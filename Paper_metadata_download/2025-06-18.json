[
  {
    "paper": {
      "id": "2506.14429",
      "authors": [
        {
          "_id": "68521a9a0164cd131671045c",
          "name": "Xiaoran Liu",
          "hidden": false
        },
        {
          "_id": "68521a9a0164cd131671045d",
          "name": "Zhigeng Liu",
          "hidden": false
        },
        {
          "_id": "68521a9a0164cd131671045e",
          "name": "Zengfeng Huang",
          "hidden": false
        },
        {
          "_id": "68521a9a0164cd131671045f",
          "name": "Qipeng Guo",
          "hidden": false
        },
        {
          "_id": "68521a9a0164cd1316710460",
          "name": "Ziwei He",
          "hidden": false
        },
        {
          "_id": "68521a9a0164cd1316710461",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-17T11:45:37.000Z",
      "submittedOnDailyAt": "2025-06-18T00:18:23.135Z",
      "title": "LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs",
      "submittedOnDailyBy": {
        "_id": "64f033ef82c6eea604c4da8b",
        "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
        "isPro": false,
        "fullname": "Liu Xiaoran",
        "user": "LiuXR",
        "type": "user"
      },
      "summary": "Large Language Diffusion Models, or diffusion LLMs, have emerged as a\nsignificant focus in NLP research, with substantial effort directed toward\nunderstanding their scalability and downstream task performance. However, their\nlong-context capabilities remain unexplored, lacking systematic analysis or\nmethods for context extension. In this work, we present the first systematic\ninvestigation comparing the long-context performance of diffusion LLMs and\ntraditional auto-regressive LLMs. We first identify a unique characteristic of\ndiffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably\n\\textit{stable perplexity} during direct context extrapolation.\nFurthermore, where auto-regressive models fail outright during the\nNeedle-In-A-Haystack task with context exceeding their pretrained length, we\ndiscover diffusion LLMs exhibit a distinct \\textit{local perception}\nphenomenon, enabling successful retrieval from recent context segments. We\nexplain both phenomena through the lens of Rotary Position Embedding (RoPE)\nscaling theory. Building on these observations, we propose LongLLaDA, a\ntraining-free method that integrates LLaDA with the NTK-based RoPE\nextrapolation. Our results validate that established extrapolation scaling laws\nremain effective for extending the context windows of diffusion LLMs.\nFurthermore, we identify long-context tasks where diffusion LLMs outperform\nauto-regressive LLMs and others where they fall short. Consequently, this study\nestablishes the first context extrapolation method for diffusion LLMs while\nproviding essential theoretical insights and empirical benchmarks critical for\nadvancing future research on long-context diffusion LLMs.",
      "upvotes": 21,
      "discussionId": "68521a9a0164cd1316710462",
      "ai_summary": "This study investigates long-context performance of diffusion LLMs compared to auto-regressive LLMs, identifies their unique characteristics, and proposes LongLLaDA, a training-free method for extending context windows.",
      "ai_keywords": [
        "diffusion LLMs",
        "auto-regressive LLMs",
        "stable perplexity",
        "local perception",
        "Rotary Position Embedding (RoPE) scaling theory",
        "LongLLaDA",
        "NTK-based RoPE extrapolation",
        "context extrapolation scaling laws",
        "long-context tasks"
      ]
    },
    "publishedAt": "2025-06-17T07:45:37.000Z",
    "title": "LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs",
    "summary": "Large Language Diffusion Models, or diffusion LLMs, have emerged as a\nsignificant focus in NLP research, with substantial effort directed toward\nunderstanding their scalability and downstream task performance. However, their\nlong-context capabilities remain unexplored, lacking systematic analysis or\nmethods for context extension. In this work, we present the first systematic\ninvestigation comparing the long-context performance of diffusion LLMs and\ntraditional auto-regressive LLMs. We first identify a unique characteristic of\ndiffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably\n\\textit{stable perplexity} during direct context extrapolation.\nFurthermore, where auto-regressive models fail outright during the\nNeedle-In-A-Haystack task with context exceeding their pretrained length, we\ndiscover diffusion LLMs exhibit a distinct \\textit{local perception}\nphenomenon, enabling successful retrieval from recent context segments. We\nexplain both phenomena through the lens of Rotary Position Embedding (RoPE)\nscaling theory. Building on these observations, we propose LongLLaDA, a\ntraining-free method that integrates LLaDA with the NTK-based RoPE\nextrapolation. Our results validate that established extrapolation scaling laws\nremain effective for extending the context windows of diffusion LLMs.\nFurthermore, we identify long-context tasks where diffusion LLMs outperform\nauto-regressive LLMs and others where they fall short. Consequently, this study\nestablishes the first context extrapolation method for diffusion LLMs while\nproviding essential theoretical insights and empirical benchmarks critical for\nadvancing future research on long-context diffusion LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14429.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f033ef82c6eea604c4da8b",
      "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
      "fullname": "Liu Xiaoran",
      "name": "LiuXR",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14234",
      "authors": [
        {
          "_id": "68522d7f0164cd13167104ee",
          "name": "Md Tanzib Hosain",
          "hidden": false
        },
        {
          "_id": "68522d7f0164cd13167104ef",
          "name": "Salman Rahman",
          "hidden": false
        },
        {
          "_id": "68522d7f0164cd13167104f0",
          "name": "Md Kishor Morol",
          "hidden": false
        },
        {
          "_id": "68522d7f0164cd13167104f1",
          "name": "Md Rizwan Parvez",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-17T06:47:19.000Z",
      "submittedOnDailyAt": "2025-06-18T01:39:00.207Z",
      "title": "Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just\n  Like an Olympiad Team",
      "submittedOnDailyBy": {
        "_id": "65ae1c4468139e3c42973fe4",
        "avatarUrl": "/avatars/b065a857dd763410caadea37a2dc01c4.svg",
        "isPro": false,
        "fullname": "Md Rizwan Parvez",
        "user": "mparvez",
        "type": "user"
      },
      "summary": "Despite impressive progress on complex reasoning, current large language\nmodels (LLMs) typically operate in isolation - treating each problem as an\nindependent attempt, without accumulating or integrating experiential\nknowledge. In contrast, expert problem solvers - such as Olympiad or\nprogramming contest teams - leverage a rich tapestry of experiences: absorbing\nmentorship from coaches, developing intuition from past problems, leveraging\nknowledge of tool usage and library functionality, adapting strategies based on\nthe expertise and experiences of peers, continuously refining their reasoning\nthrough trial and error, and learning from other related problems even during\ncompetition. We introduce Xolver, a training-free multi-agent reasoning\nframework that equips a black-box LLM with a persistent, evolving memory of\nholistic experience. Xolver integrates diverse experience modalities, including\nexternal and self-retrieval, tool use, collaborative interactions, agent-driven\nevaluation, and iterative refinement. By learning from relevant strategies,\ncode fragments, and abstract reasoning patterns at inference time, Xolver\navoids generating solutions from scratch - marking a transition from isolated\ninference toward experience-aware language agents. Built on both open-weight\nand proprietary models, Xolver consistently outperforms specialized reasoning\nagents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses\nadvanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high.\nWith o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24\n(94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) -\nhighlighting holistic experience learning as a key step toward generalist\nagents capable of expert-level reasoning. Code and data are available at\nhttps://kagnlp.github.io/xolver.github.io/.",
      "upvotes": 16,
      "discussionId": "68522d7f0164cd13167104f2",
      "ai_summary": "Xolver, a multi-agent reasoning framework, enhances large language models with persistent memory and diverse experience modalities, improving performance on complex reasoning tasks by avoiding generating solutions from scratch.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "multi-agent reasoning framework",
        "persistent memory",
        "experience-aware language agents",
        "external and self-retrieval",
        "tool use",
        "collaborative interactions",
        "agent-driven evaluation",
        "iterative refinement",
        "GSM8K",
        "AIME'24",
        "AIME'25",
        "Math-500",
        "LiveCodeBench-V5",
        "generalist agents",
        "expert-level reasoning"
      ]
    },
    "publishedAt": "2025-06-17T02:47:19.000Z",
    "title": "Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just\n  Like an Olympiad Team",
    "summary": "Despite impressive progress on complex reasoning, current large language\nmodels (LLMs) typically operate in isolation - treating each problem as an\nindependent attempt, without accumulating or integrating experiential\nknowledge. In contrast, expert problem solvers - such as Olympiad or\nprogramming contest teams - leverage a rich tapestry of experiences: absorbing\nmentorship from coaches, developing intuition from past problems, leveraging\nknowledge of tool usage and library functionality, adapting strategies based on\nthe expertise and experiences of peers, continuously refining their reasoning\nthrough trial and error, and learning from other related problems even during\ncompetition. We introduce Xolver, a training-free multi-agent reasoning\nframework that equips a black-box LLM with a persistent, evolving memory of\nholistic experience. Xolver integrates diverse experience modalities, including\nexternal and self-retrieval, tool use, collaborative interactions, agent-driven\nevaluation, and iterative refinement. By learning from relevant strategies,\ncode fragments, and abstract reasoning patterns at inference time, Xolver\navoids generating solutions from scratch - marking a transition from isolated\ninference toward experience-aware language agents. Built on both open-weight\nand proprietary models, Xolver consistently outperforms specialized reasoning\nagents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses\nadvanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high.\nWith o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24\n(94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) -\nhighlighting holistic experience learning as a key step toward generalist\nagents capable of expert-level reasoning. Code and data are available at\nhttps://kagnlp.github.io/xolver.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14234.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65ae1c4468139e3c42973fe4",
      "avatarUrl": "/avatars/b065a857dd763410caadea37a2dc01c4.svg",
      "fullname": "Md Rizwan Parvez",
      "name": "mparvez",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.13642",
      "authors": [
        {
          "_id": "685129448a68fee7f6ba4c04",
          "name": "Shaolei Zhang",
          "hidden": false
        },
        {
          "_id": "685129448a68fee7f6ba4c05",
          "name": "Shoutao Guo",
          "hidden": false
        },
        {
          "_id": "685129448a68fee7f6ba4c06",
          "name": "Qingkai Fang",
          "hidden": false
        },
        {
          "_id": "685129448a68fee7f6ba4c07",
          "name": "Yan Zhou",
          "hidden": false
        },
        {
          "_id": "685129448a68fee7f6ba4c08",
          "name": "Yang Feng",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64803e5dc57f629056c601f1/MBm95m2RAX6iKKBaKTma8.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/64803e5dc57f629056c601f1/PoLupV32gI1iLxILZccQS.mp4"
      ],
      "publishedAt": "2025-06-16T16:06:45.000Z",
      "submittedOnDailyAt": "2025-06-18T00:16:02.465Z",
      "title": "Stream-Omni: Simultaneous Multimodal Interactions with Large\n  Language-Vision-Speech Model",
      "submittedOnDailyBy": {
        "_id": "64803e5dc57f629056c601f1",
        "avatarUrl": "/avatars/a9e9c97c70714e3a29bef2cf929ee6b3.svg",
        "isPro": false,
        "fullname": "Shaolei Zhang",
        "user": "zhangshaolei",
        "type": "user"
      },
      "summary": "The emergence of GPT-4o-like large multimodal models (LMMs) has raised the\nexploration of integrating text, vision, and speech modalities to support more\nflexible multimodal interaction. Existing LMMs typically concatenate\nrepresentation of modalities along the sequence dimension and feed them into a\nlarge language model (LLM) backbone. While sequence-dimension concatenation is\nstraightforward for modality integration, it often relies heavily on\nlarge-scale data to learn modality alignments. In this paper, we aim to model\nthe relationships between modalities more purposefully, thereby achieving more\nefficient and flexible modality alignments. To this end, we propose\nStream-Omni, a large language-vision-speech model with efficient modality\nalignments, which can simultaneously support interactions under various\nmodality combinations. Stream-Omni employs LLM as the backbone and aligns the\nvision and speech to the text based on their relationships. For vision that is\nsemantically complementary to text, Stream-Omni uses sequence-dimension\nconcatenation to achieve vision-text alignment. For speech that is semantically\nconsistent with text, Stream-Omni introduces a CTC-based layer-dimension\nmapping to achieve speech-text alignment. In this way, Stream-Omni can achieve\nmodality alignments with less data (especially speech), enabling the transfer\nof text capabilities to other modalities. Experiments on various benchmarks\ndemonstrate that Stream-Omni achieves strong performance on visual\nunderstanding, speech interaction, and vision-grounded speech interaction\ntasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously\nprovide intermediate text outputs (such as ASR transcriptions and model\nresponses) during speech interaction, offering users a comprehensive multimodal\nexperience.",
      "upvotes": 16,
      "discussionId": "685129458a68fee7f6ba4c09",
      "projectPage": "https://github.com/ictnlp/Stream-Omni",
      "githubRepo": "https://github.com/ictnlp/Stream-Omni",
      "ai_summary": "Stream-Omni, a large multimodal model, integrates text, vision, and speech by efficiently aligning modalities using sequence-dimension concatenation for vision and layer-dimension mapping for speech, achieving strong performance with less data.",
      "ai_keywords": [
        "GPT-4o-like",
        "large multimodal models",
        "LLM backbone",
        "modality alignments",
        "sequence-dimension concatenation",
        "CTC-based layer-dimension mapping",
        "visual understanding",
        "speech interaction",
        "vision-grounded speech interaction",
        "ASR transcriptions",
        "model responses"
      ]
    },
    "publishedAt": "2025-06-16T12:06:45.000Z",
    "title": "Stream-Omni: Simultaneous Multimodal Interactions with Large\n  Language-Vision-Speech Model",
    "summary": "The emergence of GPT-4o-like large multimodal models (LMMs) has raised the\nexploration of integrating text, vision, and speech modalities to support more\nflexible multimodal interaction. Existing LMMs typically concatenate\nrepresentation of modalities along the sequence dimension and feed them into a\nlarge language model (LLM) backbone. While sequence-dimension concatenation is\nstraightforward for modality integration, it often relies heavily on\nlarge-scale data to learn modality alignments. In this paper, we aim to model\nthe relationships between modalities more purposefully, thereby achieving more\nefficient and flexible modality alignments. To this end, we propose\nStream-Omni, a large language-vision-speech model with efficient modality\nalignments, which can simultaneously support interactions under various\nmodality combinations. Stream-Omni employs LLM as the backbone and aligns the\nvision and speech to the text based on their relationships. For vision that is\nsemantically complementary to text, Stream-Omni uses sequence-dimension\nconcatenation to achieve vision-text alignment. For speech that is semantically\nconsistent with text, Stream-Omni introduces a CTC-based layer-dimension\nmapping to achieve speech-text alignment. In this way, Stream-Omni can achieve\nmodality alignments with less data (especially speech), enabling the transfer\nof text capabilities to other modalities. Experiments on various benchmarks\ndemonstrate that Stream-Omni achieves strong performance on visual\nunderstanding, speech interaction, and vision-grounded speech interaction\ntasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously\nprovide intermediate text outputs (such as ASR transcriptions and model\nresponses) during speech interaction, offering users a comprehensive multimodal\nexperience.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64803e5dc57f629056c601f1/MBm95m2RAX6iKKBaKTma8.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/64803e5dc57f629056c601f1/PoLupV32gI1iLxILZccQS.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13642.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64803e5dc57f629056c601f1",
      "avatarUrl": "/avatars/a9e9c97c70714e3a29bef2cf929ee6b3.svg",
      "fullname": "Shaolei Zhang",
      "name": "zhangshaolei",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14758",
      "authors": [
        {
          "_id": "685239610164cd1316710553",
          "user": {
            "_id": "649e6761f9134a06ed1e0cea",
            "avatarUrl": "/avatars/00b5dcb744c54a4aa18fe08efd70d6ff.svg",
            "isPro": false,
            "fullname": "Daixuan Cheng",
            "user": "daixuancheng",
            "type": "user"
          },
          "name": "Daixuan Cheng",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-18T06:57:21.864Z",
          "hidden": false
        },
        {
          "_id": "685239610164cd1316710554",
          "name": "Shaohan Huang",
          "hidden": false
        },
        {
          "_id": "685239610164cd1316710555",
          "name": "Xuekai Zhu",
          "hidden": false
        },
        {
          "_id": "685239610164cd1316710556",
          "name": "Bo Dai",
          "hidden": false
        },
        {
          "_id": "685239610164cd1316710557",
          "name": "Wayne Xin Zhao",
          "hidden": false
        },
        {
          "_id": "685239610164cd1316710558",
          "name": "Zhenliang Zhang",
          "hidden": false
        },
        {
          "_id": "685239610164cd1316710559",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/649e6761f9134a06ed1e0cea/UekvaawzSgcb5I120mngD.png",
        "https://cdn-uploads.huggingface.co/production/uploads/649e6761f9134a06ed1e0cea/UbAwRMdcT31OV6ORPZG52.png"
      ],
      "publishedAt": "2025-06-17T17:54:03.000Z",
      "submittedOnDailyAt": "2025-06-18T02:33:04.259Z",
      "title": "Reasoning with Exploration: An Entropy Perspective",
      "submittedOnDailyBy": {
        "_id": "649e6761f9134a06ed1e0cea",
        "avatarUrl": "/avatars/00b5dcb744c54a4aa18fe08efd70d6ff.svg",
        "isPro": false,
        "fullname": "Daixuan Cheng",
        "user": "daixuancheng",
        "type": "user"
      },
      "summary": "Balancing exploration and exploitation is a central goal in reinforcement\nlearning (RL). Despite recent advances in enhancing language model (LM)\nreasoning, most methods lean toward exploitation, and increasingly encounter\nperformance plateaus. In this work, we revisit entropy -- a signal of\nexploration in RL -- and examine its relationship to exploratory reasoning in\nLMs. Through empirical analysis, we uncover strong positive correlations\nbetween high-entropy regions and three types of exploratory reasoning actions:\n(1) pivotal tokens that determine or connect logical steps, (2) reflective\nactions such as self-verification and correction, and (3) rare behaviors\nunder-explored by the base LMs. Motivated by this, we introduce a minimal\nmodification to standard RL with only one line of code: augmenting the\nadvantage function with an entropy-based term. Unlike traditional\nmaximum-entropy methods which encourage exploration by promoting uncertainty,\nwe encourage exploration by promoting longer and deeper reasoning chains.\nNotably, our method achieves significant gains on the Pass@K metric -- an\nupper-bound estimator of LM reasoning capabilities -- even when evaluated with\nextremely large K values, pushing the boundaries of LM reasoning.",
      "upvotes": 15,
      "discussionId": "685239610164cd131671055a",
      "ai_summary": "Introducing an entropy-based term to the advantage function in reinforcement learning enhances exploratory reasoning in language models, leading to improved performance on complex reasoning tasks.",
      "ai_keywords": [
        "reinforcement learning",
        "entropy",
        "exploratory reasoning",
        "pivotal tokens",
        "reflective actions",
        "rare behaviors",
        "advantage function",
        "Pass@K"
      ]
    },
    "publishedAt": "2025-06-17T13:54:03.000Z",
    "title": "Reasoning with Exploration: An Entropy Perspective",
    "summary": "Balancing exploration and exploitation is a central goal in reinforcement\nlearning (RL). Despite recent advances in enhancing language model (LM)\nreasoning, most methods lean toward exploitation, and increasingly encounter\nperformance plateaus. In this work, we revisit entropy -- a signal of\nexploration in RL -- and examine its relationship to exploratory reasoning in\nLMs. Through empirical analysis, we uncover strong positive correlations\nbetween high-entropy regions and three types of exploratory reasoning actions:\n(1) pivotal tokens that determine or connect logical steps, (2) reflective\nactions such as self-verification and correction, and (3) rare behaviors\nunder-explored by the base LMs. Motivated by this, we introduce a minimal\nmodification to standard RL with only one line of code: augmenting the\nadvantage function with an entropy-based term. Unlike traditional\nmaximum-entropy methods which encourage exploration by promoting uncertainty,\nwe encourage exploration by promoting longer and deeper reasoning chains.\nNotably, our method achieves significant gains on the Pass@K metric -- an\nupper-bound estimator of LM reasoning capabilities -- even when evaluated with\nextremely large K values, pushing the boundaries of LM reasoning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/649e6761f9134a06ed1e0cea/UekvaawzSgcb5I120mngD.png",
      "https://cdn-uploads.huggingface.co/production/uploads/649e6761f9134a06ed1e0cea/UbAwRMdcT31OV6ORPZG52.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14758.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "649e6761f9134a06ed1e0cea",
      "avatarUrl": "/avatars/00b5dcb744c54a4aa18fe08efd70d6ff.svg",
      "fullname": "Daixuan Cheng",
      "name": "daixuancheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.12928",
      "authors": [
        {
          "_id": "6851dd060164cd13167103d7",
          "name": "King Zhu",
          "hidden": false
        },
        {
          "_id": "6851dd060164cd13167103d8",
          "name": "Hanhao Li",
          "hidden": false
        },
        {
          "_id": "6851dd060164cd13167103d9",
          "name": "Siwei Wu",
          "hidden": false
        },
        {
          "_id": "6851dd060164cd13167103da",
          "name": "Tianshun Xing",
          "hidden": false
        },
        {
          "_id": "6851dd060164cd13167103db",
          "name": "Dehua Ma",
          "hidden": false
        },
        {
          "_id": "6851dd060164cd13167103dc",
          "name": "Xiangru Tang",
          "hidden": false
        },
        {
          "_id": "6851dd060164cd13167103dd",
          "name": "Minghao Liu",
          "hidden": false
        },
        {
          "_id": "6851dd060164cd13167103de",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "6851dd060164cd13167103df",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "6851dd060164cd13167103e0",
          "name": "Yuchen Eleanor Jiang",
          "hidden": false
        },
        {
          "_id": "6851dd060164cd13167103e1",
          "name": "Changwang Zhang",
          "hidden": false
        },
        {
          "_id": "6851dd060164cd13167103e2",
          "name": "Chenghua Lin",
          "hidden": false
        },
        {
          "_id": "6851dd060164cd13167103e3",
          "name": "Jun Wang",
          "hidden": false
        },
        {
          "_id": "6851dd060164cd13167103e4",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "6851dd060164cd13167103e5",
          "name": "Wangchunshu Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-15T17:59:47.000Z",
      "submittedOnDailyAt": "2025-06-18T04:21:02.464Z",
      "title": "Scaling Test-time Compute for LLM Agents",
      "submittedOnDailyBy": {
        "_id": "638efcf4c67af472d316d424",
        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
        "isPro": false,
        "fullname": "Ge Zhang",
        "user": "zhangysk",
        "type": "user"
      },
      "summary": "Scaling test time compute has shown remarkable success in improving the\nreasoning abilities of large language models (LLMs). In this work, we conduct\nthe first systematic exploration of applying test-time scaling methods to\nlanguage agents and investigate the extent to which it improves their\neffectiveness. Specifically, we explore different test-time scaling strategies,\nincluding: (1) parallel sampling algorithms; (2) sequential revision\nstrategies; (3) verifiers and merging methods; (4)strategies for diversifying\nrollouts.We carefully analyze and ablate the impact of different design\nstrategies on applying test-time scaling on language agents, and have follow\nfindings: 1. Scaling test time compute could improve the performance of agents.\n2. Knowing when to reflect is important for agents. 3. Among different\nverification and result merging approaches, the list-wise method performs best.\n4. Increasing diversified rollouts exerts a positive effect on the agent's task\nperformance.",
      "upvotes": 15,
      "discussionId": "6851dd060164cd13167103e6",
      "ai_summary": "Systematic exploration of test-time scaling methods in large language agents reveals that computational scaling improves performance, especially through parallel sampling, sequential revision, effective verification, and increased rollout diversity.",
      "ai_keywords": [
        "parallel sampling algorithms",
        "sequential revision strategies",
        "verifiers",
        "merging methods",
        "diversified rollouts",
        "test-time scaling",
        "large language models"
      ]
    },
    "publishedAt": "2025-06-15T13:59:47.000Z",
    "title": "Scaling Test-time Compute for LLM Agents",
    "summary": "Scaling test time compute has shown remarkable success in improving the\nreasoning abilities of large language models (LLMs). In this work, we conduct\nthe first systematic exploration of applying test-time scaling methods to\nlanguage agents and investigate the extent to which it improves their\neffectiveness. Specifically, we explore different test-time scaling strategies,\nincluding: (1) parallel sampling algorithms; (2) sequential revision\nstrategies; (3) verifiers and merging methods; (4)strategies for diversifying\nrollouts.We carefully analyze and ablate the impact of different design\nstrategies on applying test-time scaling on language agents, and have follow\nfindings: 1. Scaling test time compute could improve the performance of agents.\n2. Knowing when to reflect is important for agents. 3. Among different\nverification and result merging approaches, the list-wise method performs best.\n4. Increasing diversified rollouts exerts a positive effect on the agent's task\nperformance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12928.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "638efcf4c67af472d316d424",
      "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
      "fullname": "Ge Zhang",
      "name": "zhangysk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 49
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.12278",
      "authors": [
        {
          "_id": "685234030164cd1316710502",
          "name": "Zheyuan Yang",
          "hidden": false
        },
        {
          "_id": "685234030164cd1316710503",
          "name": "Zexi Kuang",
          "hidden": false
        },
        {
          "_id": "685234030164cd1316710504",
          "name": "Xue Xia",
          "hidden": false
        },
        {
          "_id": "685234030164cd1316710505",
          "name": "Yilun Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-13T23:56:17.000Z",
      "submittedOnDailyAt": "2025-06-18T02:08:35.444Z",
      "title": "Can LLMs Generate High-Quality Test Cases for Algorithm Problems?\n  TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure",
      "submittedOnDailyBy": {
        "_id": "62f662bcc58915315c4eccea",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
        "isPro": true,
        "fullname": "Yilun Zhao",
        "user": "yilunzhao",
        "type": "user"
      },
      "summary": "We introduce TestCase-Eval, a new benchmark for systematic evaluation of LLMs\nin test-case generation. TestCase-Eval includes 500 algorithm problems and\n100,000 human-crafted solutions from the Codeforces platform. It focuses on two\npivotal tasks: (1) Fault Coverage, which measures how well LLM-generated test\nsets probe diverse input scenarios and cover a wide range of potential failure\nmodes. (2) Fault Exposure, which evaluates whether LLMs can craft a tailored\ntest input that reveals a specific incorrect code implementation. We provide a\ncomprehensive assessment of 19 state-of-the-art open-source and proprietary\nLLMs on TestCase-Eval, offering insights into their strengths and limitations\nin generating effective test cases for algorithm problems.",
      "upvotes": 12,
      "discussionId": "685234030164cd1316710506",
      "githubRepo": "https://github.com/FlowRays/TestCase-Eval",
      "ai_summary": "TestCase-Eval is a benchmark for evaluating LLMs in generating comprehensive and targeted test cases for algorithm problems.",
      "ai_keywords": [
        "test-case generation",
        "Fault Coverage",
        "Fault Exposure",
        "LLMs",
        "algorithm problems",
        "human-crafted solutions",
        "Codeforces",
        "test sets",
        "failure modes",
        "incorrect code implementation"
      ]
    },
    "publishedAt": "2025-06-13T19:56:17.000Z",
    "title": "Can LLMs Generate High-Quality Test Cases for Algorithm Problems?\n  TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure",
    "summary": "We introduce TestCase-Eval, a new benchmark for systematic evaluation of LLMs\nin test-case generation. TestCase-Eval includes 500 algorithm problems and\n100,000 human-crafted solutions from the Codeforces platform. It focuses on two\npivotal tasks: (1) Fault Coverage, which measures how well LLM-generated test\nsets probe diverse input scenarios and cover a wide range of potential failure\nmodes. (2) Fault Exposure, which evaluates whether LLMs can craft a tailored\ntest input that reveals a specific incorrect code implementation. We provide a\ncomprehensive assessment of 19 state-of-the-art open-source and proprietary\nLLMs on TestCase-Eval, offering insights into their strengths and limitations\nin generating effective test cases for algorithm problems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12278.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62f662bcc58915315c4eccea",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
      "fullname": "Yilun Zhao",
      "name": "yilunzhao",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14606",
      "authors": [
        {
          "_id": "68521f240164cd131671047a",
          "name": "Ahmed Heakl",
          "hidden": false
        },
        {
          "_id": "68521f240164cd131671047b",
          "name": "Sarim Hashmi",
          "hidden": false
        },
        {
          "_id": "68521f240164cd131671047c",
          "name": "Chaimaa Abi",
          "hidden": false
        },
        {
          "_id": "68521f240164cd131671047d",
          "name": "Celine Lee",
          "hidden": false
        },
        {
          "_id": "68521f240164cd131671047e",
          "name": "Abdulrahman Mahmoud",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/G_EGzMfb1C6fX_o-yLFbl.png",
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/G2UhoU9mbZ8tQ0VzmKLV5.png"
      ],
      "publishedAt": "2025-06-17T15:06:54.000Z",
      "submittedOnDailyAt": "2025-06-18T00:38:14.336Z",
      "title": "Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC\n  Transpilation with Testing Guarantees",
      "submittedOnDailyBy": {
        "_id": "656864e12d73834278a8dea7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
        "isPro": true,
        "fullname": "Ahmed Heakl",
        "user": "ahmedheakl",
        "type": "user"
      },
      "summary": "The hardware ecosystem is rapidly evolving, with increasing interest in\ntranslating low-level programs across different instruction set architectures\n(ISAs) in a quick, flexible, and correct way to enhance the portability and\nlongevity of existing code. A particularly challenging class of this\ntranspilation problem is translating between complex- (CISC) and reduced-\n(RISC) hardware architectures, due to fundamental differences in instruction\ncomplexity, memory models, and execution paradigms. In this work, we introduce\nGG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the\ntranslation power of pre-trained large language models (LLMs) with the rigor of\nestablished software testing constructs. Our method generates candidate\ntranslations using an LLM from one ISA to another, and embeds such translations\nwithin a software-testing framework to build quantifiable confidence in the\ntranslation. We evaluate our GG approach over two diverse datasets, enforce\nhigh code coverage (>98%) across unit tests, and achieve functional/semantic\ncorrectness of 99% on HumanEval programs and 49% on BringupBench programs,\nrespectively. Further, we compare our approach to the state-of-the-art Rosetta\n2 framework on Apple Silicon, showcasing 1.73x faster runtime performance,\n1.47x better energy efficiency, and 2.41x better memory usage for our\ntranspiled code, demonstrating the effectiveness of GG for real-world\nCISC-to-RISC translation tasks. We will open-source our codes, data, models,\nand benchmarks to establish a common foundation for ISA-level code translation\nresearch.",
      "upvotes": 10,
      "discussionId": "68521f240164cd131671047f",
      "projectPage": "https://ahmedheakl.github.io/Guaranteed-Guess/",
      "githubRepo": "https://github.com/ahmedheakl/Guaranteed-Guess",
      "ai_summary": "A novel ISA-centric transpilation pipeline using LLMs and software testing achieves high correctness and efficiency in translating between complex and reduced hardware architectures.",
      "ai_keywords": [
        "pre-trained large language models",
        "software testing constructs",
        "ISA-centric transpilation",
        "complex-instruction set computing (CISC)",
        "reduced-instruction set computing (RISC)",
        "instruction set architecture (ISA)",
        "HumanEval",
        "BringupBench",
        "Rosetta 2 framework",
        "functional/semantic correctness",
        "real-world CISC-to-RISC translation",
        "memory models",
        "execution paradigms",
        "transpilation",
        "hardware ecosystem",
        "low-level programs",
        "code portability",
        "longevity"
      ]
    },
    "publishedAt": "2025-06-17T11:06:54.000Z",
    "title": "Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC\n  Transpilation with Testing Guarantees",
    "summary": "The hardware ecosystem is rapidly evolving, with increasing interest in\ntranslating low-level programs across different instruction set architectures\n(ISAs) in a quick, flexible, and correct way to enhance the portability and\nlongevity of existing code. A particularly challenging class of this\ntranspilation problem is translating between complex- (CISC) and reduced-\n(RISC) hardware architectures, due to fundamental differences in instruction\ncomplexity, memory models, and execution paradigms. In this work, we introduce\nGG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the\ntranslation power of pre-trained large language models (LLMs) with the rigor of\nestablished software testing constructs. Our method generates candidate\ntranslations using an LLM from one ISA to another, and embeds such translations\nwithin a software-testing framework to build quantifiable confidence in the\ntranslation. We evaluate our GG approach over two diverse datasets, enforce\nhigh code coverage (>98%) across unit tests, and achieve functional/semantic\ncorrectness of 99% on HumanEval programs and 49% on BringupBench programs,\nrespectively. Further, we compare our approach to the state-of-the-art Rosetta\n2 framework on Apple Silicon, showcasing 1.73x faster runtime performance,\n1.47x better energy efficiency, and 2.41x better memory usage for our\ntranspiled code, demonstrating the effectiveness of GG for real-world\nCISC-to-RISC translation tasks. We will open-source our codes, data, models,\nand benchmarks to establish a common foundation for ISA-level code translation\nresearch.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/G_EGzMfb1C6fX_o-yLFbl.png",
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/G2UhoU9mbZ8tQ0VzmKLV5.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14606.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "656864e12d73834278a8dea7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
      "fullname": "Ahmed Heakl",
      "name": "ahmedheakl",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 41
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14245",
      "authors": [
        {
          "_id": "68521c2a0164cd131671046b",
          "name": "Xumeng Wen",
          "hidden": false
        },
        {
          "_id": "68521c2a0164cd131671046c",
          "name": "Zihan Liu",
          "hidden": false
        },
        {
          "_id": "68521c2a0164cd131671046d",
          "user": {
            "_id": "64a7a2bad001860e0c34f7f2",
            "avatarUrl": "/avatars/2433104071e4ae1c3e2d755d81d7964b.svg",
            "isPro": false,
            "fullname": "Shun Zheng",
            "user": "shun-zheng",
            "type": "user"
          },
          "name": "Shun Zheng",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-06-18T02:37:06.379Z",
          "hidden": false
        },
        {
          "_id": "68521c2a0164cd131671046e",
          "name": "Zhijian Xu",
          "hidden": false
        },
        {
          "_id": "68521c2a0164cd131671046f",
          "name": "Shengyu Ye",
          "hidden": false
        },
        {
          "_id": "68521c2a0164cd1316710470",
          "name": "Zhirong Wu",
          "hidden": false
        },
        {
          "_id": "68521c2a0164cd1316710471",
          "name": "Xiao Liang",
          "hidden": false
        },
        {
          "_id": "68521c2a0164cd1316710472",
          "name": "Yang Wang",
          "hidden": false
        },
        {
          "_id": "68521c2a0164cd1316710473",
          "name": "Junjie Li",
          "hidden": false
        },
        {
          "_id": "68521c2a0164cd1316710474",
          "name": "Ziming Miao",
          "hidden": false
        },
        {
          "_id": "68521c2a0164cd1316710475",
          "name": "Jiang Bian",
          "hidden": false
        },
        {
          "_id": "68521c2a0164cd1316710476",
          "name": "Mao Yang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64a7a2bad001860e0c34f7f2/zpklFaaRznQyEa0t9Ji70.png"
      ],
      "publishedAt": "2025-06-17T07:06:56.000Z",
      "submittedOnDailyAt": "2025-06-18T01:24:22.712Z",
      "title": "Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes\n  Correct Reasoning in Base LLMs",
      "submittedOnDailyBy": {
        "_id": "64a7a2bad001860e0c34f7f2",
        "avatarUrl": "/avatars/2433104071e4ae1c3e2d755d81d7964b.svg",
        "isPro": false,
        "fullname": "Shun Zheng",
        "user": "shun-zheng",
        "type": "user"
      },
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npromising paradigm for advancing the reasoning capabilities of Large Language\nModels (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned\nmodels often underperform their base models on the Pass@K metric for\nsolution-finding, leading to the hypothesis that RLVR merely re-weights\nexisting reasoning paths at the cost of reasoning diversity. In this work, we\nresolve this contradiction by identifying the source of the problem: the\nPass@K metric itself is a flawed measure of reasoning, as it credits correct\nfinal answers that probably arise from inaccurate or incomplete chains of\nthought (CoTs). To address this, we introduce a more precise evaluation metric,\nCoT-Pass@K, which mandates that both the reasoning path and the final\nanswer be correct. We provide a new theoretical foundation that formalizes how\nRLVR, unlike traditional RL, is uniquely structured to incentivize logical\nintegrity. Our empirical results are supportive: using CoT-Pass@K, we\nobserve that RLVR can incentivize the generalization of correct reasoning for\nall values of K. Furthermore, by analyzing the training dynamics, we find\nthat this enhanced reasoning capability emerges early in the training process\nand smoothly generalizes. Our work provides a clear perspective on the role of\nRLVR, offers a more reliable method for its evaluation, and confirms its\npotential to genuinely advance machine reasoning.",
      "upvotes": 10,
      "discussionId": "68521c2a0164cd1316710477",
      "ai_summary": "RLVR advances machine reasoning by incentivizing correct and logical thought chains, addressing limitations identified by a more precise evaluation metric, $CoT$-$Pass@K$.",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Rewards",
        "LLMS",
        "Pass@K",
        "chains of thought",
        "CoT-Pass@K",
        "logical integrity",
        "machine reasoning",
        "training dynamics"
      ]
    },
    "publishedAt": "2025-06-17T03:06:56.000Z",
    "title": "Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes\n  Correct Reasoning in Base LLMs",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npromising paradigm for advancing the reasoning capabilities of Large Language\nModels (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned\nmodels often underperform their base models on the Pass@K metric for\nsolution-finding, leading to the hypothesis that RLVR merely re-weights\nexisting reasoning paths at the cost of reasoning diversity. In this work, we\nresolve this contradiction by identifying the source of the problem: the\nPass@K metric itself is a flawed measure of reasoning, as it credits correct\nfinal answers that probably arise from inaccurate or incomplete chains of\nthought (CoTs). To address this, we introduce a more precise evaluation metric,\nCoT-Pass@K, which mandates that both the reasoning path and the final\nanswer be correct. We provide a new theoretical foundation that formalizes how\nRLVR, unlike traditional RL, is uniquely structured to incentivize logical\nintegrity. Our empirical results are supportive: using CoT-Pass@K, we\nobserve that RLVR can incentivize the generalization of correct reasoning for\nall values of K. Furthermore, by analyzing the training dynamics, we find\nthat this enhanced reasoning capability emerges early in the training process\nand smoothly generalizes. Our work provides a clear perspective on the role of\nRLVR, offers a more reliable method for its evaluation, and confirms its\npotential to genuinely advance machine reasoning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a7a2bad001860e0c34f7f2/zpklFaaRznQyEa0t9Ji70.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14245.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a7a2bad001860e0c34f7f2",
      "avatarUrl": "/avatars/2433104071e4ae1c3e2d755d81d7964b.svg",
      "fullname": "Shun Zheng",
      "name": "shun-zheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.12860",
      "authors": [
        {
          "_id": "685226a40164cd13167104bd",
          "name": "Wanlong Liu",
          "hidden": false
        },
        {
          "_id": "685226a40164cd13167104be",
          "name": "Junxiao Xu",
          "hidden": false
        },
        {
          "_id": "685226a40164cd13167104bf",
          "name": "Fei Yu",
          "hidden": false
        },
        {
          "_id": "685226a40164cd13167104c0",
          "name": "Yukang Lin",
          "hidden": false
        },
        {
          "_id": "685226a40164cd13167104c1",
          "name": "Ke Ji",
          "hidden": false
        },
        {
          "_id": "685226a40164cd13167104c2",
          "name": "Wenyu Chen",
          "hidden": false
        },
        {
          "_id": "685226a40164cd13167104c3",
          "name": "Yan Xu",
          "hidden": false
        },
        {
          "_id": "685226a40164cd13167104c4",
          "name": "Yasheng Wang",
          "hidden": false
        },
        {
          "_id": "685226a40164cd13167104c5",
          "name": "Lifeng Shang",
          "hidden": false
        },
        {
          "_id": "685226a40164cd13167104c6",
          "name": "Benyou Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-15T14:21:28.000Z",
      "submittedOnDailyAt": "2025-06-18T02:06:38.533Z",
      "title": "QFFT, Question-Free Fine-Tuning for Adaptive Reasoning",
      "submittedOnDailyBy": {
        "_id": "64eb333e6878d90b031fa5c5",
        "avatarUrl": "/avatars/a0d875b49d1c56be88f34854647306da.svg",
        "isPro": false,
        "fullname": "Wanlong Liu",
        "user": "lwl-uestc",
        "type": "user"
      },
      "summary": "Recent advancements in Long Chain-of-Thought (CoT) reasoning models have\nimproved performance on complex tasks, but they suffer from overthinking, which\ngenerates redundant reasoning steps, especially for simple questions. This\npaper revisits the reasoning patterns of Long and Short CoT models, observing\nthat the Short CoT patterns offer concise reasoning efficiently, while the Long\nCoT patterns excel in challenging scenarios where the Short CoT patterns\nstruggle. To enable models to leverage both patterns, we propose Question-Free\nFine-Tuning (QFFT), a fine-tuning approach that removes the input question\nduring training and learns exclusively from Long CoT responses. This approach\nenables the model to adaptively employ both reasoning patterns: it prioritizes\nthe Short CoT patterns and activates the Long CoT patterns only when necessary.\nExperiments on various mathematical datasets demonstrate that QFFT reduces\naverage response length by more than 50\\%, while achieving performance\ncomparable to Supervised Fine-Tuning (SFT). Additionally, QFFT exhibits\nsuperior performance compared to SFT in noisy, out-of-domain, and low-resource\nscenarios.",
      "upvotes": 10,
      "discussionId": "685226a40164cd13167104c7",
      "githubRepo": "https://github.com/LWL-cpu/Question-Free-Fine-Tuning",
      "ai_summary": "Question-Free Fine-Tuning (QFFT) improves efficiency and adaptability in cognitive models by leveraging both short and long chain-of-thought patterns, reducing response length while maintaining performance across various scenarios.",
      "ai_keywords": [
        "Long Chain-of-Thought",
        "Short Chain-of-Thought",
        "Question-Free Fine-Tuning",
        "fine-tuning",
        "Supervised Fine-Tuning"
      ]
    },
    "publishedAt": "2025-06-15T10:21:28.000Z",
    "title": "QFFT, Question-Free Fine-Tuning for Adaptive Reasoning",
    "summary": "Recent advancements in Long Chain-of-Thought (CoT) reasoning models have\nimproved performance on complex tasks, but they suffer from overthinking, which\ngenerates redundant reasoning steps, especially for simple questions. This\npaper revisits the reasoning patterns of Long and Short CoT models, observing\nthat the Short CoT patterns offer concise reasoning efficiently, while the Long\nCoT patterns excel in challenging scenarios where the Short CoT patterns\nstruggle. To enable models to leverage both patterns, we propose Question-Free\nFine-Tuning (QFFT), a fine-tuning approach that removes the input question\nduring training and learns exclusively from Long CoT responses. This approach\nenables the model to adaptively employ both reasoning patterns: it prioritizes\nthe Short CoT patterns and activates the Long CoT patterns only when necessary.\nExperiments on various mathematical datasets demonstrate that QFFT reduces\naverage response length by more than 50\\%, while achieving performance\ncomparable to Supervised Fine-Tuning (SFT). Additionally, QFFT exhibits\nsuperior performance compared to SFT in noisy, out-of-domain, and low-resource\nscenarios.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12860.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64eb333e6878d90b031fa5c5",
      "avatarUrl": "/avatars/a0d875b49d1c56be88f34854647306da.svg",
      "fullname": "Wanlong Liu",
      "name": "lwl-uestc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.13363",
      "authors": [
        {
          "_id": "685234100164cd1316710508",
          "name": "Lijun Liu",
          "hidden": false
        },
        {
          "_id": "685234100164cd1316710509",
          "name": "Ruiyang Li",
          "hidden": false
        },
        {
          "_id": "685234100164cd131671050a",
          "name": "Zhaocheng Liu",
          "hidden": false
        },
        {
          "_id": "685234100164cd131671050b",
          "name": "Chenglin Zhu",
          "hidden": false
        },
        {
          "_id": "685234100164cd131671050c",
          "name": "Chong Li",
          "hidden": false
        },
        {
          "_id": "685234100164cd131671050d",
          "name": "Jiehan Cheng",
          "hidden": false
        },
        {
          "_id": "685234100164cd131671050e",
          "name": "Qiang Ju",
          "hidden": false
        },
        {
          "_id": "685234100164cd131671050f",
          "name": "Jian Xie",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/633e570be7d5ce7bfe037a53/-7W-jpcvZwQ046FQHgVdY.qt"
      ],
      "publishedAt": "2025-06-16T11:10:25.000Z",
      "submittedOnDailyAt": "2025-06-18T02:12:29.534Z",
      "title": "Efficient Medical VIE via Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "633e570be7d5ce7bfe037a53",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633e570be7d5ce7bfe037a53/zV8ULv4Mu7YIGZ8D3JtmK.jpeg",
        "isPro": false,
        "fullname": "Zhaocheng Liu",
        "user": "zhaocheng",
        "type": "user"
      },
      "summary": "Visual Information Extraction (VIE) converts unstructured document images\ninto structured formats like JSON, critical for medical applications such as\nreport analysis and online consultations. Traditional methods rely on OCR and\nlanguage models, while end-to-end multimodal models offer direct JSON\ngeneration. However, domain-specific schemas and high annotation costs limit\ntheir effectiveness in medical VIE. We base our approach on the Reinforcement\nLearning with Verifiable Rewards (RLVR) framework to address these challenges\nusing only 100 annotated samples. Our approach ensures dataset diversity, a\nbalanced precision-recall reward mechanism to reduce hallucinations and improve\nfield coverage, and innovative sampling strategies to enhance reasoning\ncapabilities. Fine-tuning Qwen2.5-VL-7B with our RLVR method, we achieve\nstate-of-the-art performance on medical VIE tasks, significantly improving F1,\nprecision, and recall. While our models excel on tasks similar to medical\ndatasets, performance drops on dissimilar tasks, highlighting the need for\ndomain-specific optimization. Case studies further demonstrate the value of\nreasoning during training and inference for VIE.",
      "upvotes": 9,
      "discussionId": "685234100164cd1316710510",
      "ai_summary": "An RLVR framework using fine-tuned Qwen2.5-VL-7B achieves state-of-the-art performance in medical VIE with limited annotated samples, enhancing reasoning and balance between precision and recall.",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Rewards (RLVR)",
        "JSON generation",
        "multimodal models",
        "dataset diversity",
        "precision-recall reward mechanism",
        "hallucinations",
        "field coverage",
        "sampling strategies",
        "fine-tuning",
        "Qwen2.5-VL-7B",
        "F1 score",
        "case studies"
      ]
    },
    "publishedAt": "2025-06-16T07:10:25.000Z",
    "title": "Efficient Medical VIE via Reinforcement Learning",
    "summary": "Visual Information Extraction (VIE) converts unstructured document images\ninto structured formats like JSON, critical for medical applications such as\nreport analysis and online consultations. Traditional methods rely on OCR and\nlanguage models, while end-to-end multimodal models offer direct JSON\ngeneration. However, domain-specific schemas and high annotation costs limit\ntheir effectiveness in medical VIE. We base our approach on the Reinforcement\nLearning with Verifiable Rewards (RLVR) framework to address these challenges\nusing only 100 annotated samples. Our approach ensures dataset diversity, a\nbalanced precision-recall reward mechanism to reduce hallucinations and improve\nfield coverage, and innovative sampling strategies to enhance reasoning\ncapabilities. Fine-tuning Qwen2.5-VL-7B with our RLVR method, we achieve\nstate-of-the-art performance on medical VIE tasks, significantly improving F1,\nprecision, and recall. While our models excel on tasks similar to medical\ndatasets, performance drops on dissimilar tasks, highlighting the need for\ndomain-specific optimization. Case studies further demonstrate the value of\nreasoning during training and inference for VIE.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/633e570be7d5ce7bfe037a53/-7W-jpcvZwQ046FQHgVdY.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13363.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "633e570be7d5ce7bfe037a53",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633e570be7d5ce7bfe037a53/zV8ULv4Mu7YIGZ8D3JtmK.jpeg",
      "fullname": "Zhaocheng Liu",
      "name": "zhaocheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14603",
      "authors": [
        {
          "_id": "68521f8a0164cd1316710481",
          "name": "Amirmojtaba Sabour",
          "hidden": false
        },
        {
          "_id": "68521f8a0164cd1316710482",
          "name": "Sanja Fidler",
          "hidden": false
        },
        {
          "_id": "68521f8a0164cd1316710483",
          "name": "Karsten Kreis",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/_yuUKzjzngNZzaUJCVwjR.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/2FmUhOtaDCtAzFkNEx6Di.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/MC_ZLBqT81dOpANroMLrm.jpeg",
        "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/degPI6Z5IEf4v_y2_UL9V.jpeg"
      ],
      "publishedAt": "2025-06-17T15:06:07.000Z",
      "submittedOnDailyAt": "2025-06-18T00:42:20.168Z",
      "title": "Align Your Flow: Scaling Continuous-Time Flow Map Distillation",
      "submittedOnDailyBy": {
        "_id": "656015f28138827138c70858",
        "avatarUrl": "/avatars/b8471ae4d80f078c7c928fc3d8f49126.svg",
        "isPro": false,
        "fullname": "Amirmojtaba Sabour",
        "user": "amsabour",
        "type": "user"
      },
      "summary": "Diffusion- and flow-based models have emerged as state-of-the-art generative\nmodeling approaches, but they require many sampling steps. Consistency models\ncan distill these models into efficient one-step generators; however, unlike\nflow- and diffusion-based methods, their performance inevitably degrades when\nincreasing the number of steps, which we show both analytically and\nempirically. Flow maps generalize these approaches by connecting any two noise\nlevels in a single step and remain effective across all step counts. In this\npaper, we introduce two new continuous-time objectives for training flow maps,\nalong with additional novel training techniques, generalizing existing\nconsistency and flow matching objectives. We further demonstrate that\nautoguidance can improve performance, using a low-quality model for guidance\nduring distillation, and an additional boost can be achieved by adversarial\nfinetuning, with minimal loss in sample diversity. We extensively validate our\nflow map models, called Align Your Flow, on challenging image generation\nbenchmarks and achieve state-of-the-art few-step generation performance on both\nImageNet 64x64 and 512x512, using small and efficient neural networks. Finally,\nwe show text-to-image flow map models that outperform all existing\nnon-adversarially trained few-step samplers in text-conditioned synthesis.",
      "upvotes": 6,
      "discussionId": "68521f8a0164cd1316710484",
      "projectPage": "https://research.nvidia.com/labs/toronto-ai/AlignYourFlow/",
      "ai_summary": "Flow maps, introduced with new continuous-time objectives and training techniques, achieve state-of-the-art performance in few-step image and text-to-image generation.",
      "ai_keywords": [
        "diffusion models",
        "flow-based models",
        "consistency models",
        "flow maps",
        "noise levels",
        "autoguidance",
        "adversarial finetuning",
        "Align Your Flow",
        "ImageNet",
        "text-to-image synthesis"
      ]
    },
    "publishedAt": "2025-06-17T11:06:07.000Z",
    "title": "Align Your Flow: Scaling Continuous-Time Flow Map Distillation",
    "summary": "Diffusion- and flow-based models have emerged as state-of-the-art generative\nmodeling approaches, but they require many sampling steps. Consistency models\ncan distill these models into efficient one-step generators; however, unlike\nflow- and diffusion-based methods, their performance inevitably degrades when\nincreasing the number of steps, which we show both analytically and\nempirically. Flow maps generalize these approaches by connecting any two noise\nlevels in a single step and remain effective across all step counts. In this\npaper, we introduce two new continuous-time objectives for training flow maps,\nalong with additional novel training techniques, generalizing existing\nconsistency and flow matching objectives. We further demonstrate that\nautoguidance can improve performance, using a low-quality model for guidance\nduring distillation, and an additional boost can be achieved by adversarial\nfinetuning, with minimal loss in sample diversity. We extensively validate our\nflow map models, called Align Your Flow, on challenging image generation\nbenchmarks and achieve state-of-the-art few-step generation performance on both\nImageNet 64x64 and 512x512, using small and efficient neural networks. Finally,\nwe show text-to-image flow map models that outperform all existing\nnon-adversarially trained few-step samplers in text-conditioned synthesis.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/_yuUKzjzngNZzaUJCVwjR.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/2FmUhOtaDCtAzFkNEx6Di.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/MC_ZLBqT81dOpANroMLrm.jpeg",
      "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/degPI6Z5IEf4v_y2_UL9V.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14603.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "656015f28138827138c70858",
      "avatarUrl": "/avatars/b8471ae4d80f078c7c928fc3d8f49126.svg",
      "fullname": "Amirmojtaba Sabour",
      "name": "amsabour",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.13977",
      "authors": [
        {
          "_id": "68524ff90164cd13167105aa",
          "name": "Shiting Huang",
          "hidden": false
        },
        {
          "_id": "68524ff90164cd13167105ab",
          "name": "Zhen Fang",
          "hidden": false
        },
        {
          "_id": "68524ff90164cd13167105ac",
          "name": "Zehui Chen",
          "hidden": false
        },
        {
          "_id": "68524ff90164cd13167105ad",
          "name": "Siyu Yuan",
          "hidden": false
        },
        {
          "_id": "68524ff90164cd13167105ae",
          "name": "Junjie Ye",
          "hidden": false
        },
        {
          "_id": "68524ff90164cd13167105af",
          "name": "Yu Zeng",
          "hidden": false
        },
        {
          "_id": "68524ff90164cd13167105b0",
          "name": "Lin Chen",
          "hidden": false
        },
        {
          "_id": "68524ff90164cd13167105b1",
          "name": "Qi Mao",
          "hidden": false
        },
        {
          "_id": "68524ff90164cd13167105b2",
          "name": "Feng Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T17:59:18.000Z",
      "submittedOnDailyAt": "2025-06-18T04:10:42.444Z",
      "title": "CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language\n  Models in Tool-Calling Error Scenarios",
      "submittedOnDailyBy": {
        "_id": "64b0a5037a475fba70a7260d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b0a5037a475fba70a7260d/MauBbb6raMA23yrR1Zq21.jpeg",
        "isPro": false,
        "fullname": "Zhen Fang",
        "user": "CostaliyA",
        "type": "user"
      },
      "summary": "The ability of large language models (LLMs) to utilize external tools has\nenabled them to tackle an increasingly diverse range of tasks. However, as the\ntasks become more complex and long-horizon, the intricate tool utilization\nprocess may trigger various unexpected errors. Therefore, how to effectively\nhandle such errors, including identifying, diagnosing, and recovering from\nthem, has emerged as a key research direction for advancing tool learning. In\nthis work, we first extensively analyze the types of errors encountered during\nthe function-calling process on several competitive tool evaluation benchmarks.\nBased on it, we introduce CRITICTOOL, a comprehensive critique evaluation\nbenchmark specialized for tool learning. Building upon a novel evolutionary\nstrategy for dataset construction, CRITICTOOL holds diverse tool-use errors\nwith varying complexities, which better reflects real-world scenarios. We\nconduct extensive experiments on CRITICTOOL, and validate the generalization\nand effectiveness of our constructed benchmark strategy. We also provide an\nin-depth analysis of the tool reflection ability on various LLMs, offering a\nnew perspective on the field of tool learning in LLMs. The code is available at\nhttps://github.com/Shellorley0513/CriticTool{https://github.com/Shellorley0513/CriticTool}.",
      "upvotes": 5,
      "discussionId": "68524ff90164cd13167105b3",
      "githubRepo": "https://github.com/Shellorley0513/CriticTool",
      "ai_summary": "A comprehensive benchmark, CRITICTOOL, evaluates and enhances the robustness of large language models in handling errors during tool usage.",
      "ai_keywords": [
        "large language models",
        "tool learning",
        "function-calling process",
        "error identification",
        "error diagnosis",
        "error recovery",
        "evolutionary strategy",
        "dataset construction",
        "tool reflection ability",
        "critique evaluation benchmark"
      ]
    },
    "publishedAt": "2025-06-11T13:59:18.000Z",
    "title": "CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language\n  Models in Tool-Calling Error Scenarios",
    "summary": "The ability of large language models (LLMs) to utilize external tools has\nenabled them to tackle an increasingly diverse range of tasks. However, as the\ntasks become more complex and long-horizon, the intricate tool utilization\nprocess may trigger various unexpected errors. Therefore, how to effectively\nhandle such errors, including identifying, diagnosing, and recovering from\nthem, has emerged as a key research direction for advancing tool learning. In\nthis work, we first extensively analyze the types of errors encountered during\nthe function-calling process on several competitive tool evaluation benchmarks.\nBased on it, we introduce CRITICTOOL, a comprehensive critique evaluation\nbenchmark specialized for tool learning. Building upon a novel evolutionary\nstrategy for dataset construction, CRITICTOOL holds diverse tool-use errors\nwith varying complexities, which better reflects real-world scenarios. We\nconduct extensive experiments on CRITICTOOL, and validate the generalization\nand effectiveness of our constructed benchmark strategy. We also provide an\nin-depth analysis of the tool reflection ability on various LLMs, offering a\nnew perspective on the field of tool learning in LLMs. The code is available at\nhttps://github.com/Shellorley0513/CriticTool{https://github.com/Shellorley0513/CriticTool}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13977.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b0a5037a475fba70a7260d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b0a5037a475fba70a7260d/MauBbb6raMA23yrR1Zq21.jpeg",
      "fullname": "Zhen Fang",
      "name": "CostaliyA",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14002",
      "authors": [
        {
          "_id": "685210eb0164cd131671043e",
          "name": "Siyu Chen",
          "hidden": false
        },
        {
          "_id": "685210eb0164cd131671043f",
          "name": "Heejune Sheen",
          "hidden": false
        },
        {
          "_id": "685210eb0164cd1316710440",
          "name": "Xuyuan Xiong",
          "hidden": false
        },
        {
          "_id": "685210eb0164cd1316710441",
          "name": "Tianhao Wang",
          "hidden": false
        },
        {
          "_id": "685210eb0164cd1316710442",
          "name": "Zhuoran Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-16T20:58:05.000Z",
      "submittedOnDailyAt": "2025-06-18T00:09:08.222Z",
      "title": "Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse\n  Autoencoders",
      "submittedOnDailyBy": {
        "_id": "683229900411a9d65cd410c0",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VqwvpUYF8CQAKPHMNfLyw.png",
        "isPro": false,
        "fullname": "Siyu Chen",
        "user": "Siyuc",
        "type": "user"
      },
      "summary": "We study the challenge of achieving theoretically grounded feature recovery\nusing Sparse Autoencoders (SAEs) for the interpretation of Large Language\nModels. Existing SAE training algorithms often lack rigorous mathematical\nguarantees and suffer from practical limitations such as hyperparameter\nsensitivity and instability. To address these issues, we first propose a novel\nstatistical framework for the feature recovery problem, which includes a new\nnotion of feature identifiability by modeling polysemantic features as sparse\nmixtures of underlying monosemantic concepts. Building on this framework, we\nintroduce a new SAE training algorithm based on ``bias adaptation'', a\ntechnique that adaptively adjusts neural network bias parameters to ensure\nappropriate activation sparsity. We theoretically prove that this\nalgorithm correctly recovers all monosemantic features when input data is\nsampled from our proposed statistical model. Furthermore, we develop an\nimproved empirical variant, Group Bias Adaptation (GBA), and\ndemonstrate its superior performance against benchmark methods when\napplied to LLMs with up to 1.5 billion parameters. This work represents a\nfoundational step in demystifying SAE training by providing the first SAE\nalgorithm with theoretical recovery guarantees, thereby advancing the\ndevelopment of more transparent and trustworthy AI systems through enhanced\nmechanistic interpretability.",
      "upvotes": 4,
      "discussionId": "685210ec0164cd1316710443",
      "ai_summary": "A new statistical framework and training algorithm, Group Bias Adaptation, enhance Sparse Autoencoders for recovering monosemantic features in Large Language Models, offering theoretical guarantees and superior performance.",
      "ai_keywords": [
        "Sparse Autoencoders",
        "feature recovery",
        "statistical framework",
        "feature identifiability",
        "polysemantic features",
        "monosemantic concepts",
        "bias adaptation",
        "Group Bias Adaptation",
        "Large Language Models",
        "theoretical recovery guarantees",
        "mechnistic interpretability"
      ]
    },
    "publishedAt": "2025-06-16T16:58:05.000Z",
    "title": "Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse\n  Autoencoders",
    "summary": "We study the challenge of achieving theoretically grounded feature recovery\nusing Sparse Autoencoders (SAEs) for the interpretation of Large Language\nModels. Existing SAE training algorithms often lack rigorous mathematical\nguarantees and suffer from practical limitations such as hyperparameter\nsensitivity and instability. To address these issues, we first propose a novel\nstatistical framework for the feature recovery problem, which includes a new\nnotion of feature identifiability by modeling polysemantic features as sparse\nmixtures of underlying monosemantic concepts. Building on this framework, we\nintroduce a new SAE training algorithm based on ``bias adaptation'', a\ntechnique that adaptively adjusts neural network bias parameters to ensure\nappropriate activation sparsity. We theoretically prove that this\nalgorithm correctly recovers all monosemantic features when input data is\nsampled from our proposed statistical model. Furthermore, we develop an\nimproved empirical variant, Group Bias Adaptation (GBA), and\ndemonstrate its superior performance against benchmark methods when\napplied to LLMs with up to 1.5 billion parameters. This work represents a\nfoundational step in demystifying SAE training by providing the first SAE\nalgorithm with theoretical recovery guarantees, thereby advancing the\ndevelopment of more transparent and trustworthy AI systems through enhanced\nmechanistic interpretability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14002.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "683229900411a9d65cd410c0",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VqwvpUYF8CQAKPHMNfLyw.png",
      "fullname": "Siyu Chen",
      "name": "Siyuc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.10100",
      "authors": [
        {
          "_id": "68522b190164cd13167104d9",
          "name": "Yantai Yang",
          "hidden": false
        },
        {
          "_id": "68522b190164cd13167104da",
          "name": "Yuhao Wang",
          "hidden": false
        },
        {
          "_id": "68522b190164cd13167104db",
          "name": "Zichen Wen",
          "hidden": false
        },
        {
          "_id": "68522b190164cd13167104dc",
          "name": "Luo Zhongwei",
          "hidden": false
        },
        {
          "_id": "68522b190164cd13167104dd",
          "name": "Chang Zou",
          "hidden": false
        },
        {
          "_id": "68522b190164cd13167104de",
          "name": "Zhipeng Zhang",
          "hidden": false
        },
        {
          "_id": "68522b190164cd13167104df",
          "name": "Chuan Wen",
          "hidden": false
        },
        {
          "_id": "68522b190164cd13167104e0",
          "name": "Linfeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-11T18:34:57.000Z",
      "submittedOnDailyAt": "2025-06-18T01:30:42.708Z",
      "title": "EfficientVLA: Training-Free Acceleration and Compression for\n  Vision-Language-Action Models",
      "submittedOnDailyBy": {
        "_id": "653b8c3e97a4d71d950e2f20",
        "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
        "isPro": false,
        "fullname": "Zichen Wen",
        "user": "zichenwen",
        "type": "user"
      },
      "summary": "Vision-Language-Action (VLA) models, particularly diffusion-based\narchitectures, demonstrate transformative potential for embodied intelligence\nbut are severely hampered by high computational and memory demands stemming\nfrom extensive inherent and inference-time redundancies. While existing\nacceleration efforts often target isolated inefficiencies, such piecemeal\nsolutions typically fail to holistically address the varied computational and\nmemory bottlenecks across the entire VLA pipeline, thereby limiting practical\ndeployability. We introduce EfficientVLA, a structured and training-free\ninference acceleration framework that systematically eliminates these barriers\nby cohesively exploiting multifaceted redundancies. EfficientVLA\nsynergistically integrates three targeted strategies: (1) pruning of\nfunctionally inconsequential layers from the language module, guided by an\nanalysis of inter-layer redundancies; (2) optimizing the visual processing\npathway through a task-aware strategy that selects a compact, diverse set of\nvisual tokens, balancing task-criticality with informational coverage; and (3)\nalleviating temporal computational redundancy within the iterative\ndiffusion-based action head by strategically caching and reusing key\nintermediate features. We apply our method to a standard VLA model CogACT,\nyielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6%\nsuccess rate drop in the SIMPLER benchmark.",
      "upvotes": 4,
      "discussionId": "68522b190164cd13167104e1",
      "ai_summary": "EfficientVLA accelerates Vision-Language-Action models by pruning language layers, optimizing visual token selection, and caching intermediate features in the diffusion-based action head.",
      "ai_keywords": [
        "diffusion-based architectures",
        "inference acceleration framework",
        "pruning",
        "inter-layer redundancies",
        "visual tokens",
        "task-aware strategy",
        "iterative diffusion-based action head",
        "caching",
        "FLOPs",
        "SIMPLER benchmark"
      ]
    },
    "publishedAt": "2025-06-11T14:34:57.000Z",
    "title": "EfficientVLA: Training-Free Acceleration and Compression for\n  Vision-Language-Action Models",
    "summary": "Vision-Language-Action (VLA) models, particularly diffusion-based\narchitectures, demonstrate transformative potential for embodied intelligence\nbut are severely hampered by high computational and memory demands stemming\nfrom extensive inherent and inference-time redundancies. While existing\nacceleration efforts often target isolated inefficiencies, such piecemeal\nsolutions typically fail to holistically address the varied computational and\nmemory bottlenecks across the entire VLA pipeline, thereby limiting practical\ndeployability. We introduce EfficientVLA, a structured and training-free\ninference acceleration framework that systematically eliminates these barriers\nby cohesively exploiting multifaceted redundancies. EfficientVLA\nsynergistically integrates three targeted strategies: (1) pruning of\nfunctionally inconsequential layers from the language module, guided by an\nanalysis of inter-layer redundancies; (2) optimizing the visual processing\npathway through a task-aware strategy that selects a compact, diverse set of\nvisual tokens, balancing task-criticality with informational coverage; and (3)\nalleviating temporal computational redundancy within the iterative\ndiffusion-based action head by strategically caching and reusing key\nintermediate features. We apply our method to a standard VLA model CogACT,\nyielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6%\nsuccess rate drop in the SIMPLER benchmark.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10100.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653b8c3e97a4d71d950e2f20",
      "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
      "fullname": "Zichen Wen",
      "name": "zichenwen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.13651",
      "authors": [
        {
          "_id": "6850cf555e07650ecce88fe2",
          "name": "Kaiyuan Chen",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88fe3",
          "name": "Yixin Ren",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88fe4",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88fe5",
          "name": "Xiaobo Hu",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88fe6",
          "name": "Haotong Tian",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88fe7",
          "name": "Tianbao Xie",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88fe8",
          "name": "Fangfu Liu",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88fe9",
          "name": "Haoye Zhang",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88fea",
          "name": "Hongzhang Liu",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88feb",
          "name": "Yuan Gong",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88fec",
          "name": "Chen Sun",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88fed",
          "name": "Han Hou",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88fee",
          "name": "Hui Yang",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88fef",
          "name": "James Pan",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88ff0",
          "name": "Jianan Lou",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88ff1",
          "name": "Jiayi Mao",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88ff2",
          "name": "Jizheng Liu",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88ff3",
          "name": "Jinpeng Li",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88ff4",
          "name": "Kangyi Liu",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88ff5",
          "name": "Kenkun Liu",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88ff6",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88ff7",
          "name": "Run Li",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88ff8",
          "name": "Tong Niu",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88ff9",
          "name": "Wenlong Zhang",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88ffa",
          "name": "Wenqi Yan",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88ffb",
          "name": "Xuanzheng Wang",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88ffc",
          "name": "Yuchen Zhang",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88ffd",
          "name": "Yi-Hsin Hung",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88ffe",
          "name": "Yuan Jiang",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce88fff",
          "name": "Zexuan Liu",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce89000",
          "name": "Zihan Yin",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce89001",
          "name": "Zijian Ma",
          "hidden": false
        },
        {
          "_id": "6850cf555e07650ecce89002",
          "name": "Zhiwen Mo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-16T16:16:14.000Z",
      "submittedOnDailyAt": "2025-06-18T04:38:47.336Z",
      "title": "xbench: Tracking Agents Productivity Scaling with Profession-Aligned\n  Real-World Evaluations",
      "submittedOnDailyBy": {
        "_id": "6505a02f9310ce8c400edc63",
        "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
        "isPro": false,
        "fullname": "Fangfu Liu",
        "user": "Liuff23",
        "type": "user"
      },
      "summary": "We introduce xbench, a dynamic, profession-aligned evaluation suite designed\nto bridge the gap between AI agent capabilities and real-world productivity.\nWhile existing benchmarks often focus on isolated technical skills, they may\nnot accurately reflect the economic value agents deliver in professional\nsettings. To address this, xbench targets commercially significant domains with\nevaluation tasks defined by industry professionals. Our framework creates\nmetrics that strongly correlate with productivity value, enables prediction of\nTechnology-Market Fit (TMF), and facilitates tracking of product capabilities\nover time. As our initial implementations, we present two benchmarks:\nRecruitment and Marketing. For Recruitment, we collect 50 tasks from real-world\nheadhunting business scenarios to evaluate agents' abilities in company\nmapping, information retrieval, and talent sourcing. For Marketing, we assess\nagents' ability to match influencers with advertiser needs, evaluating their\nperformance across 50 advertiser requirements using a curated pool of 836\ncandidate influencers. We present initial evaluation results for leading\ncontemporary agents, establishing a baseline for these professional domains.\nOur continuously updated evalsets and evaluations are available at\nhttps://xbench.org.",
      "upvotes": 3,
      "discussionId": "6850cf555e07650ecce89003",
      "projectPage": "https://xbench.org/",
      "githubRepo": "https://github.com/xbench-ai/xbench-evals"
    },
    "publishedAt": "2025-06-16T12:16:14.000Z",
    "title": "xbench: Tracking Agents Productivity Scaling with Profession-Aligned\n  Real-World Evaluations",
    "summary": "We introduce xbench, a dynamic, profession-aligned evaluation suite designed\nto bridge the gap between AI agent capabilities and real-world productivity.\nWhile existing benchmarks often focus on isolated technical skills, they may\nnot accurately reflect the economic value agents deliver in professional\nsettings. To address this, xbench targets commercially significant domains with\nevaluation tasks defined by industry professionals. Our framework creates\nmetrics that strongly correlate with productivity value, enables prediction of\nTechnology-Market Fit (TMF), and facilitates tracking of product capabilities\nover time. As our initial implementations, we present two benchmarks:\nRecruitment and Marketing. For Recruitment, we collect 50 tasks from real-world\nheadhunting business scenarios to evaluate agents' abilities in company\nmapping, information retrieval, and talent sourcing. For Marketing, we assess\nagents' ability to match influencers with advertiser needs, evaluating their\nperformance across 50 advertiser requirements using a curated pool of 836\ncandidate influencers. We present initial evaluation results for leading\ncontemporary agents, establishing a baseline for these professional domains.\nOur continuously updated evalsets and evaluations are available at\nhttps://xbench.org.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13651.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6505a02f9310ce8c400edc63",
      "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
      "fullname": "Fangfu Liu",
      "name": "Liuff23",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.10038",
      "authors": [
        {
          "_id": "68523b4a0164cd131671055d",
          "name": "Giannis Daras",
          "hidden": false
        },
        {
          "_id": "68523b4a0164cd131671055e",
          "user": {
            "_id": "67d204c05422de5644126f0b",
            "avatarUrl": "/avatars/8a9ac73d93785f48e63184d612b9fff1.svg",
            "isPro": false,
            "fullname": "Adrian Rodriguez Munoz",
            "user": "adrianrm",
            "type": "user"
          },
          "name": "Adrian Rodriguez-Munoz",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-06-18T04:06:37.569Z",
          "hidden": false
        },
        {
          "_id": "68523b4a0164cd131671055f",
          "name": "Adam Klivans",
          "hidden": false
        },
        {
          "_id": "68523b4a0164cd1316710560",
          "name": "Antonio Torralba",
          "hidden": false
        },
        {
          "_id": "68523b4a0164cd1316710561",
          "name": "Constantinos Daskalakis",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-10T22:37:39.000Z",
      "submittedOnDailyAt": "2025-06-18T02:40:58.837Z",
      "title": "Ambient Diffusion Omni: Training Good Models with Bad Data",
      "submittedOnDailyBy": {
        "_id": "5f45f44b79c1ba4c353d1035",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5f45f44b79c1ba4c353d1035/6piqagYr7RNCy6XwJGWCG.jpeg",
        "isPro": false,
        "fullname": "Giannis Daras",
        "user": "giannisdaras",
        "type": "user"
      },
      "summary": "We show how to use low-quality, synthetic, and out-of-distribution images to\nimprove the quality of a diffusion model. Typically, diffusion models are\ntrained on curated datasets that emerge from highly filtered data pools from\nthe Web and other sources. We show that there is immense value in the\nlower-quality images that are often discarded. We present Ambient Diffusion\nOmni, a simple, principled framework to train diffusion models that can extract\nsignal from all available images during training. Our framework exploits two\nproperties of natural images -- spectral power law decay and locality. We first\nvalidate our framework by successfully training diffusion models with images\nsynthetically corrupted by Gaussian blur, JPEG compression, and motion blur. We\nthen use our framework to achieve state-of-the-art ImageNet FID, and we show\nsignificant improvements in both image quality and diversity for text-to-image\ngenerative modeling. The core insight is that noise dampens the initial skew\nbetween the desired high-quality distribution and the mixed distribution we\nactually observe. We provide rigorous theoretical justification for our\napproach by analyzing the trade-off between learning from biased data versus\nlimited unbiased data across diffusion times.",
      "upvotes": 3,
      "discussionId": "68523b4a0164cd1316710562",
      "projectPage": "https://giannisdaras.github.io/publication/ambient_omni",
      "githubRepo": "https://github.com/giannisdaras/ambient-omni",
      "ai_summary": "Ambient Diffusion Omni framework leverages low-quality images to enhance diffusion models by utilizing properties of natural images and shows improvements in ImageNet FID and text-to-image quality.",
      "ai_keywords": [
        "diffusion models",
        "synthetic images",
        "out-of-distribution images",
        "Ambient Diffusion Omni",
        "spectral power law decay",
        "locality",
        "Gaussian blur",
        "JPEG compression",
        "motion blur",
        "ImageNet FID",
        "text-to-image generative modeling",
        "noise dampening",
        "biased data",
        "limited unbiased data"
      ]
    },
    "publishedAt": "2025-06-10T18:37:39.000Z",
    "title": "Ambient Diffusion Omni: Training Good Models with Bad Data",
    "summary": "We show how to use low-quality, synthetic, and out-of-distribution images to\nimprove the quality of a diffusion model. Typically, diffusion models are\ntrained on curated datasets that emerge from highly filtered data pools from\nthe Web and other sources. We show that there is immense value in the\nlower-quality images that are often discarded. We present Ambient Diffusion\nOmni, a simple, principled framework to train diffusion models that can extract\nsignal from all available images during training. Our framework exploits two\nproperties of natural images -- spectral power law decay and locality. We first\nvalidate our framework by successfully training diffusion models with images\nsynthetically corrupted by Gaussian blur, JPEG compression, and motion blur. We\nthen use our framework to achieve state-of-the-art ImageNet FID, and we show\nsignificant improvements in both image quality and diversity for text-to-image\ngenerative modeling. The core insight is that noise dampens the initial skew\nbetween the desired high-quality distribution and the mixed distribution we\nactually observe. We provide rigorous theoretical justification for our\napproach by analyzing the trade-off between learning from biased data versus\nlimited unbiased data across diffusion times.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10038.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f45f44b79c1ba4c353d1035",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5f45f44b79c1ba4c353d1035/6piqagYr7RNCy6XwJGWCG.jpeg",
      "fullname": "Giannis Daras",
      "name": "giannisdaras",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.05336",
      "authors": [
        {
          "_id": "68425a7ab63271ff41652734",
          "name": "Ghazi Shazan Ahmad",
          "hidden": false
        },
        {
          "_id": "68425a7ab63271ff41652735",
          "user": {
            "_id": "656864e12d73834278a8dea7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
            "isPro": true,
            "fullname": "Ahmed Heakl",
            "user": "ahmedheakl",
            "type": "user"
          },
          "name": "Ahmed Heakl",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-06-07T05:48:53.401Z",
          "hidden": false
        },
        {
          "_id": "68425a7ab63271ff41652736",
          "name": "Hanan Gani",
          "hidden": false
        },
        {
          "_id": "68425a7ab63271ff41652737",
          "name": "Abdelrahman Shaker",
          "hidden": false
        },
        {
          "_id": "68425a7ab63271ff41652738",
          "name": "Zhiqiang Shen",
          "hidden": false
        },
        {
          "_id": "68425a7ab63271ff41652739",
          "name": "Ranjay Krishna",
          "hidden": false
        },
        {
          "_id": "68425a7ab63271ff4165273a",
          "name": "Fahad Shahbaz Khan",
          "hidden": false
        },
        {
          "_id": "68425a7ab63271ff4165273b",
          "name": "Salman Khan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/7r2x8C0UYFCrqk95QpBGr.mp4"
      ],
      "publishedAt": "2025-06-05T17:59:29.000Z",
      "submittedOnDailyAt": "2025-06-18T01:30:20.599Z",
      "title": "VideoMolmo: Spatio-Temporal Grounding Meets Pointing",
      "submittedOnDailyBy": {
        "_id": "656864e12d73834278a8dea7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
        "isPro": true,
        "fullname": "Ahmed Heakl",
        "user": "ahmedheakl",
        "type": "user"
      },
      "summary": "Spatio-temporal localization is vital for precise interactions across diverse\ndomains, from biological research to autonomous navigation and interactive\ninterfaces. Current video-based approaches, while proficient in tracking, lack\nthe sophisticated reasoning capabilities of large language models, limiting\ntheir contextual understanding and generalization. We introduce VideoMolmo, a\nlarge multimodal model tailored for fine-grained spatio-temporal pointing\nconditioned on textual descriptions. Building upon the Molmo architecture,\nVideoMolmo incorporates a temporal module utilizing an attention mechanism to\ncondition each frame on preceding frames, ensuring temporal consistency.\nAdditionally, our novel temporal mask fusion pipeline employs SAM2 for\nbidirectional point propagation, significantly enhancing coherence across video\nsequences. This two-step decomposition, i.e., first using the LLM to generate\nprecise pointing coordinates, then relying on a sequential mask-fusion module\nto produce coherent segmentation, not only simplifies the task for the language\nmodel but also enhances interpretability. Due to the lack of suitable datasets,\nwe curate a comprehensive dataset comprising 72k video-caption pairs annotated\nwith 100k object points. To evaluate the generalization of VideoMolmo, we\nintroduce VPoS-Bench, a challenging out-of-distribution benchmark spanning five\nreal-world scenarios: Cell Tracking, Egocentric Vision, Autonomous Driving,\nVideo-GUI Interaction, and Robotics. We also evaluate our model on Referring\nVideo Object Segmentation (Refer-VOS) and Reasoning VOS tasks. In comparison to\nexisting models, VideoMolmo substantially improves spatio-temporal pointing\naccuracy and reasoning capability. Our code and models are publicly available\nat https://github.com/mbzuai-oryx/VideoMolmo.",
      "upvotes": 3,
      "discussionId": "68425a80b63271ff416528f2",
      "projectPage": "https://mbzuai-oryx.github.io/VideoMolmo/",
      "githubRepo": "https://github.com/mbzuai-oryx/VideoMolmo",
      "ai_summary": "VideoMolmo, a multimodal model incorporating a temporal attention mechanism and SAM2 for mask fusion, enhances spatio-temporal pointing accuracy and reasoning capabilities in diverse real-world scenarios.",
      "ai_keywords": [
        "Molmo",
        "attention mechanism",
        "temporal mask fusion",
        "SAM2",
        "bidirectional point propagation",
        "VideoMolmo",
        "LLM",
        "sequential mask-fusion module",
        "VPoS-Bench",
        "Referring Video Object Segmentation",
        "Reasoning VOS"
      ]
    },
    "publishedAt": "2025-06-05T13:59:29.000Z",
    "title": "VideoMolmo: Spatio-Temporal Grounding Meets Pointing",
    "summary": "Spatio-temporal localization is vital for precise interactions across diverse\ndomains, from biological research to autonomous navigation and interactive\ninterfaces. Current video-based approaches, while proficient in tracking, lack\nthe sophisticated reasoning capabilities of large language models, limiting\ntheir contextual understanding and generalization. We introduce VideoMolmo, a\nlarge multimodal model tailored for fine-grained spatio-temporal pointing\nconditioned on textual descriptions. Building upon the Molmo architecture,\nVideoMolmo incorporates a temporal module utilizing an attention mechanism to\ncondition each frame on preceding frames, ensuring temporal consistency.\nAdditionally, our novel temporal mask fusion pipeline employs SAM2 for\nbidirectional point propagation, significantly enhancing coherence across video\nsequences. This two-step decomposition, i.e., first using the LLM to generate\nprecise pointing coordinates, then relying on a sequential mask-fusion module\nto produce coherent segmentation, not only simplifies the task for the language\nmodel but also enhances interpretability. Due to the lack of suitable datasets,\nwe curate a comprehensive dataset comprising 72k video-caption pairs annotated\nwith 100k object points. To evaluate the generalization of VideoMolmo, we\nintroduce VPoS-Bench, a challenging out-of-distribution benchmark spanning five\nreal-world scenarios: Cell Tracking, Egocentric Vision, Autonomous Driving,\nVideo-GUI Interaction, and Robotics. We also evaluate our model on Referring\nVideo Object Segmentation (Refer-VOS) and Reasoning VOS tasks. In comparison to\nexisting models, VideoMolmo substantially improves spatio-temporal pointing\naccuracy and reasoning capability. Our code and models are publicly available\nat https://github.com/mbzuai-oryx/VideoMolmo.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/7r2x8C0UYFCrqk95QpBGr.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05336.png",
    "numComments": 5,
    "submittedBy": {
      "_id": "656864e12d73834278a8dea7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
      "fullname": "Ahmed Heakl",
      "name": "ahmedheakl",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 41
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2506.14755",
      "authors": [
        {
          "_id": "685225250164cd13167104aa",
          "name": "Zhengxiang Cheng",
          "hidden": false
        },
        {
          "_id": "685225250164cd13167104ab",
          "name": "Dongping Chen",
          "hidden": false
        },
        {
          "_id": "685225250164cd13167104ac",
          "name": "Mingyang Fu",
          "hidden": false
        },
        {
          "_id": "685225250164cd13167104ad",
          "name": "Tianyi Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-17T17:50:16.000Z",
      "submittedOnDailyAt": "2025-06-18T01:05:46.554Z",
      "title": "Optimizing Length Compression in Large Reasoning Models",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "Large Reasoning Models (LRMs) have achieved remarkable success, yet they\noften suffer from producing unnecessary and verbose reasoning chains. We\nidentify a core aspect of this issue as \"invalid thinking\" -- models tend to\nrepeatedly double-check their work after having derived the correct answer. To\naddress this specific inefficiency, we move beyond the general principles of\nEfficacy and Efficiency to propose two new, fine-grained principles: Brevity,\nwhich advocates for eliminating redundancy, and Sufficiency, which ensures\ncritical reasoning steps are preserved. Guided by these principles, we\nintroduce LC-R1, a post-training method based on Group Relative Policy\nOptimization (GRPO). LC-R1 employs a novel combination of a Length Reward for\noverall conciseness and a Compress Reward that is specifically designed to\nremove the invalid portion of the thinking process. Extensive experiments on\nmultiple reasoning benchmarks demonstrate that LC-R1 achieves a significant\nreduction in sequence length (~50%) with only a marginal (~2%) drop in\naccuracy, achieving a favorable trade-off point on the Pareto frontier that\nprioritizes high compression. Our analysis further validates the robustness of\nLC-R1 and provides valuable insights for developing more powerful yet\ncomputationally efficient LRMs. Our code is released at\nhttps://github.com/zxiangx/LC-R1.",
      "upvotes": 1,
      "discussionId": "685225250164cd13167104ae",
      "githubRepo": "https://github.com/zxiangx/LC-R1",
      "ai_summary": "LC-R1, a post-training method guided by Brevity and Sufficiency principles, reduces unnecessary reasoning in Large Reasoning Models with minimal accuracy loss.",
      "ai_keywords": [
        "Large Reasoning Models",
        "LRM",
        "post-training method",
        "Group Relative Policy Optimization",
        "GRPO",
        "Length Reward",
        "Compress Reward",
        "reasoning benchmarks",
        "Pareto frontier"
      ]
    },
    "publishedAt": "2025-06-17T13:50:16.000Z",
    "title": "Optimizing Length Compression in Large Reasoning Models",
    "summary": "Large Reasoning Models (LRMs) have achieved remarkable success, yet they\noften suffer from producing unnecessary and verbose reasoning chains. We\nidentify a core aspect of this issue as \"invalid thinking\" -- models tend to\nrepeatedly double-check their work after having derived the correct answer. To\naddress this specific inefficiency, we move beyond the general principles of\nEfficacy and Efficiency to propose two new, fine-grained principles: Brevity,\nwhich advocates for eliminating redundancy, and Sufficiency, which ensures\ncritical reasoning steps are preserved. Guided by these principles, we\nintroduce LC-R1, a post-training method based on Group Relative Policy\nOptimization (GRPO). LC-R1 employs a novel combination of a Length Reward for\noverall conciseness and a Compress Reward that is specifically designed to\nremove the invalid portion of the thinking process. Extensive experiments on\nmultiple reasoning benchmarks demonstrate that LC-R1 achieves a significant\nreduction in sequence length (~50%) with only a marginal (~2%) drop in\naccuracy, achieving a favorable trade-off point on the Pareto frontier that\nprioritizes high compression. Our analysis further validates the robustness of\nLC-R1 and provides valuable insights for developing more powerful yet\ncomputationally efficient LRMs. Our code is released at\nhttps://github.com/zxiangx/LC-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14755.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14731",
      "authors": [
        {
          "_id": "685236ac0164cd1316710512",
          "name": "Ring Team",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710513",
          "name": "Bin Hu",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710514",
          "name": "Cai Chen",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710515",
          "name": "Deng Zhao",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710516",
          "name": "Ding Liu",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710517",
          "name": "Dingnan Jin",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710518",
          "name": "Feng Zhu",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710519",
          "name": "Hao Dai",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671051a",
          "name": "Hongzhi Luan",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671051b",
          "name": "Jia Guo",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671051c",
          "name": "Jiaming Liu",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671051d",
          "name": "Jiewei Wu",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671051e",
          "name": "Jun Mei",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671051f",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710520",
          "name": "Junbo Zhao",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710521",
          "name": "Junwu Xiong",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710522",
          "name": "Kaihong Zhang",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710523",
          "name": "Kuan Xu",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710524",
          "name": "Lei Liang",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710525",
          "name": "Liang Jiang",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710526",
          "name": "Liangcheng Fu",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710527",
          "name": "Longfei Zheng",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710528",
          "name": "Qiang Gao",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710529",
          "name": "Qing Cui",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671052a",
          "name": "Quan Wan",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671052b",
          "name": "Shaomian Zheng",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671052c",
          "name": "Shuaicheng Li",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671052d",
          "name": "Tongkai Yang",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671052e",
          "name": "Wang Ren",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671052f",
          "name": "Xiaodong Yan",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710530",
          "name": "Xiaopei Wan",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710531",
          "name": "Xiaoyun Feng",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710532",
          "name": "Xin Zhao",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710533",
          "name": "Xinxing Yang",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710534",
          "name": "Xinyu Kong",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710535",
          "name": "Xuemin Yang",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710536",
          "name": "Yang Li",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710537",
          "name": "Yingting Wu",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710538",
          "name": "Yongkang Liu",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd1316710539",
          "name": "Zhankai Xu",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671053a",
          "name": "Zhenduo Zhang",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671053b",
          "name": "Zhenglei Zhou",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671053c",
          "name": "Zhenyu Huang",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671053d",
          "name": "Zhiqiang Zhang",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671053e",
          "name": "Zihao Wang",
          "hidden": false
        },
        {
          "_id": "685236ac0164cd131671053f",
          "name": "Zujie Wen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-17T17:12:34.000Z",
      "submittedOnDailyAt": "2025-06-18T03:14:38.830Z",
      "title": "Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning\n  for LLMs",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": true,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model\noptimized via reinforcement learning (RL) to achieve efficient and robust\nreasoning capabilities. Built upon the publicly available Ling-lite model, a\n16.8 billion parameter model with 2.75 billion activated parameters, our\napproach matches the performance of state-of-the-art (SOTA) small-scale\nreasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench,\nGPQA-Diamond) while activating only one-third of the parameters required by\ncomparable models. To accomplish this, we introduce a joint training pipeline\nintegrating distillation with RL, revealing undocumented challenges in MoE RL\ntraining. First, we identify optimization instability during RL training, and\nwe propose Constrained Contextual Computation Policy Optimization(C3PO), a\nnovel approach that enhances training stability and improves computational\nthroughput via algorithm-system co-design methodology. Second, we empirically\ndemonstrate that selecting distillation checkpoints based on entropy loss for\nRL training, rather than validation metrics, yields superior\nperformance-efficiency trade-offs in subsequent RL training. Finally, we\ndevelop a two-stage training paradigm to harmonize multi-domain data\nintegration, addressing domain conflicts that arise in training with mixed\ndataset. We will release the model, dataset, and code.",
      "upvotes": 1,
      "discussionId": "685236ad0164cd1316710540",
      "ai_summary": "Ring-lite uses a MoE architecture and reinforcement learning to efficiently match SOTA reasoning models while activating fewer parameters and addressing challenges specific to MoE training.",
      "ai_keywords": [
        "Mixture-of-Experts (MoE)",
        "reinforcement learning (RL)",
        "Ling-lite",
        "AIME",
        "LiveCodeBench",
        "GPQA-Diamond",
        "Constrained Contextual Computation Policy Optimization(C3PO)",
        "entropy loss",
        "two-stage training paradigm"
      ]
    },
    "publishedAt": "2025-06-17T13:12:34.000Z",
    "title": "Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning\n  for LLMs",
    "summary": "We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model\noptimized via reinforcement learning (RL) to achieve efficient and robust\nreasoning capabilities. Built upon the publicly available Ling-lite model, a\n16.8 billion parameter model with 2.75 billion activated parameters, our\napproach matches the performance of state-of-the-art (SOTA) small-scale\nreasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench,\nGPQA-Diamond) while activating only one-third of the parameters required by\ncomparable models. To accomplish this, we introduce a joint training pipeline\nintegrating distillation with RL, revealing undocumented challenges in MoE RL\ntraining. First, we identify optimization instability during RL training, and\nwe propose Constrained Contextual Computation Policy Optimization(C3PO), a\nnovel approach that enhances training stability and improves computational\nthroughput via algorithm-system co-design methodology. Second, we empirically\ndemonstrate that selecting distillation checkpoints based on entropy loss for\nRL training, rather than validation metrics, yields superior\nperformance-efficiency trade-offs in subsequent RL training. Finally, we\ndevelop a two-stage training paradigm to harmonize multi-domain data\nintegration, addressing domain conflicts that arise in training with mixed\ndataset. We will release the model, dataset, and code.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14731.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": true,
      "isHf": true,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7129
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.14702",
      "authors": [
        {
          "_id": "6852331e0164cd13167104fb",
          "name": "Daniel D'souza",
          "hidden": false
        },
        {
          "_id": "6852331e0164cd13167104fc",
          "name": "Julia Kreutzer",
          "hidden": false
        },
        {
          "_id": "6852331e0164cd13167104fd",
          "name": "Adrien Morisot",
          "hidden": false
        },
        {
          "_id": "6852331e0164cd13167104fe",
          "name": "Ahmet Üstün",
          "hidden": false
        },
        {
          "_id": "6852331e0164cd13167104ff",
          "name": "Sara Hooker",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6658011eaba105a066e37e1b/RBQEHm8CJA9fKsg4bupgs.png"
      ],
      "publishedAt": "2025-06-17T16:40:42.000Z",
      "submittedOnDailyAt": "2025-06-18T02:55:26.808Z",
      "title": "Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time\n  Markers",
      "submittedOnDailyBy": {
        "_id": "6658011eaba105a066e37e1b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6658011eaba105a066e37e1b/VPwyTv1bnVMQbVMoMQzcf.jpeg",
        "isPro": false,
        "fullname": "Daniel D'souza",
        "user": "dsouzadaniel",
        "type": "user"
      },
      "summary": "One of the most profound challenges of modern machine learning is performing\nwell on the long-tail of rare and underrepresented features. Large\ngeneral-purpose models are trained for many tasks, but work best on\nhigh-frequency use cases. After training, it is hard to adapt a model to\nperform well on specific use cases underrepresented in the training corpus.\nRelying on prompt engineering or few-shot examples to maximize the output\nquality on a particular test case can be frustrating, as models can be highly\nsensitive to small changes, react in unpredicted ways or rely on a fixed system\nprompt for maintaining performance. In this work, we ask: \"Can we optimize our\ntraining protocols to both improve controllability and performance on\nunderrepresented use cases at inference time?\" We revisit the divide between\ntraining and inference techniques to improve long-tail performance while\nproviding users with a set of control levers the model is trained to be\nresponsive to. We create a detailed taxonomy of data characteristics and task\nprovenance to explicitly control generation attributes and implicitly condition\ngenerations at inference time. We fine-tune a base model to infer these markers\nautomatically, which makes them optional at inference time. This principled and\nflexible approach yields pronounced improvements in performance, especially on\nexamples from the long tail of the training distribution. While we observe an\naverage lift of 5.7% win rates in open-ended generation quality with our\nmarkers, we see over 9.1% gains in underrepresented domains. We also observe\nrelative lifts of up to 14.1% on underrepresented tasks like CodeRepair and\nabsolute improvements of 35.3% on length instruction following evaluations.",
      "upvotes": 1,
      "discussionId": "6852331e0164cd1316710500",
      "ai_summary": "A principled approach to fine-tuning models for better performance and controllability on underrepresented use cases is developed through automatic inference of generation attributes.",
      "ai_keywords": [
        "prompt engineering",
        "few-shot examples",
        "controllability",
        "performance",
        "long-tail",
        "training protocols",
        "inference techniques",
        "taxonomy",
        "data characteristics",
        "task provenance",
        "fine-tuning",
        "generation attributes",
        "markers",
        "underrepresented domains",
        "CodeRepair",
        "length instruction following"
      ]
    },
    "publishedAt": "2025-06-17T12:40:42.000Z",
    "title": "Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time\n  Markers",
    "summary": "One of the most profound challenges of modern machine learning is performing\nwell on the long-tail of rare and underrepresented features. Large\ngeneral-purpose models are trained for many tasks, but work best on\nhigh-frequency use cases. After training, it is hard to adapt a model to\nperform well on specific use cases underrepresented in the training corpus.\nRelying on prompt engineering or few-shot examples to maximize the output\nquality on a particular test case can be frustrating, as models can be highly\nsensitive to small changes, react in unpredicted ways or rely on a fixed system\nprompt for maintaining performance. In this work, we ask: \"Can we optimize our\ntraining protocols to both improve controllability and performance on\nunderrepresented use cases at inference time?\" We revisit the divide between\ntraining and inference techniques to improve long-tail performance while\nproviding users with a set of control levers the model is trained to be\nresponsive to. We create a detailed taxonomy of data characteristics and task\nprovenance to explicitly control generation attributes and implicitly condition\ngenerations at inference time. We fine-tune a base model to infer these markers\nautomatically, which makes them optional at inference time. This principled and\nflexible approach yields pronounced improvements in performance, especially on\nexamples from the long tail of the training distribution. While we observe an\naverage lift of 5.7% win rates in open-ended generation quality with our\nmarkers, we see over 9.1% gains in underrepresented domains. We also observe\nrelative lifts of up to 14.1% on underrepresented tasks like CodeRepair and\nabsolute improvements of 35.3% on length instruction following evaluations.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6658011eaba105a066e37e1b/RBQEHm8CJA9fKsg4bupgs.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14702.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6658011eaba105a066e37e1b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6658011eaba105a066e37e1b/VPwyTv1bnVMQbVMoMQzcf.jpeg",
      "fullname": "Daniel D'souza",
      "name": "dsouzadaniel",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.13901",
      "authors": [
        {
          "_id": "685252860164cd13167105c7",
          "name": "Abhilekh Borah",
          "hidden": false
        },
        {
          "_id": "685252860164cd13167105c8",
          "name": "Chhavi Sharma",
          "hidden": false
        },
        {
          "_id": "685252860164cd13167105c9",
          "name": "Danush Khanna",
          "hidden": false
        },
        {
          "_id": "685252860164cd13167105ca",
          "name": "Utkarsh Bhatt",
          "hidden": false
        },
        {
          "_id": "685252860164cd13167105cb",
          "name": "Gurpreet Singh",
          "hidden": false
        },
        {
          "_id": "685252860164cd13167105cc",
          "name": "Hasnat Md Abdullah",
          "hidden": false
        },
        {
          "_id": "685252860164cd13167105cd",
          "name": "Raghav Kaushik Ravi",
          "hidden": false
        },
        {
          "_id": "685252860164cd13167105ce",
          "name": "Vinija Jain",
          "hidden": false
        },
        {
          "_id": "685252860164cd13167105cf",
          "name": "Jyoti Patel",
          "hidden": false
        },
        {
          "_id": "685252860164cd13167105d0",
          "name": "Shubham Singh",
          "hidden": false
        },
        {
          "_id": "685252860164cd13167105d1",
          "name": "Vasu Sharma",
          "hidden": false
        },
        {
          "_id": "685252860164cd13167105d2",
          "name": "Arpita Vats",
          "hidden": false
        },
        {
          "_id": "685252860164cd13167105d3",
          "name": "Rahul Raja",
          "hidden": false
        },
        {
          "_id": "685252860164cd13167105d4",
          "name": "Aman Chadha",
          "hidden": false
        },
        {
          "_id": "685252860164cd13167105d5",
          "name": "Amitava Das",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-16T18:22:28.000Z",
      "submittedOnDailyAt": "2025-06-18T04:17:43.427Z",
      "title": "Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic\n  Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise\n  Pooled Representations",
      "submittedOnDailyBy": {
        "_id": "63a4754927f1f64ed7238dac",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
        "isPro": false,
        "fullname": "Aman Chadha",
        "user": "amanchadha",
        "type": "user"
      },
      "summary": "Alignment is no longer a luxury, it is a necessity. As large language models\n(LLMs) enter high-stakes domains like education, healthcare, governance, and\nlaw, their behavior must reliably reflect human-aligned values and safety\nconstraints. Yet current evaluations rely heavily on behavioral proxies such as\nrefusal rates, G-Eval scores, and toxicity classifiers, all of which have\ncritical blind spots. Aligned models are often vulnerable to jailbreaking,\nstochasticity of generation, and alignment faking.\n  To address this issue, we introduce the Alignment Quality Index (AQI). This\nnovel geometric and prompt-invariant metric empirically assesses LLM alignment\nby analyzing the separation of safe and unsafe activations in latent space. By\ncombining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI),\nXie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various\nformulations, AQI captures clustering quality to detect hidden misalignments\nand jailbreak risks, even when outputs appear compliant. AQI also serves as an\nearly warning signal for alignment faking, offering a robust, decoding\ninvariant tool for behavior agnostic safety auditing.\n  Additionally, we propose the LITMUS dataset to facilitate robust evaluation\nunder these challenging conditions. Empirical tests on LITMUS across different\nmodels trained under DPO, GRPO, and RLHF conditions demonstrate AQI's\ncorrelation with external judges and ability to reveal vulnerabilities missed\nby refusal metrics. We make our implementation publicly available to foster\nfuture research in this area.",
      "upvotes": 1,
      "discussionId": "685252870164cd13167105d6",
      "ai_summary": "A new evaluation metric called Alignment Quality Index (AQI) assesses the alignment of large language models by analyzing latent space activations, capturing clustering quality to detect misalignments and fake alignment, and complementing existing behavioral proxies.",
      "ai_keywords": [
        "Alignment Quality Index (AQI)",
        "latent space",
        "Davies-Bouldin Score (DBS)",
        "Dunn Index (DI)",
        "Xie-Beni Index (XBI)",
        "Calinski-Harabasz Index (CHI)",
        "LITMUS dataset",
        "DPO",
        "GRPO",
        "RLHF",
        "alignment faking",
        "external judges"
      ]
    },
    "publishedAt": "2025-06-16T14:22:28.000Z",
    "title": "Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic\n  Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise\n  Pooled Representations",
    "summary": "Alignment is no longer a luxury, it is a necessity. As large language models\n(LLMs) enter high-stakes domains like education, healthcare, governance, and\nlaw, their behavior must reliably reflect human-aligned values and safety\nconstraints. Yet current evaluations rely heavily on behavioral proxies such as\nrefusal rates, G-Eval scores, and toxicity classifiers, all of which have\ncritical blind spots. Aligned models are often vulnerable to jailbreaking,\nstochasticity of generation, and alignment faking.\n  To address this issue, we introduce the Alignment Quality Index (AQI). This\nnovel geometric and prompt-invariant metric empirically assesses LLM alignment\nby analyzing the separation of safe and unsafe activations in latent space. By\ncombining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI),\nXie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various\nformulations, AQI captures clustering quality to detect hidden misalignments\nand jailbreak risks, even when outputs appear compliant. AQI also serves as an\nearly warning signal for alignment faking, offering a robust, decoding\ninvariant tool for behavior agnostic safety auditing.\n  Additionally, we propose the LITMUS dataset to facilitate robust evaluation\nunder these challenging conditions. Empirical tests on LITMUS across different\nmodels trained under DPO, GRPO, and RLHF conditions demonstrate AQI's\ncorrelation with external judges and ability to reveal vulnerabilities missed\nby refusal metrics. We make our implementation publicly available to foster\nfuture research in this area.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13901.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4754927f1f64ed7238dac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
      "fullname": "Aman Chadha",
      "name": "amanchadha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.13599",
      "authors": [
        {
          "_id": "685220810164cd131671048e",
          "name": "Yuwei Du",
          "hidden": false
        },
        {
          "_id": "685220810164cd131671048f",
          "name": "Jie Feng",
          "hidden": false
        },
        {
          "_id": "685220810164cd1316710490",
          "name": "Jian Yuan",
          "hidden": false
        },
        {
          "_id": "685220810164cd1316710491",
          "name": "Yong Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/t_3QiPaJ54vNcgeYZKMgy.jpeg"
      ],
      "publishedAt": "2025-06-16T15:24:07.000Z",
      "submittedOnDailyAt": "2025-06-18T01:27:33.225Z",
      "title": "CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility\n  Simulation",
      "submittedOnDailyBy": {
        "_id": "6465d3bd63e7e09dd02e95c3",
        "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
        "isPro": false,
        "fullname": "Jie Feng",
        "user": "JJ-TMT",
        "type": "user"
      },
      "summary": "Human mobility simulation plays a crucial role in various real-world\napplications. Recently, to address the limitations of traditional data-driven\napproaches, researchers have explored leveraging the commonsense knowledge and\nreasoning capabilities of large language models (LLMs) to accelerate human\nmobility simulation. However, these methods suffer from several critical\nshortcomings, including inadequate modeling of urban spaces and poor\nintegration with both individual mobility patterns and collective mobility\ndistributions. To address these challenges, we propose CityGPT-Powered\nAgentic framework for Mobility Simulation\n(CAMS), an agentic framework that leverages the language based urban\nfoundation model to simulate human mobility in urban space. CAMS\ncomprises three core modules, including MobExtractor to extract template\nmobility patterns and synthesize new ones based on user profiles, GeoGenerator\nto generate anchor points considering collective knowledge and generate\ncandidate urban geospatial knowledge using an enhanced version of CityGPT,\nTrajEnhancer to retrieve spatial knowledge based on mobility patterns and\ngenerate trajectories with real trajectory preference alignment via DPO.\nExperiments on real-world datasets show that CAMS achieves superior\nperformance without relying on externally provided geospatial information.\nMoreover, by holistically modeling both individual mobility patterns and\ncollective mobility constraints, CAMS generates more realistic and\nplausible trajectories. In general, CAMS establishes a new paradigm\nthat integrates the agentic framework with urban-knowledgeable LLMs for human\nmobility simulation.",
      "upvotes": 1,
      "discussionId": "685220820164cd1316710492",
      "ai_summary": "CAMS integrates an agentic framework with urban-knowledgeable large language models to simulate human mobility more realistically by modeling individual and collective patterns.",
      "ai_keywords": [
        "large language models",
        "CityGPT",
        "agentic framework",
        "human mobility simulation",
        "urban spaces",
        "individual mobility patterns",
        "collective mobility distributions",
        "MobExtractor",
        "GeoGenerator",
        "TrajEnhancer",
        "DPO",
        "trajectory preference alignment",
        "real-world datasets"
      ]
    },
    "publishedAt": "2025-06-16T11:24:07.000Z",
    "title": "CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility\n  Simulation",
    "summary": "Human mobility simulation plays a crucial role in various real-world\napplications. Recently, to address the limitations of traditional data-driven\napproaches, researchers have explored leveraging the commonsense knowledge and\nreasoning capabilities of large language models (LLMs) to accelerate human\nmobility simulation. However, these methods suffer from several critical\nshortcomings, including inadequate modeling of urban spaces and poor\nintegration with both individual mobility patterns and collective mobility\ndistributions. To address these challenges, we propose CityGPT-Powered\nAgentic framework for Mobility Simulation\n(CAMS), an agentic framework that leverages the language based urban\nfoundation model to simulate human mobility in urban space. CAMS\ncomprises three core modules, including MobExtractor to extract template\nmobility patterns and synthesize new ones based on user profiles, GeoGenerator\nto generate anchor points considering collective knowledge and generate\ncandidate urban geospatial knowledge using an enhanced version of CityGPT,\nTrajEnhancer to retrieve spatial knowledge based on mobility patterns and\ngenerate trajectories with real trajectory preference alignment via DPO.\nExperiments on real-world datasets show that CAMS achieves superior\nperformance without relying on externally provided geospatial information.\nMoreover, by holistically modeling both individual mobility patterns and\ncollective mobility constraints, CAMS generates more realistic and\nplausible trajectories. In general, CAMS establishes a new paradigm\nthat integrates the agentic framework with urban-knowledgeable LLMs for human\nmobility simulation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/t_3QiPaJ54vNcgeYZKMgy.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13599.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6465d3bd63e7e09dd02e95c3",
      "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
      "fullname": "Jie Feng",
      "name": "JJ-TMT",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2506.13387",
      "authors": [
        {
          "_id": "6852239f0164cd13167104a4",
          "name": "Beilei Cui",
          "hidden": false
        },
        {
          "_id": "6852239f0164cd13167104a5",
          "name": "Yiming Huang",
          "hidden": false
        },
        {
          "_id": "6852239f0164cd13167104a6",
          "name": "Long Bai",
          "hidden": false
        },
        {
          "_id": "6852239f0164cd13167104a7",
          "name": "Hongliang Ren",
          "hidden": false
        }
      ],
      "publishedAt": "2025-06-16T11:50:00.000Z",
      "submittedOnDailyAt": "2025-06-18T00:57:15.579Z",
      "title": "TR2M: Transferring Monocular Relative Depth to Metric Depth with\n  Language Descriptions and Scale-Oriented Contrast",
      "submittedOnDailyBy": {
        "_id": "68518fb45452a74491857c5b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/SOYIB8T0LsNZM3ORcHXlr.png",
        "isPro": false,
        "fullname": "Beilei Cui",
        "user": "BeileiCui",
        "type": "user"
      },
      "summary": "This work presents a generalizable framework to transfer relative depth to\nmetric depth. Current monocular depth estimation methods are mainly divided\ninto metric depth estimation (MMDE) and relative depth estimation (MRDE). MMDEs\nestimate depth in metric scale but are often limited to a specific domain.\nMRDEs generalize well across different domains, but with uncertain scales which\nhinders downstream applications. To this end, we aim to build up a framework to\nsolve scale uncertainty and transfer relative depth to metric depth. Previous\nmethods used language as input and estimated two factors for conducting\nrescaling. Our approach, TR2M, utilizes both text description and image as\ninputs and estimates two rescale maps to transfer relative depth to metric\ndepth at pixel level. Features from two modalities are fused with a\ncross-modality attention module to better capture scale information. A strategy\nis designed to construct and filter confident pseudo metric depth for more\ncomprehensive supervision. We also develop scale-oriented contrastive learning\nto utilize depth distribution as guidance to enforce the model learning about\nintrinsic knowledge aligning with the scale distribution. TR2M only exploits a\nsmall number of trainable parameters to train on datasets in various domains\nand experiments not only demonstrate TR2M's great performance in seen datasets\nbut also reveal superior zero-shot capabilities on five unseen datasets. We\nshow the huge potential in pixel-wise transferring relative depth to metric\ndepth with language assistance. (Code is available at:\nhttps://github.com/BeileiCui/TR2M)",
      "upvotes": 0,
      "discussionId": "6852239f0164cd13167104a8",
      "ai_summary": "A framework, TR2M, uses multimodal inputs to rescale relative depth to metric depth, enhancing performance across various datasets through cross-modality attention and contrastive learning.",
      "ai_keywords": [
        "relative depth estimation",
        "metric depth estimation",
        "cross-modality attention",
        "contrastive learning",
        "rescale maps",
        "pseudo metric depth",
        "intrinsically aligned scale distribution"
      ]
    },
    "publishedAt": "2025-06-16T07:50:00.000Z",
    "title": "TR2M: Transferring Monocular Relative Depth to Metric Depth with\n  Language Descriptions and Scale-Oriented Contrast",
    "summary": "This work presents a generalizable framework to transfer relative depth to\nmetric depth. Current monocular depth estimation methods are mainly divided\ninto metric depth estimation (MMDE) and relative depth estimation (MRDE). MMDEs\nestimate depth in metric scale but are often limited to a specific domain.\nMRDEs generalize well across different domains, but with uncertain scales which\nhinders downstream applications. To this end, we aim to build up a framework to\nsolve scale uncertainty and transfer relative depth to metric depth. Previous\nmethods used language as input and estimated two factors for conducting\nrescaling. Our approach, TR2M, utilizes both text description and image as\ninputs and estimates two rescale maps to transfer relative depth to metric\ndepth at pixel level. Features from two modalities are fused with a\ncross-modality attention module to better capture scale information. A strategy\nis designed to construct and filter confident pseudo metric depth for more\ncomprehensive supervision. We also develop scale-oriented contrastive learning\nto utilize depth distribution as guidance to enforce the model learning about\nintrinsic knowledge aligning with the scale distribution. TR2M only exploits a\nsmall number of trainable parameters to train on datasets in various domains\nand experiments not only demonstrate TR2M's great performance in seen datasets\nbut also reveal superior zero-shot capabilities on five unseen datasets. We\nshow the huge potential in pixel-wise transferring relative depth to metric\ndepth with language assistance. (Code is available at:\nhttps://github.com/BeileiCui/TR2M)",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13387.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "68518fb45452a74491857c5b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/SOYIB8T0LsNZM3ORcHXlr.png",
      "fullname": "Beilei Cui",
      "name": "BeileiCui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]