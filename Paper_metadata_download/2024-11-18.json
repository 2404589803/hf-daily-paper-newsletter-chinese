[
    {
        "paper": {
            "id": "2411.10440",
            "authors": [
                {
                    "_id": "673aa0816c12c4b98bda19ea",
                    "user": {
                        "_id": "63e992cdccae1fe5c6222f84",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e992cdccae1fe5c6222f84/IvksSUf2DENfUwCZSmNPd.jpeg",
                        "isPro": false,
                        "fullname": "Guowei Xu",
                        "user": "Xkev",
                        "type": "user"
                    },
                    "name": "Guowei Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-18T07:47:22.405Z",
                    "hidden": false
                },
                {
                    "_id": "673aa0816c12c4b98bda19eb",
                    "user": {
                        "_id": "651585ea18cfed0d30bee586",
                        "avatarUrl": "/avatars/579e468334102472d870875fe40302e6.svg",
                        "isPro": false,
                        "fullname": "Peng Jin",
                        "user": "Chat-UniVi",
                        "type": "user"
                    },
                    "name": "Peng Jin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-18T15:19:57.329Z",
                    "hidden": false
                },
                {
                    "_id": "673aa0816c12c4b98bda19ec",
                    "name": "Li Hao",
                    "hidden": false
                },
                {
                    "_id": "673aa0816c12c4b98bda19ed",
                    "user": {
                        "_id": "62c51800cb7033fd49b8efb7",
                        "avatarUrl": "/avatars/06c2be0015f8022f9912f2279f2b3597.svg",
                        "isPro": false,
                        "fullname": "Song",
                        "user": "Yibing",
                        "type": "user"
                    },
                    "name": "Yibing Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-18T07:49:29.520Z",
                    "hidden": false
                },
                {
                    "_id": "673aa0816c12c4b98bda19ee",
                    "user": {
                        "_id": "65a52766215aabac489e3468",
                        "avatarUrl": "/avatars/fe05e22cd7e12e961296426434e17c76.svg",
                        "isPro": false,
                        "fullname": "Lichao Sun",
                        "user": "sunlichao137",
                        "type": "user"
                    },
                    "name": "Lichao Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-18T07:49:20.178Z",
                    "hidden": false
                },
                {
                    "_id": "673aa0816c12c4b98bda19ef",
                    "user": {
                        "_id": "614030bd8cbdb613b82f36a8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1631596713749-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Li Yuan",
                        "user": "LiYuan",
                        "type": "user"
                    },
                    "name": "Li Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-18T07:49:13.527Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-15T18:58:31.000Z",
            "title": "LLaVA-o1: Let Vision Language Models Reason Step-by-Step",
            "summary": "Large language models have demonstrated substantial advancements in reasoning\ncapabilities, particularly through inference-time scaling, as illustrated by\nmodels such as OpenAI's o1. However, current Vision-Language Models (VLMs)\noften struggle to perform systematic and structured reasoning, especially when\nhandling complex visual question-answering tasks. In this work, we introduce\nLLaVA-o1, a novel VLM designed to conduct autonomous multistage reasoning.\nUnlike chain-of-thought prompting, LLaVA-o1 independently engages in sequential\nstages of summarization, visual interpretation, logical reasoning, and\nconclusion generation. This structured approach enables LLaVA-o1 to achieve\nmarked improvements in precision on reasoning-intensive tasks. To accomplish\nthis, we compile the LLaVA-o1-100k dataset, integrating samples from various\nvisual question answering sources and providing structured reasoning\nannotations. Besides, we propose an inference-time stage-level beam search\nmethod, which enables effective inference-time scaling. Remarkably, with only\n100k training samples and a simple yet effective inference time scaling method,\nLLaVA-o1 not only outperforms its base model by 8.9% on a wide range of\nmultimodal reasoning benchmarks, but also surpasses the performance of larger\nand even closed-source models, such as Gemini-1.5-pro, GPT-4o-mini, and\nLlama-3.2-90B-Vision-Instruct.",
            "upvotes": 53,
            "discussionId": "673aa0826c12c4b98bda1a43"
        },
        "publishedAt": "2024-11-18T00:35:01.195Z",
        "title": "LLaVA-o1: Let Vision Language Models Reason Step-by-Step",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.10440.png",
        "numComments": 4,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e992cdccae1fe5c6222f84/IvksSUf2DENfUwCZSmNPd.jpeg",
            "fullname": "Guowei Xu",
            "name": "Xkev",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        }
    },
    {
        "paper": {
            "id": "2411.06558",
            "authors": [
                {
                    "_id": "6738f5e35604c3c005194615",
                    "user": {
                        "_id": "66449e619ff401732687f013",
                        "avatarUrl": "/avatars/251897d1324a70a9bf761513871c5841.svg",
                        "isPro": false,
                        "fullname": "chen",
                        "user": "zhen-nan",
                        "type": "user"
                    },
                    "name": "Zhennan Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-18T16:17:22.954Z",
                    "hidden": false
                },
                {
                    "_id": "6738f5e35604c3c005194616",
                    "user": {
                        "_id": "6669a6e593194f6b71bfa4ef",
                        "avatarUrl": "/avatars/373c5de91c25f2cb5f2ff8a4e27df3d5.svg",
                        "isPro": false,
                        "fullname": "YajieLi",
                        "user": "YajieLife",
                        "type": "user"
                    },
                    "name": "Yajie Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-16T19:45:14.424Z",
                    "hidden": false
                },
                {
                    "_id": "6738f5e35604c3c005194617",
                    "user": {
                        "_id": "637745113a63a2983ffbde13",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669187672174-637745113a63a2983ffbde13.jpeg",
                        "isPro": false,
                        "fullname": "Haofan Wang",
                        "user": "wanghaofan",
                        "type": "user"
                    },
                    "name": "Haofan Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-16T19:45:21.938Z",
                    "hidden": false
                },
                {
                    "_id": "6738f5e35604c3c005194618",
                    "user": {
                        "_id": "66d963e52e82d53d3b81031b",
                        "avatarUrl": "/avatars/302dbffc033ff47813a2435a2cec02f1.svg",
                        "isPro": false,
                        "fullname": "Zhibo Chen",
                        "user": "winhelp",
                        "type": "user"
                    },
                    "name": "Zhibo Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-16T19:46:09.076Z",
                    "hidden": false
                },
                {
                    "_id": "6738f5e35604c3c005194619",
                    "user": {
                        "_id": "6371dda2ffc0489ed7d712c2",
                        "avatarUrl": "/avatars/dcb80cae00535c22af99baf86cb70d93.svg",
                        "isPro": false,
                        "fullname": "zkjiang",
                        "user": "justin-zk",
                        "type": "user"
                    },
                    "name": "Zhengkai Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-18T16:17:32.848Z",
                    "hidden": false
                },
                {
                    "_id": "6738f5e35604c3c00519461a",
                    "user": {
                        "_id": "6666095cd22ee88501ed2717",
                        "avatarUrl": "/avatars/51d691e5351a277dcf0eb500ba108d0e.svg",
                        "isPro": false,
                        "fullname": "Jun Li",
                        "user": "junli14",
                        "type": "user"
                    },
                    "name": "Jun Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-16T19:46:32.226Z",
                    "hidden": false
                },
                {
                    "_id": "6738f5e35604c3c00519461b",
                    "name": "Qian Wang",
                    "hidden": false
                },
                {
                    "_id": "6738f5e35604c3c00519461c",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "6738f5e35604c3c00519461d",
                    "user": {
                        "_id": "65734004769f3ee9bde1af10",
                        "avatarUrl": "/avatars/d6310ed861972fd691687d8f47413f33.svg",
                        "isPro": false,
                        "fullname": "Ying Tai",
                        "user": "yingtai",
                        "type": "user"
                    },
                    "name": "Ying Tai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-16T19:47:58.217Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-10T18:45:41.000Z",
            "title": "Region-Aware Text-to-Image Generation via Hard Binding and Soft\n  Refinement",
            "summary": "In this paper, we present RAG, a Regional-Aware text-to-image Generation\nmethod conditioned on regional descriptions for precise layout composition.\nRegional prompting, or compositional generation, which enables fine-grained\nspatial control, has gained increasing attention for its practicality in\nreal-world applications. However, previous methods either introduce additional\ntrainable modules, thus only applicable to specific models, or manipulate on\nscore maps within cross-attention layers using attention masks, resulting in\nlimited control strength when the number of regions increases. To handle these\nlimitations, we decouple the multi-region generation into two sub-tasks, the\nconstruction of individual region (Regional Hard Binding) that ensures the\nregional prompt is properly executed, and the overall detail refinement\n(Regional Soft Refinement) over regions that dismiss the visual boundaries and\nenhance adjacent interactions. Furthermore, RAG novelly makes repainting\nfeasible, where users can modify specific unsatisfied regions in the last\ngeneration while keeping all other regions unchanged, without relying on\nadditional inpainting models. Our approach is tuning-free and applicable to\nother frameworks as an enhancement to the prompt following property.\nQuantitative and qualitative experiments demonstrate that RAG achieves superior\nperformance over attribute binding and object relationship than previous\ntuning-free methods.",
            "upvotes": 21,
            "discussionId": "6738f5e45604c3c00519465d"
        },
        "publishedAt": "2024-11-18T13:45:41.593Z",
        "title": "Region-Aware Text-to-Image Generation via Hard Binding and Soft Refinement",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.06558.png",
        "numComments": 3,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5161
        }
    },
    {
        "paper": {
            "id": "2411.08033",
            "authors": [
                {
                    "_id": "673aa0f7c36fe6d2f0a1a9a9",
                    "user": {
                        "_id": "64b55f62e6b0c53f97e64034",
                        "avatarUrl": "/avatars/d5fc53de37d4fb0b5095a5fb21b43503.svg",
                        "isPro": true,
                        "fullname": "LAN YUSHI",
                        "user": "yslan",
                        "type": "user"
                    },
                    "name": "Yushi Lan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-18T07:47:19.142Z",
                    "hidden": false
                },
                {
                    "_id": "673aa0f7c36fe6d2f0a1a9aa",
                    "user": {
                        "_id": "62e57662ae9d3f10acbb1b1b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e57662ae9d3f10acbb1b1b/lg58jdbNyv6LGH2LFnZDF.png",
                        "isPro": false,
                        "fullname": "Shangchen Zhou",
                        "user": "sczhou",
                        "type": "user"
                    },
                    "name": "Shangchen Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-18T07:49:40.488Z",
                    "hidden": false
                },
                {
                    "_id": "673aa0f7c36fe6d2f0a1a9ab",
                    "user": {
                        "_id": "63f2ec797ddf724fbcc75aee",
                        "avatarUrl": "/avatars/e93432ad11da703d46fe5e594d69f8c0.svg",
                        "isPro": false,
                        "fullname": "Zhaoyang Lyu",
                        "user": "ZhaoyangLyu",
                        "type": "user"
                    },
                    "name": "Zhaoyang Lyu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-18T07:49:45.706Z",
                    "hidden": false
                },
                {
                    "_id": "673aa0f7c36fe6d2f0a1a9ac",
                    "user": {
                        "_id": "623c530013a63ea865f96c8e",
                        "avatarUrl": "/avatars/164455a1a94f92b71733fc778c21bd89.svg",
                        "isPro": false,
                        "fullname": "Fangzhou Hong",
                        "user": "hongfz16",
                        "type": "user"
                    },
                    "name": "Fangzhou Hong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-18T07:49:50.863Z",
                    "hidden": false
                },
                {
                    "_id": "673aa0f7c36fe6d2f0a1a9ad",
                    "name": "Shuai Yang",
                    "hidden": false
                },
                {
                    "_id": "673aa0f7c36fe6d2f0a1a9ae",
                    "user": {
                        "_id": "665e2bd0fbaa279db369b488",
                        "avatarUrl": "/avatars/8cbc18d17d2813617976dbe709235799.svg",
                        "isPro": false,
                        "fullname": "Bo Dai",
                        "user": "doubling",
                        "type": "user"
                    },
                    "name": "Bo Dai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-18T07:50:09.291Z",
                    "hidden": false
                },
                {
                    "_id": "673aa0f7c36fe6d2f0a1a9af",
                    "user": {
                        "_id": "646758d6a9b4610868a138ff",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646758d6a9b4610868a138ff/fNBlK5g4g7vlPAR1bQimZ.png",
                        "isPro": false,
                        "fullname": "Xingang Pan",
                        "user": "Xingang-Pan",
                        "type": "user"
                    },
                    "name": "Xingang Pan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-18T07:50:35.048Z",
                    "hidden": false
                },
                {
                    "_id": "673aa0f7c36fe6d2f0a1a9b0",
                    "name": "Chen Change Loy",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-12T18:59:32.000Z",
            "title": "GaussianAnything: Interactive Point Cloud Latent Diffusion for 3D\n  Generation",
            "summary": "While 3D content generation has advanced significantly, existing methods\nstill face challenges with input formats, latent space design, and output\nrepresentations. This paper introduces a novel 3D generation framework that\naddresses these challenges, offering scalable, high-quality 3D generation with\nan interactive Point Cloud-structured Latent space. Our framework employs a\nVariational Autoencoder (VAE) with multi-view posed RGB-D(epth)-N(ormal)\nrenderings as input, using a unique latent space design that preserves 3D shape\ninformation, and incorporates a cascaded latent diffusion model for improved\nshape-texture disentanglement. The proposed method, GaussianAnything, supports\nmulti-modal conditional 3D generation, allowing for point cloud, caption, and\nsingle/multi-view image inputs. Notably, the newly proposed latent space\nnaturally enables geometry-texture disentanglement, thus allowing 3D-aware\nediting. Experimental results demonstrate the effectiveness of our approach on\nmultiple datasets, outperforming existing methods in both text- and\nimage-conditioned 3D generation.",
            "upvotes": 18,
            "discussionId": "673aa0f9c36fe6d2f0a1aa35"
        },
        "publishedAt": "2024-11-18T00:37:20.806Z",
        "title": "GaussianAnything: Interactive Point Cloud Latent Diffusion for 3D Generation",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64b55f62e6b0c53f97e64034/RDgMtIi1feDj5zzyJgN6H.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/64b55f62e6b0c53f97e64034/3QCSx2JFSVwwCIpKtAybM.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.08033.png",
        "numComments": 5,
        "submittedBy": {
            "avatarUrl": "/avatars/d5fc53de37d4fb0b5095a5fb21b43503.svg",
            "fullname": "LAN YUSHI",
            "name": "yslan",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        }
    },
    {
        "paper": {
            "id": "2411.10323",
            "authors": [
                {
                    "_id": "673acfbc4442393f72d541f0",
                    "user": {
                        "_id": "634e4120038b5879133552f5",
                        "avatarUrl": "/avatars/34ec861b4bbf1aecf927a7d6e726c7a4.svg",
                        "isPro": true,
                        "fullname": "Siyuan",
                        "user": "SiyuanH",
                        "type": "user"
                    },
                    "name": "Siyuan Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-18T07:51:40.447Z",
                    "hidden": true
                },
                {
                    "_id": "673acfbc4442393f72d541f1",
                    "user": {
                        "_id": "634e2217c1ce28f1de921708",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634e2217c1ce28f1de921708/XTMB6alYUM0KAUptM98kP.jpeg",
                        "isPro": false,
                        "fullname": "Yang",
                        "user": "yyyang",
                        "type": "user"
                    },
                    "name": "Mingyu Ouyang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-18T15:19:06.438Z",
                    "hidden": false
                },
                {
                    "_id": "673acfbc4442393f72d541f2",
                    "user": {
                        "_id": "661abfba70ec567e154fbb00",
                        "avatarUrl": "/avatars/46c004650ba15525ccf02d40c9314085.svg",
                        "isPro": false,
                        "fullname": "Difei Gao",
                        "user": "QuStar",
                        "type": "user"
                    },
                    "name": "Difei Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-18T07:51:04.868Z",
                    "hidden": false
                },
                {
                    "_id": "673acfbc4442393f72d541f3",
                    "user": {
                        "_id": "661ab3da2b14565c7acccf5c",
                        "avatarUrl": "/avatars/fa4fc03664803e02aede4d4c3d50b393.svg",
                        "isPro": false,
                        "fullname": "Mike Zheng Shou",
                        "user": "AnalMom",
                        "type": "user"
                    },
                    "name": "Mike Zheng Shou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-18T07:51:10.297Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-15T16:23:52.000Z",
            "title": "The Dawn of GUI Agent: A Preliminary Case Study with Claude 3.5 Computer\n  Use",
            "summary": "The recently released model, Claude 3.5 Computer Use, stands out as the first\nfrontier AI model to offer computer use in public beta as a graphical user\ninterface (GUI) agent. As an early beta, its capability in the real-world\ncomplex environment remains unknown. In this case study to explore Claude 3.5\nComputer Use, we curate and organize a collection of carefully designed tasks\nspanning a variety of domains and software. Observations from these cases\ndemonstrate Claude 3.5 Computer Use's unprecedented ability in end-to-end\nlanguage to desktop actions. Along with this study, we provide an\nout-of-the-box agent framework for deploying API-based GUI automation models\nwith easy implementation. Our case studies aim to showcase a groundwork of\ncapabilities and limitations of Claude 3.5 Computer Use with detailed analyses\nand bring to the fore questions about planning, action, and critic, which must\nbe considered for future improvement. We hope this preliminary exploration will\ninspire future research into the GUI agent community. All the test cases in the\npaper can be tried through the project:\nhttps://github.com/showlab/computer_use_ootb.",
            "upvotes": 12,
            "discussionId": "673acfbf4442393f72d54317"
        },
        "publishedAt": "2024-11-18T03:55:35.539Z",
        "title": "The Dawn of GUI Agent: A Preliminary Case Study with Claude 3.5 Computer Use",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.10323.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5161
        }
    },
    {
        "paper": {
            "id": "2411.10332",
            "authors": [
                {
                    "_id": "673ad833abddf84949fd2952",
                    "user": {
                        "_id": "66f6bc97980d52c75c300511",
                        "avatarUrl": "/avatars/f7c23c4b09701580b533212ec9b6e306.svg",
                        "isPro": false,
                        "fullname": "Vitoooo",
                        "user": "Liang0223",
                        "type": "user"
                    },
                    "name": "Yongliang Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-18T07:47:14.506Z",
                    "hidden": false
                },
                {
                    "_id": "673ad833abddf84949fd2953",
                    "user": {
                        "_id": "652845524def261e0febbf5b",
                        "avatarUrl": "/avatars/92743e595cbce4e37a8fe7400b5051ba.svg",
                        "isPro": false,
                        "fullname": "Xinting Hu",
                        "user": "kaleidudu",
                        "type": "user"
                    },
                    "name": "Xinting Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-18T07:51:51.989Z",
                    "hidden": false
                },
                {
                    "_id": "673ad833abddf84949fd2954",
                    "user": {
                        "_id": "655540892da2e4f12e6c743c",
                        "avatarUrl": "/avatars/790a80f7f9c21e3bf09addb323a6467a.svg",
                        "isPro": false,
                        "fullname": "Yuyang Sun",
                        "user": "tms28k",
                        "type": "user"
                    },
                    "name": "Yuyang Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-18T07:51:58.153Z",
                    "hidden": false
                },
                {
                    "_id": "673ad833abddf84949fd2955",
                    "user": {
                        "_id": "667ea7f396704b4d8f95ba58",
                        "avatarUrl": "/avatars/a3ec12d590e99cad07d3ecb618e5aacf.svg",
                        "isPro": false,
                        "fullname": "Zhouyi Zhou",
                        "user": "zhouzhouyi",
                        "type": "user"
                    },
                    "name": "Yizhou Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-18T07:52:13.463Z",
                    "hidden": false
                },
                {
                    "_id": "673ad833abddf84949fd2956",
                    "user": {
                        "_id": "6434e89e7b82474801105f06",
                        "avatarUrl": "/avatars/25d23d564d35e2ecb5ed6b1bbd0aa489.svg",
                        "isPro": false,
                        "fullname": "Vito Zhu",
                        "user": "Vito328",
                        "type": "user"
                    },
                    "name": "Wenbo Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-18T07:47:04.587Z",
                    "hidden": false
                },
                {
                    "_id": "673ad833abddf84949fd2957",
                    "name": "Fengyun Rao",
                    "hidden": false
                },
                {
                    "_id": "673ad833abddf84949fd2958",
                    "name": "Bernt Schiele",
                    "hidden": false
                },
                {
                    "_id": "673ad833abddf84949fd2959",
                    "name": "Xu Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-15T16:32:34.000Z",
            "title": "Number it: Temporal Grounding Videos like Flipping Manga",
            "summary": "Video Large Language Models (Vid-LLMs) have made remarkable advancements in\ncomprehending video content for QA dialogue. However, they struggle to extend\nthis visual understanding to tasks requiring precise temporal localization,\nknown as Video Temporal Grounding (VTG). To address this gap, we introduce\nNumber-Prompt (NumPro), a novel method that empowers Vid-LLMs to bridge visual\ncomprehension with temporal grounding by adding unique numerical identifiers to\neach video frame. Treating a video as a sequence of numbered frame images,\nNumPro transforms VTG into an intuitive process: flipping through manga panels\nin sequence. This allows Vid-LLMs to \"read\" event timelines, accurately linking\nvisual content with corresponding temporal information. Our experiments\ndemonstrate that NumPro significantly boosts VTG performance of top-tier\nVid-LLMs without additional computational cost. Furthermore, fine-tuning on a\nNumPro-enhanced dataset defines a new state-of-the-art for VTG, surpassing\nprevious top-performing methods by up to 6.9\\% in mIoU for moment retrieval and\n8.5\\% in mAP for highlight detection. The code will be available at\nhttps://github.com/yongliang-wu/NumPro.",
            "upvotes": 8,
            "discussionId": "673ad838abddf84949fd2b0a"
        },
        "publishedAt": "2024-11-18T04:33:53.359Z",
        "title": "Number it: Temporal Grounding Videos like Flipping Manga",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.10332.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/f7c23c4b09701580b533212ec9b6e306.svg",
            "fullname": "Vitoooo",
            "name": "Liang0223",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2411.10083",
            "authors": [
                {
                    "_id": "673b59cad55948359af213ef",
                    "name": "Wang Qun",
                    "hidden": false
                },
                {
                    "_id": "673b59cad55948359af213f0",
                    "name": "Liu Yang",
                    "hidden": false
                },
                {
                    "_id": "673b59cad55948359af213f1",
                    "user": {
                        "_id": "66d53492ad293ffc4b17d99b",
                        "avatarUrl": "/avatars/1f89566c6652637731fbc7c1d69314cb.svg",
                        "isPro": false,
                        "fullname": "qq",
                        "user": "linqingquan",
                        "type": "user"
                    },
                    "name": "Lin Qingquan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-18T16:19:06.486Z",
                    "hidden": false
                },
                {
                    "_id": "673b59cad55948359af213f2",
                    "name": "Jiang Ling",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-15T10:01:52.000Z",
            "title": "Xmodel-1.5: An 1B-scale Multilingual LLM",
            "summary": "We introduce Xmodel-1.5, a novel 1-billion-parameter multilingual large model\npretrained on approximately 2 trillion tokens. The model demonstrates strong\nperformance across several languages, with particularly notable results in\nThai, Arabic, and French, alongside its effectiveness in Chinese and English.\nIn addition, we contribute to the research community by releasing a Thai\nevaluation dataset, which includes hundreds of questions annotated by students\nfrom Chulalongkorn University's School of Integrated Innovation. While the\nresults are promising, we acknowledge that there is still room for improvement.\nWe hope this work advances ongoing efforts in multilingual AI research and\npromotes better cross-linguistic understanding in various natural language\nprocessing tasks. Our models and code are publicly available on GitHub at\nhttps://github.com/XiaoduoAILab/XmodelLM.",
            "upvotes": 3,
            "discussionId": "673b59cbd55948359af21455"
        },
        "publishedAt": "2024-11-18T13:44:48.534Z",
        "title": "Xmodel-1.5: An 1B-scale Multilingual LLM",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.10083.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5161
        }
    }
]