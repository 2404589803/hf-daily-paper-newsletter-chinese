[
    {
        "paper": {
            "id": "2407.10671",
            "authors": [
                {
                    "_id": "6695e0de321386ed51decbda",
                    "name": "An Yang",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbdb",
                    "name": "Baosong Yang",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbdc",
                    "name": "Binyuan Hui",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbdd",
                    "name": "Bo Zheng",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbde",
                    "name": "Bowen Yu",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbdf",
                    "name": "Chang Zhou",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbe0",
                    "name": "Chengpeng Li",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbe1",
                    "name": "Chengyuan Li",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbe2",
                    "name": "Dayiheng Liu",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbe3",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbe4",
                    "name": "Guanting Dong",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbe5",
                    "name": "Haoran Wei",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbe6",
                    "name": "Huan Lin",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbe7",
                    "name": "Jialong Tang",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbe8",
                    "name": "Jialin Wang",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbe9",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbea",
                    "name": "Jianhong Tu",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbeb",
                    "name": "Jianwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbec",
                    "name": "Jianxin Ma",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbed",
                    "name": "Jin Xu",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbee",
                    "name": "Jingren Zhou",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbef",
                    "name": "Jinze Bai",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbf0",
                    "name": "Jinzheng He",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbf1",
                    "name": "Junyang Lin",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbf2",
                    "name": "Kai Dang",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbf3",
                    "name": "Keming Lu",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbf4",
                    "name": "Keqin Chen",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbf5",
                    "name": "Kexin Yang",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbf6",
                    "name": "Mei Li",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbf7",
                    "name": "Mingfeng Xue",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbf8",
                    "name": "Na Ni",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbf9",
                    "name": "Pei Zhang",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbfa",
                    "name": "Peng Wang",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbfb",
                    "name": "Ru Peng",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbfc",
                    "name": "Rui Men",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbfd",
                    "name": "Ruize Gao",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbfe",
                    "name": "Runji Lin",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decbff",
                    "name": "Shijie Wang",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decc00",
                    "name": "Shuai Bai",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decc01",
                    "name": "Sinan Tan",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decc02",
                    "name": "Tianhang Zhu",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decc03",
                    "name": "Tianhao Li",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decc04",
                    "name": "Tianyu Liu",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decc05",
                    "name": "Wenbin Ge",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decc06",
                    "name": "Xiaodong Deng",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decc07",
                    "name": "Xiaohuan Zhou",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decc08",
                    "name": "Xingzhang Ren",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decc09",
                    "name": "Xinyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decc0a",
                    "name": "Xipin Wei",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decc0b",
                    "name": "Xuancheng Ren",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decc0c",
                    "name": "Yang Fan",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decc0d",
                    "name": "Yang Yao",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decc0e",
                    "name": "Yichang Zhang",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decc0f",
                    "name": "Yu Wan",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decc10",
                    "name": "Yunfei Chu",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decc11",
                    "name": "Zeyu Cui",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decc12",
                    "name": "Zhenru Zhang",
                    "hidden": false
                },
                {
                    "_id": "6695e0de321386ed51decc13",
                    "name": "Zhihao Fan",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-15T12:35:42.000Z",
            "title": "Qwen2 Technical Report",
            "summary": "This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face1 and ModelScope2, and the\nsupplementary materials including example code on GitHub3. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors.",
            "upvotes": 80
        },
        "publishedAt": "2024-07-16T01:24:43.373Z",
        "title": "Qwen2 Technical Report",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.10671.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61e4c4ca1ab24785ac11ba69/1Q1zhhyGSJ9RJG9MzwxVv.jpeg",
            "fullname": "Binyuan Hui",
            "name": "huybery",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.10058",
            "authors": [
                {
                    "_id": "66960be0659a52fa2757ae13",
                    "name": "Zhenhua Liu",
                    "hidden": false
                },
                {
                    "_id": "66960be0659a52fa2757ae14",
                    "name": "Tong Zhu",
                    "hidden": false
                },
                {
                    "_id": "66960be0659a52fa2757ae15",
                    "name": "Chuanyuan Tan",
                    "hidden": false
                },
                {
                    "_id": "66960be0659a52fa2757ae16",
                    "name": "Wenliang Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-14T03:05:53.000Z",
            "title": "Learning to Refuse: Towards Mitigating Privacy Risks in LLMs",
            "summary": "Large language models (LLMs) exhibit remarkable capabilities in understanding\nand generating natural language. However, these models can inadvertently\nmemorize private information, posing significant privacy risks. This study\naddresses the challenge of enabling LLMs to protect specific individuals'\nprivate data without the need for complete retraining. We propose \\return, a\nReal-world pErsonal daTa UnleaRNing dataset, comprising 2,492 individuals from\nWikipedia with associated QA pairs, to evaluate machine unlearning (MU) methods\nfor protecting personal data in a realistic scenario. Additionally, we\nintroduce the Name-Aware Unlearning Framework (NAUF) for Privacy Protection,\nwhich enables the model to learn which individuals' information should be\nprotected without affecting its ability to answer questions related to other\nunrelated individuals. Our extensive experiments demonstrate that NAUF achieves\na state-of-the-art average unlearning score, surpassing the best baseline\nmethod by 5.65 points, effectively protecting target individuals' personal data\nwhile maintaining the model's general capabilities.",
            "upvotes": 15
        },
        "publishedAt": "2024-07-16T04:32:22.089Z",
        "title": "Learning to Refuse: Towards Mitigating Privacy Risks in LLMs",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.10058.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629454301ae2138079f7ff31/rVtbF-j06gDiYzomTeVTc.jpeg",
            "fullname": "Tong Zhu",
            "name": "Spico",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.10457",
            "authors": [
                {
                    "_id": "6695cf864c450902c8cf3e5a",
                    "name": "Yifan Song",
                    "hidden": false
                },
                {
                    "_id": "6695cf864c450902c8cf3e5b",
                    "name": "Guoyin Wang",
                    "hidden": false
                },
                {
                    "_id": "6695cf864c450902c8cf3e5c",
                    "name": "Sujian Li",
                    "hidden": false
                },
                {
                    "_id": "6695cf864c450902c8cf3e5d",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/607f666a4ad99100d63ce35c/QxhxnvfeV6efkxwUFHwjI.png",
                        "isPro": false,
                        "fullname": "Bill Yuchen Lin",
                        "user": "yuchenlin",
                        "type": "user"
                    },
                    "name": "Bill Yuchen Lin",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-07-16T01:40:23.582Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-15T06:12:17.000Z",
            "title": "The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore\n  Non-Determinism",
            "summary": "Current evaluations of large language models (LLMs) often overlook\nnon-determinism, typically focusing on a single output per example. This limits\nour understanding of LLM performance variability in real-world applications.\nOur study addresses this issue by exploring key questions about the performance\ndifferences between greedy decoding and sampling, identifying benchmarks'\nconsistency regarding non-determinism, and examining unique model behaviors.\nThrough extensive experiments, we observe that greedy decoding generally\noutperforms sampling methods for most evaluated tasks. We also observe\nconsistent performance across different LLM sizes and alignment methods, noting\nthat alignment can reduce sampling variance. Moreover, our best-of-N sampling\napproach demonstrates that smaller LLMs can match or surpass larger models such\nas GPT-4-Turbo, highlighting the untapped potential of smaller LLMs. This\nresearch shows the importance of considering non-determinism in LLM evaluations\nand provides insights for future LLM development and evaluation.",
            "upvotes": 11
        },
        "publishedAt": "2024-07-16T00:10:32.923Z",
        "title": "The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.10457.png",
        "numComments": 3,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/607f666a4ad99100d63ce35c/QxhxnvfeV6efkxwUFHwjI.png",
            "fullname": "Bill Yuchen Lin",
            "name": "yuchenlin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.10969",
            "authors": [
                {
                    "_id": "669615f637b573b6007c5224",
                    "name": "Hongyu Wang",
                    "hidden": false
                },
                {
                    "_id": "669615f637b573b6007c5225",
                    "name": "Shuming Ma",
                    "hidden": false
                },
                {
                    "_id": "669615f637b573b6007c5226",
                    "name": "Ruiping Wang",
                    "hidden": false
                },
                {
                    "_id": "669615f637b573b6007c5227",
                    "name": "Furu Wei",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-15T17:59:29.000Z",
            "title": "Q-Sparse: All Large Language Models can be Fully Sparsely-Activated",
            "summary": "We introduce, Q-Sparse, a simple yet effective approach to training\nsparsely-activated large language models (LLMs). Q-Sparse enables full sparsity\nof activations in LLMs which can bring significant efficiency gains in\ninference. This is achieved by applying top-K sparsification to the activations\nand the straight-through-estimator to the training. The key results from this\nwork are, (1) Q-Sparse can achieve results comparable to those of baseline LLMs\nwhile being much more efficient at inference time; (2) We present an\ninference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse is\neffective in different settings, including training-from-scratch,\ncontinue-training of off-the-shelf LLMs, and finetuning; (4) Q-Sparse works for\nboth full-precision and 1-bit LLMs (e.g., BitNet b1.58). Particularly, the\nsynergy of BitNet b1.58 and Q-Sparse (can be equipped with MoE) provides the\ncornerstone and a clear path to revolutionize the efficiency, including cost\nand energy consumption, of future LLMs.",
            "upvotes": 8
        },
        "publishedAt": "2024-07-16T05:11:40.286Z",
        "title": "Q-Sparse: All Large Language Models can be Fully Sparsely-Activated",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.10969.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/3965175b320d753d9a5ccb0c7d9298a4.svg",
            "fullname": "Shuming Ma",
            "name": "shumingma",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.10943",
            "authors": [
                {
                    "_id": "669606b85e5e72a43406f888",
                    "name": "Hanqing Wang",
                    "hidden": false
                },
                {
                    "_id": "669606b85e5e72a43406f889",
                    "name": "Jiahe Chen",
                    "hidden": false
                },
                {
                    "_id": "669606b85e5e72a43406f88a",
                    "name": "Wensi Huang",
                    "hidden": false
                },
                {
                    "_id": "669606b85e5e72a43406f88b",
                    "name": "Qingwei Ben",
                    "hidden": false
                },
                {
                    "_id": "669606b85e5e72a43406f88c",
                    "name": "Tai Wang",
                    "hidden": false
                },
                {
                    "_id": "669606b85e5e72a43406f88d",
                    "name": "Boyu Mi",
                    "hidden": false
                },
                {
                    "_id": "669606b85e5e72a43406f88e",
                    "name": "Tao Huang",
                    "hidden": false
                },
                {
                    "_id": "669606b85e5e72a43406f88f",
                    "name": "Siheng Zhao",
                    "hidden": false
                },
                {
                    "_id": "669606b85e5e72a43406f890",
                    "name": "Yilun Chen",
                    "hidden": false
                },
                {
                    "_id": "669606b85e5e72a43406f891",
                    "name": "Sizhe Yang",
                    "hidden": false
                },
                {
                    "_id": "669606b85e5e72a43406f892",
                    "name": "Peizhou Cao",
                    "hidden": false
                },
                {
                    "_id": "669606b85e5e72a43406f893",
                    "name": "Wenye Yu",
                    "hidden": false
                },
                {
                    "_id": "669606b85e5e72a43406f894",
                    "name": "Zichao Ye",
                    "hidden": false
                },
                {
                    "_id": "669606b85e5e72a43406f895",
                    "name": "Jialun Li",
                    "hidden": false
                },
                {
                    "_id": "669606b85e5e72a43406f896",
                    "name": "Junfeng Long",
                    "hidden": false
                },
                {
                    "_id": "669606b85e5e72a43406f897",
                    "name": "Zirui Wang",
                    "hidden": false
                },
                {
                    "_id": "669606b85e5e72a43406f898",
                    "name": "Huiling Wang",
                    "hidden": false
                },
                {
                    "_id": "669606b85e5e72a43406f899",
                    "name": "Ying Zhao",
                    "hidden": false
                },
                {
                    "_id": "669606b85e5e72a43406f89a",
                    "name": "Zhongying Tu",
                    "hidden": false
                },
                {
                    "_id": "669606b85e5e72a43406f89b",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "669606b85e5e72a43406f89c",
                    "name": "Dahua Lin",
                    "hidden": false
                },
                {
                    "_id": "669606b85e5e72a43406f89d",
                    "name": "Jiangmiao Pang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-15T17:40:46.000Z",
            "title": "GRUtopia: Dream General Robots in a City at Scale",
            "summary": "Recent works have been exploring the scaling laws in the field of Embodied\nAI. Given the prohibitive costs of collecting real-world data, we believe the\nSimulation-to-Real (Sim2Real) paradigm is a crucial step for scaling the\nlearning of embodied models. This paper introduces project GRUtopia, the first\nsimulated interactive 3D society designed for various robots. It features\nseveral advancements: (a) The scene dataset, GRScenes, includes 100k\ninteractive, finely annotated scenes, which can be freely combined into\ncity-scale environments. In contrast to previous works mainly focusing on home,\nGRScenes covers 89 diverse scene categories, bridging the gap of\nservice-oriented environments where general robots would be initially deployed.\n(b) GRResidents, a Large Language Model (LLM) driven Non-Player Character (NPC)\nsystem that is responsible for social interaction, task generation, and task\nassignment, thus simulating social scenarios for embodied AI applications. (c)\nThe benchmark, GRBench, supports various robots but focuses on legged robots as\nprimary agents and poses moderately challenging tasks involving Object\nLoco-Navigation, Social Loco-Navigation, and Loco-Manipulation. We hope that\nthis work can alleviate the scarcity of high-quality data in this field and\nprovide a more comprehensive assessment of Embodied AI research. The project is\navailable at https://github.com/OpenRobotLab/GRUtopia.",
            "upvotes": 8
        },
        "publishedAt": "2024-07-16T04:09:28.024Z",
        "title": "GRUtopia: Dream General Robots in a City at Scale",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64e6d9d229a548f66aff6e5b/DTTn485gq7tYZXMARte9D.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/64e6d9d229a548f66aff6e5b/CeuVv0cvmKCtjsbzIi6Up.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.10943.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6d9d229a548f66aff6e5b/yQ9E2TyzM4CfSjMPigcey.jpeg",
            "fullname": "Tai Wang",
            "name": "taiwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.10973",
            "authors": [
                {
                    "_id": "6695ea8d461de4eea59b32a0",
                    "name": "Yongyuan Liang",
                    "hidden": false
                },
                {
                    "_id": "6695ea8d461de4eea59b32a1",
                    "name": "Tingqiang Xu",
                    "hidden": false
                },
                {
                    "_id": "6695ea8d461de4eea59b32a2",
                    "name": "Kaizhe Hu",
                    "hidden": false
                },
                {
                    "_id": "6695ea8d461de4eea59b32a3",
                    "name": "Guangqi Jiang",
                    "hidden": false
                },
                {
                    "_id": "6695ea8d461de4eea59b32a4",
                    "name": "Furong Huang",
                    "hidden": false
                },
                {
                    "_id": "6695ea8d461de4eea59b32a5",
                    "name": "Huazhe Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-15T17:59:57.000Z",
            "title": "Make-An-Agent: A Generalizable Policy Network Generator with\n  Behavior-Prompted Diffusion",
            "summary": "Can we generate a control policy for an agent using just one demonstration of\ndesired behaviors as a prompt, as effortlessly as creating an image from a\ntextual description? In this paper, we present Make-An-Agent, a novel policy\nparameter generator that leverages the power of conditional diffusion models\nfor behavior-to-policy generation. Guided by behavior embeddings that encode\ntrajectory information, our policy generator synthesizes latent parameter\nrepresentations, which can then be decoded into policy networks. Trained on\npolicy network checkpoints and their corresponding trajectories, our generation\nmodel demonstrates remarkable versatility and scalability on multiple tasks and\nhas a strong generalization ability on unseen tasks to output well-performed\npolicies with only few-shot demonstrations as inputs. We showcase its efficacy\nand efficiency on various domains and tasks, including varying objectives,\nbehaviors, and even across different robot manipulators. Beyond simulation, we\ndirectly deploy policies generated by Make-An-Agent onto real-world robots on\nlocomotion tasks.",
            "upvotes": 4
        },
        "publishedAt": "2024-07-16T02:07:46.209Z",
        "title": "Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.10973.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/JFH3ZTPvlaVSg4RJJBb6L.jpeg",
            "fullname": "Yongyuan Liang",
            "name": "cheryyunl",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.10387",
            "authors": [
                {
                    "_id": "6695ec67659a52fa274d8939",
                    "name": "Santiago Pascual",
                    "hidden": false
                },
                {
                    "_id": "6695ec67659a52fa274d893a",
                    "name": "Chunghsin Yeh",
                    "hidden": false
                },
                {
                    "_id": "6695ec67659a52fa274d893b",
                    "name": "Ioannis Tsiamas",
                    "hidden": false
                },
                {
                    "_id": "6695ec67659a52fa274d893c",
                    "name": "Joan Serrà",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-15T01:49:59.000Z",
            "title": "Masked Generative Video-to-Audio Transformers with Enhanced\n  Synchronicity",
            "summary": "Video-to-audio (V2A) generation leverages visual-only video features to\nrender plausible sounds that match the scene. Importantly, the generated sound\nonsets should match the visual actions that are aligned with them, otherwise\nunnatural synchronization artifacts arise. Recent works have explored the\nprogression of conditioning sound generators on still images and then video\nfeatures, focusing on quality and semantic matching while ignoring\nsynchronization, or by sacrificing some amount of quality to focus on improving\nsynchronization only. In this work, we propose a V2A generative model, named\nMaskVAT, that interconnects a full-band high-quality general audio codec with a\nsequence-to-sequence masked generative model. This combination allows modeling\nboth high audio quality, semantic matching, and temporal synchronicity at the\nsame time. Our results show that, by combining a high-quality codec with the\nproper pre-trained audio-visual features and a sequence-to-sequence parallel\nstructure, we are able to yield highly synchronized results on one hand, whilst\nbeing competitive with the state of the art of non-codec generative audio\nmodels. Sample videos and generated audios are available at\nhttps://maskvat.github.io .",
            "upvotes": 3
        },
        "publishedAt": "2024-07-16T02:13:40.563Z",
        "title": "Masked Generative Video-to-Audio Transformers with Enhanced Synchronicity",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.10387.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.07523",
            "authors": [
                {
                    "_id": "668f489bb93b425b7da8978b",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b4a717aa03b6520839e9b8/_rcsJh0ummwK-oAMAT4oY.jpeg",
                        "isPro": false,
                        "fullname": "Haiwen Diao",
                        "user": "Paranioar",
                        "type": "user"
                    },
                    "name": "Haiwen Diao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-07-11T08:25:52.287Z",
                    "hidden": false
                },
                {
                    "_id": "668f489bb93b425b7da8978c",
                    "name": "Bo Wan",
                    "hidden": false
                },
                {
                    "_id": "668f489bb93b425b7da8978d",
                    "name": "Xu Jia",
                    "hidden": false
                },
                {
                    "_id": "668f489bb93b425b7da8978e",
                    "name": "Yunzhi Zhuge",
                    "hidden": false
                },
                {
                    "_id": "668f489bb93b425b7da8978f",
                    "name": "Ying Zhang",
                    "hidden": false
                },
                {
                    "_id": "668f489bb93b425b7da89790",
                    "name": "Huchuan Lu",
                    "hidden": false
                },
                {
                    "_id": "668f489bb93b425b7da89791",
                    "name": "Long Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-10T10:22:35.000Z",
            "title": "SHERL: Synthesizing High Accuracy and Efficient Memory for\n  Resource-Limited Transfer Learning",
            "summary": "Parameter-efficient transfer learning (PETL) has emerged as a flourishing\nresearch field for adapting large pre-trained models to downstream tasks,\ngreatly reducing trainable parameters while grappling with memory challenges\nduring fine-tuning. To address it, memory-efficient series (METL) avoid\nbackpropagating gradients through the large backbone. However, they compromise\nby exclusively relying on frozen intermediate outputs and limiting the\nexhaustive exploration of prior knowledge from pre-trained models. Moreover,\nthe dependency and redundancy between cross-layer features are frequently\noverlooked, thereby submerging more discriminative representations and causing\nan inherent performance gap (vs. conventional PETL methods). Hence, we propose\nan innovative METL strategy called SHERL for resource-limited scenarios to\ndecouple the entire adaptation into two successive and complementary processes.\nIn the early route, intermediate outputs are consolidated via an\nanti-redundancy operation, enhancing their compatibility for subsequent\ninteractions; thereby in the late route, utilizing minimal late pre-trained\nlayers could alleviate the peak demand on memory overhead and regulate these\nfairly flexible features into more adaptive and powerful representations for\nnew domains. Extensive ablations on vision-and-language and language-only tasks\nshow that SHERL combines the strengths of both parameter and memory-efficient\ntechniques, performing on-par or better across diverse architectures with lower\nmemory during fine-tuning. Our code is publicly available at:\nhttps://github.com/Paranioar/SHERL.",
            "upvotes": 3
        },
        "publishedAt": "2024-07-16T01:39:22.750Z",
        "title": "SHERL: Synthesizing High Accuracy and Efficient Memory for Resource-Limited Transfer Learning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.07523.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b4a717aa03b6520839e9b8/_rcsJh0ummwK-oAMAT4oY.jpeg",
            "fullname": "Haiwen Diao",
            "name": "Paranioar",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.09533",
            "authors": [
                {
                    "_id": "6695e1627c6c4ffc86b928a3",
                    "name": "Manan Tomar",
                    "hidden": false
                },
                {
                    "_id": "6695e1627c6c4ffc86b928a4",
                    "name": "Philippe Hansen-Estruch",
                    "hidden": false
                },
                {
                    "_id": "6695e1627c6c4ffc86b928a5",
                    "name": "Philip Bachman",
                    "hidden": false
                },
                {
                    "_id": "6695e1627c6c4ffc86b928a6",
                    "name": "Alex Lamb",
                    "hidden": false
                },
                {
                    "_id": "6695e1627c6c4ffc86b928a7",
                    "name": "John Langford",
                    "hidden": false
                },
                {
                    "_id": "6695e1627c6c4ffc86b928a8",
                    "name": "Matthew E. Taylor",
                    "hidden": false
                },
                {
                    "_id": "6695e1627c6c4ffc86b928a9",
                    "name": "Sergey Levine",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-25T17:57:38.000Z",
            "title": "Video Occupancy Models",
            "summary": "We introduce a new family of video prediction models designed to support\ndownstream control tasks. We call these models Video Occupancy models (VOCs).\nVOCs operate in a compact latent space, thus avoiding the need to make\npredictions about individual pixels. Unlike prior latent-space world models,\nVOCs directly predict the discounted distribution of future states in a single\nstep, thus avoiding the need for multistep roll-outs. We show that both\nproperties are beneficial when building predictive models of video for use in\ndownstream control. Code is available at\nhttps://github.com/manantomar/video-occupancy-models{github.com/manantomar/video-occupancy-models}.",
            "upvotes": 3
        },
        "publishedAt": "2024-07-16T01:26:45.936Z",
        "title": "Video Occupancy Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.09533.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.10285",
            "authors": [
                {
                    "_id": "6695e7d1496ec7b160331ead",
                    "name": "Qinyu Yang",
                    "hidden": false
                },
                {
                    "_id": "6695e7d1496ec7b160331eae",
                    "name": "Haoxin Chen",
                    "hidden": false
                },
                {
                    "_id": "6695e7d1496ec7b160331eaf",
                    "name": "Yong Zhang",
                    "hidden": false
                },
                {
                    "_id": "6695e7d1496ec7b160331eb0",
                    "name": "Menghan Xia",
                    "hidden": false
                },
                {
                    "_id": "6695e7d1496ec7b160331eb1",
                    "name": "Xiaodong Cun",
                    "hidden": false
                },
                {
                    "_id": "6695e7d1496ec7b160331eb2",
                    "name": "Zhixun Su",
                    "hidden": false
                },
                {
                    "_id": "6695e7d1496ec7b160331eb3",
                    "name": "Ying Shan",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-14T17:59:56.000Z",
            "title": "Noise Calibration: Plug-and-play Content-Preserving Video Enhancement\n  using Pre-trained Video Diffusion Models",
            "summary": "In order to improve the quality of synthesized videos, currently, one\npredominant method involves retraining an expert diffusion model and then\nimplementing a noising-denoising process for refinement. Despite the\nsignificant training costs, maintaining consistency of content between the\noriginal and enhanced videos remains a major challenge. To tackle this\nchallenge, we propose a novel formulation that considers both visual quality\nand consistency of content. Consistency of content is ensured by a proposed\nloss function that maintains the structure of the input, while visual quality\nis improved by utilizing the denoising process of pretrained diffusion models.\nTo address the formulated optimization problem, we have developed a\nplug-and-play noise optimization strategy, referred to as Noise Calibration. By\nrefining the initial random noise through a few iterations, the content of\noriginal video can be largely preserved, and the enhancement effect\ndemonstrates a notable improvement. Extensive experiments have demonstrated the\neffectiveness of the proposed method.",
            "upvotes": 2
        },
        "publishedAt": "2024-07-16T01:54:18.179Z",
        "title": "Noise Calibration: Plug-and-play Content-Preserving Video Enhancement using Pre-trained Video Diffusion Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.10285.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.10827",
            "authors": [
                {
                    "_id": "6696183c2dcf962073b57c65",
                    "name": "Curt Tigges",
                    "hidden": false
                },
                {
                    "_id": "6696183c2dcf962073b57c66",
                    "name": "Michael Hanna",
                    "hidden": false
                },
                {
                    "_id": "6696183c2dcf962073b57c67",
                    "name": "Qinan Yu",
                    "hidden": false
                },
                {
                    "_id": "6696183c2dcf962073b57c68",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60347d3660e3dd96631c9093/B3fuZer5N04tZIAYrLnz4.jpeg",
                        "isPro": false,
                        "fullname": "Stella Biderman",
                        "user": "stellaathena",
                        "type": "user"
                    },
                    "name": "Stella Biderman",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-07-16T06:50:37.147Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-15T15:38:51.000Z",
            "title": "LLM Circuit Analyses Are Consistent Across Training and Scale",
            "summary": "Most currently deployed large language models (LLMs) undergo continuous\ntraining or additional finetuning. By contrast, most research into LLMs'\ninternal mechanisms focuses on models at one snapshot in time (the end of\npre-training), raising the question of whether their results generalize to\nreal-world settings. Existing studies of mechanisms over time focus on\nencoder-only or toy models, which differ significantly from most deployed\nmodels. In this study, we track how model mechanisms, operationalized as\ncircuits, emerge and evolve across 300 billion tokens of training in\ndecoder-only LLMs, in models ranging from 70 million to 2.8 billion parameters.\nWe find that task abilities and the functional components that support them\nemerge consistently at similar token counts across scale. Moreover, although\nsuch components may be implemented by different attention heads over time, the\noverarching algorithm that they implement remains. Surprisingly, both these\nalgorithms and the types of components involved therein can replicate across\nmodel scale. These results suggest that circuit analyses conducted on small\nmodels at the end of pre-training can provide insights that still apply after\nadditional pre-training and over model scale.",
            "upvotes": 1
        },
        "publishedAt": "2024-07-16T14:29:50.534Z",
        "title": "LLM Circuit Analyses Are Consistent Across Training and Scale",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/eM1SQ6wngwsXjsNVyny45.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.10827.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670231290373-5e7749883d77a72421292d07.jpeg",
            "fullname": "Gabriele Sarti",
            "name": "gsarti",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.10910",
            "authors": [
                {
                    "_id": "66960f292d25bd04e9943741",
                    "name": "Jae Myung Kim",
                    "hidden": false
                },
                {
                    "_id": "66960f292d25bd04e9943742",
                    "name": "Jessica Bader",
                    "hidden": false
                },
                {
                    "_id": "66960f292d25bd04e9943743",
                    "name": "Stephan Alaniz",
                    "hidden": false
                },
                {
                    "_id": "66960f292d25bd04e9943744",
                    "name": "Cordelia Schmid",
                    "hidden": false
                },
                {
                    "_id": "66960f292d25bd04e9943745",
                    "name": "Zeynep Akata",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-15T17:10:31.000Z",
            "title": "DataDream: Few-shot Guided Dataset Generation",
            "summary": "While text-to-image diffusion models have been shown to achieve\nstate-of-the-art results in image synthesis, they have yet to prove their\neffectiveness in downstream applications. Previous work has proposed to\ngenerate data for image classifier training given limited real data access.\nHowever, these methods struggle to generate in-distribution images or depict\nfine-grained features, thereby hindering the generalization of classification\nmodels trained on synthetic datasets. We propose DataDream, a framework for\nsynthesizing classification datasets that more faithfully represents the real\ndata distribution when guided by few-shot examples of the target classes.\nDataDream fine-tunes LoRA weights for the image generation model on the few\nreal images before generating the training data using the adapted model. We\nthen fine-tune LoRA weights for CLIP using the synthetic data to improve\ndownstream image classification over previous approaches on a large variety of\ndatasets. We demonstrate the efficacy of DataDream through extensive\nexperiments, surpassing state-of-the-art classification accuracy with few-shot\ndata across 7 out of 10 datasets, while being competitive on the other 3.\nAdditionally, we provide insights into the impact of various factors, such as\nthe number of real-shot and generated images as well as the fine-tuning compute\non model performance. The code is available at\nhttps://github.com/ExplainableML/DataDream.",
            "upvotes": 1
        },
        "publishedAt": "2024-07-16T04:42:22.220Z",
        "title": "DataDream: Few-shot Guided Dataset Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.10910.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1627505688463-60107b385ac3e86b3ea4fc34.jpeg",
            "fullname": "Daniel van Strien",
            "name": "davanstrien",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.10953",
            "authors": [
                {
                    "_id": "6695fdc00e08a6505b828cdd",
                    "name": "Chengguang Gan",
                    "hidden": false
                },
                {
                    "_id": "6695fdc00e08a6505b828cde",
                    "name": "Qingyu Yin",
                    "hidden": false
                },
                {
                    "_id": "6695fdc00e08a6505b828cdf",
                    "name": "Xinyang He",
                    "hidden": false
                },
                {
                    "_id": "6695fdc00e08a6505b828ce0",
                    "name": "Hanjun Wei",
                    "hidden": false
                },
                {
                    "_id": "6695fdc00e08a6505b828ce1",
                    "name": "Yunhao Liang",
                    "hidden": false
                },
                {
                    "_id": "6695fdc00e08a6505b828ce2",
                    "name": "Younghun Lim",
                    "hidden": false
                },
                {
                    "_id": "6695fdc00e08a6505b828ce3",
                    "name": "Shijian Wang",
                    "hidden": false
                },
                {
                    "_id": "6695fdc00e08a6505b828ce4",
                    "name": "Hexiang Huang",
                    "hidden": false
                },
                {
                    "_id": "6695fdc00e08a6505b828ce5",
                    "name": "Qinghao Zhang",
                    "hidden": false
                },
                {
                    "_id": "6695fdc00e08a6505b828ce6",
                    "name": "Shiwen Ni",
                    "hidden": false
                },
                {
                    "_id": "6695fdc00e08a6505b828ce7",
                    "name": "Tatsunori Mori",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-15T17:50:43.000Z",
            "title": "MMM: Multilingual Mutual Reinforcement Effect Mix Datasets & Test with\n  Open-domain Information Extraction Large Language Models",
            "summary": "The Mutual Reinforcement Effect (MRE) represents a promising avenue in\ninformation extraction and multitasking research. Nevertheless, its\napplicability has been constrained due to the exclusive availability of MRE mix\ndatasets in Japanese, thereby limiting comprehensive exploration by the global\nresearch community. To address this limitation, we introduce a Multilingual MRE\nmix dataset (MMM) that encompasses 21 sub-datasets in English, Japanese, and\nChinese. In this paper, we also propose a method for dataset translation\nassisted by Large Language Models (LLMs), which significantly reduces the\nmanual annotation time required for dataset construction by leveraging LLMs to\ntranslate the original Japanese datasets. Additionally, we have enriched the\ndataset by incorporating open-domain Named Entity Recognition (NER) and\nsentence classification tasks. Utilizing this expanded dataset, we developed a\nunified input-output framework to train an Open-domain Information Extraction\nLarge Language Model (OIELLM). The OIELLM model demonstrates the capability to\neffectively process novel MMM datasets, exhibiting significant improvements in\nperformance.",
            "upvotes": 1
        },
        "publishedAt": "2024-07-16T04:37:33.104Z",
        "title": "MMM: Multilingual Mutual Reinforcement Effect Mix Datasets & Test with Open-domain Information Extraction Large Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.10953.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1627505688463-60107b385ac3e86b3ea4fc34.jpeg",
            "fullname": "Daniel van Strien",
            "name": "davanstrien",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.10362",
            "authors": [
                {
                    "_id": "6695cb23d4ca2767b9d5a15f",
                    "name": "Jon M. Laurent",
                    "hidden": false
                },
                {
                    "_id": "6695cb23d4ca2767b9d5a160",
                    "name": "Joseph D. Janizek",
                    "hidden": false
                },
                {
                    "_id": "6695cb23d4ca2767b9d5a161",
                    "name": "Michael Ruzo",
                    "hidden": false
                },
                {
                    "_id": "6695cb23d4ca2767b9d5a162",
                    "name": "Michaela M. Hinks",
                    "hidden": false
                },
                {
                    "_id": "6695cb23d4ca2767b9d5a163",
                    "name": "Michael J. Hammerling",
                    "hidden": false
                },
                {
                    "_id": "6695cb23d4ca2767b9d5a164",
                    "name": "Siddharth Narayanan",
                    "hidden": false
                },
                {
                    "_id": "6695cb23d4ca2767b9d5a165",
                    "name": "Manvitha Ponnapati",
                    "hidden": false
                },
                {
                    "_id": "6695cb23d4ca2767b9d5a166",
                    "name": "Andrew D. White",
                    "hidden": false
                },
                {
                    "_id": "6695cb23d4ca2767b9d5a167",
                    "name": "Samuel G. Rodriques",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-14T23:52:25.000Z",
            "title": "LAB-Bench: Measuring Capabilities of Language Models for Biology\n  Research",
            "summary": "There is widespread optimism that frontier Large Language Models (LLMs) and\nLLM-augmented systems have the potential to rapidly accelerate scientific\ndiscovery across disciplines. Today, many benchmarks exist to measure LLM\nknowledge and reasoning on textbook-style science questions, but few if any\nbenchmarks are designed to evaluate language model performance on practical\ntasks required for scientific research, such as literature search, protocol\nplanning, and data analysis. As a step toward building such benchmarks, we\nintroduce the Language Agent Biology Benchmark (LAB-Bench), a broad dataset of\nover 2,400 multiple choice questions for evaluating AI systems on a range of\npractical biology research capabilities, including recall and reasoning over\nliterature, interpretation of figures, access and navigation of databases, and\ncomprehension and manipulation of DNA and protein sequences. Importantly, in\ncontrast to previous scientific benchmarks, we expect that an AI system that\ncan achieve consistently high scores on the more difficult LAB-Bench tasks\nwould serve as a useful assistant for researchers in areas such as literature\nsearch and molecular cloning. As an initial assessment of the emergent\nscientific task capabilities of frontier language models, we measure\nperformance of several against our benchmark and report results compared to\nhuman expert biology researchers. We will continue to update and expand\nLAB-Bench over time, and expect it to serve as a useful tool in the development\nof automated research systems going forward. A public subset of LAB-Bench is\navailable for use at the following URL:\nhttps://huggingface.co/datasets/futurehouse/lab-bench",
            "upvotes": 1
        },
        "publishedAt": "2024-07-16T02:11:47.869Z",
        "title": "LAB-Bench: Measuring Capabilities of Language Models for Biology Research",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.10362.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    }
]