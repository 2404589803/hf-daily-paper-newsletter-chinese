[
  {
    "paper": {
      "id": "2503.16419",
      "authors": [
        {
          "_id": "67dcdbfc71027d42fa46e3f2",
          "name": "Yang Sui",
          "hidden": false
        },
        {
          "_id": "67dcdbfc71027d42fa46e3f3",
          "name": "Yu-Neng Chuang",
          "hidden": false
        },
        {
          "_id": "67dcdbfc71027d42fa46e3f4",
          "name": "Guanchu Wang",
          "hidden": false
        },
        {
          "_id": "67dcdbfc71027d42fa46e3f5",
          "name": "Jiamu Zhang",
          "hidden": false
        },
        {
          "_id": "67dcdbfc71027d42fa46e3f6",
          "name": "Tianyi Zhang",
          "hidden": false
        },
        {
          "_id": "67dcdbfc71027d42fa46e3f7",
          "name": "Jiayi Yuan",
          "hidden": false
        },
        {
          "_id": "67dcdbfc71027d42fa46e3f8",
          "name": "Hongyi Liu",
          "hidden": false
        },
        {
          "_id": "67dcdbfc71027d42fa46e3f9",
          "name": "Andrew Wen",
          "hidden": false
        },
        {
          "_id": "67dcdbfc71027d42fa46e3fa",
          "name": "Shaochen",
          "hidden": false
        },
        {
          "_id": "67dcdbfc71027d42fa46e3fb",
          "name": "Zhong",
          "hidden": false
        },
        {
          "_id": "67dcdbfc71027d42fa46e3fc",
          "name": "Hanjie Chen",
          "hidden": false
        },
        {
          "_id": "67dcdbfc71027d42fa46e3fd",
          "name": "Xia Hu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T17:59:38.000Z",
      "submittedOnDailyAt": "2025-03-21T01:56:58.604Z",
      "title": "Stop Overthinking: A Survey on Efficient Reasoning for Large Language\n  Models",
      "submittedOnDailyBy": {
        "_id": "63787b13500186f250ba377c",
        "avatarUrl": "/avatars/96bb6051662109e9cd25e6df7d738f17.svg",
        "isPro": false,
        "fullname": "yangsui",
        "user": "yangsui",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomplex tasks. Recent advancements in Large Reasoning Models (LRMs), such as\nOpenAI o1 and DeepSeek-R1, have further improved performance in System-2\nreasoning domains like mathematics and programming by harnessing supervised\nfine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the\nChain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences\nimprove performance, they also introduce significant computational overhead due\nto verbose and redundant outputs, known as the \"overthinking phenomenon\". In\nthis paper, we provide the first structured survey to systematically\ninvestigate and explore the current progress toward achieving efficient\nreasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we\ncategorize existing works into several key directions: (1) model-based\nefficient reasoning, which considers optimizing full-length reasoning models\ninto more concise reasoning models or directly training efficient reasoning\nmodels; (2) reasoning output-based efficient reasoning, which aims to\ndynamically reduce reasoning steps and length during inference; (3) input\nprompts-based efficient reasoning, which seeks to enhance reasoning efficiency\nbased on input prompt properties such as difficulty or length control.\nAdditionally, we introduce the use of efficient data for training reasoning\nmodels, explore the reasoning capabilities of small language models, and\ndiscuss evaluation methods and benchmarking.",
      "upvotes": 26,
      "discussionId": "67dcdbfd71027d42fa46e439",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Large Reasoning Models (LRMs)",
        "OpenAI o1",
        "DeepSeek-R1",
        "supervised fine-tuning (SFT)",
        "reinforcement learning (RL)",
        "Chain-of-Thought (CoT) reasoning",
        "overthinking phenomenon",
        "model-based efficient reasoning",
        "reasoning output-based efficient reasoning",
        "input prompts-based efficient reasoning",
        "efficient data",
        "small language models",
        "evaluation methods",
        "benchmarking"
      ]
    },
    "publishedAt": "2025-03-20T13:59:38.000Z",
    "title": "Stop Overthinking: A Survey on Efficient Reasoning for Large Language\n  Models",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomplex tasks. Recent advancements in Large Reasoning Models (LRMs), such as\nOpenAI o1 and DeepSeek-R1, have further improved performance in System-2\nreasoning domains like mathematics and programming by harnessing supervised\nfine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the\nChain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences\nimprove performance, they also introduce significant computational overhead due\nto verbose and redundant outputs, known as the \"overthinking phenomenon\". In\nthis paper, we provide the first structured survey to systematically\ninvestigate and explore the current progress toward achieving efficient\nreasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we\ncategorize existing works into several key directions: (1) model-based\nefficient reasoning, which considers optimizing full-length reasoning models\ninto more concise reasoning models or directly training efficient reasoning\nmodels; (2) reasoning output-based efficient reasoning, which aims to\ndynamically reduce reasoning steps and length during inference; (3) input\nprompts-based efficient reasoning, which seeks to enhance reasoning efficiency\nbased on input prompt properties such as difficulty or length control.\nAdditionally, we introduce the use of efficient data for training reasoning\nmodels, explore the reasoning capabilities of small language models, and\ndiscuss evaluation methods and benchmarking.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16419.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63787b13500186f250ba377c",
      "avatarUrl": "/avatars/96bb6051662109e9cd25e6df7d738f17.svg",
      "fullname": "yangsui",
      "name": "yangsui",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16302",
      "authors": [
        {
          "_id": "67dce2d2068292e7ef79b3dd",
          "name": "Zeqiang Lai",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3de",
          "name": "Yunfei Zhao",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3df",
          "name": "Zibo Zhao",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3e0",
          "name": "Haolin Liu",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3e1",
          "name": "Fuyun Wang",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3e2",
          "name": "Huiwen Shi",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3e3",
          "name": "Xianghui Yang",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3e4",
          "name": "Qinxiang Lin",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3e5",
          "name": "Jinwei Huang",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3e6",
          "name": "Yuhong Liu",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3e7",
          "name": "Jie Jiang",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3e8",
          "name": "Chunchao Guo",
          "hidden": false
        },
        {
          "_id": "67dce2d2068292e7ef79b3e9",
          "name": "Xiangyu Yue",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63044b89eedc089484c995ad/ukrgJYM5cBYEzAo7b9J7U.mp4"
      ],
      "publishedAt": "2025-03-20T16:23:44.000Z",
      "submittedOnDailyAt": "2025-03-21T02:25:30.177Z",
      "title": "Unleashing Vecset Diffusion Model for Fast Shape Generation",
      "submittedOnDailyBy": {
        "_id": "63044b89eedc089484c995ad",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/T4mOIQLaQdM5_oviaw_Cp.png",
        "isPro": false,
        "fullname": "Zeqiang Lai",
        "user": "ZeqiangLai",
        "type": "user"
      },
      "summary": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM.",
      "upvotes": 20,
      "discussionId": "67dce2d6068292e7ef79b556",
      "githubRepo": "https://github.com/Tencent/FlashVDM",
      "ai_keywords": [
        "3D diffusion",
        "Vecset Diffusion Model (VDM)",
        "diffusion sampling",
        "VAE",
        "DiT",
        "Progressive Flow Distillation",
        "lightning vecset decoder",
        "Adaptive KV Selection",
        "Hierarchical Volume Decoding",
        "Efficient Network Design",
        "FLOPs",
        "decoding overhead",
        "Hunyuan3D-2",
        "Hunyuan3D-2 Turbo"
      ]
    },
    "publishedAt": "2025-03-20T12:23:44.000Z",
    "title": "Unleashing Vecset Diffusion Model for Fast Shape Generation",
    "summary": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63044b89eedc089484c995ad/ukrgJYM5cBYEzAo7b9J7U.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16302.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63044b89eedc089484c995ad",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/T4mOIQLaQdM5_oviaw_Cp.png",
      "fullname": "Zeqiang Lai",
      "name": "ZeqiangLai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16212",
      "authors": [
        {
          "_id": "67dcd33626989570158ce8cf",
          "name": "Qizhi Pei",
          "hidden": false
        },
        {
          "_id": "67dcd33626989570158ce8d0",
          "name": "Lijun Wu",
          "hidden": false
        },
        {
          "_id": "67dcd33626989570158ce8d1",
          "name": "Zhuoshi Pan",
          "hidden": false
        },
        {
          "_id": "67dcd33626989570158ce8d2",
          "name": "Yu Li",
          "hidden": false
        },
        {
          "_id": "67dcd33626989570158ce8d3",
          "name": "Honglin Lin",
          "hidden": false
        },
        {
          "_id": "67dcd33626989570158ce8d4",
          "name": "Chenlin Ming",
          "hidden": false
        },
        {
          "_id": "67dcd33626989570158ce8d5",
          "name": "Xin Gao",
          "hidden": false
        },
        {
          "_id": "67dcd33626989570158ce8d6",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "67dcd33626989570158ce8d7",
          "name": "Rui Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T15:00:41.000Z",
      "submittedOnDailyAt": "2025-03-21T01:18:55.765Z",
      "title": "MathFusion: Enhancing Mathematic Problem-solving of LLM through\n  Instruction Fusion",
      "submittedOnDailyBy": {
        "_id": "6397f6081323f19c578f142e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
        "isPro": false,
        "fullname": "QizhiPei",
        "user": "QizhiPei",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have shown impressive progress in mathematical\nreasoning. While data augmentation is promising to enhance mathematical\nproblem-solving ability, current approaches are predominantly limited to\ninstance-level modifications-such as rephrasing or generating syntactic\nvariations-which fail to capture and leverage the intrinsic relational\nstructures inherent in mathematical knowledge. Inspired by human learning\nprocesses, where mathematical proficiency develops through systematic exposure\nto interconnected concepts, we introduce MathFusion, a novel framework that\nenhances mathematical reasoning through cross-problem instruction synthesis.\nMathFusion implements this through three fusion strategies: (1) sequential\nfusion, which chains related problems to model solution dependencies; (2)\nparallel fusion, which combines analogous problems to reinforce conceptual\nunderstanding; and (3) conditional fusion, which creates context-aware\nselective problems to enhance reasoning flexibility. By applying these\nstrategies, we generate a new dataset, MathFusionQA, followed by\nfine-tuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental\nresults demonstrate that MathFusion achieves substantial improvements in\nmathematical reasoning while maintaining high data efficiency, boosting\nperformance by 18.0 points in accuracy across diverse benchmarks while\nrequiring only 45K additional synthetic instructions, representing a\nsubstantial improvement over traditional single-instruction approaches. Our\ndatasets, models, and code are publicly available at\nhttps://github.com/QizhiPei/mathfusion.",
      "upvotes": 13,
      "discussionId": "67dcd33726989570158ce90a",
      "githubRepo": "https://github.com/QizhiPei/MathFusion",
      "ai_keywords": [
        "MathFusion",
        "cross-problem instruction synthesis",
        "sequential fusion",
        "parallel fusion",
        "conditional fusion",
        "MathFusionQA",
        "DeepSeekMath-7B",
        "Mistral-7B",
        "Llama3-8B",
        "mathematical reasoning",
        "data efficiency"
      ]
    },
    "publishedAt": "2025-03-20T11:00:41.000Z",
    "title": "MathFusion: Enhancing Mathematic Problem-solving of LLM through\n  Instruction Fusion",
    "summary": "Large Language Models (LLMs) have shown impressive progress in mathematical\nreasoning. While data augmentation is promising to enhance mathematical\nproblem-solving ability, current approaches are predominantly limited to\ninstance-level modifications-such as rephrasing or generating syntactic\nvariations-which fail to capture and leverage the intrinsic relational\nstructures inherent in mathematical knowledge. Inspired by human learning\nprocesses, where mathematical proficiency develops through systematic exposure\nto interconnected concepts, we introduce MathFusion, a novel framework that\nenhances mathematical reasoning through cross-problem instruction synthesis.\nMathFusion implements this through three fusion strategies: (1) sequential\nfusion, which chains related problems to model solution dependencies; (2)\nparallel fusion, which combines analogous problems to reinforce conceptual\nunderstanding; and (3) conditional fusion, which creates context-aware\nselective problems to enhance reasoning flexibility. By applying these\nstrategies, we generate a new dataset, MathFusionQA, followed by\nfine-tuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental\nresults demonstrate that MathFusion achieves substantial improvements in\nmathematical reasoning while maintaining high data efficiency, boosting\nperformance by 18.0 points in accuracy across diverse benchmarks while\nrequiring only 45K additional synthetic instructions, representing a\nsubstantial improvement over traditional single-instruction approaches. Our\ndatasets, models, and code are publicly available at\nhttps://github.com/QizhiPei/mathfusion.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16212.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6397f6081323f19c578f142e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6397f6081323f19c578f142e/it7FYYKjlLX8wSsMLm8EO.jpeg",
      "fullname": "QizhiPei",
      "name": "QizhiPei",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.15558",
      "authors": [
        {
          "_id": "67dcadafb2cd7d4f3a266037",
          "name": "NVIDIA",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266039",
          "name": "Alisson Azzolini",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26603a",
          "name": "Hannah Brandon",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26603b",
          "name": "Prithvijit Chattopadhyay",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26603c",
          "name": "Huayu Chen",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26603d",
          "name": "Jinju Chu",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26603e",
          "name": "Yin Cui",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26603f",
          "name": "Jenna Diamond",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266040",
          "name": "Yifan Ding",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266041",
          "name": "Francesco Ferroni",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266042",
          "name": "Rama Govindaraju",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266043",
          "name": "Jinwei Gu",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266044",
          "name": "Siddharth Gururani",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266045",
          "name": "Imad El Hanafi",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266046",
          "name": "Zekun Hao",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266047",
          "name": "Jacob Huffman",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266048",
          "name": "Jingyi Jin",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266049",
          "name": "Brendan Johnson",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26604a",
          "name": "Rizwan Khan",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26604b",
          "name": "George Kurian",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26604c",
          "name": "Elena Lantz",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26604d",
          "name": "Nayeon Lee",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26604e",
          "name": "Zhaoshuo Li",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26604f",
          "name": "Xuan Li",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266050",
          "name": "Tsung-Yi Lin",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266051",
          "name": "Yen-Chen Lin",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266052",
          "name": "Ming-Yu Liu",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266053",
          "name": "Andrew Mathau",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266054",
          "name": "Yun Ni",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266055",
          "name": "Lindsey Pavao",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266056",
          "name": "Wei Ping",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266057",
          "name": "David W. Romero",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266058",
          "name": "Misha Smelyanskiy",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266059",
          "name": "Shuran Song",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26605a",
          "name": "Lyne Tchapmi",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26605b",
          "name": "Andrew Z. Wang",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26605c",
          "name": "Boxin Wang",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26605d",
          "name": "Haoxiang Wang",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26605e",
          "name": "Fangyin Wei",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a26605f",
          "name": "Jiashu Xu",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266060",
          "name": "Yao Xu",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266061",
          "name": "Xiaodong Yang",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266062",
          "name": "Zhuolin Yang",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266063",
          "name": "Xiaohui Zeng",
          "hidden": false
        },
        {
          "_id": "67dcadafb2cd7d4f3a266064",
          "name": "Zhe Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T22:06:58.000Z",
      "submittedOnDailyAt": "2025-03-21T01:09:51.519Z",
      "title": "Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning",
      "submittedOnDailyBy": {
        "_id": "649f05367b57fab3a5b27c8b",
        "avatarUrl": "/avatars/749eca8ba898685c90c305f4e3549ba1.svg",
        "isPro": false,
        "fullname": "Yin Cui",
        "user": "richardaecn",
        "type": "user"
      },
      "summary": "Physical AI systems need to perceive, understand, and perform complex actions\nin the physical world. In this paper, we present the Cosmos-Reason1 models that\ncan understand the physical world and generate appropriate embodied decisions\n(e.g., next step action) in natural language through long chain-of-thought\nreasoning processes. We begin by defining key capabilities for Physical AI\nreasoning, with a focus on physical common sense and embodied reasoning. To\nrepresent physical common sense, we use a hierarchical ontology that captures\nfundamental knowledge about space, time, and physics. For embodied reasoning,\nwe rely on a two-dimensional ontology that generalizes across different\nphysical embodiments. Building on these capabilities, we develop two multimodal\nlarge language models, Cosmos-Reason1-8B and Cosmos-Reason1-56B. We curate data\nand train our models in four stages: vision pre-training, general supervised\nfine-tuning (SFT), Physical AI SFT, and Physical AI reinforcement learning (RL)\nas the post-training. To evaluate our models, we build comprehensive benchmarks\nfor physical common sense and embodied reasoning according to our ontologies.\nEvaluation results show that Physical AI SFT and reinforcement learning bring\nsignificant improvements. To facilitate the development of Physical AI, we will\nmake our code and pre-trained models available under the NVIDIA Open Model\nLicense at https://github.com/nvidia-cosmos/cosmos-reason1.",
      "upvotes": 13,
      "discussionId": "67dcadb1b2cd7d4f3a2660f4",
      "githubRepo": "https://github.com/nvidia-cosmos/cosmos-reason1",
      "ai_keywords": [
        "hierarchical ontology",
        "two-dimensional ontology",
        "multimodal large language models",
        "vision pre-training",
        "long chain-of-thought reasoning",
        "Physical AI SFT",
        "Physical AI reinforcement learning",
        "embodied reasoning",
        "physical common sense"
      ]
    },
    "publishedAt": "2025-03-18T18:06:58.000Z",
    "title": "Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning",
    "summary": "Physical AI systems need to perceive, understand, and perform complex actions\nin the physical world. In this paper, we present the Cosmos-Reason1 models that\ncan understand the physical world and generate appropriate embodied decisions\n(e.g., next step action) in natural language through long chain-of-thought\nreasoning processes. We begin by defining key capabilities for Physical AI\nreasoning, with a focus on physical common sense and embodied reasoning. To\nrepresent physical common sense, we use a hierarchical ontology that captures\nfundamental knowledge about space, time, and physics. For embodied reasoning,\nwe rely on a two-dimensional ontology that generalizes across different\nphysical embodiments. Building on these capabilities, we develop two multimodal\nlarge language models, Cosmos-Reason1-8B and Cosmos-Reason1-56B. We curate data\nand train our models in four stages: vision pre-training, general supervised\nfine-tuning (SFT), Physical AI SFT, and Physical AI reinforcement learning (RL)\nas the post-training. To evaluate our models, we build comprehensive benchmarks\nfor physical common sense and embodied reasoning according to our ontologies.\nEvaluation results show that Physical AI SFT and reinforcement learning bring\nsignificant improvements. To facilitate the development of Physical AI, we will\nmake our code and pre-trained models available under the NVIDIA Open Model\nLicense at https://github.com/nvidia-cosmos/cosmos-reason1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15558.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649f05367b57fab3a5b27c8b",
      "avatarUrl": "/avatars/749eca8ba898685c90c305f4e3549ba1.svg",
      "fullname": "Yin Cui",
      "name": "richardaecn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16365",
      "authors": [
        {
          "_id": "67dcdc98e406e84ea880ccab",
          "name": "Muyao Li",
          "hidden": false
        },
        {
          "_id": "67dcdc98e406e84ea880ccac",
          "name": "Zihao Wang",
          "hidden": false
        },
        {
          "_id": "67dcdc98e406e84ea880ccad",
          "name": "Kaichen He",
          "hidden": false
        },
        {
          "_id": "67dcdc98e406e84ea880ccae",
          "name": "Xiaojian Ma",
          "hidden": false
        },
        {
          "_id": "67dcdc98e406e84ea880ccaf",
          "name": "Yitao Liang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/QK0LQhSbfUSqe9FuDmxJu.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/VHbOI8bWxJLDd1aOBjoRT.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/xBWzJPEtxSx-8agJJeIXc.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/y2QTnsahG2XEhyG35nhab.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/W_gV_JuPtKvZjdI9Wv1Jb.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/YDIIWH_7nTy1Xm2pTRt5B.mp4"
      ],
      "publishedAt": "2025-03-20T17:21:58.000Z",
      "submittedOnDailyAt": "2025-03-21T01:59:54.135Z",
      "title": "JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play\n  Visual Games with Keyboards and Mouse",
      "submittedOnDailyBy": {
        "_id": "642e8c99c1b0f8e4e76bcaab",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642e8c99c1b0f8e4e76bcaab/BOs9r0P9KyT9pEba9v0H4.png",
        "isPro": false,
        "fullname": "Zihao Wang",
        "user": "zhwang4ai",
        "type": "user"
      },
      "summary": "Recently, action-based decision-making in open-world environments has gained\nsignificant attention. Visual Language Action (VLA) models, pretrained on\nlarge-scale web datasets, have shown promise in decision-making tasks. However,\nprevious work has primarily focused on action post-training, often neglecting\nenhancements to the foundational model itself. In response, we introduce a\nnovel approach, Act from Visual Language Post-Training, which refines Visual\nLanguage Models (VLMs) through visual and linguistic guidance in a\nself-supervised manner. This enhancement improves the models' capabilities in\nworld knowledge, visual recognition, and spatial grounding in open-world\nenvironments. Following the above post-training paradigms, we obtain the first\nVLA models in Minecraft that can follow human instructions on over 1k different\natomic tasks, including crafting, smelting, cooking, mining, and killing. Our\nexperiments demonstrate that post-training on non-trajectory tasks leads to a\nsignificant 40% improvement over the best agent baseline on a diverse set of\natomic tasks. Furthermore, we demonstrate that our approach surpasses\ntraditional imitation learning-based policies in Minecraft, achieving\nstate-of-the-art performance. We have open-sourced the code, models, and\ndatasets to foster further research. The project page can be found in\nhttps://craftjarvis.github.io/JarvisVLA.",
      "upvotes": 10,
      "discussionId": "67dcdc9ce406e84ea880ce67",
      "projectPage": "https://craftjarvis.github.io/JarvisVLA/",
      "githubRepo": "https://github.com/CraftJarvis/JarvisVLA",
      "ai_keywords": [
        "Visual Language Action (VLA) models",
        "Visual Language Models (VLMs)",
        "self-supervised manner",
        "world knowledge",
        "visual recognition",
        "spatial grounding",
        "atomic tasks",
        "crafting",
        "smelting",
        "cooking",
        "mining",
        "killing",
        "non-trajectory tasks",
        "imitation learning-based policies"
      ]
    },
    "publishedAt": "2025-03-20T13:21:58.000Z",
    "title": "JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play\n  Visual Games with Keyboards and Mouse",
    "summary": "Recently, action-based decision-making in open-world environments has gained\nsignificant attention. Visual Language Action (VLA) models, pretrained on\nlarge-scale web datasets, have shown promise in decision-making tasks. However,\nprevious work has primarily focused on action post-training, often neglecting\nenhancements to the foundational model itself. In response, we introduce a\nnovel approach, Act from Visual Language Post-Training, which refines Visual\nLanguage Models (VLMs) through visual and linguistic guidance in a\nself-supervised manner. This enhancement improves the models' capabilities in\nworld knowledge, visual recognition, and spatial grounding in open-world\nenvironments. Following the above post-training paradigms, we obtain the first\nVLA models in Minecraft that can follow human instructions on over 1k different\natomic tasks, including crafting, smelting, cooking, mining, and killing. Our\nexperiments demonstrate that post-training on non-trajectory tasks leads to a\nsignificant 40% improvement over the best agent baseline on a diverse set of\natomic tasks. Furthermore, we demonstrate that our approach surpasses\ntraditional imitation learning-based policies in Minecraft, achieving\nstate-of-the-art performance. We have open-sourced the code, models, and\ndatasets to foster further research. The project page can be found in\nhttps://craftjarvis.github.io/JarvisVLA.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/QK0LQhSbfUSqe9FuDmxJu.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/VHbOI8bWxJLDd1aOBjoRT.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/xBWzJPEtxSx-8agJJeIXc.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/y2QTnsahG2XEhyG35nhab.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/W_gV_JuPtKvZjdI9Wv1Jb.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/642e8c99c1b0f8e4e76bcaab/YDIIWH_7nTy1Xm2pTRt5B.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16365.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "642e8c99c1b0f8e4e76bcaab",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642e8c99c1b0f8e4e76bcaab/BOs9r0P9KyT9pEba9v0H4.png",
      "fullname": "Zihao Wang",
      "name": "zhwang4ai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16356",
      "authors": [
        {
          "_id": "67dcd18ad2550735425351bf",
          "name": "Yunzhi Yao",
          "hidden": false
        },
        {
          "_id": "67dcd18ad2550735425351c0",
          "name": "Jizhan Fang",
          "hidden": false
        },
        {
          "_id": "67dcd18ad2550735425351c1",
          "name": "Jia-Chen Gu",
          "hidden": false
        },
        {
          "_id": "67dcd18ad2550735425351c2",
          "name": "Ningyu Zhang",
          "hidden": false
        },
        {
          "_id": "67dcd18ad2550735425351c3",
          "name": "Shumin Deng",
          "hidden": false
        },
        {
          "_id": "67dcd18ad2550735425351c4",
          "name": "Huajun Chen",
          "hidden": false
        },
        {
          "_id": "67dcd18ad2550735425351c5",
          "name": "Nanyun Peng",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/aC9fMp8dvcRIzvms4CAu_.png"
      ],
      "publishedAt": "2025-03-20T17:14:34.000Z",
      "submittedOnDailyAt": "2025-03-21T01:12:07.189Z",
      "title": "CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners",
      "submittedOnDailyBy": {
        "_id": "620b3bbb0668e435407c8d0a",
        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
        "isPro": false,
        "fullname": "Ningyu Zhang",
        "user": "Ningyu",
        "type": "user"
      },
      "summary": "Knowledge Editing (KE) enables the modification of outdated or incorrect\ninformation in large language models (LLMs). While existing KE methods can\nupdate isolated facts, they struggle to generalize these updates to multi-hop\nreasoning tasks that depend on the modified knowledge. Through an analysis of\nreasoning circuits -- the neural pathways LLMs use for knowledge-based\ninference, we observe that current layer-localized KE approaches, such as MEMIT\nand WISE, which edit only single or a few model layers, struggle to effectively\nincorporate updated information into these reasoning pathways. To address this\nlimitation, we propose CaKE (Circuit-aware Knowledge Editing), a novel method\nthat enables more effective integration of updated knowledge in LLMs. CaKE\nleverages strategically curated data, guided by our circuits-based analysis,\nthat enforces the model to utilize the modified knowledge, stimulating the\nmodel to develop appropriate reasoning circuits for newly integrated knowledge.\nExperimental results show that CaKE enables more accurate and consistent use of\nupdated knowledge across related reasoning tasks, leading to an average of 20%\nimprovement in multi-hop reasoning accuracy on MQuAKE dataset compared to\nexisting KE methods. We release the code and data in\nhttps://github.com/zjunlp/CaKE.",
      "upvotes": 9,
      "discussionId": "67dcd18bd255073542535223",
      "githubRepo": "https://github.com/zjunlp/CaKE",
      "ai_keywords": [
        "Knowledge Editing (KE)",
        "large language models (LLMs)",
        "multi-hop reasoning tasks",
        "reasoning circuits",
        "neural pathways",
        "knowledge-based inference",
        "MEMIT",
        "WISE",
        "layer-localized KE approaches",
        "CaKE (Circuit-aware Knowledge Editing)",
        "strategically curated data",
        "circuits-based analysis",
        "MQuAKE dataset"
      ]
    },
    "publishedAt": "2025-03-20T13:14:34.000Z",
    "title": "CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners",
    "summary": "Knowledge Editing (KE) enables the modification of outdated or incorrect\ninformation in large language models (LLMs). While existing KE methods can\nupdate isolated facts, they struggle to generalize these updates to multi-hop\nreasoning tasks that depend on the modified knowledge. Through an analysis of\nreasoning circuits -- the neural pathways LLMs use for knowledge-based\ninference, we observe that current layer-localized KE approaches, such as MEMIT\nand WISE, which edit only single or a few model layers, struggle to effectively\nincorporate updated information into these reasoning pathways. To address this\nlimitation, we propose CaKE (Circuit-aware Knowledge Editing), a novel method\nthat enables more effective integration of updated knowledge in LLMs. CaKE\nleverages strategically curated data, guided by our circuits-based analysis,\nthat enforces the model to utilize the modified knowledge, stimulating the\nmodel to develop appropriate reasoning circuits for newly integrated knowledge.\nExperimental results show that CaKE enables more accurate and consistent use of\nupdated knowledge across related reasoning tasks, leading to an average of 20%\nimprovement in multi-hop reasoning accuracy on MQuAKE dataset compared to\nexisting KE methods. We release the code and data in\nhttps://github.com/zjunlp/CaKE.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/aC9fMp8dvcRIzvms4CAu_.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16356.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620b3bbb0668e435407c8d0a",
      "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
      "fullname": "Ningyu Zhang",
      "name": "Ningyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16418",
      "authors": [
        {
          "_id": "67dcd0fe1f94b594ef4f3e8e",
          "name": "Liming Jiang",
          "hidden": false
        },
        {
          "_id": "67dcd0fe1f94b594ef4f3e8f",
          "name": "Qing Yan",
          "hidden": false
        },
        {
          "_id": "67dcd0fe1f94b594ef4f3e90",
          "name": "Yumin Jia",
          "hidden": false
        },
        {
          "_id": "67dcd0fe1f94b594ef4f3e91",
          "name": "Zichuan Liu",
          "hidden": false
        },
        {
          "_id": "67dcd0fe1f94b594ef4f3e92",
          "name": "Hao Kang",
          "hidden": false
        },
        {
          "_id": "67dcd0fe1f94b594ef4f3e93",
          "name": "Xin Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T17:59:34.000Z",
      "submittedOnDailyAt": "2025-03-21T02:41:07.989Z",
      "title": "InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Achieving flexible and high-fidelity identity-preserved image generation\nremains formidable, particularly with advanced Diffusion Transformers (DiTs)\nlike FLUX. We introduce InfiniteYou (InfU), one of the earliest robust\nframeworks leveraging DiTs for this task. InfU addresses significant issues of\nexisting methods, such as insufficient identity similarity, poor text-image\nalignment, and low generation quality and aesthetics. Central to InfU is\nInfuseNet, a component that injects identity features into the DiT base model\nvia residual connections, enhancing identity similarity while maintaining\ngeneration capabilities. A multi-stage training strategy, including pretraining\nand supervised fine-tuning (SFT) with synthetic single-person-multiple-sample\n(SPMS) data, further improves text-image alignment, ameliorates image quality,\nand alleviates face copy-pasting. Extensive experiments demonstrate that InfU\nachieves state-of-the-art performance, surpassing existing baselines. In\naddition, the plug-and-play design of InfU ensures compatibility with various\nexisting methods, offering a valuable contribution to the broader community.",
      "upvotes": 7,
      "discussionId": "67dcd1001f94b594ef4f3f44",
      "ai_keywords": [
        "Diffusion Transformers (DiTs)",
        "FLUX",
        "InfiniteYou (InfU)",
        "InfuseNet",
        "residual connections",
        "synthetic single-person-multiple-sample (SPMS) data",
        "pretraining",
        "supervised fine-tuning (SFT)"
      ]
    },
    "publishedAt": "2025-03-20T13:59:34.000Z",
    "title": "InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity",
    "summary": "Achieving flexible and high-fidelity identity-preserved image generation\nremains formidable, particularly with advanced Diffusion Transformers (DiTs)\nlike FLUX. We introduce InfiniteYou (InfU), one of the earliest robust\nframeworks leveraging DiTs for this task. InfU addresses significant issues of\nexisting methods, such as insufficient identity similarity, poor text-image\nalignment, and low generation quality and aesthetics. Central to InfU is\nInfuseNet, a component that injects identity features into the DiT base model\nvia residual connections, enhancing identity similarity while maintaining\ngeneration capabilities. A multi-stage training strategy, including pretraining\nand supervised fine-tuning (SFT) with synthetic single-person-multiple-sample\n(SPMS) data, further improves text-image alignment, ameliorates image quality,\nand alleviates face copy-pasting. Extensive experiments demonstrate that InfU\nachieves state-of-the-art performance, surpassing existing baselines. In\naddition, the plug-and-play design of InfU ensures compatibility with various\nexisting methods, offering a valuable contribution to the broader community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16418.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6415
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16057",
      "authors": [
        {
          "_id": "67dd04563b4c256a9809cc96",
          "name": "Yike Yuan",
          "hidden": false
        },
        {
          "_id": "67dd04563b4c256a9809cc97",
          "name": "Ziyu Wang",
          "hidden": false
        },
        {
          "_id": "67dd04563b4c256a9809cc98",
          "name": "Zihao Huang",
          "hidden": false
        },
        {
          "_id": "67dd04563b4c256a9809cc99",
          "name": "Defa Zhu",
          "hidden": false
        },
        {
          "_id": "67dd04563b4c256a9809cc9a",
          "name": "Xun Zhou",
          "hidden": false
        },
        {
          "_id": "67dd04563b4c256a9809cc9b",
          "name": "Jingyi Yu",
          "hidden": false
        },
        {
          "_id": "67dd04563b4c256a9809cc9c",
          "name": "Qiyang Min",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T11:45:08.000Z",
      "submittedOnDailyAt": "2025-03-21T04:50:45.905Z",
      "title": "Expert Race: A Flexible Routing Strategy for Scaling Diffusion\n  Transformer with Mixture of Experts",
      "submittedOnDailyBy": {
        "_id": "667505f4361b960c79e35486",
        "avatarUrl": "/avatars/d352639c520075220f6abaae23c39376.svg",
        "isPro": false,
        "fullname": "Defa Zhu",
        "user": "mathfinder",
        "type": "user"
      },
      "summary": "Diffusion models have emerged as mainstream framework in visual generation.\nBuilding upon this success, the integration of Mixture of Experts (MoE) methods\nhas shown promise in enhancing model scalability and performance. In this\npaper, we introduce Race-DiT, a novel MoE model for diffusion transformers with\na flexible routing strategy, Expert Race. By allowing tokens and experts to\ncompete together and select the top candidates, the model learns to dynamically\nassign experts to critical tokens. Additionally, we propose per-layer\nregularization to address challenges in shallow layer learning, and router\nsimilarity loss to prevent mode collapse, ensuring better expert utilization.\nExtensive experiments on ImageNet validate the effectiveness of our approach,\nshowcasing significant performance gains while promising scaling properties.",
      "upvotes": 7,
      "discussionId": "67dd045a3b4c256a9809cdb1",
      "ai_keywords": [
        "diffusion models",
        "Mixture of Experts (MoE)",
        "Race-DiT",
        "diffusion transformers",
        "Expert Race",
        "tokens",
        "experts",
        "per-layer regularization",
        "router similarity loss",
        "mode collapse",
        "ImageNet"
      ]
    },
    "publishedAt": "2025-03-20T07:45:08.000Z",
    "title": "Expert Race: A Flexible Routing Strategy for Scaling Diffusion\n  Transformer with Mixture of Experts",
    "summary": "Diffusion models have emerged as mainstream framework in visual generation.\nBuilding upon this success, the integration of Mixture of Experts (MoE) methods\nhas shown promise in enhancing model scalability and performance. In this\npaper, we introduce Race-DiT, a novel MoE model for diffusion transformers with\na flexible routing strategy, Expert Race. By allowing tokens and experts to\ncompete together and select the top candidates, the model learns to dynamically\nassign experts to critical tokens. Additionally, we propose per-layer\nregularization to address challenges in shallow layer learning, and router\nsimilarity loss to prevent mode collapse, ensuring better expert utilization.\nExtensive experiments on ImageNet validate the effectiveness of our approach,\nshowcasing significant performance gains while promising scaling properties.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16057.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "667505f4361b960c79e35486",
      "avatarUrl": "/avatars/d352639c520075220f6abaae23c39376.svg",
      "fullname": "Defa Zhu",
      "name": "mathfinder",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16422",
      "authors": [
        {
          "_id": "67dcd5da7f5c5665205b11c0",
          "name": "Yuheng Yuan",
          "hidden": false
        },
        {
          "_id": "67dcd5da7f5c5665205b11c1",
          "name": "Qiuhong Shen",
          "hidden": false
        },
        {
          "_id": "67dcd5da7f5c5665205b11c2",
          "name": "Xingyi Yang",
          "hidden": false
        },
        {
          "_id": "67dcd5da7f5c5665205b11c3",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/634cfebc350bcee9bed20a4d/O7634-Dy0NBVM_UZzq5Nm.mp4"
      ],
      "publishedAt": "2025-03-20T17:59:44.000Z",
      "submittedOnDailyAt": "2025-03-21T01:29:12.177Z",
      "title": "1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering",
      "submittedOnDailyBy": {
        "_id": "634cfebc350bcee9bed20a4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
        "isPro": false,
        "fullname": "Xingyi Yang",
        "user": "adamdad",
        "type": "user"
      },
      "summary": "4D Gaussian Splatting (4DGS) has recently gained considerable attention as a\nmethod for reconstructing dynamic scenes. Despite achieving superior quality,\n4DGS typically requires substantial storage and suffers from slow rendering\nspeed. In this work, we delve into these issues and identify two key sources of\ntemporal redundancy. (Q1) Short-Lifespan Gaussians: 4DGS uses a large\nportion of Gaussians with short temporal span to represent scene dynamics,\nleading to an excessive number of Gaussians. (Q2) Inactive Gaussians:\nWhen rendering, only a small subset of Gaussians contributes to each frame.\nDespite this, all Gaussians are processed during rasterization, resulting in\nredundant computation overhead. To address these redundancies, we present\n4DGS-1K, which runs at over 1000 FPS on modern GPUs. For Q1, we\nintroduce the Spatial-Temporal Variation Score, a new pruning criterion that\neffectively removes short-lifespan Gaussians while encouraging 4DGS to capture\nscene dynamics using Gaussians with longer temporal spans. For Q2, we store a\nmask for active Gaussians across consecutive frames, significantly reducing\nredundant computations in rendering. Compared to vanilla 4DGS, our method\nachieves a 41times reduction in storage and 9times faster rasterization\nspeed on complex dynamic scenes, while maintaining comparable visual quality.\nPlease see our project page at https://4DGS-1K.github.io.",
      "upvotes": 5,
      "discussionId": "67dcd5e17f5c5665205b1422",
      "ai_keywords": [
        "4D Gaussian Splatting (4DGS)",
        "temporal redundancy",
        "Short-Lifespan Gaussians",
        "inactive Gaussians",
        "rasterization",
        "Spatial-Temporal Variation Score",
        "4DGS-1K",
        "FPS",
        "modern GPUs",
        "storage",
        "rasterization speed",
        "visual quality"
      ]
    },
    "publishedAt": "2025-03-20T13:59:44.000Z",
    "title": "1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering",
    "summary": "4D Gaussian Splatting (4DGS) has recently gained considerable attention as a\nmethod for reconstructing dynamic scenes. Despite achieving superior quality,\n4DGS typically requires substantial storage and suffers from slow rendering\nspeed. In this work, we delve into these issues and identify two key sources of\ntemporal redundancy. (Q1) Short-Lifespan Gaussians: 4DGS uses a large\nportion of Gaussians with short temporal span to represent scene dynamics,\nleading to an excessive number of Gaussians. (Q2) Inactive Gaussians:\nWhen rendering, only a small subset of Gaussians contributes to each frame.\nDespite this, all Gaussians are processed during rasterization, resulting in\nredundant computation overhead. To address these redundancies, we present\n4DGS-1K, which runs at over 1000 FPS on modern GPUs. For Q1, we\nintroduce the Spatial-Temporal Variation Score, a new pruning criterion that\neffectively removes short-lifespan Gaussians while encouraging 4DGS to capture\nscene dynamics using Gaussians with longer temporal spans. For Q2, we store a\nmask for active Gaussians across consecutive frames, significantly reducing\nredundant computations in rendering. Compared to vanilla 4DGS, our method\nachieves a 41times reduction in storage and 9times faster rasterization\nspeed on complex dynamic scenes, while maintaining comparable visual quality.\nPlease see our project page at https://4DGS-1K.github.io.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/634cfebc350bcee9bed20a4d/O7634-Dy0NBVM_UZzq5Nm.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16422.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "634cfebc350bcee9bed20a4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
      "fullname": "Xingyi Yang",
      "name": "adamdad",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 12
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16413",
      "authors": [
        {
          "_id": "67dccb8ca33f11a56567bd61",
          "name": "Xueyan Zou",
          "hidden": false
        },
        {
          "_id": "67dccb8ca33f11a56567bd62",
          "name": "Yuchen Song",
          "hidden": false
        },
        {
          "_id": "67dccb8ca33f11a56567bd63",
          "name": "Ri-Zhao Qiu",
          "hidden": false
        },
        {
          "_id": "67dccb8ca33f11a56567bd64",
          "name": "Xuanbin Peng",
          "hidden": false
        },
        {
          "_id": "67dccb8ca33f11a56567bd65",
          "name": "Jianglong Ye",
          "hidden": false
        },
        {
          "_id": "67dccb8ca33f11a56567bd66",
          "name": "Sifei Liu",
          "hidden": false
        },
        {
          "_id": "67dccb8ca33f11a56567bd67",
          "name": "Xiaolong Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T17:59:12.000Z",
      "submittedOnDailyAt": "2025-03-21T00:52:49.431Z",
      "title": "M3: 3D-Spatial MultiModal Memory",
      "submittedOnDailyBy": {
        "_id": "62520988818a5dc29ab91d6f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1671670066375-62520988818a5dc29ab91d6f.png",
        "isPro": false,
        "fullname": "Xueyan Zou",
        "user": "xueyanz",
        "type": "user"
      },
      "summary": "We present 3D Spatial MultiModal Memory (M3), a multimodal memory system\ndesigned to retain information about medium-sized static scenes through video\nsources for visual perception. By integrating 3D Gaussian Splatting techniques\nwith foundation models, M3 builds a multimodal memory capable of rendering\nfeature representations across granularities, encompassing a wide range of\nknowledge. In our exploration, we identify two key challenges in previous works\non feature splatting: (1) computational constraints in storing high-dimensional\nfeatures for each Gaussian primitive, and (2) misalignment or information loss\nbetween distilled features and foundation model features. To address these\nchallenges, we propose M3 with key components of principal scene components and\nGaussian memory attention, enabling efficient training and inference. To\nvalidate M3, we conduct comprehensive quantitative evaluations of feature\nsimilarity and downstream tasks, as well as qualitative visualizations to\nhighlight the pixel trace of Gaussian memory attention. Our approach\nencompasses a diverse range of foundation models, including vision-language\nmodels (VLMs), perception models, and large multimodal and language models\n(LMMs/LLMs). Furthermore, to demonstrate real-world applicability, we deploy\nM3's feature field in indoor scenes on a quadruped robot. Notably, we claim\nthat M3 is the first work to address the core compression challenges in 3D\nfeature distillation.",
      "upvotes": 5,
      "discussionId": "67dccb92a33f11a56567bf43",
      "ai_keywords": [
        "3D Spatial MultiModal Memory (M3)",
        "3D Gaussian Splatting",
        "feature representations",
        "granularities",
        "principal scene components",
        "Gaussian memory attention",
        "feature splatting",
        "computational constraints",
        "high-dimensional features",
        "Gaussian primitive",
        "misalignment",
        "information loss",
        "distilled features",
        "foundation models",
        "vision-language models (VLMs)",
        "perception models",
        "large multimodal and language models (LMMs/LLMs)",
        "feature field",
        "quadruped robot",
        "core compression challenges",
        "3D feature distillation"
      ]
    },
    "publishedAt": "2025-03-20T13:59:12.000Z",
    "title": "M3: 3D-Spatial MultiModal Memory",
    "summary": "We present 3D Spatial MultiModal Memory (M3), a multimodal memory system\ndesigned to retain information about medium-sized static scenes through video\nsources for visual perception. By integrating 3D Gaussian Splatting techniques\nwith foundation models, M3 builds a multimodal memory capable of rendering\nfeature representations across granularities, encompassing a wide range of\nknowledge. In our exploration, we identify two key challenges in previous works\non feature splatting: (1) computational constraints in storing high-dimensional\nfeatures for each Gaussian primitive, and (2) misalignment or information loss\nbetween distilled features and foundation model features. To address these\nchallenges, we propose M3 with key components of principal scene components and\nGaussian memory attention, enabling efficient training and inference. To\nvalidate M3, we conduct comprehensive quantitative evaluations of feature\nsimilarity and downstream tasks, as well as qualitative visualizations to\nhighlight the pixel trace of Gaussian memory attention. Our approach\nencompasses a diverse range of foundation models, including vision-language\nmodels (VLMs), perception models, and large multimodal and language models\n(LMMs/LLMs). Furthermore, to demonstrate real-world applicability, we deploy\nM3's feature field in indoor scenes on a quadruped robot. Notably, we claim\nthat M3 is the first work to address the core compression challenges in 3D\nfeature distillation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16413.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62520988818a5dc29ab91d6f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1671670066375-62520988818a5dc29ab91d6f.png",
      "fullname": "Xueyan Zou",
      "name": "xueyanz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16322",
      "authors": [
        {
          "_id": "67dce4c10784200359ab2494",
          "name": "Ruonan Yu",
          "hidden": false
        },
        {
          "_id": "67dce4c10784200359ab2495",
          "name": "Songhua Liu",
          "hidden": false
        },
        {
          "_id": "67dce4c10784200359ab2496",
          "name": "Zhenxiong Tan",
          "hidden": false
        },
        {
          "_id": "67dce4c10784200359ab2497",
          "name": "Xinchao Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T16:44:43.000Z",
      "submittedOnDailyAt": "2025-03-21T02:36:06.925Z",
      "title": "Ultra-Resolution Adaptation with Ease",
      "submittedOnDailyBy": {
        "_id": "6486fb33570a419f41a882e4",
        "avatarUrl": "/avatars/860a42074439a23c629cd23851ae4da6.svg",
        "isPro": false,
        "fullname": "Ruonan Yu",
        "user": "roseannelexie",
        "type": "user"
      },
      "summary": "Text-to-image diffusion models have achieved remarkable progress in recent\nyears. However, training models for high-resolution image generation remains\nchallenging, particularly when training data and computational resources are\nlimited. In this paper, we explore this practical problem from two key\nperspectives: data and parameter efficiency, and propose a set of key\nguidelines for ultra-resolution adaptation termed URAE. For data\nefficiency, we theoretically and empirically demonstrate that synthetic data\ngenerated by some teacher models can significantly promote training\nconvergence. For parameter efficiency, we find that tuning minor components of\nthe weight matrices outperforms widely-used low-rank adapters when synthetic\ndata are unavailable, offering substantial performance gains while maintaining\nefficiency. Additionally, for models leveraging guidance distillation, such as\nFLUX, we show that disabling classifier-free guidance, i.e., setting\nthe guidance scale to 1 during adaptation, is crucial for satisfactory\nperformance. Extensive experiments validate that URAE achieves comparable\n2K-generation performance to state-of-the-art closed-source models like FLUX1.1\n[Pro] Ultra with only 3K samples and 2K iterations, while setting new\nbenchmarks for 4K-resolution generation. Codes are available\nhttps://github.com/Huage001/URAE{here}.",
      "upvotes": 5,
      "discussionId": "67dce4c50784200359ab25dc",
      "ai_keywords": [
        "text-to-image diffusion models",
        "high-resolution image generation",
        "training data",
        "computational resources",
        "data efficiency",
        "synthetic data",
        "teacher models",
        "training convergence",
        "parameter efficiency",
        "weight matrices",
        "low-rank adapters",
        "guidance distillation",
        "FLUX",
        "classifier-free guidance",
        "guidance scale",
        "2K-generation performance",
        "FLUX1.1",
        "4K-resolution generation"
      ]
    },
    "publishedAt": "2025-03-20T12:44:43.000Z",
    "title": "Ultra-Resolution Adaptation with Ease",
    "summary": "Text-to-image diffusion models have achieved remarkable progress in recent\nyears. However, training models for high-resolution image generation remains\nchallenging, particularly when training data and computational resources are\nlimited. In this paper, we explore this practical problem from two key\nperspectives: data and parameter efficiency, and propose a set of key\nguidelines for ultra-resolution adaptation termed URAE. For data\nefficiency, we theoretically and empirically demonstrate that synthetic data\ngenerated by some teacher models can significantly promote training\nconvergence. For parameter efficiency, we find that tuning minor components of\nthe weight matrices outperforms widely-used low-rank adapters when synthetic\ndata are unavailable, offering substantial performance gains while maintaining\nefficiency. Additionally, for models leveraging guidance distillation, such as\nFLUX, we show that disabling classifier-free guidance, i.e., setting\nthe guidance scale to 1 during adaptation, is crucial for satisfactory\nperformance. Extensive experiments validate that URAE achieves comparable\n2K-generation performance to state-of-the-art closed-source models like FLUX1.1\n[Pro] Ultra with only 3K samples and 2K iterations, while setting new\nbenchmarks for 4K-resolution generation. Codes are available\nhttps://github.com/Huage001/URAE{here}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16322.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6486fb33570a419f41a882e4",
      "avatarUrl": "/avatars/860a42074439a23c629cd23851ae4da6.svg",
      "fullname": "Ruonan Yu",
      "name": "roseannelexie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16188",
      "authors": [
        {
          "_id": "67dce5afbabeda89ca6071c3",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "67dce5afbabeda89ca6071c4",
          "name": "Shitian Zhao",
          "hidden": false
        },
        {
          "_id": "67dce5afbabeda89ca6071c5",
          "name": "Jike Zhong",
          "hidden": false
        },
        {
          "_id": "67dce5afbabeda89ca6071c6",
          "name": "Yuxiang Lai",
          "hidden": false
        },
        {
          "_id": "67dce5afbabeda89ca6071c7",
          "name": "Kaipeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T14:37:45.000Z",
      "submittedOnDailyAt": "2025-03-21T03:47:03.083Z",
      "title": "CLS-RL: Image Classification with Rule-Based Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "65f1713552c38a91e0a445e8",
        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
        "isPro": false,
        "fullname": "kaipeng",
        "user": "kpzhang996",
        "type": "user"
      },
      "summary": "Classification is a core task in machine learning. Recent research has shown\nthat although Multimodal Large Language Models (MLLMs) are initially poor at\nimage classification, fine-tuning them with an adequate amount of data can\nsignificantly enhance their performance, making them comparable to SOTA\nclassification models. However, acquiring large-scale labeled data is\nexpensive. In this paper, we explore few-shot MLLM classification fine-tuning.\nWe found that SFT can cause severe overfitting issues and may even degrade\nperformance over the zero-shot approach. To address this challenge, inspired by\nthe recent successes in rule-based reinforcement learning, we propose CLS-RL,\nwhich uses verifiable signals as reward to fine-tune MLLMs. We discovered that\nCLS-RL outperforms SFT in most datasets and has a much higher average accuracy\non both base-to-new and few-shot learning setting. Moreover, we observed a\nfree-lunch phenomenon for CLS-RL; when models are fine-tuned on a particular\ndataset, their performance on other distinct datasets may also improve over\nzero-shot models, even if those datasets differ in distribution and class\nnames. This suggests that RL-based methods effectively teach models the\nfundamentals of classification. Lastly, inspired by recent works in inference\ntime thinking, we re-examine the `thinking process' during fine-tuning, a\ncritical aspect of RL-based methods, in the context of visual classification.\nWe question whether such tasks require extensive thinking process during\nfine-tuning, proposing that this may actually detract from performance. Based\non this premise, we introduce the No-Thinking-CLS-RL method, which minimizes\nthinking processes during training by setting an equality accuracy reward. Our\nfindings indicate that, with much less fine-tuning time, No-Thinking-CLS-RL\nmethod achieves superior in-domain performance and generalization capabilities\nthan CLS-RL.",
      "upvotes": 5,
      "discussionId": "67dce5b1babeda89ca607241",
      "ai_keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "few-shot MLLM classification fine-tuning",
        "SFT",
        "overfitting issues",
        "zero-shot approach",
        "CLS-RL",
        "verifiable signals",
        "reward",
        "free-lunch phenomenon",
        "base-to-new",
        "few-shot learning",
        "RL-based methods",
        "inference time thinking",
        "thinking process",
        "No-Thinking-CLS-RL",
        "equality accuracy reward"
      ]
    },
    "publishedAt": "2025-03-20T10:37:45.000Z",
    "title": "CLS-RL: Image Classification with Rule-Based Reinforcement Learning",
    "summary": "Classification is a core task in machine learning. Recent research has shown\nthat although Multimodal Large Language Models (MLLMs) are initially poor at\nimage classification, fine-tuning them with an adequate amount of data can\nsignificantly enhance their performance, making them comparable to SOTA\nclassification models. However, acquiring large-scale labeled data is\nexpensive. In this paper, we explore few-shot MLLM classification fine-tuning.\nWe found that SFT can cause severe overfitting issues and may even degrade\nperformance over the zero-shot approach. To address this challenge, inspired by\nthe recent successes in rule-based reinforcement learning, we propose CLS-RL,\nwhich uses verifiable signals as reward to fine-tune MLLMs. We discovered that\nCLS-RL outperforms SFT in most datasets and has a much higher average accuracy\non both base-to-new and few-shot learning setting. Moreover, we observed a\nfree-lunch phenomenon for CLS-RL; when models are fine-tuned on a particular\ndataset, their performance on other distinct datasets may also improve over\nzero-shot models, even if those datasets differ in distribution and class\nnames. This suggests that RL-based methods effectively teach models the\nfundamentals of classification. Lastly, inspired by recent works in inference\ntime thinking, we re-examine the `thinking process' during fine-tuning, a\ncritical aspect of RL-based methods, in the context of visual classification.\nWe question whether such tasks require extensive thinking process during\nfine-tuning, proposing that this may actually detract from performance. Based\non this premise, we introduce the No-Thinking-CLS-RL method, which minimizes\nthinking processes during training by setting an equality accuracy reward. Our\nfindings indicate that, with much less fine-tuning time, No-Thinking-CLS-RL\nmethod achieves superior in-domain performance and generalization capabilities\nthan CLS-RL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16188.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f1713552c38a91e0a445e8",
      "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
      "fullname": "kaipeng",
      "name": "kpzhang996",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16421",
      "authors": [
        {
          "_id": "67dcd5913713a0e1da19bbe5",
          "name": "Quanhao Li",
          "hidden": false
        },
        {
          "_id": "67dcd5913713a0e1da19bbe6",
          "name": "Zhen Xing",
          "hidden": false
        },
        {
          "_id": "67dcd5913713a0e1da19bbe7",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "67dcd5913713a0e1da19bbe8",
          "name": "Hui Zhang",
          "hidden": false
        },
        {
          "_id": "67dcd5913713a0e1da19bbe9",
          "name": "Qi Dai",
          "hidden": false
        },
        {
          "_id": "67dcd5913713a0e1da19bbea",
          "name": "Zuxuan Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T17:59:42.000Z",
      "submittedOnDailyAt": "2025-03-21T01:28:06.140Z",
      "title": "MagicMotion: Controllable Video Generation with Dense-to-Sparse\n  Trajectory Guidance",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Recent advances in video generation have led to remarkable improvements in\nvisual quality and temporal coherence. Upon this, trajectory-controllable video\ngeneration has emerged to enable precise object motion control through\nexplicitly defined spatial paths. However, existing methods struggle with\ncomplex object movements and multi-object motion control, resulting in\nimprecise trajectory adherence, poor object consistency, and compromised visual\nquality. Furthermore, these methods only support trajectory control in a single\nformat, limiting their applicability in diverse scenarios. Additionally, there\nis no publicly available dataset or benchmark specifically tailored for\ntrajectory-controllable video generation, hindering robust training and\nsystematic evaluation. To address these challenges, we introduce MagicMotion, a\nnovel image-to-video generation framework that enables trajectory control\nthrough three levels of conditions from dense to sparse: masks, bounding boxes,\nand sparse boxes. Given an input image and trajectories, MagicMotion seamlessly\nanimates objects along defined trajectories while maintaining object\nconsistency and visual quality. Furthermore, we present MagicData, a\nlarge-scale trajectory-controlled video dataset, along with an automated\npipeline for annotation and filtering. We also introduce MagicBench, a\ncomprehensive benchmark that assesses both video quality and trajectory control\naccuracy across different numbers of objects. Extensive experiments demonstrate\nthat MagicMotion outperforms previous methods across various metrics. Our\nproject page are publicly available at\nhttps://quanhaol.github.io/magicmotion-site.",
      "upvotes": 4,
      "discussionId": "67dcd5953713a0e1da19bd51",
      "projectPage": "https://quanhaol.github.io/magicmotion-site",
      "ai_keywords": [
        "trajectory-controllable video generation",
        "dense conditions",
        "sparse conditions",
        "masks",
        "bounding boxes",
        "sparse boxes",
        "object consistency",
        "MagicMotion",
        "MagicData",
        "MagicBench"
      ]
    },
    "publishedAt": "2025-03-20T13:59:42.000Z",
    "title": "MagicMotion: Controllable Video Generation with Dense-to-Sparse\n  Trajectory Guidance",
    "summary": "Recent advances in video generation have led to remarkable improvements in\nvisual quality and temporal coherence. Upon this, trajectory-controllable video\ngeneration has emerged to enable precise object motion control through\nexplicitly defined spatial paths. However, existing methods struggle with\ncomplex object movements and multi-object motion control, resulting in\nimprecise trajectory adherence, poor object consistency, and compromised visual\nquality. Furthermore, these methods only support trajectory control in a single\nformat, limiting their applicability in diverse scenarios. Additionally, there\nis no publicly available dataset or benchmark specifically tailored for\ntrajectory-controllable video generation, hindering robust training and\nsystematic evaluation. To address these challenges, we introduce MagicMotion, a\nnovel image-to-video generation framework that enables trajectory control\nthrough three levels of conditions from dense to sparse: masks, bounding boxes,\nand sparse boxes. Given an input image and trajectories, MagicMotion seamlessly\nanimates objects along defined trajectories while maintaining object\nconsistency and visual quality. Furthermore, we present MagicData, a\nlarge-scale trajectory-controlled video dataset, along with an automated\npipeline for annotation and filtering. We also introduce MagicBench, a\ncomprehensive benchmark that assesses both video quality and trajectory control\naccuracy across different numbers of objects. Extensive experiments demonstrate\nthat MagicMotion outperforms previous methods across various metrics. Our\nproject page are publicly available at\nhttps://quanhaol.github.io/magicmotion-site.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16421.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6415
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16278",
      "authors": [
        {
          "_id": "67dcc98b54dcfdc1fd17d9b6",
          "name": "Shuqi Lu",
          "hidden": false
        },
        {
          "_id": "67dcc98b54dcfdc1fd17d9b7",
          "name": "Haowei Lin",
          "hidden": false
        },
        {
          "_id": "67dcc98b54dcfdc1fd17d9b8",
          "name": "Lin Yao",
          "hidden": false
        },
        {
          "_id": "67dcc98b54dcfdc1fd17d9b9",
          "name": "Zhifeng Gao",
          "hidden": false
        },
        {
          "_id": "67dcc98b54dcfdc1fd17d9ba",
          "name": "Xiaohong Ji",
          "hidden": false
        },
        {
          "_id": "67dcc98b54dcfdc1fd17d9bb",
          "name": "Weinan E",
          "hidden": false
        },
        {
          "_id": "67dcc98b54dcfdc1fd17d9bc",
          "name": "Linfeng Zhang",
          "hidden": false
        },
        {
          "_id": "67dcc98b54dcfdc1fd17d9bd",
          "name": "Guolin Ke",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T16:07:04.000Z",
      "submittedOnDailyAt": "2025-03-21T00:36:39.769Z",
      "title": "Uni-3DAR: Unified 3D Generation and Understanding via Autoregression on\n  Compressed Spatial Tokens",
      "submittedOnDailyBy": {
        "_id": "6348de0c62c668c7b48d83c9",
        "avatarUrl": "/avatars/7296ea9bb301e19c10926022959b2023.svg",
        "isPro": false,
        "fullname": "Guolin Ke",
        "user": "guolinke",
        "type": "user"
      },
      "summary": "Recent advancements in large language models and their multi-modal extensions\nhave demonstrated the effectiveness of unifying generation and understanding\nthrough autoregressive next-token prediction. However, despite the critical\nrole of 3D structural generation and understanding ({3D GU}) in AI for science,\nthese tasks have largely evolved independently, with autoregressive methods\nremaining underexplored. To bridge this gap, we introduce Uni-3DAR, a unified\nframework that seamlessly integrates {3D GU} tasks via autoregressive\nprediction. At its core, Uni-3DAR employs a novel hierarchical tokenization\nthat compresses 3D space using an octree, leveraging the inherent sparsity of\n3D structures. It then applies an additional tokenization for fine-grained\nstructural details, capturing key attributes such as atom types and precise\nspatial coordinates in microscopic 3D structures. We further propose two\noptimizations to enhance efficiency and effectiveness. The first is a two-level\nsubtree compression strategy, which reduces the octree token sequence by up to\n8x. The second is a masked next-token prediction mechanism tailored for\ndynamically varying token positions, significantly boosting model performance.\nBy combining these strategies, Uni-3DAR successfully unifies diverse {3D GU}\ntasks within a single autoregressive framework. Extensive experiments across\nmultiple microscopic {3D GU} tasks, including molecules, proteins, polymers,\nand crystals, validate its effectiveness and versatility. Notably, Uni-3DAR\nsurpasses previous state-of-the-art diffusion models by a substantial margin,\nachieving up to 256\\% relative improvement while delivering inference speeds up\nto 21.8x faster. The code is publicly available at\nhttps://github.com/dptech-corp/Uni-3DAR.",
      "upvotes": 4,
      "discussionId": "67dcc98c54dcfdc1fd17da0f",
      "githubRepo": "https://github.com/dptech-corp/Uni-3DAR",
      "ai_keywords": [
        "hierarchical tokenization",
        "octree",
        "two-level subtree compression strategy",
        "masked next-token prediction mechanism",
        "Uni-3DAR",
        "3D GU (3D generation and understanding)",
        "autoregressive prediction",
        "state-of-the-art diffusion models"
      ]
    },
    "publishedAt": "2025-03-20T12:07:04.000Z",
    "title": "Uni-3DAR: Unified 3D Generation and Understanding via Autoregression on\n  Compressed Spatial Tokens",
    "summary": "Recent advancements in large language models and their multi-modal extensions\nhave demonstrated the effectiveness of unifying generation and understanding\nthrough autoregressive next-token prediction. However, despite the critical\nrole of 3D structural generation and understanding ({3D GU}) in AI for science,\nthese tasks have largely evolved independently, with autoregressive methods\nremaining underexplored. To bridge this gap, we introduce Uni-3DAR, a unified\nframework that seamlessly integrates {3D GU} tasks via autoregressive\nprediction. At its core, Uni-3DAR employs a novel hierarchical tokenization\nthat compresses 3D space using an octree, leveraging the inherent sparsity of\n3D structures. It then applies an additional tokenization for fine-grained\nstructural details, capturing key attributes such as atom types and precise\nspatial coordinates in microscopic 3D structures. We further propose two\noptimizations to enhance efficiency and effectiveness. The first is a two-level\nsubtree compression strategy, which reduces the octree token sequence by up to\n8x. The second is a masked next-token prediction mechanism tailored for\ndynamically varying token positions, significantly boosting model performance.\nBy combining these strategies, Uni-3DAR successfully unifies diverse {3D GU}\ntasks within a single autoregressive framework. Extensive experiments across\nmultiple microscopic {3D GU} tasks, including molecules, proteins, polymers,\nand crystals, validate its effectiveness and versatility. Notably, Uni-3DAR\nsurpasses previous state-of-the-art diffusion models by a substantial margin,\nachieving up to 256\\% relative improvement while delivering inference speeds up\nto 21.8x faster. The code is publicly available at\nhttps://github.com/dptech-corp/Uni-3DAR.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16278.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6348de0c62c668c7b48d83c9",
      "avatarUrl": "/avatars/7296ea9bb301e19c10926022959b2023.svg",
      "fullname": "Guolin Ke",
      "name": "guolinke",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.15567",
      "authors": [
        {
          "_id": "67dcc734067589b43b19af7a",
          "name": "Yanchen Luo",
          "hidden": false
        },
        {
          "_id": "67dcc734067589b43b19af7b",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "67dcc734067589b43b19af7c",
          "name": "Yi Zhao",
          "hidden": false
        },
        {
          "_id": "67dcc734067589b43b19af7d",
          "name": "Sihang Li",
          "hidden": false
        },
        {
          "_id": "67dcc734067589b43b19af7e",
          "name": "Kenji Kawaguchi",
          "hidden": false
        },
        {
          "_id": "67dcc734067589b43b19af7f",
          "name": "Tat-Seng Chua",
          "hidden": false
        },
        {
          "_id": "67dcc734067589b43b19af80",
          "name": "Xiang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T08:56:13.000Z",
      "submittedOnDailyAt": "2025-03-21T00:33:21.187Z",
      "title": "Towards Unified Latent Space for 3D Molecular Latent Diffusion Modeling",
      "submittedOnDailyBy": {
        "_id": "64f04a28f3cd962c21726459",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/MOTc7SWbzc4jdJbMcWMcK.jpeg",
        "isPro": false,
        "fullname": "LuoYanchen",
        "user": "lyc0930",
        "type": "user"
      },
      "summary": "3D molecule generation is crucial for drug discovery and material science,\nrequiring models to process complex multi-modalities, including atom types,\nchemical bonds, and 3D coordinates. A key challenge is integrating these\nmodalities of different shapes while maintaining SE(3) equivariance for 3D\ncoordinates. To achieve this, existing approaches typically maintain separate\nlatent spaces for invariant and equivariant modalities, reducing efficiency in\nboth training and sampling. In this work, we propose Unified\nVariational Auto-Encoder for 3D Molecular Latent\nDiffusion Modeling (UAE-3D), a multi-modal VAE that compresses 3D\nmolecules into latent sequences from a unified latent space, while maintaining\nnear-zero reconstruction error. This unified latent space eliminates the\ncomplexities of handling multi-modality and equivariance when performing latent\ndiffusion modeling. We demonstrate this by employing the Diffusion\nTransformer--a general-purpose diffusion model without any molecular inductive\nbias--for latent generation. Extensive experiments on GEOM-Drugs and QM9\ndatasets demonstrate that our method significantly establishes new benchmarks\nin both de novo and conditional 3D molecule generation, achieving\nleading efficiency and quality.",
      "upvotes": 4,
      "discussionId": "67dcc735067589b43b19afd9",
      "ai_keywords": [
        "SE(3) equivariance",
        "latent spaces",
        "multi-modal VAE",
        "latent sequences",
        "unified latent space",
        "latent diffusion modeling",
        "Diffusion Transformer",
        "GEOM-Drugs dataset",
        "QM9 dataset",
        "de novo molecule generation",
        "conditional molecule generation"
      ]
    },
    "publishedAt": "2025-03-19T04:56:13.000Z",
    "title": "Towards Unified Latent Space for 3D Molecular Latent Diffusion Modeling",
    "summary": "3D molecule generation is crucial for drug discovery and material science,\nrequiring models to process complex multi-modalities, including atom types,\nchemical bonds, and 3D coordinates. A key challenge is integrating these\nmodalities of different shapes while maintaining SE(3) equivariance for 3D\ncoordinates. To achieve this, existing approaches typically maintain separate\nlatent spaces for invariant and equivariant modalities, reducing efficiency in\nboth training and sampling. In this work, we propose Unified\nVariational Auto-Encoder for 3D Molecular Latent\nDiffusion Modeling (UAE-3D), a multi-modal VAE that compresses 3D\nmolecules into latent sequences from a unified latent space, while maintaining\nnear-zero reconstruction error. This unified latent space eliminates the\ncomplexities of handling multi-modality and equivariance when performing latent\ndiffusion modeling. We demonstrate this by employing the Diffusion\nTransformer--a general-purpose diffusion model without any molecular inductive\nbias--for latent generation. Extensive experiments on GEOM-Drugs and QM9\ndatasets demonstrate that our method significantly establishes new benchmarks\nin both de novo and conditional 3D molecule generation, achieving\nleading efficiency and quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15567.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f04a28f3cd962c21726459",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/MOTc7SWbzc4jdJbMcWMcK.jpeg",
      "fullname": "LuoYanchen",
      "name": "lyc0930",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16428",
      "authors": [
        {
          "_id": "67dcd7a53c21e084fe58c3a8",
          "name": "Ruyi Xu",
          "hidden": false
        },
        {
          "_id": "67dcd7a53c21e084fe58c3a9",
          "name": "Guangxuan Xiao",
          "hidden": false
        },
        {
          "_id": "67dcd7a53c21e084fe58c3aa",
          "user": {
            "_id": "63797f727df2fefdcaf3ff7e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668906853549-noauth.jpeg",
            "isPro": false,
            "fullname": "Song",
            "user": "songhan",
            "type": "user"
          },
          "name": "Haofeng Huang",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-21T03:06:14.875Z",
          "hidden": false
        },
        {
          "_id": "67dcd7a53c21e084fe58c3ab",
          "name": "Junxian Guo",
          "hidden": false
        },
        {
          "_id": "67dcd7a53c21e084fe58c3ac",
          "name": "Song Han",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T17:59:58.000Z",
      "submittedOnDailyAt": "2025-03-21T01:36:33.593Z",
      "title": "XAttention: Block Sparse Attention with Antidiagonal Scoring",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Long-Context Transformer Models (LCTMs) are vital for real-world applications\nbut suffer high computational costs due to attention's quadratic complexity.\nBlock-sparse attention mitigates this by focusing computation on critical\nregions, yet existing methods struggle with balancing accuracy and efficiency\ndue to costly block importance measurements. In this paper, we introduce\nXAttention, a plug-and-play framework that dramatically accelerates\nlong-context inference in Transformers models using sparse attention.\nXAttention's key innovation is the insight that the sum of antidiagonal values\n(i.e., from the lower-left to upper-right) in the attention matrix provides a\npowerful proxy for block importance. This allows for precise identification and\npruning of non-essential blocks, resulting in high sparsity and dramatically\naccelerated inference. Across comprehensive evaluations on demanding\nlong-context benchmarks-including RULER and LongBench for language, VideoMME\nfor video understanding, and VBench for video generation. XAttention achieves\naccuracy comparable to full attention while delivering substantial\ncomputational gains. We demonstrate up to 13.5x acceleration in attention\ncomputation. These results underscore XAttention's ability to unlock the\npractical potential of block sparse attention, paving the way for scalable and\nefficient deployment of LCTMs in real-world applications. Code is available at\nhttps://github.com/mit-han-lab/x-attention.",
      "upvotes": 3,
      "discussionId": "67dcd7a63c21e084fe58c422",
      "ai_keywords": [
        "Long-Context Transformer Models (LCTMs)",
        "attention's quadratic complexity",
        "block-sparse attention",
        "block importance",
        "XAttention",
        "sparse attention",
        "attention matrix",
        "antidiagonal values",
        "block importance proxy",
        "precision identification",
        "block pruning",
        "high sparsity",
        "inference acceleration",
        "RULER benchmark",
        "LongBench benchmark",
        "VideoMME benchmark",
        "VBench benchmark",
        "video understanding",
        "video generation"
      ]
    },
    "publishedAt": "2025-03-20T13:59:58.000Z",
    "title": "XAttention: Block Sparse Attention with Antidiagonal Scoring",
    "summary": "Long-Context Transformer Models (LCTMs) are vital for real-world applications\nbut suffer high computational costs due to attention's quadratic complexity.\nBlock-sparse attention mitigates this by focusing computation on critical\nregions, yet existing methods struggle with balancing accuracy and efficiency\ndue to costly block importance measurements. In this paper, we introduce\nXAttention, a plug-and-play framework that dramatically accelerates\nlong-context inference in Transformers models using sparse attention.\nXAttention's key innovation is the insight that the sum of antidiagonal values\n(i.e., from the lower-left to upper-right) in the attention matrix provides a\npowerful proxy for block importance. This allows for precise identification and\npruning of non-essential blocks, resulting in high sparsity and dramatically\naccelerated inference. Across comprehensive evaluations on demanding\nlong-context benchmarks-including RULER and LongBench for language, VideoMME\nfor video understanding, and VBench for video generation. XAttention achieves\naccuracy comparable to full attention while delivering substantial\ncomputational gains. We demonstrate up to 13.5x acceleration in attention\ncomputation. These results underscore XAttention's ability to unlock the\npractical potential of block sparse attention, paving the way for scalable and\nefficient deployment of LCTMs in real-world applications. Code is available at\nhttps://github.com/mit-han-lab/x-attention.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16428.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6415
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.15851",
      "authors": [
        {
          "_id": "67dcff844aa37abf77ae7338",
          "name": "Zhou Zhenglin",
          "hidden": false
        },
        {
          "_id": "67dcff844aa37abf77ae7339",
          "name": "Ma Fan",
          "hidden": false
        },
        {
          "_id": "67dcff844aa37abf77ae733a",
          "name": "Fan Hehe",
          "hidden": false
        },
        {
          "_id": "67dcff844aa37abf77ae733b",
          "name": "Chua Tat-Seng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T05:07:46.000Z",
      "submittedOnDailyAt": "2025-03-21T04:27:49.029Z",
      "title": "Zero-1-to-A: Zero-Shot One Image to Animatable Head Avatars Using Video\n  Diffusion",
      "submittedOnDailyBy": {
        "_id": "6425318d175bd2952281065e",
        "avatarUrl": "/avatars/37deb6ceb1552dece43a1c8c13c1c871.svg",
        "isPro": false,
        "fullname": "ZhenglinZhou",
        "user": "zhenglin",
        "type": "user"
      },
      "summary": "Animatable head avatar generation typically requires extensive data for\ntraining. To reduce the data requirements, a natural solution is to leverage\nexisting data-free static avatar generation methods, such as pre-trained\ndiffusion models with score distillation sampling (SDS), which align avatars\nwith pseudo ground-truth outputs from the diffusion model. However, directly\ndistilling 4D avatars from video diffusion often leads to over-smooth results\ndue to spatial and temporal inconsistencies in the generated video. To address\nthis issue, we propose Zero-1-to-A, a robust method that synthesizes a spatial\nand temporal consistency dataset for 4D avatar reconstruction using the video\ndiffusion model. Specifically, Zero-1-to-A iteratively constructs video\ndatasets and optimizes animatable avatars in a progressive manner, ensuring\nthat avatar quality increases smoothly and consistently throughout the learning\nprocess. This progressive learning involves two stages: (1) Spatial Consistency\nLearning fixes expressions and learns from front-to-side views, and (2)\nTemporal Consistency Learning fixes views and learns from relaxed to\nexaggerated expressions, generating 4D avatars in a simple-to-complex manner.\nExtensive experiments demonstrate that Zero-1-to-A improves fidelity, animation\nquality, and rendering speed compared to existing diffusion-based methods,\nproviding a solution for lifelike avatar creation. Code is publicly available\nat: https://github.com/ZhenglinZhou/Zero-1-to-A.",
      "upvotes": 3,
      "discussionId": "67dcff884aa37abf77ae7415",
      "ai_keywords": [
        "diffusion models",
        "score distillation sampling (SDS)",
        "pseudo ground-truth outputs",
        "video diffusion",
        "spatial consistency",
        "temporal consistency",
        "Zero-1-to-A",
        "front-to-side views",
        "progressive learning",
        "spatial consistency learning",
        "temporal consistency learning",
        "4D avatars",
        "avatar quality",
        "fidelity",
        "animation quality",
        "rendering speed"
      ]
    },
    "publishedAt": "2025-03-20T01:07:46.000Z",
    "title": "Zero-1-to-A: Zero-Shot One Image to Animatable Head Avatars Using Video\n  Diffusion",
    "summary": "Animatable head avatar generation typically requires extensive data for\ntraining. To reduce the data requirements, a natural solution is to leverage\nexisting data-free static avatar generation methods, such as pre-trained\ndiffusion models with score distillation sampling (SDS), which align avatars\nwith pseudo ground-truth outputs from the diffusion model. However, directly\ndistilling 4D avatars from video diffusion often leads to over-smooth results\ndue to spatial and temporal inconsistencies in the generated video. To address\nthis issue, we propose Zero-1-to-A, a robust method that synthesizes a spatial\nand temporal consistency dataset for 4D avatar reconstruction using the video\ndiffusion model. Specifically, Zero-1-to-A iteratively constructs video\ndatasets and optimizes animatable avatars in a progressive manner, ensuring\nthat avatar quality increases smoothly and consistently throughout the learning\nprocess. This progressive learning involves two stages: (1) Spatial Consistency\nLearning fixes expressions and learns from front-to-side views, and (2)\nTemporal Consistency Learning fixes views and learns from relaxed to\nexaggerated expressions, generating 4D avatars in a simple-to-complex manner.\nExtensive experiments demonstrate that Zero-1-to-A improves fidelity, animation\nquality, and rendering speed compared to existing diffusion-based methods,\nproviding a solution for lifelike avatar creation. Code is publicly available\nat: https://github.com/ZhenglinZhou/Zero-1-to-A.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15851.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6425318d175bd2952281065e",
      "avatarUrl": "/avatars/37deb6ceb1552dece43a1c8c13c1c871.svg",
      "fullname": "ZhenglinZhou",
      "name": "zhenglin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16252",
      "authors": [
        {
          "_id": "67dcc7b29c17514cb9815abd",
          "name": "Zhaowei Liu",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815abe",
          "name": "Xin Guo",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815abf",
          "name": "Fangqi Lou",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815ac0",
          "name": "Lingfeng Zeng",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815ac1",
          "name": "Jinyi Niu",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815ac2",
          "name": "Zixuan Wang",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815ac3",
          "name": "Jiajie Xu",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815ac4",
          "name": "Weige Cai",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815ac5",
          "name": "Ziwei Yang",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815ac6",
          "name": "Xueqian Zhao",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815ac7",
          "name": "Chao Li",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815ac8",
          "name": "Sheng Xu",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815ac9",
          "name": "Dezhi Chen",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815aca",
          "name": "Yun Chen",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815acb",
          "name": "Zuo Bai",
          "hidden": false
        },
        {
          "_id": "67dcc7b29c17514cb9815acc",
          "name": "Liwen Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T15:46:18.000Z",
      "submittedOnDailyAt": "2025-03-21T00:54:01.337Z",
      "title": "Fin-R1: A Large Language Model for Financial Reasoning through\n  Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "60f1abe7544c2adfd699860c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
        "isPro": false,
        "fullname": "AK",
        "user": "akhaliq",
        "type": "user"
      },
      "summary": "Reasoning large language models are rapidly evolving across various domains.\nHowever, their capabilities in handling complex financial tasks still require\nin-depth exploration. In this paper, we introduce Fin-R1, a reasoning large\nlanguage model specifically designed for the financial sector. Fin-R1 is built\nusing a two-stage architecture, leveraging a financial reasoning dataset\ndistilled and processed based on DeepSeek-R1. Through supervised fine-tuning\n(SFT) and reinforcement learning (RL) training, it demonstrates performance\nclose to DeepSeek-R1 with a parameter size of 7 billion across a range of\nfinancial reasoning tasks. It achieves the state-of-the-art (SOTA) in the FinQA\nand ConvFinQA tasks between those LLMs in our evaluation, surpassing larger\nmodels in other tasks as well. Fin-R1 showcases strong reasoning and\ndecision-making capabilities, providing solutions to various problems\nencountered in the financial domain. Our code is available at\nhttps://github.com/SUFE-AIFLM-Lab/Fin-R1.",
      "upvotes": 2,
      "discussionId": "67dcc7b69c17514cb9815c1d",
      "githubRepo": "https://github.com/SUFE-AIFLM-Lab/Fin-R1",
      "ai_keywords": [
        "reasoning large language models",
        "two-stage architecture",
        "financial reasoning dataset",
        "DeepSeek-R1",
        "supervised fine-tuning (SFT)",
        "reinforcement learning (RL)",
        "state-of-the-art (SOTA)",
        "FinQA",
        "ConvFinQA"
      ]
    },
    "publishedAt": "2025-03-20T11:46:18.000Z",
    "title": "Fin-R1: A Large Language Model for Financial Reasoning through\n  Reinforcement Learning",
    "summary": "Reasoning large language models are rapidly evolving across various domains.\nHowever, their capabilities in handling complex financial tasks still require\nin-depth exploration. In this paper, we introduce Fin-R1, a reasoning large\nlanguage model specifically designed for the financial sector. Fin-R1 is built\nusing a two-stage architecture, leveraging a financial reasoning dataset\ndistilled and processed based on DeepSeek-R1. Through supervised fine-tuning\n(SFT) and reinforcement learning (RL) training, it demonstrates performance\nclose to DeepSeek-R1 with a parameter size of 7 billion across a range of\nfinancial reasoning tasks. It achieves the state-of-the-art (SOTA) in the FinQA\nand ConvFinQA tasks between those LLMs in our evaluation, surpassing larger\nmodels in other tasks as well. Fin-R1 showcases strong reasoning and\ndecision-making capabilities, providing solutions to various problems\nencountered in the financial domain. Our code is available at\nhttps://github.com/SUFE-AIFLM-Lab/Fin-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16252.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isHfAdmin": true,
      "isMod": false,
      "followerCount": 6415
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16194",
      "authors": [
        {
          "_id": "67dcf6375fd14aedd3005237",
          "name": "Ziyao Guo",
          "hidden": false
        },
        {
          "_id": "67dcf6375fd14aedd3005238",
          "name": "Kaipeng Zhang",
          "hidden": false
        },
        {
          "_id": "67dcf6375fd14aedd3005239",
          "name": "Michael Qizhe Shieh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T14:41:29.000Z",
      "submittedOnDailyAt": "2025-03-21T03:46:49.486Z",
      "title": "Improving Autoregressive Image Generation through Coarse-to-Fine Token\n  Prediction",
      "submittedOnDailyBy": {
        "_id": "65f1713552c38a91e0a445e8",
        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
        "isPro": false,
        "fullname": "kaipeng",
        "user": "kpzhang996",
        "type": "user"
      },
      "summary": "Autoregressive models have shown remarkable success in image generation by\nadapting sequential prediction techniques from language modeling. However,\napplying these approaches to images requires discretizing continuous pixel data\nthrough vector quantization methods like VQ-VAE. To alleviate the quantization\nerrors that existed in VQ-VAE, recent works tend to use larger codebooks.\nHowever, this will accordingly expand vocabulary size, complicating the\nautoregressive modeling task. This paper aims to find a way to enjoy the\nbenefits of large codebooks without making autoregressive modeling more\ndifficult. Through empirical investigation, we discover that tokens with\nsimilar codeword representations produce similar effects on the final generated\nimage, revealing significant redundancy in large codebooks. Based on this\ninsight, we propose to predict tokens from coarse to fine (CTF), realized by\nassigning the same coarse label for similar tokens. Our framework consists of\ntwo stages: (1) an autoregressive model that sequentially predicts coarse\nlabels for each token in the sequence, and (2) an auxiliary model that\nsimultaneously predicts fine-grained labels for all tokens conditioned on their\ncoarse labels. Experiments on ImageNet demonstrate our method's superior\nperformance, achieving an average improvement of 59 points in Inception Score\ncompared to baselines. Notably, despite adding an inference step, our approach\nachieves faster sampling speeds.",
      "upvotes": 2,
      "discussionId": "67dcf6375fd14aedd300527b",
      "ai_keywords": [
        "autoregressive models",
        "image generation",
        "sequential prediction",
        "language modeling",
        "VQ-VAE",
        "vector quantization",
        "codebooks",
        "token",
        "codeword representations",
        "coarse to fine (CTF)",
        "inference step",
        "Inception Score",
        "sampling speeds"
      ]
    },
    "publishedAt": "2025-03-20T10:41:29.000Z",
    "title": "Improving Autoregressive Image Generation through Coarse-to-Fine Token\n  Prediction",
    "summary": "Autoregressive models have shown remarkable success in image generation by\nadapting sequential prediction techniques from language modeling. However,\napplying these approaches to images requires discretizing continuous pixel data\nthrough vector quantization methods like VQ-VAE. To alleviate the quantization\nerrors that existed in VQ-VAE, recent works tend to use larger codebooks.\nHowever, this will accordingly expand vocabulary size, complicating the\nautoregressive modeling task. This paper aims to find a way to enjoy the\nbenefits of large codebooks without making autoregressive modeling more\ndifficult. Through empirical investigation, we discover that tokens with\nsimilar codeword representations produce similar effects on the final generated\nimage, revealing significant redundancy in large codebooks. Based on this\ninsight, we propose to predict tokens from coarse to fine (CTF), realized by\nassigning the same coarse label for similar tokens. Our framework consists of\ntwo stages: (1) an autoregressive model that sequentially predicts coarse\nlabels for each token in the sequence, and (2) an auxiliary model that\nsimultaneously predicts fine-grained labels for all tokens conditioned on their\ncoarse labels. Experiments on ImageNet demonstrate our method's superior\nperformance, achieving an average improvement of 59 points in Inception Score\ncompared to baselines. Notably, despite adding an inference step, our approach\nachieves faster sampling speeds.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16194.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f1713552c38a91e0a445e8",
      "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
      "fullname": "kaipeng",
      "name": "kpzhang996",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16031",
      "authors": [
        {
          "_id": "67dcc5e41f94b594ef4c0312",
          "user": {
            "_id": "651692d718f3a57f869a5a0a",
            "avatarUrl": "/avatars/d5fe48de11e46675b05e1e2e1cf3505c.svg",
            "isPro": false,
            "fullname": "Sai Kartheek Reddy",
            "user": "UVSKKR",
            "type": "user"
          },
          "name": "Sai Kartheek Reddy Kasu",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-21T01:51:18.039Z",
          "hidden": false
        },
        {
          "_id": "67dcc5e41f94b594ef4c0313",
          "name": "Shankar Biradar",
          "hidden": false
        },
        {
          "_id": "67dcc5e41f94b594ef4c0314",
          "name": "Sunil Saumya",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T10:58:02.000Z",
      "submittedOnDailyAt": "2025-03-21T00:28:13.365Z",
      "title": "Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging\n  Fabricated Claims with Humorous Content",
      "submittedOnDailyBy": {
        "_id": "651692d718f3a57f869a5a0a",
        "avatarUrl": "/avatars/d5fe48de11e46675b05e1e2e1cf3505c.svg",
        "isPro": false,
        "fullname": "Sai Kartheek Reddy",
        "user": "UVSKKR",
        "type": "user"
      },
      "summary": "This paper presents the Deceptive Humor Dataset (DHD), a novel resource for\nstudying humor derived from fabricated claims and misinformation. In an era of\nrampant misinformation, understanding how humor intertwines with deception is\nessential. DHD consists of humor-infused comments generated from false\nnarratives, incorporating fabricated claims and manipulated information using\nthe ChatGPT-4o model. Each instance is labeled with a Satire Level, ranging\nfrom 1 for subtle satire to 3 for high-level satire and classified into five\ndistinct Humor Categories: Dark Humor, Irony, Social Commentary, Wordplay, and\nAbsurdity. The dataset spans multiple languages including English, Telugu,\nHindi, Kannada, Tamil, and their code-mixed variants (Te-En, Hi-En, Ka-En,\nTa-En), making it a valuable multilingual benchmark. By introducing DHD, we\nestablish a structured foundation for analyzing humor in deceptive contexts,\npaving the way for a new research direction that explores how humor not only\ninteracts with misinformation but also influences its perception and spread. We\nestablish strong baselines for the proposed dataset, providing a foundation for\nfuture research to benchmark and advance deceptive humor detection models.",
      "upvotes": 2,
      "discussionId": "67dcc5e41f94b594ef4c0353"
    },
    "publishedAt": "2025-03-20T06:58:02.000Z",
    "title": "Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging\n  Fabricated Claims with Humorous Content",
    "summary": "This paper presents the Deceptive Humor Dataset (DHD), a novel resource for\nstudying humor derived from fabricated claims and misinformation. In an era of\nrampant misinformation, understanding how humor intertwines with deception is\nessential. DHD consists of humor-infused comments generated from false\nnarratives, incorporating fabricated claims and manipulated information using\nthe ChatGPT-4o model. Each instance is labeled with a Satire Level, ranging\nfrom 1 for subtle satire to 3 for high-level satire and classified into five\ndistinct Humor Categories: Dark Humor, Irony, Social Commentary, Wordplay, and\nAbsurdity. The dataset spans multiple languages including English, Telugu,\nHindi, Kannada, Tamil, and their code-mixed variants (Te-En, Hi-En, Ka-En,\nTa-En), making it a valuable multilingual benchmark. By introducing DHD, we\nestablish a structured foundation for analyzing humor in deceptive contexts,\npaving the way for a new research direction that explores how humor not only\ninteracts with misinformation but also influences its perception and spread. We\nestablish strong baselines for the proposed dataset, providing a foundation for\nfuture research to benchmark and advance deceptive humor detection models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16031.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "651692d718f3a57f869a5a0a",
      "avatarUrl": "/avatars/d5fe48de11e46675b05e1e2e1cf3505c.svg",
      "fullname": "Sai Kartheek Reddy",
      "name": "UVSKKR",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.12689",
      "authors": [
        {
          "_id": "67dcc2e10df3501c657ef478",
          "name": "Hengjia Li",
          "hidden": false
        },
        {
          "_id": "67dcc2e10df3501c657ef479",
          "name": "Lifan Jiang",
          "hidden": false
        },
        {
          "_id": "67dcc2e10df3501c657ef47a",
          "name": "Xi Xiao",
          "hidden": false
        },
        {
          "_id": "67dcc2e10df3501c657ef47b",
          "name": "Tianyang Wang",
          "hidden": false
        },
        {
          "_id": "67dcc2e10df3501c657ef47c",
          "name": "Hongwei Yi",
          "hidden": false
        },
        {
          "_id": "67dcc2e10df3501c657ef47d",
          "name": "Boxi Wu",
          "hidden": false
        },
        {
          "_id": "67dcc2e10df3501c657ef47e",
          "name": "Deng Cai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-16T23:15:09.000Z",
      "submittedOnDailyAt": "2025-03-21T02:25:48.893Z",
      "title": "MagicID: Hybrid Preference Optimization for ID-Consistent and\n  Dynamic-Preserved Video Customization",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "Video identity customization seeks to produce high-fidelity videos that\nmaintain consistent identity and exhibit significant dynamics based on users'\nreference images. However, existing approaches face two key challenges:\nidentity degradation over extended video length and reduced dynamics during\ntraining, primarily due to their reliance on traditional self-reconstruction\ntraining with static images. To address these issues, we introduce\nMagicID, a novel framework designed to directly promote the\ngeneration of identity-consistent and dynamically rich videos tailored to user\npreferences. Specifically, we propose constructing pairwise preference video\ndata with explicit identity and dynamic rewards for preference learning,\ninstead of sticking to the traditional self-reconstruction. To address the\nconstraints of customized preference data, we introduce a hybrid sampling\nstrategy. This approach first prioritizes identity preservation by leveraging\nstatic videos derived from reference images, then enhances dynamic motion\nquality in the generated videos using a Frontier-based sampling method. By\nutilizing these hybrid preference pairs, we optimize the model to align with\nthe reward differences between pairs of customized preferences. Extensive\nexperiments show that MagicID successfully achieves consistent identity and\nnatural dynamics, surpassing existing methods across various metrics.",
      "upvotes": 2,
      "discussionId": "67dcc2e50df3501c657ef56b",
      "projectPage": "https://echopluto.github.io/MagicID-project/",
      "githubRepo": "https://github.com/EchoPluto/MagicID",
      "ai_keywords": [
        "pairwise preference video data",
        "identity and dynamic rewards",
        "preference learning",
        "hybrid sampling strategy",
        "static videos",
        "Frontier-based sampling method",
        "reward differences",
        "customized preferences",
        "identity-preserving",
        "dynamic motion quality",
        "high-fidelity videos",
        "consistent identity",
        "natural dynamics"
      ]
    },
    "publishedAt": "2025-03-16T19:15:09.000Z",
    "title": "MagicID: Hybrid Preference Optimization for ID-Consistent and\n  Dynamic-Preserved Video Customization",
    "summary": "Video identity customization seeks to produce high-fidelity videos that\nmaintain consistent identity and exhibit significant dynamics based on users'\nreference images. However, existing approaches face two key challenges:\nidentity degradation over extended video length and reduced dynamics during\ntraining, primarily due to their reliance on traditional self-reconstruction\ntraining with static images. To address these issues, we introduce\nMagicID, a novel framework designed to directly promote the\ngeneration of identity-consistent and dynamically rich videos tailored to user\npreferences. Specifically, we propose constructing pairwise preference video\ndata with explicit identity and dynamic rewards for preference learning,\ninstead of sticking to the traditional self-reconstruction. To address the\nconstraints of customized preference data, we introduce a hybrid sampling\nstrategy. This approach first prioritizes identity preservation by leveraging\nstatic videos derived from reference images, then enhances dynamic motion\nquality in the generated videos using a Frontier-based sampling method. By\nutilizing these hybrid preference pairs, we optimize the model to align with\nthe reward differences between pairs of customized preferences. Extensive\nexperiments show that MagicID successfully achieves consistent identity and\nnatural dynamics, surpassing existing methods across various metrics.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.12689.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 35
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.16055",
      "authors": [
        {
          "_id": "67dd0d7c68dc6463747e6cac",
          "name": "Abdelrahman Elsayed",
          "hidden": false
        },
        {
          "_id": "67dd0d7c68dc6463747e6cad",
          "name": "Sarim Hashmi",
          "hidden": false
        },
        {
          "_id": "67dd0d7c68dc6463747e6cae",
          "name": "Mohammed Elseiagy",
          "hidden": false
        },
        {
          "_id": "67dd0d7c68dc6463747e6caf",
          "name": "Hu Wang",
          "hidden": false
        },
        {
          "_id": "67dd0d7c68dc6463747e6cb0",
          "name": "Mohammad Yaqub",
          "hidden": false
        },
        {
          "_id": "67dd0d7c68dc6463747e6cb1",
          "name": "Ibrahim Almakky",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-20T11:42:41.000Z",
      "submittedOnDailyAt": "2025-03-21T05:26:30.501Z",
      "title": "SALT: Singular Value Adaptation with Low-Rank Transformation",
      "submittedOnDailyBy": {
        "_id": "62676a94dacab364889bb36c",
        "avatarUrl": "/avatars/0ead41b44957eb30564ea685ed22781a.svg",
        "isPro": false,
        "fullname": "SARIM HASHMI",
        "user": "Sarim-Hash",
        "type": "user"
      },
      "summary": "The complex nature of medical image segmentation calls for models that are\nspecifically designed to capture detailed, domain-specific features. Large\nfoundation models offer considerable flexibility, yet the cost of fine-tuning\nthese models remains a significant barrier. Parameter-Efficient Fine-Tuning\n(PEFT) methods, such as Low-Rank Adaptation (LoRA), efficiently update model\nweights with low-rank matrices but may suffer from underfitting when the chosen\nrank is insufficient to capture domain-specific nuances. Conversely, full-rank\nSingular Value Decomposition (SVD) based methods provide comprehensive updates\nby modifying all singular values, yet they often lack flexibility and exhibit\nvariable performance across datasets. We propose SALT (Singular Value\nAdaptation with Low-Rank Transformation), a method that selectively adapts the\nmost influential singular values using trainable scale and shift parameters\nwhile complementing this with a low-rank update for the remaining subspace.\nThis hybrid approach harnesses the advantages of both LoRA and SVD, enabling\neffective adaptation without relying on increasing model size or depth.\nEvaluated on 5 challenging medical datasets, ranging from as few as 20 samples\nto 1000, SALT outperforms state-of-the-art PEFT (LoRA and SVD) by 2% to 5% in\nDice with only 3.9% trainable parameters, demonstrating robust adaptation even\nin low-resource settings. The code for SALT is available at:\nhttps://github.com/BioMedIA-MBZUAI/SALT",
      "upvotes": 1,
      "discussionId": "67dd0d7e68dc6463747e6d03",
      "ai_keywords": [
        "SALT",
        "Singular Value Adaptation with Low-Rank Transformation",
        "Low-Rank Adaptation",
        "LoRA",
        "Singular Value Decomposition",
        "SVD",
        "Dice",
        "Parameter-Efficient Fine-Tuning",
        "trainable parameters"
      ]
    },
    "publishedAt": "2025-03-20T07:42:41.000Z",
    "title": "SALT: Singular Value Adaptation with Low-Rank Transformation",
    "summary": "The complex nature of medical image segmentation calls for models that are\nspecifically designed to capture detailed, domain-specific features. Large\nfoundation models offer considerable flexibility, yet the cost of fine-tuning\nthese models remains a significant barrier. Parameter-Efficient Fine-Tuning\n(PEFT) methods, such as Low-Rank Adaptation (LoRA), efficiently update model\nweights with low-rank matrices but may suffer from underfitting when the chosen\nrank is insufficient to capture domain-specific nuances. Conversely, full-rank\nSingular Value Decomposition (SVD) based methods provide comprehensive updates\nby modifying all singular values, yet they often lack flexibility and exhibit\nvariable performance across datasets. We propose SALT (Singular Value\nAdaptation with Low-Rank Transformation), a method that selectively adapts the\nmost influential singular values using trainable scale and shift parameters\nwhile complementing this with a low-rank update for the remaining subspace.\nThis hybrid approach harnesses the advantages of both LoRA and SVD, enabling\neffective adaptation without relying on increasing model size or depth.\nEvaluated on 5 challenging medical datasets, ranging from as few as 20 samples\nto 1000, SALT outperforms state-of-the-art PEFT (LoRA and SVD) by 2% to 5% in\nDice with only 3.9% trainable parameters, demonstrating robust adaptation even\nin low-resource settings. The code for SALT is available at:\nhttps://github.com/BioMedIA-MBZUAI/SALT",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.16055.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62676a94dacab364889bb36c",
      "avatarUrl": "/avatars/0ead41b44957eb30564ea685ed22781a.svg",
      "fullname": "SARIM HASHMI",
      "name": "Sarim-Hash",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.15451",
      "authors": [
        {
          "_id": "67dd03fb2672aa3643252e8c",
          "user": {
            "_id": "65220fedc709aaca9aa63061",
            "avatarUrl": "/avatars/36e56199fa8c98ff3430636567a0ed62.svg",
            "isPro": false,
            "fullname": "Lixing Xiao",
            "user": "lxxiao",
            "type": "user"
          },
          "name": "Lixing Xiao",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-21T06:19:01.806Z",
          "hidden": false
        },
        {
          "_id": "67dd03fb2672aa3643252e8d",
          "name": "Shunlin Lu",
          "hidden": false
        },
        {
          "_id": "67dd03fb2672aa3643252e8e",
          "name": "Huaijin Pi",
          "hidden": false
        },
        {
          "_id": "67dd03fb2672aa3643252e8f",
          "name": "Ke Fan",
          "hidden": false
        },
        {
          "_id": "67dd03fb2672aa3643252e90",
          "name": "Liang Pan",
          "hidden": false
        },
        {
          "_id": "67dd03fb2672aa3643252e91",
          "name": "Yueer Zhou",
          "hidden": false
        },
        {
          "_id": "67dd03fb2672aa3643252e92",
          "name": "Ziyong Feng",
          "hidden": false
        },
        {
          "_id": "67dd03fb2672aa3643252e93",
          "name": "Xiaowei Zhou",
          "hidden": false
        },
        {
          "_id": "67dd03fb2672aa3643252e94",
          "name": "Sida Peng",
          "hidden": false
        },
        {
          "_id": "67dd03fb2672aa3643252e95",
          "name": "Jingbo Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-19T17:32:24.000Z",
      "submittedOnDailyAt": "2025-03-21T05:04:41.437Z",
      "title": "MotionStreamer: Streaming Motion Generation via Diffusion-based\n  Autoregressive Model in Causal Latent Space",
      "submittedOnDailyBy": {
        "_id": "65220fedc709aaca9aa63061",
        "avatarUrl": "/avatars/36e56199fa8c98ff3430636567a0ed62.svg",
        "isPro": false,
        "fullname": "Lixing Xiao",
        "user": "lxxiao",
        "type": "user"
      },
      "summary": "This paper addresses the challenge of text-conditioned streaming motion\ngeneration, which requires us to predict the next-step human pose based on\nvariable-length historical motions and incoming texts. Existing methods\nstruggle to achieve streaming motion generation, e.g., diffusion models are\nconstrained by pre-defined motion lengths, while GPT-based methods suffer from\ndelayed response and error accumulation problem due to discretized non-causal\ntokenization. To solve these problems, we propose MotionStreamer, a novel\nframework that incorporates a continuous causal latent space into a\nprobabilistic autoregressive model. The continuous latents mitigate information\nloss caused by discretization and effectively reduce error accumulation during\nlong-term autoregressive generation. In addition, by establishing temporal\ncausal dependencies between current and historical motion latents, our model\nfully utilizes the available information to achieve accurate online motion\ndecoding. Experiments show that our method outperforms existing approaches\nwhile offering more applications, including multi-round generation, long-term\ngeneration, and dynamic motion composition. Project Page:\nhttps://zju3dv.github.io/MotionStreamer/",
      "upvotes": 1,
      "discussionId": "67dd03fd2672aa3643252f2a",
      "projectPage": "https://zju3dv.github.io/MotionStreamer/",
      "ai_keywords": [
        "diffusion models",
        "streaming motion generation",
        "human pose prediction",
        "historical motions",
        "incoming texts",
        "GPT-based methods",
        "continuous causal latent space",
        "probabilistic autoregressive model",
        "information loss",
        "error accumulation",
        "temporal causal dependencies",
        "online motion decoding",
        "multi-round generation",
        "long-term generation",
        "dynamic motion composition"
      ]
    },
    "publishedAt": "2025-03-19T13:32:24.000Z",
    "title": "MotionStreamer: Streaming Motion Generation via Diffusion-based\n  Autoregressive Model in Causal Latent Space",
    "summary": "This paper addresses the challenge of text-conditioned streaming motion\ngeneration, which requires us to predict the next-step human pose based on\nvariable-length historical motions and incoming texts. Existing methods\nstruggle to achieve streaming motion generation, e.g., diffusion models are\nconstrained by pre-defined motion lengths, while GPT-based methods suffer from\ndelayed response and error accumulation problem due to discretized non-causal\ntokenization. To solve these problems, we propose MotionStreamer, a novel\nframework that incorporates a continuous causal latent space into a\nprobabilistic autoregressive model. The continuous latents mitigate information\nloss caused by discretization and effectively reduce error accumulation during\nlong-term autoregressive generation. In addition, by establishing temporal\ncausal dependencies between current and historical motion latents, our model\nfully utilizes the available information to achieve accurate online motion\ndecoding. Experiments show that our method outperforms existing approaches\nwhile offering more applications, including multi-round generation, long-term\ngeneration, and dynamic motion composition. Project Page:\nhttps://zju3dv.github.io/MotionStreamer/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.15451.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65220fedc709aaca9aa63061",
      "avatarUrl": "/avatars/36e56199fa8c98ff3430636567a0ed62.svg",
      "fullname": "Lixing Xiao",
      "name": "lxxiao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.13834",
      "authors": [
        {
          "_id": "67dcf6456b575dc3179e05a2",
          "name": "JuneHyoung Kwon",
          "hidden": false
        },
        {
          "_id": "67dcf6456b575dc3179e05a3",
          "name": "MiHyeon Kim",
          "hidden": false
        },
        {
          "_id": "67dcf6456b575dc3179e05a4",
          "name": "Eunju Lee",
          "hidden": false
        },
        {
          "_id": "67dcf6456b575dc3179e05a5",
          "name": "Juhwan Choi",
          "hidden": false
        },
        {
          "_id": "67dcf6456b575dc3179e05a6",
          "name": "YoungBin Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-18T02:17:41.000Z",
      "submittedOnDailyAt": "2025-03-21T03:48:06.030Z",
      "title": "See-Saw Modality Balance: See Gradient, and Sew Impaired Vision-Language\n  Balance to Mitigate Dominant Modality Bias",
      "submittedOnDailyBy": {
        "_id": "65646b22ac9d3c2bd7b14788",
        "avatarUrl": "/avatars/0bf19dcfa568a694361fb3a63b999997.svg",
        "isPro": false,
        "fullname": "Juhwan Choi",
        "user": "c-juhwan",
        "type": "user"
      },
      "summary": "Vision-language (VL) models have demonstrated strong performance across\nvarious tasks. However, these models often rely on a specific modality for\npredictions, leading to \"dominant modality bias.'' This bias significantly\nhurts performance, especially when one modality is impaired. In this study, we\nanalyze model behavior under dominant modality bias and theoretically show that\nunaligned gradients or differences in gradient magnitudes prevent balanced\nconvergence of the loss. Based on these findings, we propose a novel framework,\nBalGrad to mitigate dominant modality bias. Our approach includes\ninter-modality gradient reweighting, adjusting the gradient of KL divergence\nbased on each modality's contribution, and inter-task gradient projection to\nalign task directions in a non-conflicting manner. Experiments on UPMC\nFood-101, Hateful Memes, and MM-IMDb datasets confirm that BalGrad effectively\nalleviates over-reliance on specific modalities when making predictions.",
      "upvotes": 1,
      "discussionId": "67dcf64a6b575dc3179e0754",
      "ai_keywords": [
        "dominant modality bias",
        "unaligned gradients",
        "gradient magnitudes",
        "inter-modality gradient reweighting",
        "KL divergence",
        "inter-task gradient projection"
      ]
    },
    "publishedAt": "2025-03-17T22:17:41.000Z",
    "title": "See-Saw Modality Balance: See Gradient, and Sew Impaired Vision-Language\n  Balance to Mitigate Dominant Modality Bias",
    "summary": "Vision-language (VL) models have demonstrated strong performance across\nvarious tasks. However, these models often rely on a specific modality for\npredictions, leading to \"dominant modality bias.'' This bias significantly\nhurts performance, especially when one modality is impaired. In this study, we\nanalyze model behavior under dominant modality bias and theoretically show that\nunaligned gradients or differences in gradient magnitudes prevent balanced\nconvergence of the loss. Based on these findings, we propose a novel framework,\nBalGrad to mitigate dominant modality bias. Our approach includes\ninter-modality gradient reweighting, adjusting the gradient of KL divergence\nbased on each modality's contribution, and inter-task gradient projection to\nalign task directions in a non-conflicting manner. Experiments on UPMC\nFood-101, Hateful Memes, and MM-IMDb datasets confirm that BalGrad effectively\nalleviates over-reliance on specific modalities when making predictions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.13834.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65646b22ac9d3c2bd7b14788",
      "avatarUrl": "/avatars/0bf19dcfa568a694361fb3a63b999997.svg",
      "fullname": "Juhwan Choi",
      "name": "c-juhwan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.10625",
      "authors": [
        {
          "_id": "67dacb439c49701f604e4257",
          "name": "Lingteng Qiu",
          "hidden": false
        },
        {
          "_id": "67dacb439c49701f604e4258",
          "name": "Xiaodong Gu",
          "hidden": false
        },
        {
          "_id": "67dacb439c49701f604e4259",
          "name": "Peihao Li",
          "hidden": false
        },
        {
          "_id": "67dacb439c49701f604e425a",
          "user": {
            "_id": "64d0d72e15b26cc7f704a60f",
            "avatarUrl": "/avatars/f450013fb2b204f1422e614629914248.svg",
            "isPro": true,
            "fullname": "Qi Zuo",
            "user": "DyrusQZ",
            "type": "user"
          },
          "name": "Qi Zuo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-20T10:46:05.121Z",
          "hidden": false
        },
        {
          "_id": "67dacb439c49701f604e425b",
          "name": "Weichao Shen",
          "hidden": false
        },
        {
          "_id": "67dacb439c49701f604e425c",
          "name": "Junfei Zhang",
          "hidden": false
        },
        {
          "_id": "67dacb439c49701f604e425d",
          "name": "Kejie Qiu",
          "hidden": false
        },
        {
          "_id": "67dacb439c49701f604e425e",
          "name": "Weihao Yuan",
          "hidden": false
        },
        {
          "_id": "67dacb439c49701f604e425f",
          "name": "Guanying Chen",
          "hidden": false
        },
        {
          "_id": "67dacb439c49701f604e4260",
          "name": "Zilong Dong",
          "hidden": false
        },
        {
          "_id": "67dacb439c49701f604e4261",
          "name": "Liefeng Bo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64d0d72e15b26cc7f704a60f/tiDonOVoU7mFVv3w6qgcR.mp4"
      ],
      "publishedAt": "2025-03-13T17:59:21.000Z",
      "submittedOnDailyAt": "2025-03-21T05:52:51.662Z",
      "title": "LHM: Large Animatable Human Reconstruction Model from a Single Image in\n  Seconds",
      "submittedOnDailyBy": {
        "_id": "64d0d72e15b26cc7f704a60f",
        "avatarUrl": "/avatars/f450013fb2b204f1422e614629914248.svg",
        "isPro": true,
        "fullname": "Qi Zuo",
        "user": "DyrusQZ",
        "type": "user"
      },
      "summary": "Animatable 3D human reconstruction from a single image is a challenging\nproblem due to the ambiguity in decoupling geometry, appearance, and\ndeformation. Recent advances in 3D human reconstruction mainly focus on static\nhuman modeling, and the reliance of using synthetic 3D scans for training\nlimits their generalization ability. Conversely, optimization-based video\nmethods achieve higher fidelity but demand controlled capture conditions and\ncomputationally intensive refinement processes. Motivated by the emergence of\nlarge reconstruction models for efficient static reconstruction, we propose LHM\n(Large Animatable Human Reconstruction Model) to infer high-fidelity avatars\nrepresented as 3D Gaussian splatting in a feed-forward pass. Our model\nleverages a multimodal transformer architecture to effectively encode the human\nbody positional features and image features with attention mechanism, enabling\ndetailed preservation of clothing geometry and texture. To further boost the\nface identity preservation and fine detail recovery, we propose a head feature\npyramid encoding scheme to aggregate multi-scale features of the head regions.\nExtensive experiments demonstrate that our LHM generates plausible animatable\nhuman in seconds without post-processing for face and hands, outperforming\nexisting methods in both reconstruction accuracy and generalization ability.",
      "upvotes": 1,
      "discussionId": "67dacb499c49701f604e4454",
      "ai_keywords": [
        "3D Gaussian splatting",
        "multimodal transformer architecture",
        "attention mechanism",
        "head feature pyramid encoding scheme"
      ]
    },
    "publishedAt": "2025-03-13T13:59:21.000Z",
    "title": "LHM: Large Animatable Human Reconstruction Model from a Single Image in\n  Seconds",
    "summary": "Animatable 3D human reconstruction from a single image is a challenging\nproblem due to the ambiguity in decoupling geometry, appearance, and\ndeformation. Recent advances in 3D human reconstruction mainly focus on static\nhuman modeling, and the reliance of using synthetic 3D scans for training\nlimits their generalization ability. Conversely, optimization-based video\nmethods achieve higher fidelity but demand controlled capture conditions and\ncomputationally intensive refinement processes. Motivated by the emergence of\nlarge reconstruction models for efficient static reconstruction, we propose LHM\n(Large Animatable Human Reconstruction Model) to infer high-fidelity avatars\nrepresented as 3D Gaussian splatting in a feed-forward pass. Our model\nleverages a multimodal transformer architecture to effectively encode the human\nbody positional features and image features with attention mechanism, enabling\ndetailed preservation of clothing geometry and texture. To further boost the\nface identity preservation and fine detail recovery, we propose a head feature\npyramid encoding scheme to aggregate multi-scale features of the head regions.\nExtensive experiments demonstrate that our LHM generates plausible animatable\nhuman in seconds without post-processing for face and hands, outperforming\nexisting methods in both reconstruction accuracy and generalization ability.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64d0d72e15b26cc7f704a60f/tiDonOVoU7mFVv3w6qgcR.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.10625.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d0d72e15b26cc7f704a60f",
      "avatarUrl": "/avatars/f450013fb2b204f1422e614629914248.svg",
      "fullname": "Qi Zuo",
      "name": "DyrusQZ",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  }
]