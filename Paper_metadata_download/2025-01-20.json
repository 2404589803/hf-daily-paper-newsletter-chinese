[
  {
    "paper": {
      "id": "2501.09891",
      "authors": [
        {
          "_id": "678dc332281a0e32feb5fbfe",
          "user": {
            "_id": "669fbf44200c01b737751dc5",
            "avatarUrl": "/avatars/2023c01b2a8cc1625cafcd0b625871dc.svg",
            "isPro": false,
            "fullname": "Kuang-Huei Lee",
            "user": "khlee112",
            "type": "user"
          },
          "name": "Kuang-Huei Lee",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-20T13:16:59.574Z",
          "hidden": false
        },
        {
          "_id": "678dc332281a0e32feb5fbff",
          "user": {
            "_id": "64ef4f9866f36326b3ec5b8c",
            "avatarUrl": "/avatars/9eea179e0797f952664f33e1aef21e88.svg",
            "isPro": false,
            "fullname": "Ian Fischer",
            "user": "Ianfischer",
            "type": "user"
          },
          "name": "Ian Fischer",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-20T13:18:01.647Z",
          "hidden": false
        },
        {
          "_id": "678dc332281a0e32feb5fc00",
          "name": "Yueh-Hua Wu",
          "hidden": false
        },
        {
          "_id": "678dc332281a0e32feb5fc01",
          "name": "Dave Marwood",
          "hidden": false
        },
        {
          "_id": "678dc332281a0e32feb5fc02",
          "name": "Shumeet Baluja",
          "hidden": false
        },
        {
          "_id": "678dc332281a0e32feb5fc03",
          "name": "Dale Schuurmans",
          "hidden": false
        },
        {
          "_id": "678dc332281a0e32feb5fc04",
          "user": {
            "_id": "64d0268001931c60161e026a",
            "avatarUrl": "/avatars/0ea48c47b0270b449fd6b97b495e64a6.svg",
            "isPro": true,
            "fullname": "Xinyun Chen",
            "user": "xinyunchen",
            "type": "user"
          },
          "name": "Xinyun Chen",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-20T13:18:37.820Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-17T00:41:44.000Z",
      "title": "Evolving Deeper LLM Thinking",
      "summary": "We explore an evolutionary search strategy for scaling inference time compute\nin Large Language Models. The proposed approach, Mind Evolution, uses a\nlanguage model to generate, recombine and refine candidate responses. The\nproposed approach avoids the need to formalize the underlying inference problem\nwhenever a solution evaluator is available. Controlling for inference cost, we\nfind that Mind Evolution significantly outperforms other inference strategies\nsuch as Best-of-N and Sequential Revision in natural language planning tasks.\nIn the TravelPlanner and Natural Plan benchmarks, Mind Evolution solves more\nthan 98% of the problem instances using Gemini 1.5 Pro without the use of a\nformal solver.",
      "upvotes": 71,
      "discussionId": "678dc333281a0e32feb5fc2c"
    },
    "publishedAt": "2025-01-19T22:30:10.779Z",
    "title": "Evolving Deeper LLM Thinking",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09891.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5713
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2501.10120",
      "authors": [
        {
          "_id": "678dc283d4a7a158a8e5cf08",
          "user": {
            "_id": "60ea81771cc8dc259c58e905",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60ea81771cc8dc259c58e905/kmGlaNvdS4EEHc_5qongT.jpeg",
            "isPro": false,
            "fullname": "yichen he",
            "user": "hyc2026",
            "type": "user"
          },
          "name": "Yichen He",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-20T13:19:13.591Z",
          "hidden": false
        },
        {
          "_id": "678dc283d4a7a158a8e5cf09",
          "name": "Guanhua Huang",
          "hidden": false
        },
        {
          "_id": "678dc283d4a7a158a8e5cf0a",
          "user": {
            "_id": "662608797670fa2bc0d0fa0a",
            "avatarUrl": "/avatars/eef336e6ba43fae56c90e17f60606f4d.svg",
            "isPro": false,
            "fullname": "fengpeiyuan",
            "user": "fpybytedance",
            "type": "user"
          },
          "name": "Peiyuan Feng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-20T13:19:41.033Z",
          "hidden": false
        },
        {
          "_id": "678dc283d4a7a158a8e5cf0b",
          "name": "Yuan Lin",
          "hidden": false
        },
        {
          "_id": "678dc283d4a7a158a8e5cf0c",
          "name": "Yuchen Zhang",
          "hidden": false
        },
        {
          "_id": "678dc283d4a7a158a8e5cf0d",
          "name": "Hang Li",
          "hidden": false
        },
        {
          "_id": "678dc283d4a7a158a8e5cf0e",
          "name": "Weinan E",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-17T11:12:28.000Z",
      "title": "PaSa: An LLM Agent for Comprehensive Academic Paper Search",
      "summary": "We introduce PaSa, an advanced Paper Search agent powered by large language\nmodels. PaSa can autonomously make a series of decisions, including invoking\nsearch tools, reading papers, and selecting relevant references, to ultimately\nobtain comprehensive and accurate results for complex scholarly queries. We\noptimize PaSa using reinforcement learning with a synthetic dataset,\nAutoScholarQuery, which includes 35k fine-grained academic queries and\ncorresponding papers sourced from top-tier AI conference publications.\nAdditionally, we develop RealScholarQuery, a benchmark collecting real-world\nacademic queries to assess PaSa performance in more realistic scenarios.\nDespite being trained on synthetic data, PaSa significantly outperforms\nexisting baselines on RealScholarQuery, including Google, Google Scholar,\nGoogle with GPT-4 for paraphrased queries, chatGPT (search-enabled GPT-4o),\nGPT-o1, and PaSa-GPT-4o (PaSa implemented by prompting GPT-4o). Notably,\nPaSa-7B surpasses the best Google-based baseline, Google with GPT-4o, by 37.78%\nin recall@20 and 39.90% in recall@50. It also exceeds PaSa-GPT-4o by 30.36% in\nrecall and 4.25% in precision. Model, datasets, and code are available at\nhttps://github.com/bytedance/pasa.",
      "upvotes": 22,
      "discussionId": "678dc284d4a7a158a8e5cf48"
    },
    "publishedAt": "2025-01-19T22:27:10.419Z",
    "title": "PaSa: An LLM Agent for Comprehensive Academic Paper Search",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.10120.png",
    "numComments": 9,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5713
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.09775",
      "authors": [
        {
          "_id": "678e1b151a99a49b98056e1c",
          "name": "Tairan Fu",
          "hidden": false
        },
        {
          "_id": "678e1b151a99a49b98056e1d",
          "name": "Javier Conde",
          "hidden": false
        },
        {
          "_id": "678e1b151a99a49b98056e1e",
          "user": {
            "_id": "64f31365ed48e3bb9c487d5d",
            "avatarUrl": "/avatars/979c1979eadbd4529c95b925bbb58d78.svg",
            "isPro": false,
            "fullname": "Gonzalo",
            "user": "gonzmart",
            "type": "user"
          },
          "name": "Gonzalo Martínez",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-20T16:59:39.580Z",
          "hidden": false
        },
        {
          "_id": "678e1b151a99a49b98056e1f",
          "user": {
            "_id": "5f9c00a5777efc07d7f1e4be",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1665073337782-5f9c00a5777efc07d7f1e4be.png",
            "isPro": false,
            "fullname": "María Grandury",
            "user": "mariagrandury",
            "type": "user"
          },
          "name": "María Grandury",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-01-20T10:35:14.566Z",
          "hidden": false
        },
        {
          "_id": "678e1b151a99a49b98056e20",
          "user": {
            "_id": "6574f66b06fdcd4ca9491299",
            "avatarUrl": "/avatars/e31821949d75efb750ab2d9ebe12b9a8.svg",
            "isPro": false,
            "fullname": "pedro reviriego",
            "user": "reviriego",
            "type": "user"
          },
          "name": "Pedro Reviriego",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-01-20T13:16:45.528Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-16T10:27:51.000Z",
      "title": "Multiple Choice Questions: Reasoning Makes Large Language Models (LLMs)\n  More Self-Confident Even When They Are Wrong",
      "summary": "One of the most widely used methods to evaluate LLMs are Multiple Choice\nQuestion (MCQ) tests. MCQ benchmarks enable the testing of LLM knowledge on\nalmost any topic at scale as the results can be processed automatically. To\nhelp the LLM answer, a few examples called few shots can be included in the\nprompt. Moreover, the LLM can be asked to answer the question directly with the\nselected option or to first provide the reasoning and then the selected answer,\nwhich is known as chain of thought. In addition to checking whether the\nselected answer is correct, the evaluation can look at the LLM-estimated\nprobability of its response as an indication of the confidence of the LLM in\nthe response. In this paper, we study how the LLM confidence in its answer\ndepends on whether the model has been asked to answer directly or to provide\nthe reasoning before answering. The results of the evaluation of questions on a\nwide range of topics in seven different models show that LLMs are more\nconfident in their answers when they provide reasoning before the answer. This\noccurs regardless of whether the selected answer is correct. Our hypothesis is\nthat this behavior is due to the reasoning that modifies the probability of the\nselected answer, as the LLM predicts the answer based on the input question and\nthe reasoning that supports the selection made. Therefore, LLM estimated\nprobabilities seem to have intrinsic limitations that should be understood in\norder to use them in evaluation procedures. Interestingly, the same behavior\nhas been observed in humans, for whom explaining an answer increases confidence\nin its correctness.",
      "upvotes": 13,
      "discussionId": "678e1b161a99a49b98056e61"
    },
    "publishedAt": "2025-01-20T04:45:24.921Z",
    "title": "Multiple Choice Questions: Reasoning Makes Large Language Models (LLMs) More Self-Confident Even When They Are Wrong",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09775.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64f31365ed48e3bb9c487d5d",
      "avatarUrl": "/avatars/979c1979eadbd4529c95b925bbb58d78.svg",
      "fullname": "Gonzalo",
      "name": "gonzmart",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2501.10020",
      "authors": [
        {
          "_id": "678dd1d27e1f344cdc26b717",
          "name": "Chao He",
          "hidden": false
        },
        {
          "_id": "678dd1d27e1f344cdc26b718",
          "name": "Jianqiang Ren",
          "hidden": false
        },
        {
          "_id": "678dd1d27e1f344cdc26b719",
          "user": {
            "_id": "63d0cc736b985b0f25d0412c",
            "avatarUrl": "/avatars/3eb8c79f9a7c4c819038ea7b04e323dd.svg",
            "isPro": false,
            "fullname": "Bo",
            "user": "Liefeng",
            "type": "user"
          },
          "name": "Liefeng Bo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-20T14:07:42.307Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-17T08:09:06.000Z",
      "title": "Textoon: Generating Vivid 2D Cartoon Characters from Text Descriptions",
      "summary": "The 2D cartoon style is a prominent art form in digital character creation,\nparticularly popular among younger audiences. While advancements in digital\nhuman technology have spurred extensive research into photorealistic digital\nhumans and 3D characters, interactive 2D cartoon characters have received\ncomparatively less attention. Unlike 3D counterparts, which require\nsophisticated construction and resource-intensive rendering, Live2D, a\nwidely-used format for 2D cartoon characters, offers a more efficient\nalternative, which allows to animate 2D characters in a manner that simulates\n3D movement without the necessity of building a complete 3D model. Furthermore,\nLive2D employs lightweight HTML5 (H5) rendering, improving both accessibility\nand efficiency. In this technical report, we introduce Textoon, an innovative\nmethod for generating diverse 2D cartoon characters in the Live2D format based\non text descriptions. The Textoon leverages cutting-edge language and vision\nmodels to comprehend textual intentions and generate 2D appearance, capable of\ncreating a wide variety of stunning and interactive 2D characters within one\nminute. The project homepage is https://human3daigc.github.io/Textoon_webpage/.",
      "upvotes": 13,
      "discussionId": "678dd1d47e1f344cdc26b7b2"
    },
    "publishedAt": "2025-01-19T23:32:27.791Z",
    "title": "Textoon: Generating Vivid 2D Cartoon Characters from Text Descriptions",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.10020.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5713
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.09825",
      "authors": [
        {
          "_id": "678e3e0c8aeb001443af5cb1",
          "user": {
            "_id": "66bb35988b09ede0b7b92313",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bb35988b09ede0b7b92313/M06mQ3ifyRwuladTNwMS2.png",
            "isPro": false,
            "fullname": "Nada Saadi",
            "user": "Nadas31",
            "type": "user"
          },
          "name": "Nada Saadi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-20T14:12:06.609Z",
          "hidden": false
        },
        {
          "_id": "678e3e0c8aeb001443af5cb2",
          "user": {
            "_id": "5f5f6c113c67af20d9945afb",
            "avatarUrl": "/avatars/06b2eb3a5d27864280d4d02e6d00d782.svg",
            "isPro": false,
            "fullname": "Tathagata Raha",
            "user": "tathagataraha",
            "type": "user"
          },
          "name": "Tathagata Raha",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-20T14:12:13.246Z",
          "hidden": false
        },
        {
          "_id": "678e3e0c8aeb001443af5cb3",
          "user": {
            "_id": "628e39f4b1596566033b8d7b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628e39f4b1596566033b8d7b/-Y807up1cgMmAQsczdOPn.jpeg",
            "isPro": false,
            "fullname": "Clément Christophe",
            "user": "cchristophe",
            "type": "user"
          },
          "name": "Clément Christophe",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-20T14:12:19.413Z",
          "hidden": false
        },
        {
          "_id": "678e3e0c8aeb001443af5cb4",
          "name": "Marco AF Pimentel",
          "hidden": false
        },
        {
          "_id": "678e3e0c8aeb001443af5cb5",
          "user": {
            "_id": "65281d6ef61ca80b9c2ee707",
            "avatarUrl": "/avatars/090ea7210a4bb6549b0f7fee71525625.svg",
            "isPro": false,
            "fullname": "Ronnie Rajan",
            "user": "ronnierajan",
            "type": "user"
          },
          "name": "Ronnie Rajan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-20T14:12:33.526Z",
          "hidden": false
        },
        {
          "_id": "678e3e0c8aeb001443af5cb6",
          "user": {
            "_id": "65280984b794fe3d06544d77",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65280984b794fe3d06544d77/tyrxbxtDG02On1uiRaVbL.jpeg",
            "isPro": false,
            "fullname": "Praveenkumar",
            "user": "pkanithi",
            "type": "user"
          },
          "name": "Praveen K Kanithi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-01-20T14:07:22.890Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-16T20:24:56.000Z",
      "title": "Bridging Language Barriers in Healthcare: A Study on Arabic LLMs",
      "summary": "This paper investigates the challenges of developing large language models\n(LLMs) proficient in both multilingual understanding and medical knowledge. We\ndemonstrate that simply translating medical data does not guarantee strong\nperformance on clinical tasks in the target language. Our experiments reveal\nthat the optimal language mix in training data varies significantly across\ndifferent medical tasks. We find that larger models with carefully calibrated\nlanguage ratios achieve superior performance on native-language clinical tasks.\nFurthermore, our results suggest that relying solely on fine-tuning may not be\nthe most effective approach for incorporating new language knowledge into LLMs.\nInstead, data and computationally intensive pretraining methods may still be\nnecessary to achieve optimal performance in multilingual medical settings.\nThese findings provide valuable guidance for building effective and inclusive\nmedical AI systems for diverse linguistic communities.",
      "upvotes": 8,
      "discussionId": "678e3e0d8aeb001443af5cf4"
    },
    "publishedAt": "2025-01-20T07:14:47.264Z",
    "title": "Bridging Language Barriers in Healthcare: A Study on Arabic LLMs",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09825.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "628e39f4b1596566033b8d7b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628e39f4b1596566033b8d7b/-Y807up1cgMmAQsczdOPn.jpeg",
      "fullname": "Clément Christophe",
      "name": "cchristophe",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2501.10021",
      "authors": [
        {
          "_id": "678dc72ab3cda33f4a7e3b94",
          "user": {
            "_id": "64a5d8219f3b568c202b3137",
            "avatarUrl": "/avatars/eef6fb7c70d272555a53183c0e50dbaf.svg",
            "isPro": false,
            "fullname": "Di Chang",
            "user": "Boese0601",
            "type": "user"
          },
          "name": "Di Chang",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-01-20T04:33:59.756Z",
          "hidden": false
        },
        {
          "_id": "678dc72ab3cda33f4a7e3b95",
          "name": "Hongyi Xu",
          "hidden": false
        },
        {
          "_id": "678dc72ab3cda33f4a7e3b96",
          "user": {
            "_id": "6408dfd4b6a334f53e24023c",
            "avatarUrl": "/avatars/b7e3fa4fbec6313e94ff3384b74dabfc.svg",
            "isPro": false,
            "fullname": "You Xie",
            "user": "youxie",
            "type": "user"
          },
          "name": "You Xie",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-20T14:20:36.757Z",
          "hidden": false
        },
        {
          "_id": "678dc72ab3cda33f4a7e3b97",
          "user": {
            "_id": "67312401d433c6b122c38202",
            "avatarUrl": "/avatars/599523d1412ab01671aa7f86c14d508a.svg",
            "isPro": false,
            "fullname": "Yipeng Gao",
            "user": "YipengGao",
            "type": "user"
          },
          "name": "Yipeng Gao",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-20T14:20:23.934Z",
          "hidden": false
        },
        {
          "_id": "678dc72ab3cda33f4a7e3b98",
          "user": {
            "_id": "673460e68f847a342114a00d",
            "avatarUrl": "/avatars/a8bed17291f166a6e1dc79413f6ce80a.svg",
            "isPro": false,
            "fullname": "Zhengfei Kuang",
            "user": "zfkuang",
            "type": "user"
          },
          "name": "Zhengfei Kuang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-20T14:20:17.627Z",
          "hidden": false
        },
        {
          "_id": "678dc72ab3cda33f4a7e3b99",
          "user": {
            "_id": "66a1da7cc9e703d2af5ad742",
            "avatarUrl": "/avatars/f9cd9ae3407e249ab4569479200feb1f.svg",
            "isPro": false,
            "fullname": "Shengqu Cai",
            "user": "primecai",
            "type": "user"
          },
          "name": "Shengqu Cai",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-20T14:20:11.259Z",
          "hidden": false
        },
        {
          "_id": "678dc72ab3cda33f4a7e3b9a",
          "name": "Chenxu Zhang",
          "hidden": false
        },
        {
          "_id": "678dc72ab3cda33f4a7e3b9b",
          "user": {
            "_id": "63086a237dc1b1a54cc6c24d",
            "avatarUrl": "/avatars/477b94134edc4c18c8f769ecbb7d8091.svg",
            "isPro": false,
            "fullname": "Song",
            "user": "guoxiansong",
            "type": "user"
          },
          "name": "Guoxian Song",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-20T14:19:38.417Z",
          "hidden": false
        },
        {
          "_id": "678dc72ab3cda33f4a7e3b9c",
          "name": "Chao Wang",
          "hidden": false
        },
        {
          "_id": "678dc72ab3cda33f4a7e3b9d",
          "user": {
            "_id": "6309ad81b105f8675bd5a796",
            "avatarUrl": "/avatars/01c2186924486da4606e128e83709164.svg",
            "isPro": true,
            "fullname": "Shi",
            "user": "Yichun",
            "type": "user"
          },
          "name": "Yichun Shi",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-20T14:19:30.964Z",
          "hidden": false
        },
        {
          "_id": "678dc72ab3cda33f4a7e3b9e",
          "name": "Zeyuan Chen",
          "hidden": false
        },
        {
          "_id": "678dc72ab3cda33f4a7e3b9f",
          "user": {
            "_id": "642a276516d4d8293c9a47e8",
            "avatarUrl": "/avatars/80e6db8bc2544f3486b11b57858a8692.svg",
            "isPro": false,
            "fullname": "Shijie Zhou",
            "user": "shijiezhou",
            "type": "user"
          },
          "name": "Shijie Zhou",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-20T14:18:48.920Z",
          "hidden": false
        },
        {
          "_id": "678dc72ab3cda33f4a7e3ba0",
          "name": "Linjie Luo",
          "hidden": false
        },
        {
          "_id": "678dc72ab3cda33f4a7e3ba1",
          "user": {
            "_id": "6694e583ac96ca2c17131505",
            "avatarUrl": "/avatars/6e7a31f257e36cf301da6f879dc0a122.svg",
            "isPro": false,
            "fullname": "Gordon Wetzstein",
            "user": "wetzste1",
            "type": "user"
          },
          "name": "Gordon Wetzstein",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-20T14:17:49.662Z",
          "hidden": false
        },
        {
          "_id": "678dc72ab3cda33f4a7e3ba2",
          "user": {
            "_id": "65fcb99d383d3f256c3a92d2",
            "avatarUrl": "/avatars/b85d32f4d7a19816b8d499e05b173ad1.svg",
            "isPro": false,
            "fullname": "Mohammad Soleymani",
            "user": "msoleymani",
            "type": "user"
          },
          "name": "Mohammad Soleymani",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-20T14:17:43.486Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-17T08:10:53.000Z",
      "title": "X-Dyna: Expressive Dynamic Human Image Animation",
      "summary": "We introduce X-Dyna, a novel zero-shot, diffusion-based pipeline for\nanimating a single human image using facial expressions and body movements\nderived from a driving video, that generates realistic, context-aware dynamics\nfor both the subject and the surrounding environment. Building on prior\napproaches centered on human pose control, X-Dyna addresses key shortcomings\ncausing the loss of dynamic details, enhancing the lifelike qualities of human\nvideo animations. At the core of our approach is the Dynamics-Adapter, a\nlightweight module that effectively integrates reference appearance context\ninto the spatial attentions of the diffusion backbone while preserving the\ncapacity of motion modules in synthesizing fluid and intricate dynamic details.\nBeyond body pose control, we connect a local control module with our model to\ncapture identity-disentangled facial expressions, facilitating accurate\nexpression transfer for enhanced realism in animated scenes. Together, these\ncomponents form a unified framework capable of learning physical human motion\nand natural scene dynamics from a diverse blend of human and scene videos.\nComprehensive qualitative and quantitative evaluations demonstrate that X-Dyna\noutperforms state-of-the-art methods, creating highly lifelike and expressive\nanimations. The code is available at https://github.com/bytedance/X-Dyna.",
      "upvotes": 6,
      "discussionId": "678dc72bb3cda33f4a7e3c15"
    },
    "publishedAt": "2025-01-20T00:21:54.930Z",
    "title": "X-Dyna: Expressive Dynamic Human Image Animation",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64a5d8219f3b568c202b3137/3siRENiyngtaOyP2D65Fk.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.10021.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64a5d8219f3b568c202b3137",
      "avatarUrl": "/avatars/eef6fb7c70d272555a53183c0e50dbaf.svg",
      "fullname": "Di Chang",
      "name": "Boese0601",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2501.10045",
      "authors": [
        {
          "_id": "678dd3044ce7abd7ef1f5345",
          "name": "Shengkui Zhao",
          "hidden": false
        },
        {
          "_id": "678dd3044ce7abd7ef1f5346",
          "name": "Kun Zhou",
          "hidden": false
        },
        {
          "_id": "678dd3044ce7abd7ef1f5347",
          "name": "Zexu Pan",
          "hidden": false
        },
        {
          "_id": "678dd3044ce7abd7ef1f5348",
          "name": "Yukun Ma",
          "hidden": false
        },
        {
          "_id": "678dd3044ce7abd7ef1f5349",
          "name": "Chong Zhang",
          "hidden": false
        },
        {
          "_id": "678dd3044ce7abd7ef1f534a",
          "name": "Bin Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-17T09:04:38.000Z",
      "title": "HiFi-SR: A Unified Generative Transformer-Convolutional Adversarial\n  Network for High-Fidelity Speech Super-Resolution",
      "summary": "The application of generative adversarial networks (GANs) has recently\nadvanced speech super-resolution (SR) based on intermediate representations\nlike mel-spectrograms. However, existing SR methods that typically rely on\nindependently trained and concatenated networks may lead to inconsistent\nrepresentations and poor speech quality, especially in out-of-domain scenarios.\nIn this work, we propose HiFi-SR, a unified network that leverages end-to-end\nadversarial training to achieve high-fidelity speech super-resolution. Our\nmodel features a unified transformer-convolutional generator designed to\nseamlessly handle both the prediction of latent representations and their\nconversion into time-domain waveforms. The transformer network serves as a\npowerful encoder, converting low-resolution mel-spectrograms into latent space\nrepresentations, while the convolutional network upscales these representations\ninto high-resolution waveforms. To enhance high-frequency fidelity, we\nincorporate a multi-band, multi-scale time-frequency discriminator, along with\na multi-scale mel-reconstruction loss in the adversarial training process.\nHiFi-SR is versatile, capable of upscaling any input speech signal between 4\nkHz and 32 kHz to a 48 kHz sampling rate. Experimental results demonstrate that\nHiFi-SR significantly outperforms existing speech SR methods across both\nobjective metrics and ABX preference tests, for both in-domain and\nout-of-domain scenarios (https://github.com/modelscope/ClearerVoice-Studio).",
      "upvotes": 5,
      "discussionId": "678dd3054ce7abd7ef1f53b8"
    },
    "publishedAt": "2025-01-19T23:37:31.452Z",
    "title": "HiFi-SR: A Unified Generative Transformer-Convolutional Adversarial Network for High-Fidelity Speech Super-Resolution",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.10045.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5713
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2501.10132",
      "authors": [
        {
          "_id": "678db8f76e06f11c16d15ff5",
          "user": {
            "_id": "60eff04e22ab0ac83b0fc9d8",
            "avatarUrl": "/avatars/99068a7a9bf44aca7e5b1cf0d184681e.svg",
            "isPro": false,
            "fullname": "lucen zhong",
            "user": "anchorzhong",
            "type": "user"
          },
          "name": "Lucen Zhong",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-20T14:32:47.710Z",
          "hidden": false
        },
        {
          "_id": "678db8f76e06f11c16d15ff6",
          "user": {
            "_id": "63033dc4e1e7f0e03a5e1a31",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661157784937-63033dc4e1e7f0e03a5e1a31.jpeg",
            "isPro": false,
            "fullname": "Zhengxiao Du",
            "user": "zxdu20",
            "type": "user"
          },
          "name": "Zhengxiao Du",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-20T14:32:53.512Z",
          "hidden": false
        },
        {
          "_id": "678db8f76e06f11c16d15ff7",
          "name": "Xiaohan Zhang",
          "hidden": false
        },
        {
          "_id": "678db8f76e06f11c16d15ff8",
          "user": {
            "_id": "66e40e556998c3d86c3e9263",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/3aZyGNr8bbnY0Ry0iObr4.jpeg",
            "isPro": false,
            "fullname": "Haiyi Hu",
            "user": "haithesea",
            "type": "user"
          },
          "name": "Haiyi Hu",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-20T14:33:03.717Z",
          "hidden": false
        },
        {
          "_id": "678db8f76e06f11c16d15ff9",
          "user": {
            "_id": "640dff05474aa6f89556677e",
            "avatarUrl": "/avatars/1b4591c7322d649c797b3125148f1915.svg",
            "isPro": false,
            "fullname": "Jie Tang",
            "user": "jerytang",
            "type": "user"
          },
          "name": "Jie Tang",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-20T14:33:09.675Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-17T11:41:53.000Z",
      "title": "ComplexFuncBench: Exploring Multi-Step and Constrained Function Calling\n  under Long-Context Scenario",
      "summary": "Enhancing large language models (LLMs) with real-time APIs can help generate\nmore accurate and up-to-date responses. However, evaluating the function\ncalling abilities of LLMs in real-world scenarios remains under-explored due to\nthe complexity of data collection and evaluation. In this work, we introduce\nComplexFuncBench, a benchmark for complex function calling across five\nreal-world scenarios. Compared to existing benchmarks, ComplexFuncBench\nencompasses multi-step and constrained function calling, which requires\nlong-parameter filing, parameter value reasoning, and 128k long context.\nAdditionally, we propose an automatic framework, ComplexEval, for\nquantitatively evaluating complex function calling tasks. Through comprehensive\nexperiments, we demonstrate the deficiencies of state-of-the-art LLMs in\nfunction calling and suggest future directions for optimizing these\ncapabilities. The data and code are available at\nhttps://github.com/THUDM/ComplexFuncBench.",
      "upvotes": 4,
      "discussionId": "678db8f86e06f11c16d16040"
    },
    "publishedAt": "2025-01-19T21:57:49.821Z",
    "title": "ComplexFuncBench: Exploring Multi-Step and Constrained Function Calling under Long-Context Scenario",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.10132.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60eff04e22ab0ac83b0fc9d8",
      "avatarUrl": "/avatars/99068a7a9bf44aca7e5b1cf0d184681e.svg",
      "fullname": "lucen zhong",
      "name": "anchorzhong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2501.09978",
      "authors": [
        {
          "_id": "678dcddd1c0169c0bc73364e",
          "name": "Xiangyue Liu",
          "hidden": false
        },
        {
          "_id": "678dcddd1c0169c0bc73364f",
          "user": {
            "_id": "64baa2a530a1f0f0f03e758c",
            "avatarUrl": "/avatars/100deaba962f49520a028c94b51720b1.svg",
            "isPro": false,
            "fullname": "Kunming Luo",
            "user": "coolbeam",
            "type": "user"
          },
          "name": "Kunming Luo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-01-20T14:37:28.443Z",
          "hidden": false
        },
        {
          "_id": "678dcddd1c0169c0bc733650",
          "name": "Heng Li",
          "hidden": false
        },
        {
          "_id": "678dcddd1c0169c0bc733651",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "678dcddd1c0169c0bc733652",
          "name": "Yuan Liu",
          "hidden": false
        },
        {
          "_id": "678dcddd1c0169c0bc733653",
          "name": "Li Yi",
          "hidden": false
        },
        {
          "_id": "678dcddd1c0169c0bc733654",
          "name": "Ping Tan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-17T06:40:20.000Z",
      "title": "GaussianAvatar-Editor: Photorealistic Animatable Gaussian Head Avatar\n  Editor",
      "summary": "We introduce GaussianAvatar-Editor, an innovative framework for text-driven\nediting of animatable Gaussian head avatars that can be fully controlled in\nexpression, pose, and viewpoint. Unlike static 3D Gaussian editing, editing\nanimatable 4D Gaussian avatars presents challenges related to motion occlusion\nand spatial-temporal inconsistency. To address these issues, we propose the\nWeighted Alpha Blending Equation (WABE). This function enhances the blending\nweight of visible Gaussians while suppressing the influence on non-visible\nGaussians, effectively handling motion occlusion during editing. Furthermore,\nto improve editing quality and ensure 4D consistency, we incorporate\nconditional adversarial learning into the editing process. This strategy helps\nto refine the edited results and maintain consistency throughout the animation.\nBy integrating these methods, our GaussianAvatar-Editor achieves photorealistic\nand consistent results in animatable 4D Gaussian editing. We conduct\ncomprehensive experiments across various subjects to validate the effectiveness\nof our proposed techniques, which demonstrates the superiority of our approach\nover existing methods. More results and code are available at: [Project\nLink](https://xiangyueliu.github.io/GaussianAvatar-Editor/).",
      "upvotes": 2,
      "discussionId": "678dcde21c0169c0bc7337fb"
    },
    "publishedAt": "2025-01-19T23:15:32.498Z",
    "title": "GaussianAvatar-Editor: Photorealistic Animatable Gaussian Head Avatar Editor",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09978.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 5713
    },
    "isAuthorParticipating": false
  }
]