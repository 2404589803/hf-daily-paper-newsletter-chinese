[
    {
        "paper": {
            "id": "2411.07184",
            "authors": [
                {
                    "_id": "67341bc8bd4f7e2dfe198a26",
                    "name": "Yunhan Yang",
                    "hidden": false
                },
                {
                    "_id": "67341bc8bd4f7e2dfe198a27",
                    "name": "Yukun Huang",
                    "hidden": false
                },
                {
                    "_id": "67341bc8bd4f7e2dfe198a28",
                    "name": "Yuan-Chen Guo",
                    "hidden": false
                },
                {
                    "_id": "67341bc8bd4f7e2dfe198a29",
                    "name": "Liangjun Lu",
                    "hidden": false
                },
                {
                    "_id": "67341bc8bd4f7e2dfe198a2a",
                    "name": "Xiaoyang Wu",
                    "hidden": false
                },
                {
                    "_id": "67341bc8bd4f7e2dfe198a2b",
                    "name": "Edmund Y. Lam",
                    "hidden": false
                },
                {
                    "_id": "67341bc8bd4f7e2dfe198a2c",
                    "name": "Yan-Pei Cao",
                    "hidden": false
                },
                {
                    "_id": "67341bc8bd4f7e2dfe198a2d",
                    "name": "Xihui Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-11T17:59:10.000Z",
            "title": "SAMPart3D: Segment Any Part in 3D Objects",
            "summary": "3D part segmentation is a crucial and challenging task in 3D perception,\nplaying a vital role in applications such as robotics, 3D generation, and 3D\nediting. Recent methods harness the powerful Vision Language Models (VLMs) for\n2D-to-3D knowledge distillation, achieving zero-shot 3D part segmentation.\nHowever, these methods are limited by their reliance on text prompts, which\nrestricts the scalability to large-scale unlabeled datasets and the flexibility\nin handling part ambiguities. In this work, we introduce SAMPart3D, a scalable\nzero-shot 3D part segmentation framework that segments any 3D object into\nsemantic parts at multiple granularities, without requiring predefined part\nlabel sets as text prompts. For scalability, we use text-agnostic vision\nfoundation models to distill a 3D feature extraction backbone, allowing scaling\nto large unlabeled 3D datasets to learn rich 3D priors. For flexibility, we\ndistill scale-conditioned part-aware 3D features for 3D part segmentation at\nmultiple granularities. Once the segmented parts are obtained from the\nscale-conditioned part-aware 3D features, we use VLMs to assign semantic labels\nto each part based on the multi-view renderings. Compared to previous methods,\nour SAMPart3D can scale to the recent large-scale 3D object dataset Objaverse\nand handle complex, non-ordinary objects. Additionally, we contribute a new 3D\npart segmentation benchmark to address the lack of diversity and complexity of\nobjects and parts in existing benchmarks. Experiments show that our SAMPart3D\nsignificantly outperforms existing zero-shot 3D part segmentation methods, and\ncan facilitate various applications such as part-level editing and interactive\nsegmentation.",
            "upvotes": 15,
            "discussionId": "67341bcabd4f7e2dfe198af8"
        },
        "publishedAt": "2024-11-13T02:06:59.149Z",
        "title": "SAMPart3D: Segment Any Part in 3D Objects",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.07184.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b4eecf2fc8324fcb63b404/zGYqYVB4-o-GBMybJ8CDA.png",
            "fullname": "Yunhan Yang",
            "name": "yhyang-myron",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        }
    },
    {
        "paper": {
            "id": "2411.07975",
            "authors": [
                {
                    "_id": "673423249a3e8b8171a34bbe",
                    "name": "Yiyang Ma",
                    "hidden": false
                },
                {
                    "_id": "673423249a3e8b8171a34bbf",
                    "name": "Xingchao Liu",
                    "hidden": false
                },
                {
                    "_id": "673423249a3e8b8171a34bc0",
                    "name": "Xiaokang Chen",
                    "hidden": false
                },
                {
                    "_id": "673423249a3e8b8171a34bc1",
                    "name": "Wen Liu",
                    "hidden": false
                },
                {
                    "_id": "673423249a3e8b8171a34bc2",
                    "name": "Chengyue Wu",
                    "hidden": false
                },
                {
                    "_id": "673423249a3e8b8171a34bc3",
                    "name": "Zhiyu Wu",
                    "hidden": false
                },
                {
                    "_id": "673423249a3e8b8171a34bc4",
                    "name": "Zizheng Pan",
                    "hidden": false
                },
                {
                    "_id": "673423249a3e8b8171a34bc5",
                    "name": "Zhenda Xie",
                    "hidden": false
                },
                {
                    "_id": "673423249a3e8b8171a34bc6",
                    "name": "Haowei Zhang",
                    "hidden": false
                },
                {
                    "_id": "673423249a3e8b8171a34bc7",
                    "name": "Xingkai yu",
                    "hidden": false
                },
                {
                    "_id": "673423249a3e8b8171a34bc8",
                    "name": "Liang Zhao",
                    "hidden": false
                },
                {
                    "_id": "673423249a3e8b8171a34bc9",
                    "name": "Yisong Wang",
                    "hidden": false
                },
                {
                    "_id": "673423249a3e8b8171a34bca",
                    "name": "Jiaying Liu",
                    "hidden": false
                },
                {
                    "_id": "673423249a3e8b8171a34bcb",
                    "name": "Chong Ruan",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-12T17:55:10.000Z",
            "title": "JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified\n  Multimodal Understanding and Generation",
            "summary": "We present JanusFlow, a powerful framework that unifies image understanding\nand generation in a single model. JanusFlow introduces a minimalist\narchitecture that integrates autoregressive language models with rectified\nflow, a state-of-the-art method in generative modeling. Our key finding\ndemonstrates that rectified flow can be straightforwardly trained within the\nlarge language model framework, eliminating the need for complex architectural\nmodifications. To further improve the performance of our unified model, we\nadopt two key strategies: (i) decoupling the understanding and generation\nencoders, and (ii) aligning their representations during unified training.\nExtensive experiments show that JanusFlow achieves comparable or superior\nperformance to specialized models in their respective domains, while\nsignificantly outperforming existing unified approaches across standard\nbenchmarks. This work represents a step toward more efficient and versatile\nvision-language models.",
            "upvotes": 10,
            "discussionId": "673423259a3e8b8171a34bfe"
        },
        "publishedAt": "2024-11-13T04:43:51.100Z",
        "title": "JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.07975.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5113
        }
    },
    {
        "paper": {
            "id": "2411.07133",
            "authors": [
                {
                    "_id": "6734679ad3521b36246eb784",
                    "name": "Zhangchen Xu",
                    "hidden": false
                },
                {
                    "_id": "6734679ad3521b36246eb785",
                    "name": "Fengqing Jiang",
                    "hidden": false
                },
                {
                    "_id": "6734679ad3521b36246eb786",
                    "name": "Luyao Niu",
                    "hidden": false
                },
                {
                    "_id": "6734679ad3521b36246eb787",
                    "user": {
                        "_id": "607f666a4ad99100d63ce35c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/607f666a4ad99100d63ce35c/QxhxnvfeV6efkxwUFHwjI.png",
                        "isPro": false,
                        "fullname": "Bill Yuchen Lin",
                        "user": "yuchenlin",
                        "type": "user"
                    },
                    "name": "Bill Yuchen Lin",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-11-13T08:47:23.435Z",
                    "hidden": false
                },
                {
                    "_id": "6734679ad3521b36246eb788",
                    "name": "Radha Poovendran",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-11T17:06:48.000Z",
            "title": "Stronger Models are NOT Stronger Teachers for Instruction Tuning",
            "summary": "Instruction tuning has been widely adopted to ensure large language models\n(LLMs) follow user instructions effectively. The resulting\ninstruction-following capabilities of LLMs heavily rely on the instruction\ndatasets used for tuning. Recently, synthetic instruction datasets have emerged\nas an economically viable solution to provide LLMs diverse and high-quality\ninstructions. However, existing approaches typically assume that larger or\nstronger models are stronger teachers for instruction tuning, and hence simply\nadopt these models as response generators to the synthetic instructions. In\nthis paper, we challenge this commonly-adopted assumption. Our extensive\nexperiments across five base models and twenty response generators reveal that\nlarger and stronger models are not necessarily stronger teachers of smaller\nmodels. We refer to this phenomenon as the Larger Models' Paradox. We observe\nthat existing metrics cannot precisely predict the effectiveness of response\ngenerators since they ignore the compatibility between teachers and base models\nbeing fine-tuned. We thus develop a novel metric, named as\nCompatibility-Adjusted Reward (CAR) to measure the effectiveness of response\ngenerators. Our experiments across five base models demonstrate that CAR\noutperforms almost all baselines.",
            "upvotes": 7,
            "discussionId": "6734679bd3521b36246eb7d0"
        },
        "publishedAt": "2024-11-13T07:17:49.344Z",
        "title": "Stronger Models are NOT Stronger Teachers for Instruction Tuning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.07133.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/607f666a4ad99100d63ce35c/QxhxnvfeV6efkxwUFHwjI.png",
            "fullname": "Bill Yuchen Lin",
            "name": "yuchenlin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 64
        }
    },
    {
        "paper": {
            "id": "2411.08017",
            "authors": [
                {
                    "_id": "673478717adce85d6b29c398",
                    "user": {
                        "_id": "633f75ce7f69125dcea1adad",
                        "avatarUrl": "/avatars/6eb040568f7c6957f36e232f0d7fec98.svg",
                        "isPro": false,
                        "fullname": "Aditya Sanghi",
                        "user": "adityasanghi",
                        "type": "user"
                    },
                    "name": "Aditya Sanghi",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-11-13T09:59:14.593Z",
                    "hidden": false
                },
                {
                    "_id": "673478717adce85d6b29c399",
                    "name": "Aliasghar Khani",
                    "hidden": false
                },
                {
                    "_id": "673478717adce85d6b29c39a",
                    "name": "Pradyumna Reddy",
                    "hidden": false
                },
                {
                    "_id": "673478717adce85d6b29c39b",
                    "name": "Arianna Rampini",
                    "hidden": false
                },
                {
                    "_id": "673478717adce85d6b29c39c",
                    "name": "Derek Cheung",
                    "hidden": false
                },
                {
                    "_id": "673478717adce85d6b29c39d",
                    "name": "Kamal Rahimi Malekshan",
                    "hidden": false
                },
                {
                    "_id": "673478717adce85d6b29c39e",
                    "name": "Kanika Madan",
                    "hidden": false
                },
                {
                    "_id": "673478717adce85d6b29c39f",
                    "name": "Hooman Shayani",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-12T18:49:06.000Z",
            "title": "Wavelet Latent Diffusion (Wala): Billion-Parameter 3D Generative Model\n  with Compact Wavelet Encodings",
            "summary": "Large-scale 3D generative models require substantial computational resources\nyet often fall short in capturing fine details and complex geometries at high\nresolutions. We attribute this limitation to the inefficiency of current\nrepresentations, which lack the compactness required to model the generative\nmodels effectively. To address this, we introduce a novel approach called\nWavelet Latent Diffusion, or WaLa, that encodes 3D shapes into wavelet-based,\ncompact latent encodings. Specifically, we compress a 256^3 signed distance\nfield into a 12^3 times 4 latent grid, achieving an impressive 2427x\ncompression ratio with minimal loss of detail. This high level of compression\nallows our method to efficiently train large-scale generative networks without\nincreasing the inference time. Our models, both conditional and unconditional,\ncontain approximately one billion parameters and successfully generate\nhigh-quality 3D shapes at 256^3 resolution. Moreover, WaLa offers rapid\ninference, producing shapes within two to four seconds depending on the\ncondition, despite the model's scale. We demonstrate state-of-the-art\nperformance across multiple datasets, with significant improvements in\ngeneration quality, diversity, and computational efficiency. We open-source our\ncode and, to the best of our knowledge, release the largest pretrained 3D\ngenerative models across different modalities.",
            "upvotes": 3,
            "discussionId": "673478727adce85d6b29c42a"
        },
        "publishedAt": "2024-11-13T08:35:39.818Z",
        "title": "Wavelet Latent Diffusion (Wala): Billion-Parameter 3D Generative Model with Compact Wavelet Encodings",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/627a75d3adee2d5829796aeb/A87n0LoKqy8KystILTANy.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.08017.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/143b2717b7af7f1d0334d079b6d65d09.svg",
            "fullname": "Shayani",
            "name": "Hooman",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 1
        }
    },
    {
        "paper": {
            "id": "2411.07461",
            "authors": [
                {
                    "_id": "6734bf91fa1009ea2b5a1534",
                    "name": "Anas Awadalla",
                    "hidden": false
                },
                {
                    "_id": "6734bf91fa1009ea2b5a1535",
                    "name": "Le Xue",
                    "hidden": false
                },
                {
                    "_id": "6734bf91fa1009ea2b5a1536",
                    "name": "Manli Shu",
                    "hidden": false
                },
                {
                    "_id": "6734bf91fa1009ea2b5a1537",
                    "name": "An Yan",
                    "hidden": false
                },
                {
                    "_id": "6734bf91fa1009ea2b5a1538",
                    "name": "Jun Wang",
                    "hidden": false
                },
                {
                    "_id": "6734bf91fa1009ea2b5a1539",
                    "name": "Senthil Purushwalkam",
                    "hidden": false
                },
                {
                    "_id": "6734bf91fa1009ea2b5a153a",
                    "name": "Sheng Shen",
                    "hidden": false
                },
                {
                    "_id": "6734bf91fa1009ea2b5a153b",
                    "name": "Hannah Lee",
                    "hidden": false
                },
                {
                    "_id": "6734bf91fa1009ea2b5a153c",
                    "name": "Oscar Lo",
                    "hidden": false
                },
                {
                    "_id": "6734bf91fa1009ea2b5a153d",
                    "name": "Jae Sung Park",
                    "hidden": false
                },
                {
                    "_id": "6734bf91fa1009ea2b5a153e",
                    "name": "Etash Guha",
                    "hidden": false
                },
                {
                    "_id": "6734bf91fa1009ea2b5a153f",
                    "name": "Silvio Savarese",
                    "hidden": false
                },
                {
                    "_id": "6734bf91fa1009ea2b5a1540",
                    "name": "Ludwig Schmidt",
                    "hidden": false
                },
                {
                    "_id": "6734bf91fa1009ea2b5a1541",
                    "name": "Yejin Choi",
                    "hidden": false
                },
                {
                    "_id": "6734bf91fa1009ea2b5a1542",
                    "name": "Caiming Xiong",
                    "hidden": false
                },
                {
                    "_id": "6734bf91fa1009ea2b5a1543",
                    "name": "Ran Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-12T00:52:52.000Z",
            "title": "BLIP3-KALE: Knowledge Augmented Large-Scale Dense Captions",
            "summary": "We introduce BLIP3-KALE, a dataset of 218 million image-text pairs that\nbridges the gap between descriptive synthetic captions and factual web-scale\nalt-text. KALE augments synthetic dense image captions with web-scale alt-text\nto generate factually grounded image captions. Our two-stage approach leverages\nlarge vision-language models and language models to create knowledge-augmented\ncaptions, which are then used to train a specialized VLM for scaling up the\ndataset. We train vision-language models on KALE and demonstrate improvements\non vision-language tasks. Our experiments show the utility of KALE for training\nmore capable and knowledgeable multimodal models. We release the KALE dataset\nat https://huggingface.co/datasets/Salesforce/blip3-kale",
            "upvotes": 2,
            "discussionId": "6734bf93fa1009ea2b5a1592"
        },
        "publishedAt": "2024-11-13T13:32:55.875Z",
        "title": "BLIP3-KALE: Knowledge Augmented Large-Scale Dense Captions",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.07461.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5113
        }
    },
    {
        "paper": {
            "id": "2411.05197",
            "authors": [
                {
                    "_id": "67349ba576c7f079f723a657",
                    "name": "Cheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "67349ba576c7f079f723a658",
                    "name": "Hanna Foerster",
                    "hidden": false
                },
                {
                    "_id": "67349ba576c7f079f723a659",
                    "name": "Robert D. Mullins",
                    "hidden": false
                },
                {
                    "_id": "67349ba576c7f079f723a65a",
                    "name": "Yiren Zhao",
                    "hidden": false
                },
                {
                    "_id": "67349ba576c7f079f723a65b",
                    "name": "Ilia Shumailov",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-07T21:44:04.000Z",
            "title": "Hardware and Software Platform Inference",
            "summary": "It is now a common business practice to buy access to large language model\n(LLM) inference rather than self-host, because of significant upfront hardware\ninfrastructure and energy costs. However, as a buyer, there is no mechanism to\nverify the authenticity of the advertised service including the serving\nhardware platform, e.g. that it is actually being served using an NVIDIA H100.\nFurthermore, there are reports suggesting that model providers may deliver\nmodels that differ slightly from the advertised ones, often to make them run on\nless expensive hardware. That way, a client pays premium for a capable model\naccess on more expensive hardware, yet ends up being served by a (potentially\nless capable) cheaper model on cheaper hardware. In this paper we introduce\n\\textbf{hardware and software platform inference (HSPI)} -- a method\nfor identifying the underlying  architecture and software stack of a\n(black-box) machine learning model solely based on its input-output behavior.\nOur method leverages the inherent differences of various  architectures\nand compilers to distinguish between different  types and software\nstacks. By analyzing the numerical patterns in the model's outputs, we propose\na classification framework capable of accurately identifying the  used\nfor model inference as well as the underlying software configuration. Our\nfindings demonstrate the feasibility of inferring  type from black-box\nmodels. We evaluate HSPI against models served on different real hardware and\nfind that in a white-box setting we can distinguish between different s\nwith between 83.9% and 100% accuracy. Even in a black-box setting we are\nable to achieve results that are up to three times higher than random guess\naccuracy.",
            "upvotes": 1,
            "discussionId": "67349ba676c7f079f723a6af"
        },
        "publishedAt": "2024-11-13T11:00:00.979Z",
        "title": "Hardware and Software Platform Inference",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.05197.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
            "fullname": "i",
            "name": "iliashum",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2411.06307",
            "authors": [
                {
                    "_id": "6732bbb96c671fb849ad965a",
                    "name": "Zitong Lan",
                    "hidden": false
                },
                {
                    "_id": "6732bbb96c671fb849ad965b",
                    "name": "Chenhao Zheng",
                    "hidden": false
                },
                {
                    "_id": "6732bbb96c671fb849ad965c",
                    "user": {
                        "_id": "6340e9c67158d417a0fdb0c5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/OeRx05yiLURO_uUx0AYux.png",
                        "isPro": false,
                        "fullname": "Zhiwei",
                        "user": "zzwalala",
                        "type": "user"
                    },
                    "name": "Zhiwei Zheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-11-13T16:56:01.507Z",
                    "hidden": false
                },
                {
                    "_id": "6732bbb96c671fb849ad965d",
                    "name": "Mingmin Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-09T23:15:51.000Z",
            "title": "Acoustic Volume Rendering for Neural Impulse Response Fields",
            "summary": "Realistic audio synthesis that captures accurate acoustic phenomena is\nessential for creating immersive experiences in virtual and augmented reality.\nSynthesizing the sound received at any position relies on the estimation of\nimpulse response (IR), which characterizes how sound propagates in one scene\nalong different paths before arriving at the listener's position. In this\npaper, we present Acoustic Volume Rendering (AVR), a novel approach that adapts\nvolume rendering techniques to model acoustic impulse responses. While volume\nrendering has been successful in modeling radiance fields for images and neural\nscene representations, IRs present unique challenges as time-series signals. To\naddress these challenges, we introduce frequency-domain volume rendering and\nuse spherical integration to fit the IR measurements. Our method constructs an\nimpulse response field that inherently encodes wave propagation principles and\nachieves state-of-the-art performance in synthesizing impulse responses for\nnovel poses. Experiments show that AVR surpasses current leading methods by a\nsubstantial margin. Additionally, we develop an acoustic simulation platform,\nAcoustiX, which provides more accurate and realistic IR simulations than\nexisting simulators. Code for AVR and AcoustiX are available at\nhttps://zitonglan.github.io/avr.",
            "upvotes": 0,
            "discussionId": "6732bbba6c671fb849ad96ac"
        },
        "publishedAt": "2024-11-13T15:30:55.727Z",
        "title": "Acoustic Volume Rendering for Neural Impulse Response Fields",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.06307.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/OeRx05yiLURO_uUx0AYux.png",
            "fullname": "Zhiwei",
            "name": "zzwalala",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    }
]