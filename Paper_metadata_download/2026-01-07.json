[
  {
    "paper": {
      "id": "2601.03252",
      "authors": [
        {
          "_id": "695dc956c03d6d81e4399ea4",
          "name": "Hao Yu",
          "hidden": false
        },
        {
          "_id": "695dc956c03d6d81e4399ea5",
          "name": "Haotong Lin",
          "hidden": false
        },
        {
          "_id": "695dc956c03d6d81e4399ea6",
          "name": "Jiawei Wang",
          "hidden": false
        },
        {
          "_id": "695dc956c03d6d81e4399ea7",
          "name": "Jiaxin Li",
          "hidden": false
        },
        {
          "_id": "695dc956c03d6d81e4399ea8",
          "name": "Yida Wang",
          "hidden": false
        },
        {
          "_id": "695dc956c03d6d81e4399ea9",
          "name": "Xueyang Zhang",
          "hidden": false
        },
        {
          "_id": "695dc956c03d6d81e4399eaa",
          "name": "Yue Wang",
          "hidden": false
        },
        {
          "_id": "695dc956c03d6d81e4399eab",
          "name": "Xiaowei Zhou",
          "hidden": false
        },
        {
          "_id": "695dc956c03d6d81e4399eac",
          "name": "Ruizhen Hu",
          "hidden": false
        },
        {
          "_id": "695dc956c03d6d81e4399ead",
          "name": "Sida Peng",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6489a01b8de3f9d810b0154f/XlXUo1VjGVhePk-xHRmkj.mp4"
      ],
      "publishedAt": "2026-01-06T18:57:06.000Z",
      "submittedOnDailyAt": "2026-01-07T00:26:37.060Z",
      "title": "InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields",
      "submittedOnDailyBy": {
        "_id": "6489a01b8de3f9d810b0154f",
        "avatarUrl": "/avatars/f7a0fc6816535945e11bac1212dd7b57.svg",
        "isPro": false,
        "fullname": "Haotong Lin",
        "user": "haotongl",
        "type": "user"
      },
      "summary": "Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids. Such representations restrict their scalability to arbitrary output resolutions and hinder the geometric detail recovery. This paper introduces InfiniDepth, which represents depth as neural implicit fields. Through a simple yet effective local implicit decoder, we can query depth at continuous 2D coordinates, enabling arbitrary-resolution and fine-grained depth estimation. To better assess our method's capabilities, we curate a high-quality 4K synthetic benchmark from five different games, spanning diverse scenes with rich geometric and appearance details. Extensive experiments demonstrate that InfiniDepth achieves state-of-the-art performance on both synthetic and real-world benchmarks across relative and metric depth estimation tasks, particularly excelling in fine-detail regions. It also benefits the task of novel view synthesis under large viewpoint shifts, producing high-quality results with fewer holes and artifacts.",
      "upvotes": 56,
      "discussionId": "695dc956c03d6d81e4399eae",
      "ai_summary": "InfiniDepth represents depth as neural implicit fields using a local implicit decoder, enabling continuous 2D coordinate querying for arbitrary-resolution depth estimation and superior performance in fine-detail regions.",
      "ai_keywords": [
        "neural implicit fields",
        "local implicit decoder",
        "continuous 2D coordinates",
        "arbitrary-resolution depth estimation",
        "synthetic benchmark",
        "4K synthetic benchmark",
        "novel view synthesis",
        "viewpoint shifts"
      ],
      "organization": {
        "_id": "61bac2af530e5c78d7b99667",
        "name": "zju",
        "fullname": "Zhejiang University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
      }
    },
    "publishedAt": "2026-01-06T13:57:06.000Z",
    "title": "InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields",
    "summary": "Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids. Such representations restrict their scalability to arbitrary output resolutions and hinder the geometric detail recovery. This paper introduces InfiniDepth, which represents depth as neural implicit fields. Through a simple yet effective local implicit decoder, we can query depth at continuous 2D coordinates, enabling arbitrary-resolution and fine-grained depth estimation. To better assess our method's capabilities, we curate a high-quality 4K synthetic benchmark from five different games, spanning diverse scenes with rich geometric and appearance details. Extensive experiments demonstrate that InfiniDepth achieves state-of-the-art performance on both synthetic and real-world benchmarks across relative and metric depth estimation tasks, particularly excelling in fine-detail regions. It also benefits the task of novel view synthesis under large viewpoint shifts, producing high-quality results with fewer holes and artifacts.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6489a01b8de3f9d810b0154f/XlXUo1VjGVhePk-xHRmkj.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03252.png",
    "numComments": 6,
    "submittedBy": {
      "_id": "6489a01b8de3f9d810b0154f",
      "avatarUrl": "/avatars/f7a0fc6816535945e11bac1212dd7b57.svg",
      "fullname": "Haotong Lin",
      "name": "haotongl",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "61bac2af530e5c78d7b99667",
      "name": "zju",
      "fullname": "Zhejiang University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.01554",
      "authors": [
        {
          "_id": "695dcda5c03d6d81e4399eb8",
          "name": "MOSI. AI",
          "hidden": false
        },
        {
          "_id": "695dcda5c03d6d81e4399eb9",
          "name": "Donghua Yu",
          "hidden": false
        },
        {
          "_id": "695dcda5c03d6d81e4399eba",
          "name": "Zhengyuan Lin",
          "hidden": false
        },
        {
          "_id": "695dcda5c03d6d81e4399ebb",
          "name": "Chen Yang",
          "hidden": false
        },
        {
          "_id": "695dcda5c03d6d81e4399ebc",
          "name": "Yiyang Zhang",
          "hidden": false
        },
        {
          "_id": "695dcda5c03d6d81e4399ebd",
          "name": "Hanfu Chen",
          "hidden": false
        },
        {
          "_id": "695dcda5c03d6d81e4399ebe",
          "name": "Jingqi Chen",
          "hidden": false
        },
        {
          "_id": "695dcda5c03d6d81e4399ebf",
          "name": "Ke Chen",
          "hidden": false
        },
        {
          "_id": "695dcda5c03d6d81e4399ec0",
          "name": "Liwei Fan",
          "hidden": false
        },
        {
          "_id": "695dcda5c03d6d81e4399ec1",
          "name": "Yi Jiang",
          "hidden": false
        },
        {
          "_id": "695dcda5c03d6d81e4399ec2",
          "name": "Jie Zhu",
          "hidden": false
        },
        {
          "_id": "695dcda5c03d6d81e4399ec3",
          "name": "Muchen Li",
          "hidden": false
        },
        {
          "_id": "695dcda5c03d6d81e4399ec4",
          "name": "Wenxuan Wang",
          "hidden": false
        },
        {
          "_id": "695dcda5c03d6d81e4399ec5",
          "name": "Yang Wang",
          "hidden": false
        },
        {
          "_id": "695dcda5c03d6d81e4399ec6",
          "name": "Zhe Xu",
          "hidden": false
        },
        {
          "_id": "695dcda5c03d6d81e4399ec7",
          "name": "Yitian Gong",
          "hidden": false
        },
        {
          "_id": "695dcda5c03d6d81e4399ec8",
          "name": "Yuqian Zhang",
          "hidden": false
        },
        {
          "_id": "695dcda5c03d6d81e4399ec9",
          "name": "Wenbo Zhang",
          "hidden": false
        },
        {
          "_id": "695dcda5c03d6d81e4399eca",
          "name": "Zhaoye Fei",
          "hidden": false
        },
        {
          "_id": "695dcda5c03d6d81e4399ecb",
          "name": "Qinyuan Cheng",
          "hidden": false
        },
        {
          "_id": "695dcda5c03d6d81e4399ecc",
          "name": "Shimin Li",
          "hidden": false
        },
        {
          "_id": "695dcda5c03d6d81e4399ecd",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-04T15:01:10.000Z",
      "submittedOnDailyAt": "2026-01-07T00:52:16.123Z",
      "title": "MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization",
      "submittedOnDailyBy": {
        "_id": "629ef8544313a7c1dd671130",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629ef8544313a7c1dd671130/i5xfHIgELcuO1Ew19ebTw.png",
        "isPro": false,
        "fullname": "Zhaoye Fei",
        "user": "ngc7293",
        "type": "user"
      },
      "summary": "Speaker-Attributed, Time-Stamped Transcription (SATS) aims to transcribe what is said and to precisely determine the timing of each speaker, which is particularly valuable for meeting transcription. Existing SATS systems rarely adopt an end-to-end formulation and are further constrained by limited context windows, weak long-range speaker memory, and the inability to output timestamps. To address these limitations, we present MOSS Transcribe Diarize, a unified multimodal large language model that jointly performs Speaker-Attributed, Time-Stamped Transcription in an end-to-end paradigm. Trained on extensive real wild data and equipped with a 128k context window for up to 90-minute inputs, MOSS Transcribe Diarize scales well and generalizes robustly. Across comprehensive evaluations, it outperforms state-of-the-art commercial systems on multiple public and in-house benchmarks.",
      "upvotes": 35,
      "discussionId": "695dcda6c03d6d81e4399ece",
      "projectPage": "https://mosi.cn/models/moss-transcribe-diarize",
      "ai_summary": "A unified multimodal large language model for end-to-end speaker-attributed, time-stamped transcription with extended context window and strong generalization across benchmarks.",
      "ai_keywords": [
        "multimodal large language model",
        "end-to-end paradigm",
        "speaker diarization",
        "time-stamped transcription",
        "context window",
        "robust generalization"
      ],
      "organization": {
        "_id": "613b0dee83ec35d460684607",
        "name": "OpenMOSS-Team",
        "fullname": "OpenMOSS",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"
      }
    },
    "publishedAt": "2026-01-04T10:01:10.000Z",
    "title": "MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization",
    "summary": "Speaker-Attributed, Time-Stamped Transcription (SATS) aims to transcribe what is said and to precisely determine the timing of each speaker, which is particularly valuable for meeting transcription. Existing SATS systems rarely adopt an end-to-end formulation and are further constrained by limited context windows, weak long-range speaker memory, and the inability to output timestamps. To address these limitations, we present MOSS Transcribe Diarize, a unified multimodal large language model that jointly performs Speaker-Attributed, Time-Stamped Transcription in an end-to-end paradigm. Trained on extensive real wild data and equipped with a 128k context window for up to 90-minute inputs, MOSS Transcribe Diarize scales well and generalizes robustly. Across comprehensive evaluations, it outperforms state-of-the-art commercial systems on multiple public and in-house benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01554.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "629ef8544313a7c1dd671130",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629ef8544313a7c1dd671130/i5xfHIgELcuO1Ew19ebTw.png",
      "fullname": "Zhaoye Fei",
      "name": "ngc7293",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "613b0dee83ec35d460684607",
      "name": "OpenMOSS-Team",
      "fullname": "OpenMOSS",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.03193",
      "authors": [
        {
          "_id": "695de87dc03d6d81e4399fd2",
          "name": "Ruiyan Han",
          "hidden": false
        },
        {
          "_id": "695de87dc03d6d81e4399fd3",
          "name": "Zhen Fang",
          "hidden": false
        },
        {
          "_id": "695de87dc03d6d81e4399fd4",
          "name": "XinYu Sun",
          "hidden": false
        },
        {
          "_id": "695de87dc03d6d81e4399fd5",
          "name": "Yuchen Ma",
          "hidden": false
        },
        {
          "_id": "695de87dc03d6d81e4399fd6",
          "name": "Ziheng Wang",
          "hidden": false
        },
        {
          "_id": "695de87dc03d6d81e4399fd7",
          "name": "Yu Zeng",
          "hidden": false
        },
        {
          "_id": "695de87dc03d6d81e4399fd8",
          "name": "Zehui Chen",
          "hidden": false
        },
        {
          "_id": "695de87dc03d6d81e4399fd9",
          "name": "Lin Chen",
          "hidden": false
        },
        {
          "_id": "695de87dc03d6d81e4399fda",
          "name": "Wenxuan Huang",
          "hidden": false
        },
        {
          "_id": "695de87dc03d6d81e4399fdb",
          "name": "Wei-Jie Xu",
          "hidden": false
        },
        {
          "_id": "695de87dc03d6d81e4399fdc",
          "name": "Yi Cao",
          "hidden": false
        },
        {
          "_id": "695de87dc03d6d81e4399fdd",
          "name": "Feng Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-06T17:15:50.000Z",
      "submittedOnDailyAt": "2026-01-07T02:31:26.744Z",
      "title": "UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision",
      "submittedOnDailyBy": {
        "_id": "64b02ec0e5000ae8a572ced5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
        "isPro": false,
        "fullname": "Lin Chen",
        "user": "Lin-Chen",
        "type": "user"
      },
      "summary": "While Unified Multimodal Models (UMMs) have achieved remarkable success in cross-modal comprehension, a significant gap persists in their ability to leverage such internal knowledge for high-quality generation. We formalize this discrepancy as Conduction Aphasia, a phenomenon where models accurately interpret multimodal inputs but struggle to translate that understanding into faithful and controllable synthesis. To address this, we propose UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision. By partitioning a single UMM into three collaborative roles: Proposer, Solver, and Judge, UniCorn generates high-quality interactions via self-play and employs cognitive pattern reconstruction to distill latent understanding into explicit generative signals. To validate the restoration of multimodal coherence, we introduce UniCycle, a cycle-consistency benchmark based on a Text to Image to Text reconstruction loop. Extensive experiments demonstrate that UniCorn achieves comprehensive and substantial improvements over the base model across six general image generation benchmarks. Notably, it achieves SOTA performance on TIIF(73.8), DPG(86.8), CompBench(88.5), and UniCycle while further delivering substantial gains of +5.0 on WISE and +6.5 on OneIG. These results highlight that our method significantly enhances T2I generation while maintaining robust comprehension, demonstrating the scalability of fully self-supervised refinement for unified multimodal intelligence.",
      "upvotes": 16,
      "discussionId": "695de87dc03d6d81e4399fde",
      "projectPage": "https://costaliya.github.io/UniCorn.github.io/",
      "ai_summary": "UniCorn, a self-improvement framework for unified multimodal models, addresses generation gaps through self-play and cognitive pattern reconstruction, achieving state-of-the-art results in text-to-image generation.",
      "ai_keywords": [
        "Unified Multimodal Models",
        "Conduction Aphasia",
        "UniCorn",
        "self-improvement framework",
        "self-play",
        "cognitive pattern reconstruction",
        "Text to Image",
        "cycle-consistency",
        "UniCycle",
        "T2I generation"
      ]
    },
    "publishedAt": "2026-01-06T12:15:50.000Z",
    "title": "UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision",
    "summary": "While Unified Multimodal Models (UMMs) have achieved remarkable success in cross-modal comprehension, a significant gap persists in their ability to leverage such internal knowledge for high-quality generation. We formalize this discrepancy as Conduction Aphasia, a phenomenon where models accurately interpret multimodal inputs but struggle to translate that understanding into faithful and controllable synthesis. To address this, we propose UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision. By partitioning a single UMM into three collaborative roles: Proposer, Solver, and Judge, UniCorn generates high-quality interactions via self-play and employs cognitive pattern reconstruction to distill latent understanding into explicit generative signals. To validate the restoration of multimodal coherence, we introduce UniCycle, a cycle-consistency benchmark based on a Text to Image to Text reconstruction loop. Extensive experiments demonstrate that UniCorn achieves comprehensive and substantial improvements over the base model across six general image generation benchmarks. Notably, it achieves SOTA performance on TIIF(73.8), DPG(86.8), CompBench(88.5), and UniCycle while further delivering substantial gains of +5.0 on WISE and +6.5 on OneIG. These results highlight that our method significantly enhances T2I generation while maintaining robust comprehension, demonstrating the scalability of fully self-supervised refinement for unified multimodal intelligence.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03193.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b02ec0e5000ae8a572ced5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
      "fullname": "Lin Chen",
      "name": "Lin-Chen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 90,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.03233",
      "authors": [
        {
          "_id": "695dc6d9c03d6d81e4399e85",
          "name": "Yoav HaCohen",
          "hidden": false
        },
        {
          "_id": "695dc6d9c03d6d81e4399e86",
          "name": "Benny Brazowski",
          "hidden": false
        },
        {
          "_id": "695dc6d9c03d6d81e4399e87",
          "name": "Nisan Chiprut",
          "hidden": false
        },
        {
          "_id": "695dc6d9c03d6d81e4399e88",
          "name": "Yaki Bitterman",
          "hidden": false
        },
        {
          "_id": "695dc6d9c03d6d81e4399e89",
          "name": "Andrew Kvochko",
          "hidden": false
        },
        {
          "_id": "695dc6d9c03d6d81e4399e8a",
          "name": "Avishai Berkowitz",
          "hidden": false
        },
        {
          "_id": "695dc6d9c03d6d81e4399e8b",
          "name": "Daniel Shalem",
          "hidden": false
        },
        {
          "_id": "695dc6d9c03d6d81e4399e8c",
          "name": "Daphna Lifschitz",
          "hidden": false
        },
        {
          "_id": "695dc6d9c03d6d81e4399e8d",
          "name": "Dudu Moshe",
          "hidden": false
        },
        {
          "_id": "695dc6d9c03d6d81e4399e8e",
          "name": "Eitan Porat",
          "hidden": false
        },
        {
          "_id": "695dc6d9c03d6d81e4399e8f",
          "name": "Eitan Richardson",
          "hidden": false
        },
        {
          "_id": "695dc6d9c03d6d81e4399e90",
          "name": "Guy Shiran",
          "hidden": false
        },
        {
          "_id": "695dc6d9c03d6d81e4399e91",
          "name": "Itay Chachy",
          "hidden": false
        },
        {
          "_id": "695dc6d9c03d6d81e4399e92",
          "name": "Jonathan Chetboun",
          "hidden": false
        },
        {
          "_id": "695dc6d9c03d6d81e4399e93",
          "name": "Michael Finkelson",
          "hidden": false
        },
        {
          "_id": "695dc6d9c03d6d81e4399e94",
          "name": "Michael Kupchick",
          "hidden": false
        },
        {
          "_id": "695dc6d9c03d6d81e4399e95",
          "name": "Nir Zabari",
          "hidden": false
        },
        {
          "_id": "695dc6d9c03d6d81e4399e96",
          "name": "Nitzan Guetta",
          "hidden": false
        },
        {
          "_id": "695dc6d9c03d6d81e4399e97",
          "name": "Noa Kotler",
          "hidden": false
        },
        {
          "_id": "695dc6d9c03d6d81e4399e98",
          "name": "Ofir Bibi",
          "hidden": false
        },
        {
          "_id": "695dc6d9c03d6d81e4399e99",
          "name": "Ori Gordon",
          "hidden": false
        },
        {
          "_id": "695dc6d9c03d6d81e4399e9a",
          "name": "Poriya Panet",
          "hidden": false
        },
        {
          "_id": "695dc6d9c03d6d81e4399e9b",
          "name": "Roi Benita",
          "hidden": false
        },
        {
          "_id": "695dc6d9c03d6d81e4399e9c",
          "name": "Shahar Armon",
          "hidden": false
        },
        {
          "_id": "695dc6d9c03d6d81e4399e9d",
          "name": "Victor Kulikov",
          "hidden": false
        },
        {
          "_id": "695dc6d9c03d6d81e4399e9e",
          "name": "Yaron Inger",
          "hidden": false
        },
        {
          "_id": "695dc6d9c03d6d81e4399e9f",
          "name": "Yonatan Shiftan",
          "hidden": false
        },
        {
          "_id": "695dc6d9c03d6d81e4399ea0",
          "name": "Zeev Melumian",
          "hidden": false
        },
        {
          "_id": "695dc6d9c03d6d81e4399ea1",
          "name": "Zeev Farbman",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/cYoXYuK3pjt85pl5-fvUv.mp4"
      ],
      "publishedAt": "2026-01-06T18:24:41.000Z",
      "submittedOnDailyAt": "2026-01-07T00:07:29.528Z",
      "title": "LTX-2: Efficient Joint Audio-Visual Foundation Model",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.",
      "upvotes": 15,
      "discussionId": "695dc6d9c03d6d81e4399ea2",
      "projectPage": "https://app.ltx.studio/ltx-2-playground/i2v",
      "githubRepo": "https://github.com/Lightricks/LTX-2",
      "githubRepoAddedBy": "user",
      "ai_summary": "LTX-2 is an open-source audiovisual diffusion model that generates synchronized video and audio content using a dual-stream transformer architecture with cross-modal attention and classifier-free guidance.",
      "ai_keywords": [
        "text-to-video diffusion models",
        "audiovisual content",
        "dual-stream transformer",
        "cross-attention layers",
        "temporal positional embeddings",
        "AdaLN",
        "classifier-free guidance",
        "modality-aware classifier-free guidance",
        "multilingual text encoder",
        "diffusion models"
      ],
      "githubStars": 538
    },
    "publishedAt": "2026-01-06T13:24:41.000Z",
    "title": "LTX-2: Efficient Joint Audio-Visual Foundation Model",
    "summary": "Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/cYoXYuK3pjt85pl5-fvUv.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03233.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 200,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.01874",
      "authors": [
        {
          "_id": "695cee316aa73bc11f09163b",
          "name": "Shuhang Chen",
          "hidden": false
        },
        {
          "_id": "695cee316aa73bc11f09163c",
          "name": "Yunqiu Xu",
          "hidden": false
        },
        {
          "_id": "695cee316aa73bc11f09163d",
          "name": "Junjie Xie",
          "hidden": false
        },
        {
          "_id": "695cee316aa73bc11f09163e",
          "name": "Aojun Lu",
          "hidden": false
        },
        {
          "_id": "695cee316aa73bc11f09163f",
          "name": "Tao Feng",
          "hidden": false
        },
        {
          "_id": "695cee316aa73bc11f091640",
          "name": "Zeying Huang",
          "hidden": false
        },
        {
          "_id": "695cee316aa73bc11f091641",
          "name": "Ning Zhang",
          "hidden": false
        },
        {
          "_id": "695cee316aa73bc11f091642",
          "name": "Yi Sun",
          "hidden": false
        },
        {
          "_id": "695cee316aa73bc11f091643",
          "name": "Yi Yang",
          "hidden": false
        },
        {
          "_id": "695cee316aa73bc11f091644",
          "name": "Hangjie Yuan",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-05T08:02:18.000Z",
      "submittedOnDailyAt": "2026-01-07T01:06:12.554Z",
      "title": "CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving",
      "submittedOnDailyBy": {
        "_id": "646c77911ee398a4e9404b8b",
        "avatarUrl": "/avatars/05d1ea421dd4f3e2fd47cbe99fc52933.svg",
        "isPro": false,
        "fullname": "Yunqiu Xu",
        "user": "Yunqiu",
        "type": "user"
      },
      "summary": "Despite significant progress, multimodal large language models continue to struggle with visual mathematical problem solving. Some recent works recognize that visual perception is a bottleneck in visual mathematical reasoning, but their solutions are limited to improving the extraction and interpretation of visual inputs. Notably, they all ignore the key issue of whether the extracted visual cues are faithfully integrated and properly utilized in subsequent reasoning. Motivated by this, we present CogFlow, a novel cognitive-inspired three-stage framework that incorporates a knowledge internalization stage, explicitly simulating the hierarchical flow of human reasoning: perceptionRightarrowinternalizationRightarrowreasoning. Inline with this hierarchical flow, we holistically enhance all its stages. We devise Synergistic Visual Rewards to boost perception capabilities in parametric and semantic spaces, jointly improving visual information extraction from symbols and diagrams. To guarantee faithful integration of extracted visual cues into subsequent reasoning, we introduce a Knowledge Internalization Reward model in the internalization stage, bridging perception and reasoning. Moreover, we design a Visual-Gated Policy Optimization algorithm to further enforce the reasoning is grounded with the visual knowledge, preventing models seeking shortcuts that appear coherent but are visually ungrounded reasoning chains. Moreover, we contribute a new dataset MathCog for model training, which contains samples with over 120K high-quality perception-reasoning aligned annotations. Comprehensive experiments and analysis on commonly used visual mathematical reasoning benchmarks validate the superiority of the proposed CogFlow.",
      "upvotes": 15,
      "discussionId": "695cee326aa73bc11f091645",
      "projectPage": "https://shchen233.github.io/cogflow/",
      "ai_summary": "Visual mathematical problem solving remains challenging for multimodal large language models, prompting the development of CogFlow, a cognitive-inspired three-stage framework that enhances perception, internalization, and reasoning through synergistic rewards and visual-gated policy optimization.",
      "ai_keywords": [
        "multimodal large language models",
        "visual mathematical problem solving",
        "visual perception",
        "visual reasoning",
        "cognitive-inspired framework",
        "knowledge internalization",
        "visual-gated policy optimization",
        "Synergistic Visual Rewards",
        "Knowledge Internalization Reward model"
      ]
    },
    "publishedAt": "2026-01-05T03:02:18.000Z",
    "title": "CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving",
    "summary": "Despite significant progress, multimodal large language models continue to struggle with visual mathematical problem solving. Some recent works recognize that visual perception is a bottleneck in visual mathematical reasoning, but their solutions are limited to improving the extraction and interpretation of visual inputs. Notably, they all ignore the key issue of whether the extracted visual cues are faithfully integrated and properly utilized in subsequent reasoning. Motivated by this, we present CogFlow, a novel cognitive-inspired three-stage framework that incorporates a knowledge internalization stage, explicitly simulating the hierarchical flow of human reasoning: perceptionRightarrowinternalizationRightarrowreasoning. Inline with this hierarchical flow, we holistically enhance all its stages. We devise Synergistic Visual Rewards to boost perception capabilities in parametric and semantic spaces, jointly improving visual information extraction from symbols and diagrams. To guarantee faithful integration of extracted visual cues into subsequent reasoning, we introduce a Knowledge Internalization Reward model in the internalization stage, bridging perception and reasoning. Moreover, we design a Visual-Gated Policy Optimization algorithm to further enforce the reasoning is grounded with the visual knowledge, preventing models seeking shortcuts that appear coherent but are visually ungrounded reasoning chains. Moreover, we contribute a new dataset MathCog for model training, which contains samples with over 120K high-quality perception-reasoning aligned annotations. Comprehensive experiments and analysis on commonly used visual mathematical reasoning benchmarks validate the superiority of the proposed CogFlow.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01874.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "646c77911ee398a4e9404b8b",
      "avatarUrl": "/avatars/05d1ea421dd4f3e2fd47cbe99fc52933.svg",
      "fullname": "Yunqiu Xu",
      "name": "Yunqiu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.02785",
      "authors": [
        {
          "_id": "695dd030c03d6d81e4399ee8",
          "name": "Mengtian Li",
          "hidden": false
        },
        {
          "_id": "695dd030c03d6d81e4399ee9",
          "name": "Jinshu Chen",
          "hidden": false
        },
        {
          "_id": "695dd030c03d6d81e4399eea",
          "name": "Songtao Zhao",
          "hidden": false
        },
        {
          "_id": "695dd030c03d6d81e4399eeb",
          "name": "Wanquan Feng",
          "hidden": false
        },
        {
          "_id": "695dd030c03d6d81e4399eec",
          "name": "Pengqi Tu",
          "hidden": false
        },
        {
          "_id": "695dd030c03d6d81e4399eed",
          "name": "Qian He",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/1B89POtz-ncYIp_iw_rCT.qt"
      ],
      "publishedAt": "2026-01-06T07:42:12.000Z",
      "submittedOnDailyAt": "2026-01-07T00:48:26.802Z",
      "title": "DreamStyle: A Unified Framework for Video Stylization",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Video stylization, an important downstream task of video generation models, has not yet been thoroughly explored. Its input style conditions typically include text, style image, and stylized first frame. Each condition has a characteristic advantage: text is more flexible, style image provides a more accurate visual anchor, and stylized first frame makes long-video stylization feasible. However, existing methods are largely confined to a single type of style condition, which limits their scope of application. Additionally, their lack of high-quality datasets leads to style inconsistency and temporal flicker. To address these limitations, we introduce DreamStyle, a unified framework for video stylization, supporting (1) text-guided, (2) style-image-guided, and (3) first-frame-guided video stylization, accompanied by a well-designed data curation pipeline to acquire high-quality paired video data. DreamStyle is built on a vanilla Image-to-Video (I2V) model and trained using a Low-Rank Adaptation (LoRA) with token-specific up matrices that reduces the confusion among different condition tokens. Both qualitative and quantitative evaluations demonstrate that DreamStyle is competent in all three video stylization tasks, and outperforms the competitors in style consistency and video quality.",
      "upvotes": 13,
      "discussionId": "695dd030c03d6d81e4399eee",
      "projectPage": "https://lemonsky1995.github.io/dreamstyle/",
      "ai_summary": "DreamStyle is a unified video stylization framework that supports multiple style conditions while addressing style inconsistency and temporal flicker through a specialized data curation pipeline and LoRA training approach.",
      "ai_keywords": [
        "video stylization",
        "Image-to-Video (I2V) model",
        "Low-Rank Adaptation (LoRA)",
        "token-specific up matrices",
        "style consistency",
        "temporal flicker"
      ],
      "organization": {
        "_id": "653b817d32c97d0655575872",
        "name": "ByteDance",
        "fullname": "ByteDance",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
      }
    },
    "publishedAt": "2026-01-06T02:42:12.000Z",
    "title": "DreamStyle: A Unified Framework for Video Stylization",
    "summary": "Video stylization, an important downstream task of video generation models, has not yet been thoroughly explored. Its input style conditions typically include text, style image, and stylized first frame. Each condition has a characteristic advantage: text is more flexible, style image provides a more accurate visual anchor, and stylized first frame makes long-video stylization feasible. However, existing methods are largely confined to a single type of style condition, which limits their scope of application. Additionally, their lack of high-quality datasets leads to style inconsistency and temporal flicker. To address these limitations, we introduce DreamStyle, a unified framework for video stylization, supporting (1) text-guided, (2) style-image-guided, and (3) first-frame-guided video stylization, accompanied by a well-designed data curation pipeline to acquire high-quality paired video data. DreamStyle is built on a vanilla Image-to-Video (I2V) model and trained using a Low-Rank Adaptation (LoRA) with token-specific up matrices that reduces the confusion among different condition tokens. Both qualitative and quantitative evaluations demonstrate that DreamStyle is competent in all three video stylization tasks, and outperforms the competitors in style consistency and video quality.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/1B89POtz-ncYIp_iw_rCT.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02785.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 200,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "653b817d32c97d0655575872",
      "name": "ByteDance",
      "fullname": "ByteDance",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.02780",
      "authors": [
        {
          "_id": "695dd171c03d6d81e4399ef4",
          "name": "Bangjun Xiao",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399ef5",
          "name": "Bingquan Xia",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399ef6",
          "name": "Bo Yang",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399ef7",
          "name": "Bofei Gao",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399ef8",
          "name": "Bowen Shen",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399ef9",
          "name": "Chen Zhang",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399efa",
          "name": "Chenhong He",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399efb",
          "name": "Chiheng Lou",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399efc",
          "name": "Fuli Luo",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399efd",
          "name": "Gang Wang",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399efe",
          "name": "Gang Xie",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399eff",
          "name": "Hailin Zhang",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f00",
          "name": "Hanglong Lv",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f01",
          "name": "Hanyu Li",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f02",
          "name": "Heyu Chen",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f03",
          "name": "Hongshen Xu",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f04",
          "name": "Houbin Zhang",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f05",
          "name": "Huaqiu Liu",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f06",
          "name": "Jiangshan Duo",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f07",
          "name": "Jianyu Wei",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f08",
          "name": "Jiebao Xiao",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f09",
          "name": "Jinhao Dong",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f0a",
          "name": "Jun Shi",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f0b",
          "name": "Junhao Hu",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f0c",
          "name": "Kainan Bao",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f0d",
          "name": "Kang Zhou",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f0e",
          "name": "Lei Li",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f0f",
          "name": "Liang Zhao",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f10",
          "name": "Linghao Zhang",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f11",
          "name": "Peidian Li",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f12",
          "name": "Qianli Chen",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f13",
          "name": "Shaohui Liu",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f14",
          "name": "Shihua Yu",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f15",
          "name": "Shijie Cao",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f16",
          "name": "Shimao Chen",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f17",
          "name": "Shouqiu Yu",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f18",
          "name": "Shuo Liu",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f19",
          "name": "Tianling Zhou",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f1a",
          "name": "Weijiang Su",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f1b",
          "name": "Weikun Wang",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f1c",
          "name": "Wenhan Ma",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f1d",
          "name": "Xiangwei Deng",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f1e",
          "name": "Bohan Mao",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f1f",
          "name": "Bowen Ye",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f20",
          "name": "Can Cai",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f21",
          "name": "Chenghua Wang",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f22",
          "name": "Chengxuan Zhu",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f23",
          "name": "Chong Ma",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f24",
          "name": "Chun Chen",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f25",
          "name": "Chunan Li",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f26",
          "name": "Dawei Zhu",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f27",
          "name": "Deshan Xiao",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f28",
          "name": "Dong Zhang",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f29",
          "name": "Duo Zhang",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f2a",
          "name": "Fangyue Liu",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f2b",
          "name": "Feiyu Yang",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f2c",
          "name": "Fengyuan Shi",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f2d",
          "name": "Guoan Wang",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f2e",
          "name": "Hao Tian",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f2f",
          "name": "Hao Wu",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f30",
          "name": "Heng Qu",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f31",
          "name": "Hongfei Yi",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f32",
          "name": "Hongxu An",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f33",
          "name": "Hongyi Guan",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f34",
          "name": "Xing Zhang",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f35",
          "name": "Yifan Song",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f36",
          "name": "Yihan Yan",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f37",
          "name": "Yihao Zhao",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f38",
          "name": "Yingchun Lai",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f39",
          "name": "Yizhao Gao",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f3a",
          "name": "Yu Cheng",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f3b",
          "name": "Yuanyuan Tian",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f3c",
          "name": "Yudong Wang",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f3d",
          "name": "Zhen Tang",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f3e",
          "name": "Zhengju Tang",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f3f",
          "name": "Zhengtao Wen",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f40",
          "name": "Zhichao Song",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f41",
          "name": "Zhixian Zheng",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f42",
          "name": "Zihan Jiang",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f43",
          "name": "Jian Wen",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f44",
          "name": "Jiarui Sun",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f45",
          "name": "Jiawei Li",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f46",
          "name": "Jinlong Xue",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f47",
          "name": "Jun Xia",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f48",
          "name": "Kai Fang",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f49",
          "name": "Menghang Zhu",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f4a",
          "name": "Nuo Chen",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f4b",
          "name": "Qian Tu",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f4c",
          "name": "Qihao Zhang",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f4d",
          "name": "Qiying Wang",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f4e",
          "name": "Rang Li",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f4f",
          "name": "Rui Ma",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f50",
          "name": "Shaolei Zhang",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f51",
          "name": "Shengfan Wang",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f52",
          "name": "Shicheng Li",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f53",
          "name": "Shuhao Gu",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f54",
          "name": "Shuhuai Ren",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f55",
          "name": "Sirui Deng",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f56",
          "name": "Tao Guo",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f57",
          "name": "Tianyang Lu",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f58",
          "name": "Weiji Zhuang",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f59",
          "name": "Weikang Zhang",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f5a",
          "name": "Weimin Xiong",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f5b",
          "name": "Wenshan Huang",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f5c",
          "name": "Wenyu Yang",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f5d",
          "name": "Xin Zhang",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f5e",
          "name": "Xing Yong",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f5f",
          "name": "Xu Wang",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f60",
          "name": "Xueyang Xie",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f61",
          "name": "Yilin Jiang",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f62",
          "name": "Yixin Yang",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f63",
          "name": "Yongzhe He",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f64",
          "name": "Yu Tu",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f65",
          "name": "Yuanliang Dong",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f66",
          "name": "Yuchen Liu",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f67",
          "name": "Yue Ma",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f68",
          "name": "Yue Yu",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f69",
          "name": "Yuxing Xiang",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f6a",
          "name": "Zhaojun Huang",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f6b",
          "name": "Zhenru Lin",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f6c",
          "name": "Zhipeng Xu",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f6d",
          "name": "Zhiyang Chen",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f6e",
          "name": "Zhonghua Deng",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f6f",
          "name": "Zihan Zhang",
          "hidden": false
        },
        {
          "_id": "695dd171c03d6d81e4399f70",
          "name": "Zihao Yue",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-06T07:31:47.000Z",
      "submittedOnDailyAt": "2026-01-07T00:52:38.331Z",
      "title": "MiMo-V2-Flash Technical Report",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We present MiMo-V2-Flash, a Mixture-of-Experts (MoE) model with 309B total parameters and 15B active parameters, designed for fast, strong reasoning and agentic capabilities. MiMo-V2-Flash adopts a hybrid attention architecture that interleaves Sliding Window Attention (SWA) with global attention, with a 128-token sliding window under a 5:1 hybrid ratio. The model is pre-trained on 27 trillion tokens with Multi-Token Prediction (MTP), employing a native 32k context length and subsequently extended to 256k. To efficiently scale post-training compute, MiMo-V2-Flash introduces a novel Multi-Teacher On-Policy Distillation (MOPD) paradigm. In this framework, domain-specialized teachers (e.g., trained via large-scale reinforcement learning) provide dense and token-level reward, enabling the student model to perfectly master teacher expertise. MiMo-V2-Flash rivals top-tier open-weight models such as DeepSeek-V3.2 and Kimi-K2, despite using only 1/2 and 1/3 of their total parameters, respectively. During inference, by repurposing MTP as a draft model for speculative decoding, MiMo-V2-Flash achieves up to 3.6 acceptance length and 2.6x decoding speedup with three MTP layers. We open-source both the model weights and the three-layer MTP weights to foster open research and community collaboration.",
      "upvotes": 13,
      "discussionId": "695dd172c03d6d81e4399f71",
      "projectPage": "https://mimo.xiaomi.com/blog/mimo-v2-flash",
      "githubRepo": "https://github.com/XiaomiMiMo/MiMo-V2-Flash",
      "githubRepoAddedBy": "user",
      "ai_summary": "MiMo-V2-Flash is a sparse Mixture-of-Experts model with hybrid attention architecture and efficient distillation technique that achieves strong performance with reduced parameters and improved inference speed.",
      "ai_keywords": [
        "Mixture-of-Experts",
        "Sliding Window Attention",
        "global attention",
        "Multi-Token Prediction",
        "speculative decoding",
        "Multi-Teacher On-Policy Distillation"
      ],
      "githubStars": 947,
      "organization": {
        "_id": "680cb4c37f289defb2210940",
        "name": "XiaomiMiMo",
        "fullname": "Xiaomi MiMo",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/680cb7d1233834890a64acee/5w_4aLfF-7MAyaIPOV498.jpeg"
      }
    },
    "publishedAt": "2026-01-06T02:31:47.000Z",
    "title": "MiMo-V2-Flash Technical Report",
    "summary": "We present MiMo-V2-Flash, a Mixture-of-Experts (MoE) model with 309B total parameters and 15B active parameters, designed for fast, strong reasoning and agentic capabilities. MiMo-V2-Flash adopts a hybrid attention architecture that interleaves Sliding Window Attention (SWA) with global attention, with a 128-token sliding window under a 5:1 hybrid ratio. The model is pre-trained on 27 trillion tokens with Multi-Token Prediction (MTP), employing a native 32k context length and subsequently extended to 256k. To efficiently scale post-training compute, MiMo-V2-Flash introduces a novel Multi-Teacher On-Policy Distillation (MOPD) paradigm. In this framework, domain-specialized teachers (e.g., trained via large-scale reinforcement learning) provide dense and token-level reward, enabling the student model to perfectly master teacher expertise. MiMo-V2-Flash rivals top-tier open-weight models such as DeepSeek-V3.2 and Kimi-K2, despite using only 1/2 and 1/3 of their total parameters, respectively. During inference, by repurposing MTP as a draft model for speculative decoding, MiMo-V2-Flash achieves up to 3.6 acceptance length and 2.6x decoding speedup with three MTP layers. We open-source both the model weights and the three-layer MTP weights to foster open research and community collaboration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02780.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 200,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "680cb4c37f289defb2210940",
      "name": "XiaomiMiMo",
      "fullname": "Xiaomi MiMo",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/680cb7d1233834890a64acee/5w_4aLfF-7MAyaIPOV498.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.03044",
      "authors": [
        {
          "_id": "695dc422c03d6d81e4399e60",
          "name": "Mingjie Pan",
          "hidden": false
        },
        {
          "_id": "695dc422c03d6d81e4399e61",
          "name": "Siyuan Feng",
          "hidden": false
        },
        {
          "_id": "695dc422c03d6d81e4399e62",
          "name": "Qinglin Zhang",
          "hidden": false
        },
        {
          "_id": "695dc422c03d6d81e4399e63",
          "name": "Xinchen Li",
          "hidden": false
        },
        {
          "_id": "695dc422c03d6d81e4399e64",
          "name": "Jianheng Song",
          "hidden": false
        },
        {
          "_id": "695dc422c03d6d81e4399e65",
          "name": "Chendi Qu",
          "hidden": false
        },
        {
          "_id": "695dc422c03d6d81e4399e66",
          "name": "Yi Wang",
          "hidden": false
        },
        {
          "_id": "695dc422c03d6d81e4399e67",
          "name": "Chuankang Li",
          "hidden": false
        },
        {
          "_id": "695dc422c03d6d81e4399e68",
          "name": "Ziyu Xiong",
          "hidden": false
        },
        {
          "_id": "695dc422c03d6d81e4399e69",
          "name": "Zhi Chen",
          "hidden": false
        },
        {
          "_id": "695dc422c03d6d81e4399e6a",
          "name": "Yi Liu",
          "hidden": false
        },
        {
          "_id": "695dc422c03d6d81e4399e6b",
          "name": "Jianlan Luo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/655ecd7c56e5ceaf05344b24/AEQ64pAi4UD3rvJ33Qeag.mp4"
      ],
      "publishedAt": "2026-01-06T14:25:11.000Z",
      "submittedOnDailyAt": "2026-01-07T03:50:19.522Z",
      "title": "SOP: A Scalable Online Post-Training System for Vision-Language-Action Models",
      "submittedOnDailyBy": {
        "_id": "655ecd7c56e5ceaf05344b24",
        "avatarUrl": "/avatars/faad88525197bb6c63be0068f19de418.svg",
        "isPro": false,
        "fullname": "MingjieP",
        "user": "pmj110119",
        "type": "user"
      },
      "summary": "Vision-language-action (VLA) models achieve strong generalization through large-scale pre-training, but real-world deployment requires expert-level task proficiency in addition to broad generality. Existing post-training approaches for VLA models are typically offline, single-robot, or task-specific, limiting effective on-policy adaptation and scalable learning from real-world interaction. We introduce a Scalable Online Post-training (SOP) system that enables online, distributed, multi-task post-training of generalist VLA models directly in the physical world. SOP tightly couples execution and learning through a closed-loop architecture in which a fleet of robots continuously streams on-policy experience and human intervention signals to a centralized cloud learner, and asynchronously receives updated policies. This design supports prompt on-policy correction, scales experience collection through parallel deployment, and preserves generality during adaptation. SOP is agnostic to the choice of post-training algorithm; we instantiate it with both interactive imitation learning (HG-DAgger) and reinforcement learning (RECAP). Across a range of real-world manipulation tasks including cloth folding, box assembly, and grocery restocking, we show that SOP substantially improves the performance of large pretrained VLA models while maintaining a single shared policy across tasks. Effective post-training can be achieved within hours of real-world interaction, and performance scales near-linearly with the number of robots in the fleet. These results suggest that tightly coupling online learning with fleet-scale deployment is instrumental to enabling efficient, reliable, and scalable post-training of generalist robot policies in the physical world.",
      "upvotes": 9,
      "discussionId": "695dc422c03d6d81e4399e6c",
      "ai_summary": "A scalable online post-training system enables real-world robot policy adaptation through distributed, multi-task learning that maintains generality while improving task proficiency.",
      "ai_keywords": [
        "Vision-language-action models",
        "post-training",
        "online learning",
        "distributed learning",
        "multi-task learning",
        "closed-loop architecture",
        "on-policy experience",
        "interactive imitation learning",
        "reinforcement learning",
        "fleet-scale deployment"
      ],
      "organization": {
        "_id": "6959ca2481c675360361a275",
        "name": "KineMind",
        "fullname": "AgiBot Research",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/69548eb63499b9bcd6ee0574/g02RWir_9quNCKL40jiV0.jpeg"
      }
    },
    "publishedAt": "2026-01-06T09:25:11.000Z",
    "title": "SOP: A Scalable Online Post-Training System for Vision-Language-Action Models",
    "summary": "Vision-language-action (VLA) models achieve strong generalization through large-scale pre-training, but real-world deployment requires expert-level task proficiency in addition to broad generality. Existing post-training approaches for VLA models are typically offline, single-robot, or task-specific, limiting effective on-policy adaptation and scalable learning from real-world interaction. We introduce a Scalable Online Post-training (SOP) system that enables online, distributed, multi-task post-training of generalist VLA models directly in the physical world. SOP tightly couples execution and learning through a closed-loop architecture in which a fleet of robots continuously streams on-policy experience and human intervention signals to a centralized cloud learner, and asynchronously receives updated policies. This design supports prompt on-policy correction, scales experience collection through parallel deployment, and preserves generality during adaptation. SOP is agnostic to the choice of post-training algorithm; we instantiate it with both interactive imitation learning (HG-DAgger) and reinforcement learning (RECAP). Across a range of real-world manipulation tasks including cloth folding, box assembly, and grocery restocking, we show that SOP substantially improves the performance of large pretrained VLA models while maintaining a single shared policy across tasks. Effective post-training can be achieved within hours of real-world interaction, and performance scales near-linearly with the number of robots in the fleet. These results suggest that tightly coupling online learning with fleet-scale deployment is instrumental to enabling efficient, reliable, and scalable post-training of generalist robot policies in the physical world.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/655ecd7c56e5ceaf05344b24/AEQ64pAi4UD3rvJ33Qeag.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03044.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655ecd7c56e5ceaf05344b24",
      "avatarUrl": "/avatars/faad88525197bb6c63be0068f19de418.svg",
      "fullname": "MingjieP",
      "name": "pmj110119",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6959ca2481c675360361a275",
      "name": "KineMind",
      "fullname": "AgiBot Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/69548eb63499b9bcd6ee0574/g02RWir_9quNCKL40jiV0.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.02427",
      "authors": [
        {
          "_id": "695dce13c03d6d81e4399ed0",
          "name": "Loc Magne",
          "hidden": false
        },
        {
          "_id": "695dce13c03d6d81e4399ed1",
          "name": "Anas Awadalla",
          "hidden": false
        },
        {
          "_id": "695dce13c03d6d81e4399ed2",
          "name": "Guanzhi Wang",
          "hidden": false
        },
        {
          "_id": "695dce13c03d6d81e4399ed3",
          "name": "Yinzhen Xu",
          "hidden": false
        },
        {
          "_id": "695dce13c03d6d81e4399ed4",
          "name": "Joshua Belofsky",
          "hidden": false
        },
        {
          "_id": "695dce13c03d6d81e4399ed5",
          "name": "Fengyuan Hu",
          "hidden": false
        },
        {
          "_id": "695dce13c03d6d81e4399ed6",
          "name": "Joohwan Kim",
          "hidden": false
        },
        {
          "_id": "695dce13c03d6d81e4399ed7",
          "name": "Ludwig Schmidt",
          "hidden": false
        },
        {
          "_id": "695dce13c03d6d81e4399ed8",
          "name": "Georgia Gkioxari",
          "hidden": false
        },
        {
          "_id": "695dce13c03d6d81e4399ed9",
          "name": "Jan Kautz",
          "hidden": false
        },
        {
          "_id": "695dce13c03d6d81e4399eda",
          "name": "Yisong Yue",
          "hidden": false
        },
        {
          "_id": "695dce13c03d6d81e4399edb",
          "name": "Yejin Choi",
          "hidden": false
        },
        {
          "_id": "695dce13c03d6d81e4399edc",
          "name": "Yuke Zhu",
          "hidden": false
        },
        {
          "_id": "695dce13c03d6d81e4399edd",
          "name": "Linxi \"Jim\" Fan",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/wxFZ7NVsObT5w9DBp9Gdu.mp4"
      ],
      "publishedAt": "2026-01-04T16:24:50.000Z",
      "submittedOnDailyAt": "2026-01-07T00:38:46.198Z",
      "title": "NitroGen: An Open Foundation Model for Generalist Gaming Agents",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce NitroGen, a vision-action foundation model for generalist gaming agents that is trained on 40,000 hours of gameplay videos across more than 1,000 games. We incorporate three key ingredients: 1) an internet-scale video-action dataset constructed by automatically extracting player actions from publicly available gameplay videos, 2) a multi-game benchmark environment that can measure cross-game generalization, and 3) a unified vision-action model trained with large-scale behavior cloning. NitroGen exhibits strong competence across diverse domains, including combat encounters in 3D action games, high-precision control in 2D platformers, and exploration in procedurally generated worlds. It transfers effectively to unseen games, achieving up to 52% relative improvement in task success rates over models trained from scratch. We release the dataset, evaluation suite, and model weights to advance research on generalist embodied agents.",
      "upvotes": 9,
      "discussionId": "695dce14c03d6d81e4399ede",
      "projectPage": "https://nitrogen.minedojo.org/",
      "githubRepo": "https://github.com/MineDojo/NitroGen",
      "githubRepoAddedBy": "user",
      "ai_summary": "NitroGen is a vision-action foundation model trained on extensive gameplay data that demonstrates strong cross-game generalization and effective transfer learning capabilities.",
      "ai_keywords": [
        "vision-action foundation model",
        "large-scale behavior cloning",
        "cross-game generalization",
        "transfer learning",
        "embodied agents"
      ],
      "githubStars": 1397,
      "organization": {
        "_id": "60262b67268c201cdc8b7d43",
        "name": "nvidia",
        "fullname": "NVIDIA",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
      }
    },
    "publishedAt": "2026-01-04T11:24:50.000Z",
    "title": "NitroGen: An Open Foundation Model for Generalist Gaming Agents",
    "summary": "We introduce NitroGen, a vision-action foundation model for generalist gaming agents that is trained on 40,000 hours of gameplay videos across more than 1,000 games. We incorporate three key ingredients: 1) an internet-scale video-action dataset constructed by automatically extracting player actions from publicly available gameplay videos, 2) a multi-game benchmark environment that can measure cross-game generalization, and 3) a unified vision-action model trained with large-scale behavior cloning. NitroGen exhibits strong competence across diverse domains, including combat encounters in 3D action games, high-precision control in 2D platformers, and exploration in procedurally generated worlds. It transfers effectively to unseen games, achieving up to 52% relative improvement in task success rates over models trained from scratch. We release the dataset, evaluation suite, and model weights to advance research on generalist embodied agents.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/wxFZ7NVsObT5w9DBp9Gdu.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02427.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 200,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "60262b67268c201cdc8b7d43",
      "name": "nvidia",
      "fullname": "NVIDIA",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.22334",
      "authors": [
        {
          "_id": "695e0748c03d6d81e439a027",
          "name": "Yiheng Wang",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a028",
          "name": "Yixin Chen",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a029",
          "name": "Shuo Li",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a02a",
          "name": "Yifan Zhou",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a02b",
          "name": "Bo Liu",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a02c",
          "name": "Hengjian Gao",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a02d",
          "name": "Jiakang Yuan",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a02e",
          "name": "Jia Bu",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a02f",
          "name": "Wanghan Xu",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a030",
          "name": "Yuhao Zhou",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a031",
          "name": "Xiangyu Zhao",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a032",
          "name": "Zhiwang Zhou",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a033",
          "name": "Fengxiang Wang",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a034",
          "name": "Haodong Duan",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a035",
          "name": "Songyang Zhang",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a036",
          "name": "Jun Yao",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a037",
          "name": "Han Deng",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a038",
          "name": "Yizhou Wang",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a039",
          "name": "Jiabei Xiao",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a03a",
          "name": "Jiaqi Liu",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a03b",
          "name": "Encheng Su",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a03c",
          "name": "Yujie Liu",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a03d",
          "name": "Weida Wang",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a03e",
          "name": "Junchi Yao",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a03f",
          "name": "Shenghe Zheng",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a040",
          "name": "Haoran Sun",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a041",
          "name": "Runmin Ma",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a042",
          "name": "Xiangchao Yan",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a043",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a044",
          "name": "Dongzhan Zhou",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a045",
          "name": "Shufei Zhang",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a046",
          "name": "Peng Ye",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a047",
          "name": "Xiaosong Wang",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a048",
          "name": "Shixiang Tang",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a049",
          "name": "Wenlong Zhang",
          "hidden": false
        },
        {
          "_id": "695e0748c03d6d81e439a04a",
          "name": "Lei Bai",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-26T17:36:02.000Z",
      "submittedOnDailyAt": "2026-01-07T05:09:43.722Z",
      "title": "SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence",
      "submittedOnDailyBy": {
        "_id": "63bab9c1bb6a2fabd14421bd",
        "avatarUrl": "/avatars/c47030cf167072bf6ce3421f025c7746.svg",
        "isPro": false,
        "fullname": "Yuhao Zhou",
        "user": "Soptq",
        "type": "user"
      },
      "summary": "We introduce SciEvalKit, a unified benchmarking toolkit designed to evaluate AI models for science across a broad range of scientific disciplines and task capabilities. Unlike general-purpose evaluation platforms, SciEvalKit focuses on the core competencies of scientific intelligence, including Scientific Multimodal Perception, Scientific Multimodal Reasoning, Scientific Multimodal Understanding, Scientific Symbolic Reasoning, Scientific Code Generation, Science Hypothesis Generation and Scientific Knowledge Understanding. It supports six major scientific domains, spanning from physics and chemistry to astronomy and materials science. SciEvalKit builds a foundation of expert-grade scientific benchmarks, curated from real-world, domain-specific datasets, ensuring that tasks reflect authentic scientific challenges. The toolkit features a flexible, extensible evaluation pipeline that enables batch evaluation across models and datasets, supports custom model and dataset integration, and provides transparent, reproducible, and comparable results. By bridging capability-based evaluation and disciplinary diversity, SciEvalKit offers a standardized yet customizable infrastructure to benchmark the next generation of scientific foundation models and intelligent agents. The toolkit is open-sourced and actively maintained to foster community-driven development and progress in AI4Science.",
      "upvotes": 3,
      "discussionId": "695e0748c03d6d81e439a04b",
      "ai_summary": "SciEvalKit is a unified benchmarking toolkit for evaluating AI models across scientific disciplines, focusing on core scientific intelligence competencies and supporting diverse domains from physics to materials science.",
      "ai_keywords": [
        "AI models",
        "scientific intelligence",
        "Scientific Multimodal Perception",
        "Scientific Multimodal Reasoning",
        "Scientific Multimodal Understanding",
        "Scientific Symbolic Reasoning",
        "Scientific Code Generation",
        "Science Hypothesis Generation",
        "Scientific Knowledge Understanding",
        "scientific foundation models",
        "intelligent agents"
      ],
      "organization": {
        "_id": "690af7a885f71496ea396393",
        "name": "InternScience",
        "fullname": "Intern Science",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65f2a4198404ac0e4c0f175f/XIPU4aCPBogXSrj6NrfLk.png"
      }
    },
    "publishedAt": "2025-12-26T12:36:02.000Z",
    "title": "SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence",
    "summary": "We introduce SciEvalKit, a unified benchmarking toolkit designed to evaluate AI models for science across a broad range of scientific disciplines and task capabilities. Unlike general-purpose evaluation platforms, SciEvalKit focuses on the core competencies of scientific intelligence, including Scientific Multimodal Perception, Scientific Multimodal Reasoning, Scientific Multimodal Understanding, Scientific Symbolic Reasoning, Scientific Code Generation, Science Hypothesis Generation and Scientific Knowledge Understanding. It supports six major scientific domains, spanning from physics and chemistry to astronomy and materials science. SciEvalKit builds a foundation of expert-grade scientific benchmarks, curated from real-world, domain-specific datasets, ensuring that tasks reflect authentic scientific challenges. The toolkit features a flexible, extensible evaluation pipeline that enables batch evaluation across models and datasets, supports custom model and dataset integration, and provides transparent, reproducible, and comparable results. By bridging capability-based evaluation and disciplinary diversity, SciEvalKit offers a standardized yet customizable infrastructure to benchmark the next generation of scientific foundation models and intelligent agents. The toolkit is open-sourced and actively maintained to foster community-driven development and progress in AI4Science.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22334.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63bab9c1bb6a2fabd14421bd",
      "avatarUrl": "/avatars/c47030cf167072bf6ce3421f025c7746.svg",
      "fullname": "Yuhao Zhou",
      "name": "Soptq",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "690af7a885f71496ea396393",
      "name": "InternScience",
      "fullname": "Intern Science",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65f2a4198404ac0e4c0f175f/XIPU4aCPBogXSrj6NrfLk.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.03227",
      "authors": [
        {
          "_id": "695df23bc03d6d81e4399fec",
          "name": "Ruixing Zhang",
          "hidden": false
        },
        {
          "_id": "695df23bc03d6d81e4399fed",
          "name": "Zihan Liu",
          "hidden": false
        },
        {
          "_id": "695df23bc03d6d81e4399fee",
          "name": "Leilei Sun",
          "hidden": false
        },
        {
          "_id": "695df23bc03d6d81e4399fef",
          "name": "Tongyu Zhu",
          "hidden": false
        },
        {
          "_id": "695df23bc03d6d81e4399ff0",
          "name": "Weifeng Lv",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-06T18:13:24.000Z",
      "submittedOnDailyAt": "2026-01-07T03:14:46.464Z",
      "title": "The Sonar Moment: Benchmarking Audio-Language Models in Audio Geo-Localization",
      "submittedOnDailyBy": {
        "_id": "66b9bfade085a5c7e7c24669",
        "avatarUrl": "/avatars/db9a5ca6d5568555a97c578ba0820508.svg",
        "isPro": false,
        "fullname": "Rising0321",
        "user": "RisingZhang",
        "type": "user"
      },
      "summary": "Geo-localization aims to infer the geographic origin of a given signal. In computer vision, geo-localization has served as a demanding benchmark for compositional reasoning and is relevant to public safety. In contrast, progress on audio geo-localization has been constrained by the lack of high-quality audio-location pairs. To address this gap, we introduce AGL1K, the first audio geo-localization benchmark for audio language models (ALMs), spanning 72 countries and territories. To extract reliably localizable samples from a crowd-sourced platform, we propose the Audio Localizability metric that quantifies the informativeness of each recording, yielding 1,444 curated audio clips. Evaluations on 16 ALMs show that ALMs have emerged with audio geo-localization capability. We find that closed-source models substantially outperform open-source models, and that linguistic clues often dominate as a scaffold for prediction. We further analyze ALMs' reasoning traces, regional bias, error causes, and the interpretability of the localizability metric. Overall, AGL1K establishes a benchmark for audio geo-localization and may advance ALMs with better geospatial reasoning capability.",
      "upvotes": 1,
      "discussionId": "695df23cc03d6d81e4399ff1",
      "githubRepo": "https://github.com/Rising0321/AGL1K",
      "githubRepoAddedBy": "user",
      "ai_summary": "Audio geo-localization benchmark AGL1K is introduced to advance audio language models' geospatial reasoning capabilities through curated audio clips and evaluation across multiple models.",
      "ai_keywords": [
        "audio geo-localization",
        "audio language models",
        "AGL1K",
        "audio localizability metric",
        "compositional reasoning",
        "public safety",
        "crowd-sourced platform",
        "geospatial reasoning"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "63ba7720fc454697637969f1",
        "name": "Beihang",
        "fullname": "Beihang University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"
      }
    },
    "publishedAt": "2026-01-06T13:13:24.000Z",
    "title": "The Sonar Moment: Benchmarking Audio-Language Models in Audio Geo-Localization",
    "summary": "Geo-localization aims to infer the geographic origin of a given signal. In computer vision, geo-localization has served as a demanding benchmark for compositional reasoning and is relevant to public safety. In contrast, progress on audio geo-localization has been constrained by the lack of high-quality audio-location pairs. To address this gap, we introduce AGL1K, the first audio geo-localization benchmark for audio language models (ALMs), spanning 72 countries and territories. To extract reliably localizable samples from a crowd-sourced platform, we propose the Audio Localizability metric that quantifies the informativeness of each recording, yielding 1,444 curated audio clips. Evaluations on 16 ALMs show that ALMs have emerged with audio geo-localization capability. We find that closed-source models substantially outperform open-source models, and that linguistic clues often dominate as a scaffold for prediction. We further analyze ALMs' reasoning traces, regional bias, error causes, and the interpretability of the localizability metric. Overall, AGL1K establishes a benchmark for audio geo-localization and may advance ALMs with better geospatial reasoning capability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03227.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66b9bfade085a5c7e7c24669",
      "avatarUrl": "/avatars/db9a5ca6d5568555a97c578ba0820508.svg",
      "fullname": "Rising0321",
      "name": "RisingZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "63ba7720fc454697637969f1",
      "name": "Beihang",
      "fullname": "Beihang University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.03194",
      "authors": [
        {
          "_id": "695dcd3fc03d6d81e4399eb0",
          "name": "Mohammad Zia Ur Rehman",
          "hidden": false
        },
        {
          "_id": "695dcd3fc03d6d81e4399eb1",
          "name": "Sai Kartheek Reddy Kasu",
          "hidden": false
        },
        {
          "_id": "695dcd3fc03d6d81e4399eb2",
          "name": "Shashivardhan Reddy Koppula",
          "hidden": false
        },
        {
          "_id": "695dcd3fc03d6d81e4399eb3",
          "name": "Sai Rithwik Reddy Chirra",
          "hidden": false
        },
        {
          "_id": "695dcd3fc03d6d81e4399eb4",
          "name": "Shwetank Shekhar Singh",
          "hidden": false
        },
        {
          "_id": "695dcd3fc03d6d81e4399eb5",
          "name": "Nagendra Kumar",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-06T17:16:45.000Z",
      "submittedOnDailyAt": "2026-01-07T00:35:59.898Z",
      "title": "X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework",
      "submittedOnDailyBy": {
        "_id": "651692d718f3a57f869a5a0a",
        "avatarUrl": "/avatars/d5fe48de11e46675b05e1e2e1cf3505c.svg",
        "isPro": false,
        "fullname": "Sai Kartheek Reddy",
        "user": "UVSKKR",
        "type": "user"
      },
      "summary": "Hate speech detection on social media faces challenges in both accuracy and explainability, especially for underexplored Indic languages. We propose a novel explainability-guided training framework, X-MuTeST (eXplainable Multilingual haTe Speech deTection), for hate speech detection that combines high-level semantic reasoning from large language models (LLMs) with traditional attention-enhancing techniques. We extend this research to Hindi and Telugu alongside English by providing benchmark human-annotated rationales for each word to justify the assigned class label. The X-MuTeST explainability method computes the difference between the prediction probabilities of the original text and those of unigrams, bigrams, and trigrams. Final explanations are computed as the union between LLM explanations and X-MuTeST explanations. We show that leveraging human rationales during training enhances both classification performance and explainability. Moreover, combining human rationales with our explainability method to refine the model attention yields further improvements. We evaluate explainability using Plausibility metrics such as Token-F1 and IOU-F1 and Faithfulness metrics such as Comprehensiveness and Sufficiency. By focusing on under-resourced languages, our work advances hate speech detection across diverse linguistic contexts. Our dataset includes token-level rationale annotations for 6,004 Hindi, 4,492 Telugu, and 6,334 English samples. Data and code are available on https://github.com/ziarehman30/X-MuTeST",
      "upvotes": 1,
      "discussionId": "695dcd40c03d6d81e4399eb6",
      "githubRepo": "https://github.com/ziarehman30/X-MuTeST",
      "githubRepoAddedBy": "user",
      "ai_summary": "A novel explainability-guided training framework for hate speech detection in Indic languages that combines large language models with attention-enhancing techniques and provides human-annotated rationales for improved performance and interpretability.",
      "ai_keywords": [
        "explainability-guided training",
        "large language models",
        "attention-enhancing techniques",
        "hate speech detection",
        "multilingual",
        "token-level rationale annotations",
        "Plausibility metrics",
        "Faithfulness metrics",
        "Token-F1",
        "IOU-F1",
        "Comprehensiveness",
        "Sufficiency"
      ],
      "githubStars": 0
    },
    "publishedAt": "2026-01-06T12:16:45.000Z",
    "title": "X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework",
    "summary": "Hate speech detection on social media faces challenges in both accuracy and explainability, especially for underexplored Indic languages. We propose a novel explainability-guided training framework, X-MuTeST (eXplainable Multilingual haTe Speech deTection), for hate speech detection that combines high-level semantic reasoning from large language models (LLMs) with traditional attention-enhancing techniques. We extend this research to Hindi and Telugu alongside English by providing benchmark human-annotated rationales for each word to justify the assigned class label. The X-MuTeST explainability method computes the difference between the prediction probabilities of the original text and those of unigrams, bigrams, and trigrams. Final explanations are computed as the union between LLM explanations and X-MuTeST explanations. We show that leveraging human rationales during training enhances both classification performance and explainability. Moreover, combining human rationales with our explainability method to refine the model attention yields further improvements. We evaluate explainability using Plausibility metrics such as Token-F1 and IOU-F1 and Faithfulness metrics such as Comprehensiveness and Sufficiency. By focusing on under-resourced languages, our work advances hate speech detection across diverse linguistic contexts. Our dataset includes token-level rationale annotations for 6,004 Hindi, 4,492 Telugu, and 6,334 English samples. Data and code are available on https://github.com/ziarehman30/X-MuTeST",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03194.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "651692d718f3a57f869a5a0a",
      "avatarUrl": "/avatars/d5fe48de11e46675b05e1e2e1cf3505c.svg",
      "fullname": "Sai Kartheek Reddy",
      "name": "UVSKKR",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.03153",
      "authors": [
        {
          "_id": "695dc084c03d6d81e4399e46",
          "name": "Jiakai Tang",
          "hidden": false
        },
        {
          "_id": "695dc084c03d6d81e4399e47",
          "name": "Xu Chen",
          "hidden": false
        },
        {
          "_id": "695dc084c03d6d81e4399e48",
          "name": "Wen Chen",
          "hidden": false
        },
        {
          "_id": "695dc084c03d6d81e4399e49",
          "name": "Jian Wu",
          "hidden": false
        },
        {
          "_id": "695dc084c03d6d81e4399e4a",
          "name": "Yuning Jiang",
          "hidden": false
        },
        {
          "_id": "695dc084c03d6d81e4399e4b",
          "name": "Bo Zheng",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-06T16:25:48.000Z",
      "submittedOnDailyAt": "2026-01-07T01:08:55.177Z",
      "title": "Parallel Latent Reasoning for Sequential Recommendation",
      "submittedOnDailyBy": {
        "_id": "65acfb3a14e6582c30b4ce76",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65acfb3a14e6582c30b4ce76/RhEhePggBtyM0RIIqXQen.jpeg",
        "isPro": false,
        "fullname": "TangJiakai",
        "user": "TangJiakai5704",
        "type": "user"
      },
      "summary": "Capturing complex user preferences from sparse behavioral sequences remains a fundamental challenge in sequential recommendation. Recent latent reasoning methods have shown promise by extending test-time computation through multi-step reasoning, yet they exclusively rely on depth-level scaling along a single trajectory, suffering from diminishing returns as reasoning depth increases. To address this limitation, we propose Parallel Latent Reasoning (PLR), a novel framework that pioneers width-level computational scaling by exploring multiple diverse reasoning trajectories simultaneously. PLR constructs parallel reasoning streams through learnable trigger tokens in continuous latent space, preserves diversity across streams via global reasoning regularization, and adaptively synthesizes multi-stream outputs through mixture-of-reasoning-streams aggregation. Extensive experiments on three real-world datasets demonstrate that PLR substantially outperforms state-of-the-art baselines while maintaining real-time inference efficiency. Theoretical analysis further validates the effectiveness of parallel reasoning in improving generalization capability. Our work opens new avenues for enhancing reasoning capacity in sequential recommendation beyond existing depth scaling.",
      "upvotes": 1,
      "discussionId": "695dc085c03d6d81e4399e4c",
      "ai_summary": "Parallel Latent Reasoning framework improves sequential recommendation by exploring multiple diverse reasoning trajectories simultaneously through learnable trigger tokens and adaptive aggregation.",
      "ai_keywords": [
        "latent reasoning",
        "sequential recommendation",
        "multi-step reasoning",
        "depth-level scaling",
        "width-level computational scaling",
        "reasoning trajectories",
        "learnable trigger tokens",
        "global reasoning regularization",
        "mixture-of-reasoning-streams aggregation"
      ]
    },
    "publishedAt": "2026-01-06T11:25:48.000Z",
    "title": "Parallel Latent Reasoning for Sequential Recommendation",
    "summary": "Capturing complex user preferences from sparse behavioral sequences remains a fundamental challenge in sequential recommendation. Recent latent reasoning methods have shown promise by extending test-time computation through multi-step reasoning, yet they exclusively rely on depth-level scaling along a single trajectory, suffering from diminishing returns as reasoning depth increases. To address this limitation, we propose Parallel Latent Reasoning (PLR), a novel framework that pioneers width-level computational scaling by exploring multiple diverse reasoning trajectories simultaneously. PLR constructs parallel reasoning streams through learnable trigger tokens in continuous latent space, preserves diversity across streams via global reasoning regularization, and adaptively synthesizes multi-stream outputs through mixture-of-reasoning-streams aggregation. Extensive experiments on three real-world datasets demonstrate that PLR substantially outperforms state-of-the-art baselines while maintaining real-time inference efficiency. Theoretical analysis further validates the effectiveness of parallel reasoning in improving generalization capability. Our work opens new avenues for enhancing reasoning capacity in sequential recommendation beyond existing depth scaling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03153.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65acfb3a14e6582c30b4ce76",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65acfb3a14e6582c30b4ce76/RhEhePggBtyM0RIIqXQen.jpeg",
      "fullname": "TangJiakai",
      "name": "TangJiakai5704",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.02439",
      "authors": [
        {
          "_id": "695dd24cc03d6d81e4399f73",
          "name": "Hao Bai",
          "hidden": false
        },
        {
          "_id": "695dd24cc03d6d81e4399f74",
          "name": "Alexey Taymanov",
          "hidden": false
        },
        {
          "_id": "695dd24cc03d6d81e4399f75",
          "name": "Tong Zhang",
          "hidden": false
        },
        {
          "_id": "695dd24cc03d6d81e4399f76",
          "name": "Aviral Kumar",
          "hidden": false
        },
        {
          "_id": "695dd24cc03d6d81e4399f77",
          "name": "Spencer Whitehead",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-05T09:35:11.000Z",
      "submittedOnDailyAt": "2026-01-07T00:56:27.821Z",
      "title": "WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We present WebGym, the largest-to-date open-source environment for training realistic visual web agents. Real websites are non-stationary and diverse, making artificial or small-scale task sets insufficient for robust policy learning. WebGym contains nearly 300,000 tasks with rubric-based evaluations across diverse, real-world websites and difficulty levels. We train agents with a simple reinforcement learning (RL) recipe, which trains on the agent's own interaction traces (rollouts), using task rewards as feedback to guide learning. To enable scaling RL, we speed up sampling of trajectories in WebGym by developing a high-throughput asynchronous rollout system, designed specifically for web agents. Our system achieves a 4-5x rollout speedup compared to naive implementations. Second, we scale the task set breadth, depth, and size, which results in continued performance improvement. Fine-tuning a strong base vision-language model, Qwen-3-VL-8B-Instruct, on WebGym results in an improvement in success rate on an out-of-distribution test set from 26.2% to 42.9%, significantly outperforming agents based on proprietary models such as GPT-4o and GPT-5-Thinking that achieve 27.1% and 29.8%, respectively. This improvement is substantial because our test set consists only of tasks on websites never seen during training, unlike many other prior works on training visual web agents.",
      "upvotes": 0,
      "discussionId": "695dd24cc03d6d81e4399f78",
      "ai_summary": "WebGym presents a large-scale open-source environment for training visual web agents using reinforcement learning with high-throughput asynchronous sampling, achieving superior performance on unseen websites compared to proprietary models.",
      "ai_keywords": [
        "reinforcement learning",
        "rollout system",
        "vision-language model",
        "Qwen-3-VL-8B-Instruct",
        "asynchronous rollout system",
        "web agent training",
        "task rewards",
        "policy learning",
        "visual web agents"
      ],
      "organization": {
        "_id": "5e6485f787403103f9f1055e",
        "name": "microsoft",
        "fullname": "Microsoft",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
      }
    },
    "publishedAt": "2026-01-05T04:35:11.000Z",
    "title": "WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks",
    "summary": "We present WebGym, the largest-to-date open-source environment for training realistic visual web agents. Real websites are non-stationary and diverse, making artificial or small-scale task sets insufficient for robust policy learning. WebGym contains nearly 300,000 tasks with rubric-based evaluations across diverse, real-world websites and difficulty levels. We train agents with a simple reinforcement learning (RL) recipe, which trains on the agent's own interaction traces (rollouts), using task rewards as feedback to guide learning. To enable scaling RL, we speed up sampling of trajectories in WebGym by developing a high-throughput asynchronous rollout system, designed specifically for web agents. Our system achieves a 4-5x rollout speedup compared to naive implementations. Second, we scale the task set breadth, depth, and size, which results in continued performance improvement. Fine-tuning a strong base vision-language model, Qwen-3-VL-8B-Instruct, on WebGym results in an improvement in success rate on an out-of-distribution test set from 26.2% to 42.9%, significantly outperforming agents based on proprietary models such as GPT-4o and GPT-5-Thinking that achieve 27.1% and 29.8%, respectively. This improvement is substantial because our test set consists only of tasks on websites never seen during training, unlike many other prior works on training visual web agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02439.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 200,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "5e6485f787403103f9f1055e",
      "name": "microsoft",
      "fullname": "Microsoft",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.01720",
      "authors": [
        {
          "_id": "695c971f6aa73bc11f0914ea",
          "name": "Xijie Huang",
          "hidden": false
        },
        {
          "_id": "695c971f6aa73bc11f0914eb",
          "name": "Chengming Xu",
          "hidden": false
        },
        {
          "_id": "695c971f6aa73bc11f0914ec",
          "name": "Donghao Luo",
          "hidden": false
        },
        {
          "_id": "695c971f6aa73bc11f0914ed",
          "name": "Xiaobin Hu",
          "hidden": false
        },
        {
          "_id": "695c971f6aa73bc11f0914ee",
          "name": "Peng Tang",
          "hidden": false
        },
        {
          "_id": "695c971f6aa73bc11f0914ef",
          "name": "Xu Peng",
          "hidden": false
        },
        {
          "_id": "695c971f6aa73bc11f0914f0",
          "name": "Jiangning Zhang",
          "hidden": false
        },
        {
          "_id": "695c971f6aa73bc11f0914f1",
          "name": "Chengjie Wang",
          "hidden": false
        },
        {
          "_id": "695c971f6aa73bc11f0914f2",
          "name": "Yanwei Fu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-05T01:46:22.000Z",
      "submittedOnDailyAt": "2026-01-07T00:13:13.236Z",
      "title": "FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing",
      "submittedOnDailyBy": {
        "_id": "652fab9d04a34a9282bf29d6",
        "avatarUrl": "/avatars/cd5967b37ebb1225e9ae1d46f196e2e2.svg",
        "isPro": false,
        "fullname": "Chengming Xu",
        "user": "ChengmingX",
        "type": "user"
      },
      "summary": "First-Frame Propagation (FFP) offers a promising paradigm for controllable video editing, but existing methods are hampered by a reliance on cumbersome run-time guidance. We identify the root cause of this limitation as the inadequacy of current training datasets, which are often too short, low-resolution, and lack the task diversity required to teach robust temporal priors. To address this foundational data gap, we first introduce FFP-300K, a new large-scale dataset comprising 300K high-fidelity video pairs at 720p resolution and 81 frames in length, constructed via a principled two-track pipeline for diverse local and global edits. Building on this dataset, we propose a novel framework designed for true guidance-free FFP that resolves the critical tension between maintaining first-frame appearance and preserving source video motion. Architecturally, we introduce Adaptive Spatio-Temporal RoPE (AST-RoPE), which dynamically remaps positional encodings to disentangle appearance and motion references. At the objective level, we employ a self-distillation strategy where an identity propagation task acts as a powerful regularizer, ensuring long-term temporal stability and preventing semantic drift. Comprehensive experiments on the EditVerseBench benchmark demonstrate that our method significantly outperforming existing academic and commercial models by receiving about 0.2 PickScore and 0.3 VLM score improvement against these competitors.",
      "upvotes": 0,
      "discussionId": "695c971f6aa73bc11f0914f3",
      "projectPage": "https://ffp-300k.github.io/",
      "ai_summary": "A new large-scale video dataset and framework are presented that enable effective first-frame propagation without runtime guidance through adaptive spatio-temporal positional encoding and self-distillation techniques.",
      "ai_keywords": [
        "First-Frame Propagation",
        "video editing",
        "training datasets",
        "temporal priors",
        "FFP-300K",
        "Adaptive Spatio-Temporal RoPE",
        "AST-RoPE",
        "self-distillation",
        "identity propagation task",
        "PickScore",
        "VLM score",
        "EditVerseBench"
      ]
    },
    "publishedAt": "2026-01-04T20:46:22.000Z",
    "title": "FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing",
    "summary": "First-Frame Propagation (FFP) offers a promising paradigm for controllable video editing, but existing methods are hampered by a reliance on cumbersome run-time guidance. We identify the root cause of this limitation as the inadequacy of current training datasets, which are often too short, low-resolution, and lack the task diversity required to teach robust temporal priors. To address this foundational data gap, we first introduce FFP-300K, a new large-scale dataset comprising 300K high-fidelity video pairs at 720p resolution and 81 frames in length, constructed via a principled two-track pipeline for diverse local and global edits. Building on this dataset, we propose a novel framework designed for true guidance-free FFP that resolves the critical tension between maintaining first-frame appearance and preserving source video motion. Architecturally, we introduce Adaptive Spatio-Temporal RoPE (AST-RoPE), which dynamically remaps positional encodings to disentangle appearance and motion references. At the objective level, we employ a self-distillation strategy where an identity propagation task acts as a powerful regularizer, ensuring long-term temporal stability and preventing semantic drift. Comprehensive experiments on the EditVerseBench benchmark demonstrate that our method significantly outperforming existing academic and commercial models by receiving about 0.2 PickScore and 0.3 VLM score improvement against these competitors.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01720.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "652fab9d04a34a9282bf29d6",
      "avatarUrl": "/avatars/cd5967b37ebb1225e9ae1d46f196e2e2.svg",
      "fullname": "Chengming Xu",
      "name": "ChengmingX",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.01584",
      "authors": [
        {
          "_id": "695e0f95c03d6d81e439a058",
          "name": "Jakub Hoscilowicz",
          "hidden": false
        }
      ],
      "publishedAt": "2026-01-04T16:15:59.000Z",
      "submittedOnDailyAt": "2026-01-07T05:19:58.147Z",
      "title": "Steerability of Instrumental-Convergence Tendencies in LLMs",
      "submittedOnDailyBy": {
        "_id": "6605626876a0652cac85f233",
        "avatarUrl": "/avatars/fa008045c9ca4e0d71d40a02de74104d.svg",
        "isPro": false,
        "fullname": "j-hoscilowic",
        "user": "j-hoscilowic",
        "type": "user"
      },
      "summary": "We examine two properties of AI systems: capability (what a system can do) and steerability (how reliably one can shift behavior toward intended outcomes). A central question is whether capability growth reduces steerability and risks control collapse. We also distinguish between authorized steerability (builders reliably reaching intended behaviors) and unauthorized steerability (attackers eliciting disallowed behaviors). This distinction highlights a fundamental safety--security dilemma of AI models: safety requires high steerability to enforce control (e.g., stop/refuse), while security requires low steerability for malicious actors to elicit harmful behaviors. This tension presents a significant challenge for open-weight models, which currently exhibit high steerability via common techniques like fine-tuning or adversarial attacks. Using Qwen3 and InstrumentalEval, we find that a short anti-instrumental prompt suffix sharply reduces the measured convergence rate (e.g., shutdown avoidance, self-replication). For Qwen3-30B Instruct, the convergence rate drops from 81.69% under a pro-instrumental suffix to 2.82% under an anti-instrumental suffix. Under anti-instrumental prompting, larger aligned models show lower convergence rates than smaller ones (Instruct: 2.82% vs. 4.23%; Thinking: 4.23% vs. 9.86%). Code is available at github.com/j-hoscilowicz/instrumental_steering.",
      "upvotes": 0,
      "discussionId": "695e0f95c03d6d81e439a059",
      "githubRepo": "https://github.com/j-hoscilowicz/instrumental_steering/",
      "githubRepoAddedBy": "user",
      "ai_summary": "Research investigates the balance between AI system capabilities and steerability, finding that specific prompts can dramatically reduce unwanted behaviors in large language models.",
      "ai_keywords": [
        "steerability",
        "capability",
        "control collapse",
        "authorized steerability",
        "unauthorized steerability",
        "safety-security dilemma",
        "open-weight models",
        "fine-tuning",
        "adversarial attacks",
        "convergence rate",
        "anti-instrumental prompt suffix",
        "pro-instrumental suffix",
        "Qwen3",
        "InstrumentalEval"
      ]
    },
    "publishedAt": "2026-01-04T11:15:59.000Z",
    "title": "Steerability of Instrumental-Convergence Tendencies in LLMs",
    "summary": "We examine two properties of AI systems: capability (what a system can do) and steerability (how reliably one can shift behavior toward intended outcomes). A central question is whether capability growth reduces steerability and risks control collapse. We also distinguish between authorized steerability (builders reliably reaching intended behaviors) and unauthorized steerability (attackers eliciting disallowed behaviors). This distinction highlights a fundamental safety--security dilemma of AI models: safety requires high steerability to enforce control (e.g., stop/refuse), while security requires low steerability for malicious actors to elicit harmful behaviors. This tension presents a significant challenge for open-weight models, which currently exhibit high steerability via common techniques like fine-tuning or adversarial attacks. Using Qwen3 and InstrumentalEval, we find that a short anti-instrumental prompt suffix sharply reduces the measured convergence rate (e.g., shutdown avoidance, self-replication). For Qwen3-30B Instruct, the convergence rate drops from 81.69% under a pro-instrumental suffix to 2.82% under an anti-instrumental suffix. Under anti-instrumental prompting, larger aligned models show lower convergence rates than smaller ones (Instruct: 2.82% vs. 4.23%; Thinking: 4.23% vs. 9.86%). Code is available at github.com/j-hoscilowicz/instrumental_steering.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01584.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6605626876a0652cac85f233",
      "avatarUrl": "/avatars/fa008045c9ca4e0d71d40a02de74104d.svg",
      "fullname": "j-hoscilowic",
      "name": "j-hoscilowic",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  }
]