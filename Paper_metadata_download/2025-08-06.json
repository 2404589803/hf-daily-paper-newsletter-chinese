[
  {
    "paper": {
      "id": "2508.03320",
      "authors": [
        {
          "_id": "6892bd5c8da45ffb0a2b23fe",
          "name": "Peiyu Wang",
          "hidden": false
        },
        {
          "_id": "6892bd5c8da45ffb0a2b23ff",
          "name": "Yi Peng",
          "hidden": false
        },
        {
          "_id": "6892bd5c8da45ffb0a2b2400",
          "name": "Yimeng Gan",
          "hidden": false
        },
        {
          "_id": "6892bd5c8da45ffb0a2b2401",
          "name": "Liang Hu",
          "hidden": false
        },
        {
          "_id": "6892bd5c8da45ffb0a2b2402",
          "name": "Tianyidan Xie",
          "hidden": false
        },
        {
          "_id": "6892bd5c8da45ffb0a2b2403",
          "name": "Xiaokun Wang",
          "hidden": false
        },
        {
          "_id": "6892bd5c8da45ffb0a2b2404",
          "name": "Yichen Wei",
          "hidden": false
        },
        {
          "_id": "6892bd5c8da45ffb0a2b2405",
          "name": "Chuanxin Tang",
          "hidden": false
        },
        {
          "_id": "6892bd5c8da45ffb0a2b2406",
          "name": "Bo Zhu",
          "hidden": false
        },
        {
          "_id": "6892bd5c8da45ffb0a2b2407",
          "name": "Changshi Li",
          "hidden": false
        },
        {
          "_id": "6892bd5c8da45ffb0a2b2408",
          "name": "Hongyang Wei",
          "hidden": false
        },
        {
          "_id": "6892bd5c8da45ffb0a2b2409",
          "name": "Eric Li",
          "hidden": false
        },
        {
          "_id": "6892bd5c8da45ffb0a2b240a",
          "name": "Xuchen Song",
          "hidden": false
        },
        {
          "_id": "6892bd5c8da45ffb0a2b240b",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "6892bd5c8da45ffb0a2b240c",
          "name": "Yahui Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-05T10:59:01.000Z",
      "submittedOnDailyAt": "2025-08-06T01:57:00.887Z",
      "title": "Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding\n  and Generation",
      "submittedOnDailyBy": {
        "_id": "620f5a1c3f76c50e6458a9b6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620f5a1c3f76c50e6458a9b6/pXh_f5F0UvufxuUa-eS-v.jpeg",
        "isPro": false,
        "fullname": "Peiyu Wang",
        "user": "OrlandoHugBot",
        "type": "user"
      },
      "summary": "We introduce Skywork UniPic, a 1.5 billion-parameter autoregressive model\nthat unifies image understanding, text-to-image generation, and image editing\nwithin a single architecture-eliminating the need for task-specific adapters or\ninter-module connectors-and demonstrate that compact multimodal systems can\nachieve state-of-the-art performance on commodity hardware. Skywork UniPic\nachieves a GenEval score of 0.86, surpassing most existing unified models; sets\na new DPG-Bench complex-generation record of 85.5; attains 5.83 on\nGEditBench-EN and 3.49 on ImgEdit-Bench for image editing; and generates 1024 x\n1024 images with under 15 GB of GPU memory (e.g., RTX 4090). (1) a decoupled\nencoding strategy that leverages a masked autoregressive encoder for synthesis\nand a SigLIP2 encoder for understanding, all feeding a shared autoregressive\ndecoder; (2) a progressive, resolution-aware training schedule scaling from 256\nx 256 to 1024 x 1024 while dynamically unfreezing parameters to balance\ncapacity and stability; and (3) meticulously curated, 100 million-scale\ndatasets augmented with task-specific reward models to refine generation and\nediting objectives. By demonstrating that high-fidelity multimodal integration\nneed not incur prohibitive resource demands, Skywork UniPic establishes a\npractical paradigm for deployable, high-fidelity multimodal AI. Code and\nweights are publicly available at\nhttps://huggingface.co/Skywork/Skywork-UniPic-1.5B.",
      "upvotes": 29,
      "discussionId": "6892bd5c8da45ffb0a2b240d",
      "githubRepo": "https://github.com/SkyworkAI/UniPic",
      "ai_summary": "Skywork UniPic, a 1.5 billion-parameter autoregressive model, unifies image understanding, text-to-image generation, and image editing with state-of-the-art performance on commodity hardware.",
      "ai_keywords": [
        "autoregressive model",
        "image understanding",
        "text-to-image generation",
        "image editing",
        "GenEval",
        "DPG-Bench",
        "GEditBench-EN",
        "ImgEdit-Bench",
        "decoupled encoding strategy",
        "masked autoregressive encoder",
        "SigLIP2 encoder",
        "shared autoregressive decoder",
        "progressive training schedule",
        "resolution-aware training",
        "parameter unfreezing",
        "high-fidelity multimodal integration"
      ],
      "githubStars": 361
    },
    "publishedAt": "2025-08-05T06:59:01.000Z",
    "title": "Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding\n  and Generation",
    "summary": "We introduce Skywork UniPic, a 1.5 billion-parameter autoregressive model\nthat unifies image understanding, text-to-image generation, and image editing\nwithin a single architecture-eliminating the need for task-specific adapters or\ninter-module connectors-and demonstrate that compact multimodal systems can\nachieve state-of-the-art performance on commodity hardware. Skywork UniPic\nachieves a GenEval score of 0.86, surpassing most existing unified models; sets\na new DPG-Bench complex-generation record of 85.5; attains 5.83 on\nGEditBench-EN and 3.49 on ImgEdit-Bench for image editing; and generates 1024 x\n1024 images with under 15 GB of GPU memory (e.g., RTX 4090). (1) a decoupled\nencoding strategy that leverages a masked autoregressive encoder for synthesis\nand a SigLIP2 encoder for understanding, all feeding a shared autoregressive\ndecoder; (2) a progressive, resolution-aware training schedule scaling from 256\nx 256 to 1024 x 1024 while dynamically unfreezing parameters to balance\ncapacity and stability; and (3) meticulously curated, 100 million-scale\ndatasets augmented with task-specific reward models to refine generation and\nediting objectives. By demonstrating that high-fidelity multimodal integration\nneed not incur prohibitive resource demands, Skywork UniPic establishes a\npractical paradigm for deployable, high-fidelity multimodal AI. Code and\nweights are publicly available at\nhttps://huggingface.co/Skywork/Skywork-UniPic-1.5B.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03320.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "620f5a1c3f76c50e6458a9b6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620f5a1c3f76c50e6458a9b6/pXh_f5F0UvufxuUa-eS-v.jpeg",
      "fullname": "Peiyu Wang",
      "name": "OrlandoHugBot",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.03694",
      "authors": [
        {
          "_id": "6892b64d8da45ffb0a2b23d4",
          "name": "Jianxiong Gao",
          "hidden": false
        },
        {
          "_id": "6892b64d8da45ffb0a2b23d5",
          "name": "Zhaoxi Chen",
          "hidden": false
        },
        {
          "_id": "6892b64d8da45ffb0a2b23d6",
          "name": "Xian Liu",
          "hidden": false
        },
        {
          "_id": "6892b64d8da45ffb0a2b23d7",
          "name": "Jianfeng Feng",
          "hidden": false
        },
        {
          "_id": "6892b64d8da45ffb0a2b23d8",
          "name": "Chenyang Si",
          "hidden": false
        },
        {
          "_id": "6892b64d8da45ffb0a2b23d9",
          "name": "Yanwei Fu",
          "hidden": false
        },
        {
          "_id": "6892b64d8da45ffb0a2b23da",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "6892b64d8da45ffb0a2b23db",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-05T17:59:58.000Z",
      "submittedOnDailyAt": "2025-08-06T00:30:01.624Z",
      "title": "LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation",
      "submittedOnDailyBy": {
        "_id": "643815c4961bb61e463c5896",
        "avatarUrl": "/avatars/3b44592472f16c56105bff8c314d9939.svg",
        "isPro": false,
        "fullname": "Jianxiong Gao",
        "user": "Jianxiong",
        "type": "user"
      },
      "summary": "Controllable ultra-long video generation is a fundamental yet challenging\ntask. Although existing methods are effective for short clips, they struggle to\nscale due to issues such as temporal inconsistency and visual degradation. In\nthis paper, we initially investigate and identify three key factors: separate\nnoise initialization, independent control signal normalization, and the\nlimitations of single-modality guidance. To address these issues, we propose\nLongVie, an end-to-end autoregressive framework for controllable long video\ngeneration. LongVie introduces two core designs to ensure temporal consistency:\n1) a unified noise initialization strategy that maintains consistent generation\nacross clips, and 2) global control signal normalization that enforces\nalignment in the control space throughout the entire video. To mitigate visual\ndegradation, LongVie employs 3) a multi-modal control framework that integrates\nboth dense (e.g., depth maps) and sparse (e.g., keypoints) control signals,\ncomplemented by 4) a degradation-aware training strategy that adaptively\nbalances modality contributions over time to preserve visual quality. We also\nintroduce LongVGenBench, a comprehensive benchmark consisting of 100\nhigh-resolution videos spanning diverse real-world and synthetic environments,\neach lasting over one minute. Extensive experiments show that LongVie achieves\nstate-of-the-art performance in long-range controllability, consistency, and\nquality.",
      "upvotes": 24,
      "discussionId": "6892b64d8da45ffb0a2b23dc",
      "projectPage": "https://vchitect.github.io/LongVie-project/",
      "githubRepo": "https://github.com/Vchitect/LongVie",
      "ai_summary": "LongVie, an end-to-end autoregressive framework, addresses temporal consistency and visual degradation in ultra-long video generation through unified noise initialization, global control signal normalization, multi-modal control, and degradation-aware training.",
      "ai_keywords": [
        "autoregressive framework",
        "temporal consistency",
        "visual degradation",
        "unified noise initialization",
        "global control signal normalization",
        "multi-modal control",
        "degradation-aware training",
        "LongVGenBench"
      ],
      "githubStars": 9
    },
    "publishedAt": "2025-08-05T13:59:58.000Z",
    "title": "LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation",
    "summary": "Controllable ultra-long video generation is a fundamental yet challenging\ntask. Although existing methods are effective for short clips, they struggle to\nscale due to issues such as temporal inconsistency and visual degradation. In\nthis paper, we initially investigate and identify three key factors: separate\nnoise initialization, independent control signal normalization, and the\nlimitations of single-modality guidance. To address these issues, we propose\nLongVie, an end-to-end autoregressive framework for controllable long video\ngeneration. LongVie introduces two core designs to ensure temporal consistency:\n1) a unified noise initialization strategy that maintains consistent generation\nacross clips, and 2) global control signal normalization that enforces\nalignment in the control space throughout the entire video. To mitigate visual\ndegradation, LongVie employs 3) a multi-modal control framework that integrates\nboth dense (e.g., depth maps) and sparse (e.g., keypoints) control signals,\ncomplemented by 4) a degradation-aware training strategy that adaptively\nbalances modality contributions over time to preserve visual quality. We also\nintroduce LongVGenBench, a comprehensive benchmark consisting of 100\nhigh-resolution videos spanning diverse real-world and synthetic environments,\neach lasting over one minute. Extensive experiments show that LongVie achieves\nstate-of-the-art performance in long-range controllability, consistency, and\nquality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03694.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "643815c4961bb61e463c5896",
      "avatarUrl": "/avatars/3b44592472f16c56105bff8c314d9939.svg",
      "fullname": "Jianxiong Gao",
      "name": "Jianxiong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.02193",
      "authors": [
        {
          "_id": "6892c77d8da45ffb0a2b2471",
          "name": "Yuxuan Song",
          "hidden": false
        },
        {
          "_id": "6892c77d8da45ffb0a2b2472",
          "name": "Zheng Zhang",
          "hidden": false
        },
        {
          "_id": "6892c77d8da45ffb0a2b2473",
          "name": "Cheng Luo",
          "hidden": false
        },
        {
          "_id": "6892c77d8da45ffb0a2b2474",
          "name": "Pengyang Gao",
          "hidden": false
        },
        {
          "_id": "6892c77d8da45ffb0a2b2475",
          "name": "Fan Xia",
          "hidden": false
        },
        {
          "_id": "6892c77d8da45ffb0a2b2476",
          "name": "Hao Luo",
          "hidden": false
        },
        {
          "_id": "6892c77d8da45ffb0a2b2477",
          "name": "Zheng Li",
          "hidden": false
        },
        {
          "_id": "6892c77d8da45ffb0a2b2478",
          "name": "Yuehang Yang",
          "hidden": false
        },
        {
          "_id": "6892c77d8da45ffb0a2b2479",
          "name": "Hongli Yu",
          "hidden": false
        },
        {
          "_id": "6892c77d8da45ffb0a2b247a",
          "name": "Xingwei Qu",
          "hidden": false
        },
        {
          "_id": "6892c77d8da45ffb0a2b247b",
          "name": "Yuwei Fu",
          "hidden": false
        },
        {
          "_id": "6892c77d8da45ffb0a2b247c",
          "name": "Jing Su",
          "hidden": false
        },
        {
          "_id": "6892c77d8da45ffb0a2b247d",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "6892c77d8da45ffb0a2b247e",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "6892c77d8da45ffb0a2b247f",
          "name": "Mingxuan Wang",
          "hidden": false
        },
        {
          "_id": "6892c77d8da45ffb0a2b2480",
          "name": "Lin Yan",
          "hidden": false
        },
        {
          "_id": "6892c77d8da45ffb0a2b2481",
          "name": "Xiaoying Jia",
          "hidden": false
        },
        {
          "_id": "6892c77d8da45ffb0a2b2482",
          "name": "Jingjing Liu",
          "hidden": false
        },
        {
          "_id": "6892c77d8da45ffb0a2b2483",
          "name": "Wei-Ying Ma",
          "hidden": false
        },
        {
          "_id": "6892c77d8da45ffb0a2b2484",
          "name": "Ya-Qin Zhang",
          "hidden": false
        },
        {
          "_id": "6892c77d8da45ffb0a2b2485",
          "name": "Yonghui Wu",
          "hidden": false
        },
        {
          "_id": "6892c77d8da45ffb0a2b2486",
          "name": "Hao Zhou",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/66275b53d138af2d2eeb9326/dFE-umzBWqWmGHWbKItOo.png"
      ],
      "publishedAt": "2025-08-04T08:43:01.000Z",
      "submittedOnDailyAt": "2025-08-06T02:15:50.872Z",
      "title": "Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed\n  Inference",
      "submittedOnDailyBy": {
        "_id": "66275b53d138af2d2eeb9326",
        "avatarUrl": "/avatars/1103f68b978238d5bce604294d467e00.svg",
        "isPro": false,
        "fullname": "Yuxuan Song",
        "user": "yxsong",
        "type": "user"
      },
      "summary": "We present Seed Diffusion Preview, a large-scale language model based on\ndiscrete-state diffusion, offering remarkably fast inference speed. Thanks to\nnon-sequential, parallel generation, discrete diffusion models provide a\nnotable speedup to mitigate the inherent latency of token-by-token decoding, as\ndemonstrated recently (e.g., Mercury Coder, Gemini Diffusion). Seed Diffusion\nPreview achieves an inference speed of 2,146 token/s over H20 GPUs while\nmaintaining competitive performance across a sweep of standard code evaluation\nbenchmarks, significantly faster than contemporary Mercury and Gemini\nDiffusion, establishing new state of the art on the speed-quality Pareto\nfrontier for code models.",
      "upvotes": 22,
      "discussionId": "6892c77d8da45ffb0a2b2487",
      "projectPage": "https://seed.bytedance.com/en/seed_diffusion",
      "ai_summary": "Seed Diffusion Preview, a discrete-state diffusion language model, achieves fast inference speeds through parallel generation, outperforming Mercury and Gemini Diffusion in speed and quality.",
      "ai_keywords": [
        "discrete-state diffusion",
        "non-sequential",
        "parallel generation",
        "token-by-token decoding",
        "Seed Diffusion Preview",
        "H20 GPUs",
        "code evaluation benchmarks",
        "speed-quality Pareto frontier"
      ]
    },
    "publishedAt": "2025-08-04T04:43:01.000Z",
    "title": "Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed\n  Inference",
    "summary": "We present Seed Diffusion Preview, a large-scale language model based on\ndiscrete-state diffusion, offering remarkably fast inference speed. Thanks to\nnon-sequential, parallel generation, discrete diffusion models provide a\nnotable speedup to mitigate the inherent latency of token-by-token decoding, as\ndemonstrated recently (e.g., Mercury Coder, Gemini Diffusion). Seed Diffusion\nPreview achieves an inference speed of 2,146 token/s over H20 GPUs while\nmaintaining competitive performance across a sweep of standard code evaluation\nbenchmarks, significantly faster than contemporary Mercury and Gemini\nDiffusion, establishing new state of the art on the speed-quality Pareto\nfrontier for code models.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/66275b53d138af2d2eeb9326/dFE-umzBWqWmGHWbKItOo.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02193.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "66275b53d138af2d2eeb9326",
      "avatarUrl": "/avatars/1103f68b978238d5bce604294d467e00.svg",
      "fullname": "Yuxuan Song",
      "name": "yxsong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.03686",
      "authors": [
        {
          "_id": "6892f2468da45ffb0a2b2500",
          "name": "Shudong Liu",
          "hidden": false
        },
        {
          "_id": "6892f2468da45ffb0a2b2501",
          "name": "Hongwei Liu",
          "hidden": false
        },
        {
          "_id": "6892f2468da45ffb0a2b2502",
          "name": "Junnan Liu",
          "hidden": false
        },
        {
          "_id": "6892f2468da45ffb0a2b2503",
          "name": "Linchen Xiao",
          "hidden": false
        },
        {
          "_id": "6892f2468da45ffb0a2b2504",
          "name": "Songyang Gao",
          "hidden": false
        },
        {
          "_id": "6892f2468da45ffb0a2b2505",
          "name": "Chengqi Lyu",
          "hidden": false
        },
        {
          "_id": "6892f2468da45ffb0a2b2506",
          "name": "Yuzhe Gu",
          "hidden": false
        },
        {
          "_id": "6892f2468da45ffb0a2b2507",
          "name": "Wenwei Zhang",
          "hidden": false
        },
        {
          "_id": "6892f2468da45ffb0a2b2508",
          "name": "Derek F. Wong",
          "hidden": false
        },
        {
          "_id": "6892f2468da45ffb0a2b2509",
          "name": "Songyang Zhang",
          "hidden": false
        },
        {
          "_id": "6892f2468da45ffb0a2b250a",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-05T17:55:24.000Z",
      "submittedOnDailyAt": "2025-08-06T04:43:01.913Z",
      "title": "CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and\n  Outcome Reward",
      "submittedOnDailyBy": {
        "_id": "630716d11801ecc7d2595021",
        "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
        "isPro": false,
        "fullname": "Songyang Zhang",
        "user": "zsytony",
        "type": "user"
      },
      "summary": "Answer verification is crucial not only for evaluating large language models\n(LLMs) by matching their unstructured outputs against standard answers, but\nalso serves as the reward model to guide LLM optimization. Most evaluation\nframeworks rely on regularized matching or employ general LLMs for answer\nverification, which demands extensive, repetitive customization for regex rules\nor evaluation prompts. Two fundamental limitations persist in current\nmethodologies: 1) the absence of comprehensive benchmarks that systematically\nevaluate verification capabilities across different LLMs; and 2) the nascent\nstage of verifier development, where existing approaches lack both the\nrobustness to handle complex edge cases and the generalizability across\ndifferent domains. In this work, we develop CompassVerifier, an accurate and\nrobust lightweight verifier model for evaluation and outcome reward. It\ndemonstrates multi-domain competency spanning math, knowledge, and diverse\nreasoning tasks, with the capability to process various answer types, including\nmulti-subproblems, formulas, and sequence answers, while effectively\nidentifying abnormal/invalid responses. We introduce VerifierBench benchmark\ncomprising model outputs collected from multiple data sources, augmented\nthrough manual analysis of metaerror patterns to enhance CompassVerifier. We\nanticipate that CompassVerifier and VerifierBench will facilitate answer\nverification, evaluation protocols, and reinforcement learning research. Code\nand dataset are available at https://github.com/open-compass/CompassVerifier.",
      "upvotes": 14,
      "discussionId": "6892f2468da45ffb0a2b250b",
      "githubRepo": "https://github.com/open-compass/CompassVerifier",
      "ai_summary": "CompassVerifier is a lightweight, robust model for verifying LLM outputs across various domains, supported by VerifierBench, a comprehensive benchmark dataset.",
      "ai_keywords": [
        "LLMs",
        "answer verification",
        "reward model",
        "evaluation frameworks",
        "regex rules",
        "evaluation prompts",
        "benchmarks",
        "verifier development",
        "multi-domain competency",
        "math",
        "knowledge",
        "reasoning tasks",
        "multi-subproblems",
        "formulas",
        "sequence answers",
        "abnormal/invalid responses",
        "VerifierBench"
      ],
      "githubStars": 12
    },
    "publishedAt": "2025-08-05T13:55:24.000Z",
    "title": "CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and\n  Outcome Reward",
    "summary": "Answer verification is crucial not only for evaluating large language models\n(LLMs) by matching their unstructured outputs against standard answers, but\nalso serves as the reward model to guide LLM optimization. Most evaluation\nframeworks rely on regularized matching or employ general LLMs for answer\nverification, which demands extensive, repetitive customization for regex rules\nor evaluation prompts. Two fundamental limitations persist in current\nmethodologies: 1) the absence of comprehensive benchmarks that systematically\nevaluate verification capabilities across different LLMs; and 2) the nascent\nstage of verifier development, where existing approaches lack both the\nrobustness to handle complex edge cases and the generalizability across\ndifferent domains. In this work, we develop CompassVerifier, an accurate and\nrobust lightweight verifier model for evaluation and outcome reward. It\ndemonstrates multi-domain competency spanning math, knowledge, and diverse\nreasoning tasks, with the capability to process various answer types, including\nmulti-subproblems, formulas, and sequence answers, while effectively\nidentifying abnormal/invalid responses. We introduce VerifierBench benchmark\ncomprising model outputs collected from multiple data sources, augmented\nthrough manual analysis of metaerror patterns to enhance CompassVerifier. We\nanticipate that CompassVerifier and VerifierBench will facilitate answer\nverification, evaluation protocols, and reinforcement learning research. Code\nand dataset are available at https://github.com/open-compass/CompassVerifier.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03686.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "630716d11801ecc7d2595021",
      "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
      "fullname": "Songyang Zhang",
      "name": "zsytony",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.02091",
      "authors": [
        {
          "_id": "6892ef808da45ffb0a2b24e8",
          "name": "Xiaoya Li",
          "hidden": false
        },
        {
          "_id": "6892ef808da45ffb0a2b24e9",
          "name": "Xiaofei Sun",
          "hidden": false
        },
        {
          "_id": "6892ef808da45ffb0a2b24ea",
          "name": "Albert Wang",
          "hidden": false
        },
        {
          "_id": "6892ef808da45ffb0a2b24eb",
          "name": "Chris Shum",
          "hidden": false
        },
        {
          "_id": "6892ef808da45ffb0a2b24ec",
          "name": "Jiwei Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6803ddda5c044d396d02f03a/HjerqHcTmj_DCsAFjU5w3.png"
      ],
      "publishedAt": "2025-08-04T05:57:46.000Z",
      "submittedOnDailyAt": "2025-08-06T04:32:14.488Z",
      "title": "CRINN: Contrastive Reinforcement Learning for Approximate Nearest\n  Neighbor Search",
      "submittedOnDailyBy": {
        "_id": "6803ddda5c044d396d02f03a",
        "avatarUrl": "/avatars/ec583f6560d3d82fbd3bf95fb83c9356.svg",
        "isPro": false,
        "fullname": "Xiaoya Li",
        "user": "xxiaoyali",
        "type": "user"
      },
      "summary": "Approximate nearest-neighbor search (ANNS) algorithms have become\nincreasingly critical for recent AI applications, particularly in\nretrieval-augmented generation (RAG) and agent-based LLM applications. In this\npaper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS\noptimization as a reinforcement learning problem where execution speed serves\nas the reward signal. This approach enables the automatic generation of\nprogressively faster ANNS implementations while maintaining accuracy\nconstraints. Our experimental evaluation demonstrates CRINN's effectiveness\nacross six widely-used NNS benchmark datasets. When compared against\nstate-of-the-art open-source ANNS algorithms, CRINN achieves best performance\non three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and\nGloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean\nand GloVe-25-angular). The implications of CRINN's success reach well beyond\nANNS optimization: It validates that LLMs augmented with reinforcement learning\ncan function as an effective tool for automating sophisticated algorithmic\noptimizations that demand specialized knowledge and labor-intensive manual\nrefinement.Code can be found at https://github.com/deepreinforce-ai/CRINN",
      "upvotes": 7,
      "discussionId": "6892ef808da45ffb0a2b24ed",
      "ai_summary": "CRINN, a reinforcement learning-based approach, optimizes approximate nearest-neighbor search algorithms for speed while maintaining accuracy, outperforming state-of-the-art methods on several benchmarks.",
      "ai_keywords": [
        "approximate nearest-neighbor search",
        "ANNS",
        "retrieval-augmented generation",
        "RAG",
        "agent-based LLM",
        "reinforcement learning",
        "execution speed",
        "reward signal",
        "NNS benchmark datasets",
        "GIST-960-Euclidean",
        "MNIST-784-Euclidean",
        "GloVe-25-angular",
        "SIFT-128-Euclidean"
      ]
    },
    "publishedAt": "2025-08-04T01:57:46.000Z",
    "title": "CRINN: Contrastive Reinforcement Learning for Approximate Nearest\n  Neighbor Search",
    "summary": "Approximate nearest-neighbor search (ANNS) algorithms have become\nincreasingly critical for recent AI applications, particularly in\nretrieval-augmented generation (RAG) and agent-based LLM applications. In this\npaper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS\noptimization as a reinforcement learning problem where execution speed serves\nas the reward signal. This approach enables the automatic generation of\nprogressively faster ANNS implementations while maintaining accuracy\nconstraints. Our experimental evaluation demonstrates CRINN's effectiveness\nacross six widely-used NNS benchmark datasets. When compared against\nstate-of-the-art open-source ANNS algorithms, CRINN achieves best performance\non three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and\nGloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean\nand GloVe-25-angular). The implications of CRINN's success reach well beyond\nANNS optimization: It validates that LLMs augmented with reinforcement learning\ncan function as an effective tool for automating sophisticated algorithmic\noptimizations that demand specialized knowledge and labor-intensive manual\nrefinement.Code can be found at https://github.com/deepreinforce-ai/CRINN",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6803ddda5c044d396d02f03a/HjerqHcTmj_DCsAFjU5w3.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02091.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6803ddda5c044d396d02f03a",
      "avatarUrl": "/avatars/ec583f6560d3d82fbd3bf95fb83c9356.svg",
      "fullname": "Xiaoya Li",
      "name": "xxiaoyali",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.03050",
      "authors": [
        {
          "_id": "6892e5018da45ffb0a2b24dc",
          "name": "Zeyu Zhu",
          "hidden": false
        },
        {
          "_id": "6892e5018da45ffb0a2b24dd",
          "name": "Weijia Wu",
          "hidden": false
        },
        {
          "_id": "6892e5018da45ffb0a2b24de",
          "name": "Mike Zheng Shou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-05T03:54:18.000Z",
      "submittedOnDailyAt": "2025-08-06T03:46:25.473Z",
      "title": "Multi-human Interactive Talking Dataset",
      "submittedOnDailyBy": {
        "_id": "6345a93afe134dfd7a0cfabd",
        "avatarUrl": "/avatars/65130ce06b1c72ab1066678419731d88.svg",
        "isPro": false,
        "fullname": "wu weijia",
        "user": "weijiawu",
        "type": "user"
      },
      "summary": "Existing studies on talking video generation have predominantly focused on\nsingle-person monologues or isolated facial animations, limiting their\napplicability to realistic multi-human interactions. To bridge this gap, we\nintroduce MIT, a large-scale dataset specifically designed for multi-human\ntalking video generation. To this end, we develop an automatic pipeline that\ncollects and annotates multi-person conversational videos. The resulting\ndataset comprises 12 hours of high-resolution footage, each featuring two to\nfour speakers, with fine-grained annotations of body poses and speech\ninteractions. It captures natural conversational dynamics in multi-speaker\nscenario, offering a rich resource for studying interactive visual behaviors.\nTo demonstrate the potential of MIT, we furthur propose CovOG, a baseline model\nfor this novel task. It integrates a Multi-Human Pose Encoder (MPE) to handle\nvarying numbers of speakers by aggregating individual pose embeddings, and an\nInteractive Audio Driver (IAD) to modulate head dynamics based on\nspeaker-specific audio features. Together, these components showcase the\nfeasibility and challenges of generating realistic multi-human talking videos,\nestablishing MIT as a valuable benchmark for future research. The code is\navalibale at: https://github.com/showlab/Multi-human-Talking-Video-Dataset.",
      "upvotes": 3,
      "discussionId": "6892e5018da45ffb0a2b24df",
      "ai_summary": "MIT, a large-scale dataset for multi-human talking video generation, includes fine-grained annotations and is used to demonstrate CovOG, a baseline model integrating a Multi-Human Pose Encoder and an Interactive Audio Driver.",
      "ai_keywords": [
        "Multi-Human Pose Encoder",
        "Interactive Audio Driver",
        "multi-human talking video generation",
        "fine-grained annotations",
        "body poses",
        "speech interactions",
        "conversational dynamics",
        "multi-speaker scenario"
      ]
    },
    "publishedAt": "2025-08-04T23:54:18.000Z",
    "title": "Multi-human Interactive Talking Dataset",
    "summary": "Existing studies on talking video generation have predominantly focused on\nsingle-person monologues or isolated facial animations, limiting their\napplicability to realistic multi-human interactions. To bridge this gap, we\nintroduce MIT, a large-scale dataset specifically designed for multi-human\ntalking video generation. To this end, we develop an automatic pipeline that\ncollects and annotates multi-person conversational videos. The resulting\ndataset comprises 12 hours of high-resolution footage, each featuring two to\nfour speakers, with fine-grained annotations of body poses and speech\ninteractions. It captures natural conversational dynamics in multi-speaker\nscenario, offering a rich resource for studying interactive visual behaviors.\nTo demonstrate the potential of MIT, we furthur propose CovOG, a baseline model\nfor this novel task. It integrates a Multi-Human Pose Encoder (MPE) to handle\nvarying numbers of speakers by aggregating individual pose embeddings, and an\nInteractive Audio Driver (IAD) to modulate head dynamics based on\nspeaker-specific audio features. Together, these components showcase the\nfeasibility and challenges of generating realistic multi-human talking videos,\nestablishing MIT as a valuable benchmark for future research. The code is\navalibale at: https://github.com/showlab/Multi-human-Talking-Video-Dataset.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03050.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6345a93afe134dfd7a0cfabd",
      "avatarUrl": "/avatars/65130ce06b1c72ab1066678419731d88.svg",
      "fullname": "wu weijia",
      "name": "weijiawu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.03012",
      "authors": [
        {
          "_id": "6892c65c8da45ffb0a2b245e",
          "name": "Zexiong Ma",
          "hidden": false
        },
        {
          "_id": "6892c65c8da45ffb0a2b245f",
          "name": "Chao Peng",
          "hidden": false
        },
        {
          "_id": "6892c65c8da45ffb0a2b2460",
          "name": "Qunhong Zeng",
          "hidden": false
        },
        {
          "_id": "6892c65c8da45ffb0a2b2461",
          "name": "Pengfei Gao",
          "hidden": false
        },
        {
          "_id": "6892c65c8da45ffb0a2b2462",
          "name": "Yanzhen Zou",
          "hidden": false
        },
        {
          "_id": "6892c65c8da45ffb0a2b2463",
          "name": "Bing Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-05T02:44:21.000Z",
      "submittedOnDailyAt": "2025-08-06T01:36:11.665Z",
      "title": "Tool-integrated Reinforcement Learning for Repo Deep Search",
      "submittedOnDailyBy": {
        "_id": "654da66fb36f85a025bc24b6",
        "avatarUrl": "/avatars/e5542856ab4bf1845e8f546b5f17cd99.svg",
        "isPro": false,
        "fullname": "Zexiong Ma",
        "user": "mizersy",
        "type": "user"
      },
      "summary": "Issue localization, the process of identifying code locations that need\nmodification to resolve software issues, is a critical yet challenging task in\nsoftware development. The semantic gap between natural language issue\ndescriptions and faulty code requires complex multi-hop reasoning through code\ndependencies. Existing LLM-based agents attempt to address this by integrating\nrepository retrieval tools. However, this transforms issue localization into a\ndemanding task we call Repo Deep Search, which requires the LLM to effectively\nutilize various repository retrieval tools throughout a multi-step reasoning\nand navigation process. To tackle this challenge, we present ToolTrain, a\ntwo-stage tool-integrated training framework combining rejection-sampled\nsupervised fine-tuning and tool-integrated reinforcement learning to enhance\nLLMs' ability to use retrieval tools for issue localization. Experimental\nresults show that ToolTrain-trained models achieve state-of-the-art\nperformance, with our 32B model even surpassing Claude-3.7 on function-level\nlocalization. The results also show that improved localization performance\ntranslates to better end-to-end issue resolution performance. This further\ndemonstrates that training for issue localization is a viable and effective\nstrategy for improving automated software development.",
      "upvotes": 3,
      "discussionId": "6892c65d8da45ffb0a2b2464",
      "githubRepo": "https://github.com/Mizersy/RepoDeepSearch",
      "ai_summary": "ToolTrain, a two-stage training framework combining supervised fine-tuning and reinforcement learning, enhances LLMs for issue localization by integrating repository retrieval tools, achieving state-of-the-art performance.",
      "ai_keywords": [
        "LLM-based agents",
        "Repo Deep Search",
        "rejection-sampled supervised fine-tuning",
        "tool-integrated reinforcement learning",
        "function-level localization",
        "Claude-3.7",
        "automated software development"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-08-04T22:44:21.000Z",
    "title": "Tool-integrated Reinforcement Learning for Repo Deep Search",
    "summary": "Issue localization, the process of identifying code locations that need\nmodification to resolve software issues, is a critical yet challenging task in\nsoftware development. The semantic gap between natural language issue\ndescriptions and faulty code requires complex multi-hop reasoning through code\ndependencies. Existing LLM-based agents attempt to address this by integrating\nrepository retrieval tools. However, this transforms issue localization into a\ndemanding task we call Repo Deep Search, which requires the LLM to effectively\nutilize various repository retrieval tools throughout a multi-step reasoning\nand navigation process. To tackle this challenge, we present ToolTrain, a\ntwo-stage tool-integrated training framework combining rejection-sampled\nsupervised fine-tuning and tool-integrated reinforcement learning to enhance\nLLMs' ability to use retrieval tools for issue localization. Experimental\nresults show that ToolTrain-trained models achieve state-of-the-art\nperformance, with our 32B model even surpassing Claude-3.7 on function-level\nlocalization. The results also show that improved localization performance\ntranslates to better end-to-end issue resolution performance. This further\ndemonstrates that training for issue localization is a viable and effective\nstrategy for improving automated software development.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03012.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654da66fb36f85a025bc24b6",
      "avatarUrl": "/avatars/e5542856ab4bf1845e8f546b5f17cd99.svg",
      "fullname": "Zexiong Ma",
      "name": "mizersy",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.01780",
      "authors": [
        {
          "_id": "689197dcf01a094725f83589",
          "name": "Guozhao Mo",
          "hidden": false
        },
        {
          "_id": "689197dcf01a094725f8358a",
          "name": "Wenliang Zhong",
          "hidden": false
        },
        {
          "_id": "689197dcf01a094725f8358b",
          "name": "Jiawei Chen",
          "hidden": false
        },
        {
          "_id": "689197dcf01a094725f8358c",
          "name": "Xuanang Chen",
          "hidden": false
        },
        {
          "_id": "689197dcf01a094725f8358d",
          "name": "Yaojie Lu",
          "hidden": false
        },
        {
          "_id": "689197dcf01a094725f8358e",
          "name": "Hongyu Lin",
          "hidden": false
        },
        {
          "_id": "689197dcf01a094725f8358f",
          "name": "Ben He",
          "hidden": false
        },
        {
          "_id": "689197dcf01a094725f83590",
          "name": "Xianpei Han",
          "hidden": false
        },
        {
          "_id": "689197dcf01a094725f83591",
          "name": "Le Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-03T14:36:42.000Z",
      "submittedOnDailyAt": "2025-08-06T01:39:40.186Z",
      "title": "LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?",
      "submittedOnDailyBy": {
        "_id": "654c7fbe6b51714c2a6ff590",
        "avatarUrl": "/avatars/db217415c56730872b9a807f3afb4e5b.svg",
        "isPro": false,
        "fullname": "Jiawei Chen",
        "user": "jiawei-ucas",
        "type": "user"
      },
      "summary": "With the rapid development of Model Context Protocol (MCP), the number of MCP\nservers has surpassed 10,000. However, existing MCP benchmarks are limited to\nsingle-server settings with only a few tools, hindering effective evaluation of\nagent capabilities in large-scale, real-world scenarios. To address this\nlimitation, we present LiveMCPBench, the first comprehensive benchmark\ncomprising 95 real-world tasks grounded in the MCP ecosystem, designed to\nevaluate LLM agents at scale across diverse servers. To support a scalable and\nreproducible evaluation pipeline in large-scale MCP environments, we curate\nLiveMCPTool, a diverse and readily deployable collection of 70 MCP servers and\n527 tools. Furthermore, we introduce LiveMCPEval, an LLM-as-a-Judge framework\nthat enables automated and adaptive evaluation in dynamic, time-varying task\nenvironments, achieving 81% agreement with human reviewers. Finally, we propose\nthe MCP Copilot Agent, a multi-step agent that routes tools for dynamic\nplanning and executes tools for API interaction across the entire LiveMCPTool\nsuite. Our evaluation covers 10 leading models, with the best-performing model\n(Claude-Sonnet-4) reaching a 78.95% success rate. However, we observe large\nperformance variance across models, and several widely-used models perform\npoorly in LiveMCPBench's complex, tool-rich environments. Overall, LiveMCPBench\noffers the first unified framework for benchmarking LLM agents in realistic,\ntool-rich, and dynamic MCP environments, laying a solid foundation for scalable\nand reproducible research on agent capabilities. Our code and data will be\npublicly available at https://icip-cas.github.io/LiveMCPBench.",
      "upvotes": 3,
      "discussionId": "689197ddf01a094725f83592",
      "ai_summary": "LiveMCPBench provides a comprehensive benchmark for evaluating LLM agents across a diverse set of real-world tasks in the MCP ecosystem, using a scalable evaluation pipeline and adaptive judging framework.",
      "ai_keywords": [
        "Model Context Protocol",
        "MCP",
        "LiveMCPBench",
        "LLM agents",
        "real-world tasks",
        "LiveMCPTool",
        "LiveMCPEval",
        "LLM-as-a-Judge",
        "MCP Copilot Agent",
        "API interaction",
        "dynamic task environments",
        "human reviewers",
        "performance variance"
      ]
    },
    "publishedAt": "2025-08-03T10:36:42.000Z",
    "title": "LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?",
    "summary": "With the rapid development of Model Context Protocol (MCP), the number of MCP\nservers has surpassed 10,000. However, existing MCP benchmarks are limited to\nsingle-server settings with only a few tools, hindering effective evaluation of\nagent capabilities in large-scale, real-world scenarios. To address this\nlimitation, we present LiveMCPBench, the first comprehensive benchmark\ncomprising 95 real-world tasks grounded in the MCP ecosystem, designed to\nevaluate LLM agents at scale across diverse servers. To support a scalable and\nreproducible evaluation pipeline in large-scale MCP environments, we curate\nLiveMCPTool, a diverse and readily deployable collection of 70 MCP servers and\n527 tools. Furthermore, we introduce LiveMCPEval, an LLM-as-a-Judge framework\nthat enables automated and adaptive evaluation in dynamic, time-varying task\nenvironments, achieving 81% agreement with human reviewers. Finally, we propose\nthe MCP Copilot Agent, a multi-step agent that routes tools for dynamic\nplanning and executes tools for API interaction across the entire LiveMCPTool\nsuite. Our evaluation covers 10 leading models, with the best-performing model\n(Claude-Sonnet-4) reaching a 78.95% success rate. However, we observe large\nperformance variance across models, and several widely-used models perform\npoorly in LiveMCPBench's complex, tool-rich environments. Overall, LiveMCPBench\noffers the first unified framework for benchmarking LLM agents in realistic,\ntool-rich, and dynamic MCP environments, laying a solid foundation for scalable\nand reproducible research on agent capabilities. Our code and data will be\npublicly available at https://icip-cas.github.io/LiveMCPBench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01780.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654c7fbe6b51714c2a6ff590",
      "avatarUrl": "/avatars/db217415c56730872b9a807f3afb4e5b.svg",
      "fullname": "Jiawei Chen",
      "name": "jiawei-ucas",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.02079",
      "authors": [
        {
          "_id": "6892c2a18da45ffb0a2b241e",
          "name": "Amitava Das",
          "hidden": false
        },
        {
          "_id": "6892c2a18da45ffb0a2b241f",
          "name": "Abhilekh Borah",
          "hidden": false
        },
        {
          "_id": "6892c2a18da45ffb0a2b2420",
          "name": "Vinija Jain",
          "hidden": false
        },
        {
          "_id": "6892c2a18da45ffb0a2b2421",
          "name": "Aman Chadha",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-04T05:45:24.000Z",
      "submittedOnDailyAt": "2025-08-06T01:22:23.102Z",
      "title": "AlignGuard-LoRA: Alignment-Preserving Fine-Tuning via Fisher-Guided\n  Decomposition and Riemannian-Geodesic Collision Regularization",
      "submittedOnDailyBy": {
        "_id": "63a4754927f1f64ed7238dac",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
        "isPro": false,
        "fullname": "Aman Chadha",
        "user": "amanchadha",
        "type": "user"
      },
      "summary": "Low-rank adaptation (LoRA) has become a standard tool for efficiently\nfine-tuning large language models (LLMs). Yet, even minor LoRA updates can\ninduce alignment drift, weakening safety and behavioral constraints through\nentangled parameter changes. To address this, we propose AlignGuard-LoRA (AGL),\na principled framework for preserving alignment during finetuning. AGL\nintroduces several key components: a primary task loss for supervision, Fisher\nInformation Matrix-based regularization to restrict updates in\nalignment-sensitive subspaces, and task-specific regularization to stabilize\nthe integration of new knowledge. We further introduce collision-aware\nregularization, blending Riemannian overlap -- which penalizes coordinate-wise\ninterference -- and geodesic separation -- which encourages disjoint update\ngeometry. We curate DriftCaps, a targeted diagnostic benchmark of safe and\nunsafe prompts designed to quantify alignment drift and safety degradation.\nEmpirical evaluations show that AGL mitigates alignment drift by up to 50% on\nsafety-critical benchmarks without degrading downstream task performance.\nComprehensive ablation confirms that each component contributes distinctly to\npreserving latent safety behaviors. Finally, we derive and validate a scaling\nlaw for catastrophic forgetting, revealing that AGL flattens post-finetuning\nloss escalation while preserving adaptation dynamics. AGL is a structurally\ngrounded refinement of LoRA, ensuring alignment preservation with minimal\ntrade-offs. To encourage further exploration and development, we open-source\nour implementation.",
      "upvotes": 2,
      "discussionId": "6892c2a18da45ffb0a2b2422",
      "ai_summary": "AlignGuard-LoRA (AGL) is a framework that preserves alignment during fine-tuning of large language models by introducing regularization techniques and a diagnostic benchmark to mitigate alignment drift.",
      "ai_keywords": [
        "LoRA",
        "AlignGuard-LoRA",
        "primary task loss",
        "Fisher Information Matrix-based regularization",
        "task-specific regularization",
        "collision-aware regularization",
        "Riemannian overlap",
        "geodesic separation",
        "DriftCaps",
        "alignment drift",
        "catastrophic forgetting",
        "adaptation dynamics"
      ]
    },
    "publishedAt": "2025-08-04T01:45:24.000Z",
    "title": "AlignGuard-LoRA: Alignment-Preserving Fine-Tuning via Fisher-Guided\n  Decomposition and Riemannian-Geodesic Collision Regularization",
    "summary": "Low-rank adaptation (LoRA) has become a standard tool for efficiently\nfine-tuning large language models (LLMs). Yet, even minor LoRA updates can\ninduce alignment drift, weakening safety and behavioral constraints through\nentangled parameter changes. To address this, we propose AlignGuard-LoRA (AGL),\na principled framework for preserving alignment during finetuning. AGL\nintroduces several key components: a primary task loss for supervision, Fisher\nInformation Matrix-based regularization to restrict updates in\nalignment-sensitive subspaces, and task-specific regularization to stabilize\nthe integration of new knowledge. We further introduce collision-aware\nregularization, blending Riemannian overlap -- which penalizes coordinate-wise\ninterference -- and geodesic separation -- which encourages disjoint update\ngeometry. We curate DriftCaps, a targeted diagnostic benchmark of safe and\nunsafe prompts designed to quantify alignment drift and safety degradation.\nEmpirical evaluations show that AGL mitigates alignment drift by up to 50% on\nsafety-critical benchmarks without degrading downstream task performance.\nComprehensive ablation confirms that each component contributes distinctly to\npreserving latent safety behaviors. Finally, we derive and validate a scaling\nlaw for catastrophic forgetting, revealing that AGL flattens post-finetuning\nloss escalation while preserving adaptation dynamics. AGL is a structurally\ngrounded refinement of LoRA, ensuring alignment preservation with minimal\ntrade-offs. To encourage further exploration and development, we open-source\nour implementation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02079.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4754927f1f64ed7238dac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
      "fullname": "Aman Chadha",
      "name": "amanchadha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.00477",
      "authors": [
        {
          "_id": "689300478da45ffb0a2b252a",
          "name": "Yuzhuo Chen",
          "hidden": false
        },
        {
          "_id": "689300478da45ffb0a2b252b",
          "name": "Zehua Ma",
          "hidden": false
        },
        {
          "_id": "689300478da45ffb0a2b252c",
          "name": "Jianhua Wang",
          "hidden": false
        },
        {
          "_id": "689300478da45ffb0a2b252d",
          "name": "Kai Kang",
          "hidden": false
        },
        {
          "_id": "689300478da45ffb0a2b252e",
          "name": "Shunyu Yao",
          "hidden": false
        },
        {
          "_id": "689300478da45ffb0a2b252f",
          "name": "Weiming Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-01T09:51:54.000Z",
      "submittedOnDailyAt": "2025-08-06T05:43:40.504Z",
      "title": "LAMIC: Layout-Aware Multi-Image Composition via Scalability of\n  Multimodal Diffusion Transformer",
      "submittedOnDailyBy": {
        "_id": "64e5f0c23e220d8f697d1ab0",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e5f0c23e220d8f697d1ab0/qD-Egzxs-ZvJT5GXeqE3d.jpeg",
        "isPro": false,
        "fullname": "Jinsong Li",
        "user": "Jinsong-Li",
        "type": "user"
      },
      "summary": "In controllable image synthesis, generating coherent and consistent images\nfrom multiple references with spatial layout awareness remains an open\nchallenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework\nthat, for the first time, extends single-reference diffusion models to\nmulti-reference scenarios in a training-free manner. Built upon the MMDiT\nmodel, LAMIC introduces two plug-and-play attention mechanisms: 1) Group\nIsolation Attention (GIA) to enhance entity disentanglement; and 2)\nRegion-Modulated Attention (RMA) to enable layout-aware generation. To\ncomprehensively evaluate model capabilities, we further introduce three\nmetrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout\ncontrol; and 2) Background Similarity (BG-S) for measuring background\nconsistency. Extensive experiments show that LAMIC achieves state-of-the-art\nperformance across most major metrics: it consistently outperforms existing\nmulti-reference baselines in ID-S, BG-S, IN-R and AVG scores across all\nsettings, and achieves the best DPG in complex composition tasks. These results\ndemonstrate LAMIC's superior abilities in identity keeping, background\npreservation, layout control, and prompt-following, all achieved without any\ntraining or fine-tuning, showcasing strong zero-shot generalization ability. By\ninheriting the strengths of advanced single-reference models and enabling\nseamless extension to multi-image scenarios, LAMIC establishes a new\ntraining-free paradigm for controllable multi-image composition. As foundation\nmodels continue to evolve, LAMIC's performance is expected to scale\naccordingly. Our implementation is available at:\nhttps://github.com/Suchenl/LAMIC.",
      "upvotes": 1,
      "discussionId": "689300478da45ffb0a2b2530",
      "githubRepo": "https://github.com/Suchenl/LAMIC",
      "ai_summary": "LAMIC, a Layout-Aware Multi-Image Composition framework, extends single-reference diffusion models to multi-reference scenarios using attention mechanisms, achieving state-of-the-art performance in controllable image synthesis without training.",
      "ai_keywords": [
        "LAMIC",
        "Layout-Aware Multi-Image Composition",
        "diffusion models",
        "Group Isolation Attention",
        "Region-Modulated Attention",
        "Inclusion Ratio",
        "Fill Ratio",
        "Background Similarity",
        "ID-S",
        "BG-S",
        "IN-R",
        "AVG",
        "DPG",
        "zero-shot generalization"
      ],
      "githubStars": 6
    },
    "publishedAt": "2025-08-01T05:51:54.000Z",
    "title": "LAMIC: Layout-Aware Multi-Image Composition via Scalability of\n  Multimodal Diffusion Transformer",
    "summary": "In controllable image synthesis, generating coherent and consistent images\nfrom multiple references with spatial layout awareness remains an open\nchallenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework\nthat, for the first time, extends single-reference diffusion models to\nmulti-reference scenarios in a training-free manner. Built upon the MMDiT\nmodel, LAMIC introduces two plug-and-play attention mechanisms: 1) Group\nIsolation Attention (GIA) to enhance entity disentanglement; and 2)\nRegion-Modulated Attention (RMA) to enable layout-aware generation. To\ncomprehensively evaluate model capabilities, we further introduce three\nmetrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout\ncontrol; and 2) Background Similarity (BG-S) for measuring background\nconsistency. Extensive experiments show that LAMIC achieves state-of-the-art\nperformance across most major metrics: it consistently outperforms existing\nmulti-reference baselines in ID-S, BG-S, IN-R and AVG scores across all\nsettings, and achieves the best DPG in complex composition tasks. These results\ndemonstrate LAMIC's superior abilities in identity keeping, background\npreservation, layout control, and prompt-following, all achieved without any\ntraining or fine-tuning, showcasing strong zero-shot generalization ability. By\ninheriting the strengths of advanced single-reference models and enabling\nseamless extension to multi-image scenarios, LAMIC establishes a new\ntraining-free paradigm for controllable multi-image composition. As foundation\nmodels continue to evolve, LAMIC's performance is expected to scale\naccordingly. Our implementation is available at:\nhttps://github.com/Suchenl/LAMIC.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.00477.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e5f0c23e220d8f697d1ab0",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e5f0c23e220d8f697d1ab0/qD-Egzxs-ZvJT5GXeqE3d.jpeg",
      "fullname": "Jinsong Li",
      "name": "Jinsong-Li",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.02063",
      "authors": [
        {
          "_id": "6892c3aa8da45ffb0a2b242c",
          "name": "Amitava Das",
          "hidden": false
        },
        {
          "_id": "6892c3aa8da45ffb0a2b242d",
          "name": "Vinija Jain",
          "hidden": false
        },
        {
          "_id": "6892c3aa8da45ffb0a2b242e",
          "name": "Aman Chadha",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-04T05:03:35.000Z",
      "submittedOnDailyAt": "2025-08-06T01:23:46.765Z",
      "title": "TRACEALIGN -- Tracing the Drift: Attributing Alignment Failures to\n  Training-Time Belief Sources in LLMs",
      "submittedOnDailyBy": {
        "_id": "63a4754927f1f64ed7238dac",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
        "isPro": false,
        "fullname": "Aman Chadha",
        "user": "amanchadha",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) fine-tuned to align with human values often\nexhibit alignment drift, producing unsafe or policy-violating completions when\nexposed to adversarial prompts, decoding perturbations, or paraphrased\njailbreaks. While prior work has behaviorally characterized alignment failure,\nlittle is known about the training-time belief sources underlying these\nfailures. We introduce TraceAlign, a unified framework for tracing unsafe\ncompletions back to their root causes in the model's training corpus. Central\nto our approach is the Belief Conflict Index (BCI), which quantifies semantic\ninconsistency between generated spans and aligned policies, based on retrieved\ntraining documents using suffix-array matching. We propose three complementary\ninterventions: (i) TraceShield, an inference-time safety filter that refuses\ncompletions with high-BCI spans, (ii) Contrastive Belief Deconfliction Loss, a\ncontrastive fine-tuning objective penalizing high-BCI continuations during DPO,\nand (iii) Prov-Decode, a provenance-aware decoding strategy that vetoes beam\nexpansions predicted to yield high-BCI spans. Together, these defenses reduce\nalignment drift by up to 85% on our curated Alignment Drift Benchmark (ADB)\nwhile preserving utility on standard tasks, with delta less than 0.2 and\nimproved refusal quality. We further derive a theoretical upper bound on drift\nlikelihood via suffix-array span statistics, linking memorization frequency and\nlength to adversarial reactivation risk. TraceAlign thus provides the first\nscalable, traceable, and grounded toolkit for understanding and mitigating\nalignment failures at source. To encourage further exploration and development,\nwe open-source our implementation at:\nhttps://anonymous.4open.science/r/tracealign-2DA7",
      "upvotes": 0,
      "discussionId": "6892c3ab8da45ffb0a2b242f",
      "ai_summary": "TraceAlign is a framework that identifies and mitigates alignment drift in LLMs by tracing unsafe completions to their training sources and applying interventions to reduce drift while maintaining utility.",
      "ai_keywords": [
        "Large Language Models",
        "alignment drift",
        "adversarial prompts",
        "decoding perturbations",
        "paraphrased jailbreaks",
        "TraceAlign",
        "Belief Conflict Index",
        "suffix-array matching",
        "TraceShield",
        "Contrastive Belief Deconfliction Loss",
        "Prov-Decode",
        "Alignment Drift Benchmark",
        "memorization frequency",
        "adversarial reactivation risk"
      ]
    },
    "publishedAt": "2025-08-04T01:03:35.000Z",
    "title": "TRACEALIGN -- Tracing the Drift: Attributing Alignment Failures to\n  Training-Time Belief Sources in LLMs",
    "summary": "Large Language Models (LLMs) fine-tuned to align with human values often\nexhibit alignment drift, producing unsafe or policy-violating completions when\nexposed to adversarial prompts, decoding perturbations, or paraphrased\njailbreaks. While prior work has behaviorally characterized alignment failure,\nlittle is known about the training-time belief sources underlying these\nfailures. We introduce TraceAlign, a unified framework for tracing unsafe\ncompletions back to their root causes in the model's training corpus. Central\nto our approach is the Belief Conflict Index (BCI), which quantifies semantic\ninconsistency between generated spans and aligned policies, based on retrieved\ntraining documents using suffix-array matching. We propose three complementary\ninterventions: (i) TraceShield, an inference-time safety filter that refuses\ncompletions with high-BCI spans, (ii) Contrastive Belief Deconfliction Loss, a\ncontrastive fine-tuning objective penalizing high-BCI continuations during DPO,\nand (iii) Prov-Decode, a provenance-aware decoding strategy that vetoes beam\nexpansions predicted to yield high-BCI spans. Together, these defenses reduce\nalignment drift by up to 85% on our curated Alignment Drift Benchmark (ADB)\nwhile preserving utility on standard tasks, with delta less than 0.2 and\nimproved refusal quality. We further derive a theoretical upper bound on drift\nlikelihood via suffix-array span statistics, linking memorization frequency and\nlength to adversarial reactivation risk. TraceAlign thus provides the first\nscalable, traceable, and grounded toolkit for understanding and mitigating\nalignment failures at source. To encourage further exploration and development,\nwe open-source our implementation at:\nhttps://anonymous.4open.science/r/tracealign-2DA7",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02063.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4754927f1f64ed7238dac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
      "fullname": "Aman Chadha",
      "name": "amanchadha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  }
]