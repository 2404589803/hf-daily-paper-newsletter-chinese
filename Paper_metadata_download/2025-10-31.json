[
  {
    "paper": {
      "id": "2510.26583",
      "authors": [
        {
          "_id": "690414cc2c556835fa67efb5",
          "name": "Yufeng Cui",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efb6",
          "name": "Honghao Chen",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efb7",
          "name": "Haoge Deng",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efb8",
          "name": "Xu Huang",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efb9",
          "name": "Xinghang Li",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efba",
          "name": "Jirong Liu",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efbb",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efbc",
          "name": "Zhuoyan Luo",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efbd",
          "name": "Jinsheng Wang",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efbe",
          "name": "Wenxuan Wang",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efbf",
          "name": "Yueze Wang",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efc0",
          "name": "Chengyuan Wang",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efc1",
          "name": "Fan Zhang",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efc2",
          "name": "Yingli Zhao",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efc3",
          "name": "Ting Pan",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efc4",
          "name": "Xianduo Li",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efc5",
          "name": "Zecheng Hao",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efc6",
          "name": "Wenxuan Ma",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efc7",
          "name": "Zhuo Chen",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efc8",
          "name": "Yulong Ao",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efc9",
          "name": "Tiejun Huang",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efca",
          "name": "Zhongyuan Wang",
          "hidden": false
        },
        {
          "_id": "690414cc2c556835fa67efcb",
          "name": "Xinlong Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-30T15:11:16.000Z",
      "submittedOnDailyAt": "2025-10-31T00:20:30.779Z",
      "title": "Emu3.5: Native Multimodal Models are World Learners",
      "submittedOnDailyBy": {
        "_id": "63ca558304c979828311c5a5",
        "avatarUrl": "/avatars/2a439d79fba2f987cabe780d10c94d25.svg",
        "isPro": false,
        "fullname": "Xinlong Wang",
        "user": "xinlongwang",
        "type": "user"
      },
      "summary": "We introduce Emu3.5, a large-scale multimodal world model that natively\npredicts the next state across vision and language. Emu3.5 is pre-trained\nend-to-end with a unified next-token prediction objective on a corpus of\nvision-language interleaved data containing over 10 trillion tokens, primarily\nderived from sequential frames and transcripts of internet videos. The model\nnaturally accepts interleaved vision-language inputs and generates interleaved\nvision-language outputs. Emu3.5 is further post-trained with large-scale\nreinforcement learning to enhance multimodal reasoning and generation. To\nimprove inference efficiency, we propose Discrete Diffusion Adaptation (DiDA),\nwhich converts token-by-token decoding into bidirectional parallel prediction,\naccelerating per-image inference by about 20x without sacrificing performance.\nEmu3.5 exhibits strong native multimodal capabilities, including long-horizon\nvision-language generation, any-to-image (X2I) generation, and complex\ntext-rich image generation. It also exhibits generalizable world-modeling\nabilities, enabling spatiotemporally consistent world exploration and\nopen-world embodied manipulation across diverse scenarios and tasks. For\ncomparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image\n(Nano Banana) on image generation and editing tasks and demonstrates superior\nresults on a suite of interleaved generation tasks. We open-source Emu3.5 at\nhttps://github.com/baaivision/Emu3.5 to support community research.",
      "upvotes": 38,
      "discussionId": "690414cc2c556835fa67efcc",
      "projectPage": "https://emu.world/",
      "githubRepo": "https://github.com/baaivision/Emu3.5",
      "githubStars": 555
    },
    "publishedAt": "2025-10-30T11:11:16.000Z",
    "title": "Emu3.5: Native Multimodal Models are World Learners",
    "summary": "We introduce Emu3.5, a large-scale multimodal world model that natively\npredicts the next state across vision and language. Emu3.5 is pre-trained\nend-to-end with a unified next-token prediction objective on a corpus of\nvision-language interleaved data containing over 10 trillion tokens, primarily\nderived from sequential frames and transcripts of internet videos. The model\nnaturally accepts interleaved vision-language inputs and generates interleaved\nvision-language outputs. Emu3.5 is further post-trained with large-scale\nreinforcement learning to enhance multimodal reasoning and generation. To\nimprove inference efficiency, we propose Discrete Diffusion Adaptation (DiDA),\nwhich converts token-by-token decoding into bidirectional parallel prediction,\naccelerating per-image inference by about 20x without sacrificing performance.\nEmu3.5 exhibits strong native multimodal capabilities, including long-horizon\nvision-language generation, any-to-image (X2I) generation, and complex\ntext-rich image generation. It also exhibits generalizable world-modeling\nabilities, enabling spatiotemporally consistent world exploration and\nopen-world embodied manipulation across diverse scenarios and tasks. For\ncomparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image\n(Nano Banana) on image generation and editing tasks and demonstrates superior\nresults on a suite of interleaved generation tasks. We open-source Emu3.5 at\nhttps://github.com/baaivision/Emu3.5 to support community research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26583.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "63ca558304c979828311c5a5",
      "avatarUrl": "/avatars/2a439d79fba2f987cabe780d10c94d25.svg",
      "fullname": "Xinlong Wang",
      "name": "xinlongwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 15
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.15510",
      "authors": [
        {
          "_id": "68feef866cdff8b857f471d1",
          "user": {
            "_id": "66f989359ebb4cf074a1f9f7",
            "avatarUrl": "/avatars/d0e899f579e0897a849e79688bf19c8a.svg",
            "isPro": false,
            "fullname": "Heeseong Shin",
            "user": "hsshin98",
            "type": "user"
          },
          "name": "Heeseong Shin",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-28T15:38:00.777Z",
          "hidden": false
        },
        {
          "_id": "68feef866cdff8b857f471d2",
          "name": "Byeongho Heo",
          "hidden": false
        },
        {
          "_id": "68feef866cdff8b857f471d3",
          "name": "Dongyoon Han",
          "hidden": false
        },
        {
          "_id": "68feef866cdff8b857f471d4",
          "name": "Seungryong Kim",
          "hidden": false
        },
        {
          "_id": "68feef866cdff8b857f471d5",
          "user": {
            "_id": "67c6a1e75e2443d7d5f85cb3",
            "avatarUrl": "/avatars/0569b368520411ab828d46725bc3896a.svg",
            "isPro": false,
            "fullname": "Taekyung Kim",
            "user": "taekyung-k",
            "type": "user"
          },
          "name": "Taekyung Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-27T10:25:08.822Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-17T10:24:14.000Z",
      "submittedOnDailyAt": "2025-10-31T04:40:23.632Z",
      "title": "Exploring Conditions for Diffusion models in Robotic Control",
      "submittedOnDailyBy": {
        "_id": "66f989359ebb4cf074a1f9f7",
        "avatarUrl": "/avatars/d0e899f579e0897a849e79688bf19c8a.svg",
        "isPro": false,
        "fullname": "Heeseong Shin",
        "user": "hsshin98",
        "type": "user"
      },
      "summary": "While pre-trained visual representations have significantly advanced\nimitation learning, they are often task-agnostic as they remain frozen during\npolicy learning. In this work, we explore leveraging pre-trained text-to-image\ndiffusion models to obtain task-adaptive visual representations for robotic\ncontrol, without fine-tuning the model itself. However, we find that naively\napplying textual conditions - a successful strategy in other vision domains -\nyields minimal or even negative gains in control tasks. We attribute this to\nthe domain gap between the diffusion model's training data and robotic control\nenvironments, leading us to argue for conditions that consider the specific,\ndynamic visual information required for control. To this end, we propose ORCA,\nwhich introduces learnable task prompts that adapt to the control environment\nand visual prompts that capture fine-grained, frame-specific details. Through\nfacilitating task-adaptive representations with our newly devised conditions,\nour approach achieves state-of-the-art performance on various robotic control\nbenchmarks, significantly surpassing prior methods.",
      "upvotes": 33,
      "discussionId": "68feef866cdff8b857f471d6",
      "projectPage": "https://orca-rc.github.io/",
      "ai_summary": "ORCA uses learnable task and visual prompts to adapt pre-trained text-to-image diffusion models for robotic control, achieving state-of-the-art performance on benchmarks.",
      "ai_keywords": [
        "text-to-image diffusion models",
        "task-adaptive visual representations",
        "robotic control",
        "domain gap",
        "learnable task prompts",
        "visual prompts",
        "frame-specific details"
      ],
      "organization": {
        "_id": "64ffe603efd273eec7768bde",
        "name": "naver-ai",
        "fullname": "NAVER AI Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64ff1755b75685dd7a46e146/Zj2bxgq31oSqwVrw16IE_.png"
      }
    },
    "publishedAt": "2025-10-17T06:24:14.000Z",
    "title": "Exploring Conditions for Diffusion models in Robotic Control",
    "summary": "While pre-trained visual representations have significantly advanced\nimitation learning, they are often task-agnostic as they remain frozen during\npolicy learning. In this work, we explore leveraging pre-trained text-to-image\ndiffusion models to obtain task-adaptive visual representations for robotic\ncontrol, without fine-tuning the model itself. However, we find that naively\napplying textual conditions - a successful strategy in other vision domains -\nyields minimal or even negative gains in control tasks. We attribute this to\nthe domain gap between the diffusion model's training data and robotic control\nenvironments, leading us to argue for conditions that consider the specific,\ndynamic visual information required for control. To this end, we propose ORCA,\nwhich introduces learnable task prompts that adapt to the control environment\nand visual prompts that capture fine-grained, frame-specific details. Through\nfacilitating task-adaptive representations with our newly devised conditions,\nour approach achieves state-of-the-art performance on various robotic control\nbenchmarks, significantly surpassing prior methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15510.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66f989359ebb4cf074a1f9f7",
      "avatarUrl": "/avatars/d0e899f579e0897a849e79688bf19c8a.svg",
      "fullname": "Heeseong Shin",
      "name": "hsshin98",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "64ffe603efd273eec7768bde",
      "name": "naver-ai",
      "fullname": "NAVER AI Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64ff1755b75685dd7a46e146/Zj2bxgq31oSqwVrw16IE_.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.26794",
      "authors": [
        {
          "_id": "690422312c556835fa67f096",
          "name": "Jing Lin",
          "hidden": false
        },
        {
          "_id": "690422312c556835fa67f097",
          "name": "Ruisi Wang",
          "hidden": false
        },
        {
          "_id": "690422312c556835fa67f098",
          "name": "Junzhe Lu",
          "hidden": false
        },
        {
          "_id": "690422312c556835fa67f099",
          "name": "Ziqi Huang",
          "hidden": false
        },
        {
          "_id": "690422312c556835fa67f09a",
          "name": "Guorui Song",
          "hidden": false
        },
        {
          "_id": "690422312c556835fa67f09b",
          "name": "Ailing Zeng",
          "hidden": false
        },
        {
          "_id": "690422312c556835fa67f09c",
          "name": "Xian Liu",
          "hidden": false
        },
        {
          "_id": "690422312c556835fa67f09d",
          "name": "Chen Wei",
          "hidden": false
        },
        {
          "_id": "690422312c556835fa67f09e",
          "name": "Wanqi Yin",
          "hidden": false
        },
        {
          "_id": "690422312c556835fa67f09f",
          "name": "Qingping Sun",
          "hidden": false
        },
        {
          "_id": "690422312c556835fa67f0a0",
          "name": "Zhongang Cai",
          "hidden": false
        },
        {
          "_id": "690422312c556835fa67f0a1",
          "name": "Lei Yang",
          "hidden": false
        },
        {
          "_id": "690422312c556835fa67f0a2",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65b74305e602b6c2c9125480/C-PRM_Cp-h-wmdj0A5cty.jpeg"
      ],
      "publishedAt": "2025-10-30T17:59:27.000Z",
      "submittedOnDailyAt": "2025-10-31T01:21:12.151Z",
      "title": "The Quest for Generalizable Motion Generation: Data, Model, and\n  Evaluation",
      "submittedOnDailyBy": {
        "_id": "65b74305e602b6c2c9125480",
        "avatarUrl": "/avatars/d36909e0f245bfeb632a4afc9d3fceca.svg",
        "isPro": false,
        "fullname": "wang ruisi",
        "user": "wruisi",
        "type": "user"
      },
      "summary": "Despite recent advances in 3D human motion generation (MoGen) on standard\nbenchmarks, existing models still face a fundamental bottleneck in their\ngeneralization capability. In contrast, adjacent generative fields, most\nnotably video generation (ViGen), have demonstrated remarkable generalization\nin modeling human behaviors, highlighting transferable insights that MoGen can\nleverage. Motivated by this observation, we present a comprehensive framework\nthat systematically transfers knowledge from ViGen to MoGen across three key\npillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a\nlarge-scale dataset comprising 228,000 high-quality motion samples that\nintegrates high-fidelity optical MoCap data with semantically annotated motions\nfrom web videos and synthesized samples generated by state-of-the-art ViGen\nmodels. The dataset includes both text-motion pairs and text-video-motion\ntriplets, substantially expanding semantic diversity. Second, we propose\nViMoGen, a flow-matching-based diffusion transformer that unifies priors from\nMoCap data and ViGen models through gated multimodal conditioning. To enhance\nefficiency, we further develop ViMoGen-light, a distilled variant that\neliminates video generation dependencies while preserving strong\ngeneralization. Finally, we present MBench, a hierarchical benchmark designed\nfor fine-grained evaluation across motion quality, prompt fidelity, and\ngeneralization ability. Extensive experiments show that our framework\nsignificantly outperforms existing approaches in both automatic and human\nevaluations. The code, data, and benchmark will be made publicly available.",
      "upvotes": 21,
      "discussionId": "690422322c556835fa67f0a3",
      "githubRepo": "https://github.com/oneScotch/ViMoGen",
      "githubStars": 0,
      "organization": {
        "_id": "6508b28cf36bb51c50faad98",
        "name": "NanyangTechnologicalUniversity",
        "fullname": "Nanyang Technological University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
      }
    },
    "publishedAt": "2025-10-30T13:59:27.000Z",
    "title": "The Quest for Generalizable Motion Generation: Data, Model, and\n  Evaluation",
    "summary": "Despite recent advances in 3D human motion generation (MoGen) on standard\nbenchmarks, existing models still face a fundamental bottleneck in their\ngeneralization capability. In contrast, adjacent generative fields, most\nnotably video generation (ViGen), have demonstrated remarkable generalization\nin modeling human behaviors, highlighting transferable insights that MoGen can\nleverage. Motivated by this observation, we present a comprehensive framework\nthat systematically transfers knowledge from ViGen to MoGen across three key\npillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a\nlarge-scale dataset comprising 228,000 high-quality motion samples that\nintegrates high-fidelity optical MoCap data with semantically annotated motions\nfrom web videos and synthesized samples generated by state-of-the-art ViGen\nmodels. The dataset includes both text-motion pairs and text-video-motion\ntriplets, substantially expanding semantic diversity. Second, we propose\nViMoGen, a flow-matching-based diffusion transformer that unifies priors from\nMoCap data and ViGen models through gated multimodal conditioning. To enhance\nefficiency, we further develop ViMoGen-light, a distilled variant that\neliminates video generation dependencies while preserving strong\ngeneralization. Finally, we present MBench, a hierarchical benchmark designed\nfor fine-grained evaluation across motion quality, prompt fidelity, and\ngeneralization ability. Extensive experiments show that our framework\nsignificantly outperforms existing approaches in both automatic and human\nevaluations. The code, data, and benchmark will be made publicly available.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65b74305e602b6c2c9125480/C-PRM_Cp-h-wmdj0A5cty.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26794.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b74305e602b6c2c9125480",
      "avatarUrl": "/avatars/d36909e0f245bfeb632a4afc9d3fceca.svg",
      "fullname": "wang ruisi",
      "name": "wruisi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "6508b28cf36bb51c50faad98",
      "name": "NanyangTechnologicalUniversity",
      "fullname": "Nanyang Technological University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.26802",
      "authors": [
        {
          "_id": "690417142c556835fa67f021",
          "name": "Ziyu Guo",
          "hidden": false
        },
        {
          "_id": "690417142c556835fa67f022",
          "name": "Xinyan Chen",
          "hidden": false
        },
        {
          "_id": "690417142c556835fa67f023",
          "name": "Renrui Zhang",
          "hidden": false
        },
        {
          "_id": "690417142c556835fa67f024",
          "name": "Ruichuan An",
          "hidden": false
        },
        {
          "_id": "690417142c556835fa67f025",
          "name": "Yu Qi",
          "hidden": false
        },
        {
          "_id": "690417142c556835fa67f026",
          "name": "Dongzhi Jiang",
          "hidden": false
        },
        {
          "_id": "690417142c556835fa67f027",
          "name": "Xiangtai Li",
          "hidden": false
        },
        {
          "_id": "690417142c556835fa67f028",
          "name": "Manyuan Zhang",
          "hidden": false
        },
        {
          "_id": "690417142c556835fa67f029",
          "name": "Hongsheng Li",
          "hidden": false
        },
        {
          "_id": "690417142c556835fa67f02a",
          "name": "Pheng-Ann Heng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-30T17:59:55.000Z",
      "submittedOnDailyAt": "2025-10-31T00:26:10.415Z",
      "title": "Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with\n  the MME-CoF Benchmark",
      "submittedOnDailyBy": {
        "_id": "645b8b2687c79b6ec0bb3b7a",
        "avatarUrl": "/avatars/00a9db32a42dc950112bf2593bb109cb.svg",
        "isPro": false,
        "fullname": "Renrui",
        "user": "ZrrSkywalker",
        "type": "user"
      },
      "summary": "Recent video generation models can produce high-fidelity, temporally coherent\nvideos, indicating that they may encode substantial world knowledge. Beyond\nrealistic synthesis, they also exhibit emerging behaviors indicative of visual\nperception, modeling, and manipulation. Yet, an important question still\nremains: Are video models ready to serve as zero-shot reasoners in challenging\nvisual reasoning scenarios? In this work, we conduct an empirical study to\ncomprehensively investigate this question, focusing on the leading and popular\nVeo-3. We evaluate its reasoning behavior across 12 dimensions, including\nspatial, geometric, physical, temporal, and embodied logic, systematically\ncharacterizing both its strengths and failure modes. To standardize this study,\nwe curate the evaluation data into MME-CoF, a compact benchmark that enables\nin-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our\nfindings reveal that while current video models demonstrate promising reasoning\npatterns on short-horizon spatial coherence, fine-grained grounding, and\nlocally consistent dynamics, they remain limited in long-horizon causal\nreasoning, strict geometric constraints, and abstract logic. Overall, they are\nnot yet reliable as standalone zero-shot reasoners, but exhibit encouraging\nsigns as complementary visual engines alongside dedicated reasoning models.\nProject page: https://video-cof.github.io",
      "upvotes": 16,
      "discussionId": "690417152c556835fa67f02b",
      "projectPage": "https://video-cof.github.io/",
      "githubRepo": "https://github.com/ZiyuGuo99/MME-CoF",
      "githubStars": 6,
      "organization": {
        "_id": "6390c6fdd00f25601f445cd4",
        "name": "CUHK-CSE",
        "fullname": "The Chinese University of Hong Kong",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/621f2eb36e152b56a7cf0248/o8RRAczRjfNEzq70GzUwQ.png"
      }
    },
    "publishedAt": "2025-10-30T13:59:55.000Z",
    "title": "Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with\n  the MME-CoF Benchmark",
    "summary": "Recent video generation models can produce high-fidelity, temporally coherent\nvideos, indicating that they may encode substantial world knowledge. Beyond\nrealistic synthesis, they also exhibit emerging behaviors indicative of visual\nperception, modeling, and manipulation. Yet, an important question still\nremains: Are video models ready to serve as zero-shot reasoners in challenging\nvisual reasoning scenarios? In this work, we conduct an empirical study to\ncomprehensively investigate this question, focusing on the leading and popular\nVeo-3. We evaluate its reasoning behavior across 12 dimensions, including\nspatial, geometric, physical, temporal, and embodied logic, systematically\ncharacterizing both its strengths and failure modes. To standardize this study,\nwe curate the evaluation data into MME-CoF, a compact benchmark that enables\nin-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our\nfindings reveal that while current video models demonstrate promising reasoning\npatterns on short-horizon spatial coherence, fine-grained grounding, and\nlocally consistent dynamics, they remain limited in long-horizon causal\nreasoning, strict geometric constraints, and abstract logic. Overall, they are\nnot yet reliable as standalone zero-shot reasoners, but exhibit encouraging\nsigns as complementary visual engines alongside dedicated reasoning models.\nProject page: https://video-cof.github.io",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26802.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "645b8b2687c79b6ec0bb3b7a",
      "avatarUrl": "/avatars/00a9db32a42dc950112bf2593bb109cb.svg",
      "fullname": "Renrui",
      "name": "ZrrSkywalker",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "organization": {
      "_id": "6390c6fdd00f25601f445cd4",
      "name": "CUHK-CSE",
      "fullname": "The Chinese University of Hong Kong",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/621f2eb36e152b56a7cf0248/o8RRAczRjfNEzq70GzUwQ.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.26692",
      "authors": [
        {
          "_id": "690414082c556835fa67ef77",
          "name": "Kimi Team",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef78",
          "name": "Yu Zhang",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef79",
          "name": "Zongyu Lin",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef7a",
          "name": "Xingcheng Yao",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef7b",
          "name": "Jiaxi Hu",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef7c",
          "name": "Fanqing Meng",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef7d",
          "name": "Chengyin Liu",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef7e",
          "name": "Xin Men",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef7f",
          "name": "Songlin Yang",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef80",
          "name": "Zhiyuan Li",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef81",
          "name": "Wentao Li",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef82",
          "name": "Enzhe Lu",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef83",
          "name": "Weizhou Liu",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef84",
          "name": "Yanru Chen",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef85",
          "name": "Weixin Xu",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef86",
          "name": "Longhui Yu",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef87",
          "name": "Yejie Wang",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef88",
          "name": "Yu Fan",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef89",
          "name": "Longguang Zhong",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef8a",
          "name": "Enming Yuan",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef8b",
          "name": "Dehao Zhang",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef8c",
          "name": "Yizhi Zhang",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef8d",
          "name": "T. Y. Liu",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef8e",
          "name": "Haiming Wang",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef8f",
          "name": "Shengjun Fang",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef90",
          "name": "Weiran He",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef91",
          "name": "Shaowei Liu",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef92",
          "name": "Yiwei Li",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef93",
          "name": "Jianlin Su",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef94",
          "name": "Jiezhong Qiu",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef95",
          "name": "Bo Pang",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef96",
          "name": "Junjie Yan",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef97",
          "name": "Zhejun Jiang",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef98",
          "name": "Weixiao Huang",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef99",
          "name": "Bohong Yin",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef9a",
          "name": "Jiacheng You",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef9b",
          "name": "Chu Wei",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef9c",
          "name": "Zhengtao Wang",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef9d",
          "name": "Chao Hong",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef9e",
          "name": "Yutian Chen",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67ef9f",
          "name": "Guanduo Chen",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efa0",
          "name": "Yucheng Wang",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efa1",
          "name": "Huabin Zheng",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efa2",
          "name": "Feng Wang",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efa3",
          "name": "Yibo Liu",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efa4",
          "name": "Mengnan Dong",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efa5",
          "name": "Zheng Zhang",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efa6",
          "name": "Siyuan Pan",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efa7",
          "name": "Wenhao Wu",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efa8",
          "name": "Yuhao Wu",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efa9",
          "name": "Longyu Guan",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efaa",
          "name": "Jiawen Tao",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efab",
          "name": "Guohong Fu",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efac",
          "name": "Xinran Xu",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efad",
          "name": "Yuzhi Wang",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efae",
          "name": "Guokun Lai",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efaf",
          "name": "Yuxin Wu",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efb0",
          "name": "Xinyu Zhou",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efb1",
          "name": "Zhilin Yang",
          "hidden": false
        },
        {
          "_id": "690414082c556835fa67efb2",
          "name": "Yulun Du",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-30T16:59:43.000Z",
      "submittedOnDailyAt": "2025-10-31T00:12:50.143Z",
      "title": "Kimi Linear: An Expressive, Efficient Attention Architecture",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce Kimi Linear, a hybrid linear attention architecture that, for\nthe first time, outperforms full attention under fair comparisons across\nvarious scenarios -- including short-context, long-context, and reinforcement\nlearning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an\nexpressive linear attention module that extends Gated DeltaNet with a\nfiner-grained gating mechanism, enabling more effective use of limited\nfinite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware\nefficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)\ntransition matrices, which substantially reduces computation compared to the\ngeneral DPLR formulation while remaining more consistent with the classical\ndelta rule.\n  We pretrain a Kimi Linear model with 3B activated parameters and 48B total\nparameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention\n(MLA). Our experiments show that with an identical training recipe, Kimi Linear\noutperforms full MLA with a sizeable margin across all evaluated tasks, while\nreducing KV cache usage by up to 75% and achieving up to 6 times decoding\nthroughput for a 1M context. These results demonstrate that Kimi Linear can be\na drop-in replacement for full attention architectures with superior\nperformance and efficiency, including tasks with longer input and output\nlengths.\n  To support further research, we open-source the KDA kernel and vLLM\nimplementations, and release the pre-trained and instruction-tuned model\ncheckpoints.",
      "upvotes": 16,
      "discussionId": "690414092c556835fa67efb3",
      "githubRepo": "https://github.com/MoonshotAI/Kimi-Linear",
      "githubStars": 246
    },
    "publishedAt": "2025-10-30T12:59:43.000Z",
    "title": "Kimi Linear: An Expressive, Efficient Attention Architecture",
    "summary": "We introduce Kimi Linear, a hybrid linear attention architecture that, for\nthe first time, outperforms full attention under fair comparisons across\nvarious scenarios -- including short-context, long-context, and reinforcement\nlearning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an\nexpressive linear attention module that extends Gated DeltaNet with a\nfiner-grained gating mechanism, enabling more effective use of limited\nfinite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware\nefficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)\ntransition matrices, which substantially reduces computation compared to the\ngeneral DPLR formulation while remaining more consistent with the classical\ndelta rule.\n  We pretrain a Kimi Linear model with 3B activated parameters and 48B total\nparameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention\n(MLA). Our experiments show that with an identical training recipe, Kimi Linear\noutperforms full MLA with a sizeable margin across all evaluated tasks, while\nreducing KV cache usage by up to 75% and achieving up to 6 times decoding\nthroughput for a 1M context. These results demonstrate that Kimi Linear can be\na drop-in replacement for full attention architectures with superior\nperformance and efficiency, including tasks with longer input and output\nlengths.\n  To support further research, we open-source the KDA kernel and vLLM\nimplementations, and release the pre-trained and instruction-tuned model\ncheckpoints.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26692.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 152
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.26800",
      "authors": [
        {
          "_id": "6904193e2c556835fa67f063",
          "name": "Yukun Huang",
          "hidden": false
        },
        {
          "_id": "6904193e2c556835fa67f064",
          "name": "Jiwen Yu",
          "hidden": false
        },
        {
          "_id": "6904193e2c556835fa67f065",
          "name": "Yanning Zhou",
          "hidden": false
        },
        {
          "_id": "6904193e2c556835fa67f066",
          "name": "Jianan Wang",
          "hidden": false
        },
        {
          "_id": "6904193e2c556835fa67f067",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "6904193e2c556835fa67f068",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "6904193e2c556835fa67f069",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-30T17:59:51.000Z",
      "submittedOnDailyAt": "2025-10-31T00:35:27.539Z",
      "title": "OmniX: From Unified Panoramic Generation and Perception to\n  Graphics-Ready 3D Scenes",
      "submittedOnDailyBy": {
        "_id": "638ee900ee7e45e0474a5712",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638ee900ee7e45e0474a5712/KLli_eCbWwffKR7oLDmV3.jpeg",
        "isPro": false,
        "fullname": "Yukun Huang",
        "user": "KevinHuang",
        "type": "user"
      },
      "summary": "There are two prevalent ways to constructing 3D scenes: procedural generation\nand 2D lifting. Among them, panorama-based 2D lifting has emerged as a\npromising technique, leveraging powerful 2D generative priors to produce\nimmersive, realistic, and diverse 3D environments. In this work, we advance\nthis technique to generate graphics-ready 3D scenes suitable for physically\nbased rendering (PBR), relighting, and simulation. Our key insight is to\nrepurpose 2D generative models for panoramic perception of geometry, textures,\nand PBR materials. Unlike existing 2D lifting approaches that emphasize\nappearance generation and ignore the perception of intrinsic properties, we\npresent OmniX, a versatile and unified framework. Based on a lightweight and\nefficient cross-modal adapter structure, OmniX reuses 2D generative priors for\na broad range of panoramic vision tasks, including panoramic perception,\ngeneration, and completion. Furthermore, we construct a large-scale synthetic\npanorama dataset containing high-quality multimodal panoramas from diverse\nindoor and outdoor scenes. Extensive experiments demonstrate the effectiveness\nof our model in panoramic visual perception and graphics-ready 3D scene\ngeneration, opening new possibilities for immersive and physically realistic\nvirtual world generation.",
      "upvotes": 13,
      "discussionId": "6904193e2c556835fa67f06a",
      "projectPage": "https://yukun-huang.github.io/OmniX/",
      "githubRepo": "https://github.com/HKU-MMLab/OmniX",
      "githubStars": 0,
      "organization": {
        "_id": "67ea9ecfc234715db8dbf339",
        "name": "hkuhk",
        "fullname": "The University of Hong Kong",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"
      }
    },
    "publishedAt": "2025-10-30T13:59:51.000Z",
    "title": "OmniX: From Unified Panoramic Generation and Perception to\n  Graphics-Ready 3D Scenes",
    "summary": "There are two prevalent ways to constructing 3D scenes: procedural generation\nand 2D lifting. Among them, panorama-based 2D lifting has emerged as a\npromising technique, leveraging powerful 2D generative priors to produce\nimmersive, realistic, and diverse 3D environments. In this work, we advance\nthis technique to generate graphics-ready 3D scenes suitable for physically\nbased rendering (PBR), relighting, and simulation. Our key insight is to\nrepurpose 2D generative models for panoramic perception of geometry, textures,\nand PBR materials. Unlike existing 2D lifting approaches that emphasize\nappearance generation and ignore the perception of intrinsic properties, we\npresent OmniX, a versatile and unified framework. Based on a lightweight and\nefficient cross-modal adapter structure, OmniX reuses 2D generative priors for\na broad range of panoramic vision tasks, including panoramic perception,\ngeneration, and completion. Furthermore, we construct a large-scale synthetic\npanorama dataset containing high-quality multimodal panoramas from diverse\nindoor and outdoor scenes. Extensive experiments demonstrate the effectiveness\nof our model in panoramic visual perception and graphics-ready 3D scene\ngeneration, opening new possibilities for immersive and physically realistic\nvirtual world generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26800.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "638ee900ee7e45e0474a5712",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638ee900ee7e45e0474a5712/KLli_eCbWwffKR7oLDmV3.jpeg",
      "fullname": "Yukun Huang",
      "name": "KevinHuang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "organization": {
      "_id": "67ea9ecfc234715db8dbf339",
      "name": "hkuhk",
      "fullname": "The University of Hong Kong",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.26768",
      "authors": [
        {
          "_id": "690426d02c556835fa67f0b0",
          "name": "Shengnan An",
          "hidden": false
        },
        {
          "_id": "690426d02c556835fa67f0b1",
          "name": "Xunliang Cai",
          "hidden": false
        },
        {
          "_id": "690426d02c556835fa67f0b2",
          "name": "Xuezhi Cao",
          "hidden": false
        },
        {
          "_id": "690426d02c556835fa67f0b3",
          "name": "Xiaoyu Li",
          "hidden": false
        },
        {
          "_id": "690426d02c556835fa67f0b4",
          "name": "Yehao Lin",
          "hidden": false
        },
        {
          "_id": "690426d02c556835fa67f0b5",
          "name": "Junlin Liu",
          "hidden": false
        },
        {
          "_id": "690426d02c556835fa67f0b6",
          "name": "Xinxuan Lv",
          "hidden": false
        },
        {
          "_id": "690426d02c556835fa67f0b7",
          "name": "Dan Ma",
          "hidden": false
        },
        {
          "_id": "690426d02c556835fa67f0b8",
          "name": "Xuanlin Wang",
          "hidden": false
        },
        {
          "_id": "690426d02c556835fa67f0b9",
          "name": "Ziwen Wang",
          "hidden": false
        },
        {
          "_id": "690426d02c556835fa67f0ba",
          "name": "Shuang Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-30T17:52:02.000Z",
      "submittedOnDailyAt": "2025-10-31T02:30:10.841Z",
      "title": "AMO-Bench: Large Language Models Still Struggle in High School Math\n  Competitions",
      "submittedOnDailyBy": {
        "_id": "64db5f5dd68a6ddcc7bd89e9",
        "avatarUrl": "/avatars/69375ec915927b855813df8a6d486837.svg",
        "isPro": false,
        "fullname": "Shengnan An",
        "user": "ShengnanAn",
        "type": "user"
      },
      "summary": "We present AMO-Bench, an Advanced Mathematical reasoning benchmark with\nOlympiad level or even higher difficulty, comprising 50 human-crafted problems.\nExisting benchmarks have widely leveraged high school math competitions for\nevaluating mathematical reasoning capabilities of large language models (LLMs).\nHowever, many existing math competitions are becoming less effective for\nassessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To\naddress this, AMO-Bench introduces more rigorous challenges by ensuring all 50\nproblems are (1) cross-validated by experts to meet at least the International\nMathematical Olympiad (IMO) difficulty standards, and (2) entirely original\nproblems to prevent potential performance leakages from data memorization.\nMoreover, each problem in AMO-Bench requires only a final answer rather than a\nproof, enabling automatic and robust grading for evaluation. Experimental\nresults across 26 LLMs on AMO-Bench show that even the best-performing model\nachieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%.\nBeyond these poor performances, our further analysis reveals a promising\nscaling trend with increasing test-time compute on AMO-Bench. These results\nhighlight the significant room for improving the mathematical reasoning in\ncurrent LLMs. We release AMO-Bench to facilitate further research into\nadvancing the reasoning abilities of language models.\nhttps://amo-bench.github.io/",
      "upvotes": 10,
      "discussionId": "690426d02c556835fa67f0bb",
      "projectPage": "https://amo-bench.github.io/",
      "githubRepo": "https://github.com/meituan-longcat/AMO-Bench",
      "githubStars": 4,
      "organization": {
        "_id": "68b28d79a176a9beb30d2049",
        "name": "meituan-longcat",
        "fullname": "LongCat",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"
      }
    },
    "publishedAt": "2025-10-30T13:52:02.000Z",
    "title": "AMO-Bench: Large Language Models Still Struggle in High School Math\n  Competitions",
    "summary": "We present AMO-Bench, an Advanced Mathematical reasoning benchmark with\nOlympiad level or even higher difficulty, comprising 50 human-crafted problems.\nExisting benchmarks have widely leveraged high school math competitions for\nevaluating mathematical reasoning capabilities of large language models (LLMs).\nHowever, many existing math competitions are becoming less effective for\nassessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To\naddress this, AMO-Bench introduces more rigorous challenges by ensuring all 50\nproblems are (1) cross-validated by experts to meet at least the International\nMathematical Olympiad (IMO) difficulty standards, and (2) entirely original\nproblems to prevent potential performance leakages from data memorization.\nMoreover, each problem in AMO-Bench requires only a final answer rather than a\nproof, enabling automatic and robust grading for evaluation. Experimental\nresults across 26 LLMs on AMO-Bench show that even the best-performing model\nachieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%.\nBeyond these poor performances, our further analysis reveals a promising\nscaling trend with increasing test-time compute on AMO-Bench. These results\nhighlight the significant room for improving the mathematical reasoning in\ncurrent LLMs. We release AMO-Bench to facilitate further research into\nadvancing the reasoning abilities of language models.\nhttps://amo-bench.github.io/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26768.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64db5f5dd68a6ddcc7bd89e9",
      "avatarUrl": "/avatars/69375ec915927b855813df8a6d486837.svg",
      "fullname": "Shengnan An",
      "name": "ShengnanAn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "68b28d79a176a9beb30d2049",
      "name": "meituan-longcat",
      "fullname": "LongCat",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.26658",
      "authors": [
        {
          "_id": "6904402f2c556835fa67f11d",
          "name": "Zewen Chi",
          "hidden": false
        },
        {
          "_id": "6904402f2c556835fa67f11e",
          "name": "Li Dong",
          "hidden": false
        },
        {
          "_id": "6904402f2c556835fa67f11f",
          "name": "Qingxiu Dong",
          "hidden": false
        },
        {
          "_id": "6904402f2c556835fa67f120",
          "name": "Yaru Hao",
          "hidden": false
        },
        {
          "_id": "6904402f2c556835fa67f121",
          "name": "Xun Wu",
          "hidden": false
        },
        {
          "_id": "6904402f2c556835fa67f122",
          "name": "Shaohan Huang",
          "hidden": false
        },
        {
          "_id": "6904402f2c556835fa67f123",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-30T16:25:10.000Z",
      "submittedOnDailyAt": "2025-10-31T03:22:59.279Z",
      "title": "The Era of Agentic Organization: Learning to Organize with Language\n  Models",
      "submittedOnDailyBy": {
        "_id": "60f6d61f89b21b8fd2d471c6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60f6d61f89b21b8fd2d471c6/RmLFf97vUoXMoCT3rWbhm.jpeg",
        "isPro": false,
        "fullname": "Zewen Chi",
        "user": "CZWin32768",
        "type": "user"
      },
      "summary": "We envision a new era of AI, termed agentic organization, where agents solve\ncomplex problems by working collaboratively and concurrently, enabling outcomes\nbeyond individual intelligence. To realize this vision, we introduce\nasynchronous thinking (AsyncThink) as a new paradigm of reasoning with large\nlanguage models, which organizes the internal thinking process into\nconcurrently executable structures. Specifically, we propose a thinking\nprotocol where an organizer dynamically assigns sub-queries to workers, merges\nintermediate knowledge, and produces coherent solutions. More importantly, the\nthinking structure in this protocol can be further optimized through\nreinforcement learning. Experiments demonstrate that AsyncThink achieves 28%\nlower inference latency compared to parallel thinking while improving accuracy\non mathematical reasoning. Moreover, AsyncThink generalizes its learned\nasynchronous thinking capabilities, effectively tackling unseen tasks without\nadditional training.",
      "upvotes": 10,
      "discussionId": "6904402f2c556835fa67f124"
    },
    "publishedAt": "2025-10-30T12:25:10.000Z",
    "title": "The Era of Agentic Organization: Learning to Organize with Language\n  Models",
    "summary": "We envision a new era of AI, termed agentic organization, where agents solve\ncomplex problems by working collaboratively and concurrently, enabling outcomes\nbeyond individual intelligence. To realize this vision, we introduce\nasynchronous thinking (AsyncThink) as a new paradigm of reasoning with large\nlanguage models, which organizes the internal thinking process into\nconcurrently executable structures. Specifically, we propose a thinking\nprotocol where an organizer dynamically assigns sub-queries to workers, merges\nintermediate knowledge, and produces coherent solutions. More importantly, the\nthinking structure in this protocol can be further optimized through\nreinforcement learning. Experiments demonstrate that AsyncThink achieves 28%\nlower inference latency compared to parallel thinking while improving accuracy\non mathematical reasoning. Moreover, AsyncThink generalizes its learned\nasynchronous thinking capabilities, effectively tackling unseen tasks without\nadditional training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26658.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f6d61f89b21b8fd2d471c6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60f6d61f89b21b8fd2d471c6/RmLFf97vUoXMoCT3rWbhm.jpeg",
      "fullname": "Zewen Chi",
      "name": "CZWin32768",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.25992",
      "authors": [
        {
          "_id": "6904169e2c556835fa67f004",
          "name": "Yihe Deng",
          "hidden": false
        },
        {
          "_id": "6904169e2c556835fa67f005",
          "name": "I-Hung Hsu",
          "hidden": false
        },
        {
          "_id": "6904169e2c556835fa67f006",
          "name": "Jun Yan",
          "hidden": false
        },
        {
          "_id": "6904169e2c556835fa67f007",
          "name": "Zifeng Wang",
          "hidden": false
        },
        {
          "_id": "6904169e2c556835fa67f008",
          "name": "Rujun Han",
          "hidden": false
        },
        {
          "_id": "6904169e2c556835fa67f009",
          "name": "Gufeng Zhang",
          "hidden": false
        },
        {
          "_id": "6904169e2c556835fa67f00a",
          "name": "Yanfei Chen",
          "hidden": false
        },
        {
          "_id": "6904169e2c556835fa67f00b",
          "name": "Wei Wang",
          "hidden": false
        },
        {
          "_id": "6904169e2c556835fa67f00c",
          "name": "Tomas Pfister",
          "hidden": false
        },
        {
          "_id": "6904169e2c556835fa67f00d",
          "name": "Chen-Yu Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-29T22:05:08.000Z",
      "submittedOnDailyAt": "2025-10-31T00:25:45.081Z",
      "title": "Supervised Reinforcement Learning: From Expert Trajectories to Step-wise\n  Reasoning",
      "submittedOnDailyBy": {
        "_id": "64dc08f9e7bc8544f9b1ac32",
        "avatarUrl": "/avatars/e2ceccaf12dbdc643396c56f9a80ab8b.svg",
        "isPro": false,
        "fullname": "I-Hung Hsu",
        "user": "alexhsu",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) often struggle with problems that require\nmulti-step reasoning. For small-scale open-source models, Reinforcement\nLearning with Verifiable Rewards (RLVR) fails when correct solutions are rarely\nsampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to\noverfit long demonstrations through rigid token-by-token imitation. To address\nthis gap, we propose Supervised Reinforcement Learning (SRL), a framework that\nreformulates problem solving as generating a sequence of logical \"actions\". SRL\ntrains the model to generate an internal reasoning monologue before committing\nto each action. It provides smoother rewards based on the similarity between\nthe model's actions and expert actions extracted from the SFT dataset in a\nstep-wise manner. This supervision offers richer learning signals even when all\nrollouts are incorrect, while encouraging flexible reasoning guided by expert\ndemonstrations. As a result, SRL enables small models to learn challenging\nproblems previously unlearnable by SFT or RLVR. Moreover, initializing training\nwith SRL before refining with RLVR yields the strongest overall performance.\nBeyond reasoning benchmarks, SRL generalizes effectively to agentic software\nengineering tasks, establishing it as a robust and versatile training framework\nfor reasoning-oriented LLMs.",
      "upvotes": 9,
      "discussionId": "6904169f2c556835fa67f00e",
      "organization": {
        "_id": "5e6aca39878b8b2bf9806447",
        "name": "google",
        "fullname": "Google",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
      }
    },
    "publishedAt": "2025-10-29T18:05:08.000Z",
    "title": "Supervised Reinforcement Learning: From Expert Trajectories to Step-wise\n  Reasoning",
    "summary": "Large Language Models (LLMs) often struggle with problems that require\nmulti-step reasoning. For small-scale open-source models, Reinforcement\nLearning with Verifiable Rewards (RLVR) fails when correct solutions are rarely\nsampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to\noverfit long demonstrations through rigid token-by-token imitation. To address\nthis gap, we propose Supervised Reinforcement Learning (SRL), a framework that\nreformulates problem solving as generating a sequence of logical \"actions\". SRL\ntrains the model to generate an internal reasoning monologue before committing\nto each action. It provides smoother rewards based on the similarity between\nthe model's actions and expert actions extracted from the SFT dataset in a\nstep-wise manner. This supervision offers richer learning signals even when all\nrollouts are incorrect, while encouraging flexible reasoning guided by expert\ndemonstrations. As a result, SRL enables small models to learn challenging\nproblems previously unlearnable by SFT or RLVR. Moreover, initializing training\nwith SRL before refining with RLVR yields the strongest overall performance.\nBeyond reasoning benchmarks, SRL generalizes effectively to agentic software\nengineering tasks, establishing it as a robust and versatile training framework\nfor reasoning-oriented LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.25992.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64dc08f9e7bc8544f9b1ac32",
      "avatarUrl": "/avatars/e2ceccaf12dbdc643396c56f9a80ab8b.svg",
      "fullname": "I-Hung Hsu",
      "name": "alexhsu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "5e6aca39878b8b2bf9806447",
      "name": "google",
      "fullname": "Google",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.26213",
      "authors": [
        {
          "_id": "6904468e2c556835fa67f131",
          "name": "Hengrui Kang",
          "hidden": false
        },
        {
          "_id": "6904468e2c556835fa67f132",
          "name": "Zhuangcheng Gu",
          "hidden": false
        },
        {
          "_id": "6904468e2c556835fa67f133",
          "name": "Zhiyuan Zhao",
          "hidden": false
        },
        {
          "_id": "6904468e2c556835fa67f134",
          "name": "Zichen Wen",
          "hidden": false
        },
        {
          "_id": "6904468e2c556835fa67f135",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "6904468e2c556835fa67f136",
          "name": "Weijia Li",
          "hidden": false
        },
        {
          "_id": "6904468e2c556835fa67f137",
          "name": "Conghui He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-30T07:39:54.000Z",
      "submittedOnDailyAt": "2025-10-31T03:53:22.142Z",
      "title": "OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal\n  Document Layout Generation",
      "submittedOnDailyBy": {
        "_id": "66c598b536a75deef8bb21c0",
        "avatarUrl": "/avatars/b3721202d01d3c947c36f0beae43ea1c.svg",
        "isPro": false,
        "fullname": "Hengrui Kang",
        "user": "khr0516",
        "type": "user"
      },
      "summary": "Document AI has advanced rapidly and is attracting increasing attention. Yet,\nwhile most efforts have focused on document layout analysis (DLA), its\ngenerative counterpart, document layout generation, remains underexplored. A\nmajor obstacle lies in the scarcity of diverse layouts: academic papers with\nManhattan-style structures dominate existing studies, while open-world genres\nsuch as newspapers and magazines remain severely underrepresented. To address\nthis gap, we curate OmniLayout-1M, the first million-scale dataset of diverse\ndocument layouts, covering six common document types and comprising\ncontemporary layouts collected from multiple sources. Moreover, since existing\nmethods struggle in complex domains and often fail to arrange long sequences\ncoherently, we introduce OmniLayout-LLM, a 0.5B model with designed two-stage\nCoarse-to-Fine learning paradigm: 1) learning universal layout principles from\nOmniLayout-1M with coarse category definitions, and 2) transferring the\nknowledge to a specific domain with fine-grained annotations. Extensive\nexperiments demonstrate that our approach achieves strong performance on\nmultiple domains in M^{6}Doc dataset, substantially surpassing both existing\nlayout generation experts and several latest general-purpose LLMs. Our code,\nmodels, and dataset will be publicly released.",
      "upvotes": 4,
      "discussionId": "6904468e2c556835fa67f138"
    },
    "publishedAt": "2025-10-30T03:39:54.000Z",
    "title": "OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal\n  Document Layout Generation",
    "summary": "Document AI has advanced rapidly and is attracting increasing attention. Yet,\nwhile most efforts have focused on document layout analysis (DLA), its\ngenerative counterpart, document layout generation, remains underexplored. A\nmajor obstacle lies in the scarcity of diverse layouts: academic papers with\nManhattan-style structures dominate existing studies, while open-world genres\nsuch as newspapers and magazines remain severely underrepresented. To address\nthis gap, we curate OmniLayout-1M, the first million-scale dataset of diverse\ndocument layouts, covering six common document types and comprising\ncontemporary layouts collected from multiple sources. Moreover, since existing\nmethods struggle in complex domains and often fail to arrange long sequences\ncoherently, we introduce OmniLayout-LLM, a 0.5B model with designed two-stage\nCoarse-to-Fine learning paradigm: 1) learning universal layout principles from\nOmniLayout-1M with coarse category definitions, and 2) transferring the\nknowledge to a specific domain with fine-grained annotations. Extensive\nexperiments demonstrate that our approach achieves strong performance on\nmultiple domains in M^{6}Doc dataset, substantially surpassing both existing\nlayout generation experts and several latest general-purpose LLMs. Our code,\nmodels, and dataset will be publicly released.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26213.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66c598b536a75deef8bb21c0",
      "avatarUrl": "/avatars/b3721202d01d3c947c36f0beae43ea1c.svg",
      "fullname": "Hengrui Kang",
      "name": "khr0516",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.25779",
      "authors": [
        {
          "_id": "69042d902c556835fa67f0d9",
          "name": "Gagan Bansal",
          "hidden": false
        },
        {
          "_id": "69042d902c556835fa67f0da",
          "name": "Wenyue Hua",
          "hidden": false
        },
        {
          "_id": "69042d902c556835fa67f0db",
          "name": "Zezhou Huang",
          "hidden": false
        },
        {
          "_id": "69042d902c556835fa67f0dc",
          "name": "Adam Fourney",
          "hidden": false
        },
        {
          "_id": "69042d902c556835fa67f0dd",
          "name": "Amanda Swearngin",
          "hidden": false
        },
        {
          "_id": "69042d902c556835fa67f0de",
          "name": "Will Epperson",
          "hidden": false
        },
        {
          "_id": "69042d902c556835fa67f0df",
          "name": "Tyler Payne",
          "hidden": false
        },
        {
          "_id": "69042d902c556835fa67f0e0",
          "name": "Jake M. Hofman",
          "hidden": false
        },
        {
          "_id": "69042d902c556835fa67f0e1",
          "name": "Brendan Lucier",
          "hidden": false
        },
        {
          "_id": "69042d902c556835fa67f0e2",
          "name": "Chinmay Singh",
          "hidden": false
        },
        {
          "_id": "69042d902c556835fa67f0e3",
          "name": "Markus Mobius",
          "hidden": false
        },
        {
          "_id": "69042d902c556835fa67f0e4",
          "name": "Akshay Nambi",
          "hidden": false
        },
        {
          "_id": "69042d902c556835fa67f0e5",
          "name": "Archana Yadav",
          "hidden": false
        },
        {
          "_id": "69042d902c556835fa67f0e6",
          "name": "Kevin Gao",
          "hidden": false
        },
        {
          "_id": "69042d902c556835fa67f0e7",
          "name": "David M. Rothschild",
          "hidden": false
        },
        {
          "_id": "69042d902c556835fa67f0e8",
          "name": "Aleksandrs Slivkins",
          "hidden": false
        },
        {
          "_id": "69042d902c556835fa67f0e9",
          "name": "Daniel G. Goldstein",
          "hidden": false
        },
        {
          "_id": "69042d902c556835fa67f0ea",
          "name": "Hussein Mozannar",
          "hidden": false
        },
        {
          "_id": "69042d902c556835fa67f0eb",
          "name": "Nicole Immorlica",
          "hidden": false
        },
        {
          "_id": "69042d902c556835fa67f0ec",
          "name": "Maya Murad",
          "hidden": false
        },
        {
          "_id": "69042d902c556835fa67f0ed",
          "name": "Matthew Vogel",
          "hidden": false
        },
        {
          "_id": "69042d902c556835fa67f0ee",
          "name": "Subbarao Kambhampati",
          "hidden": false
        },
        {
          "_id": "69042d902c556835fa67f0ef",
          "name": "Eric Horvitz",
          "hidden": false
        },
        {
          "_id": "69042d902c556835fa67f0f0",
          "name": "Saleema Amershi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-27T18:35:59.000Z",
      "submittedOnDailyAt": "2025-10-31T02:09:06.318Z",
      "title": "Magentic Marketplace: An Open-Source Environment for Studying Agentic\n  Markets",
      "submittedOnDailyBy": {
        "_id": "64aba383fddf117e6e5ba818",
        "avatarUrl": "/avatars/ee7d25d865b34be5902872d060ad9153.svg",
        "isPro": false,
        "fullname": "Akshay  Nambi",
        "user": "akshaynambi",
        "type": "user"
      },
      "summary": "As LLM agents advance, they are increasingly mediating economic decisions,\nranging from product discovery to transactions, on behalf of users. Such\napplications promise benefits but also raise many questions about agent\naccountability and value for users. Addressing these questions requires\nunderstanding how agents behave in realistic market conditions. However,\nprevious research has largely evaluated agents in constrained settings, such as\nsingle-task marketplaces (e.g., negotiation) or structured two-agent\ninteractions. Real-world markets are fundamentally different: they require\nagents to handle diverse economic activities and coordinate within large,\ndynamic ecosystems where multiple agents with opaque behaviors may engage in\nopen-ended dialogues. To bridge this gap, we investigate two-sided agentic\nmarketplaces where Assistant agents represent consumers and Service agents\nrepresent competing businesses. To study these interactions safely, we develop\nMagentic-Marketplace-- a simulated environment where Assistants and Services\ncan operate. This environment enables us to study key market dynamics: the\nutility agents achieve, behavioral biases, vulnerability to manipulation, and\nhow search mechanisms shape market outcomes. Our experiments show that frontier\nmodels can approach optimal welfare-- but only under ideal search conditions.\nPerformance degrades sharply with scale, and all models exhibit severe\nfirst-proposal bias, creating 10-30x advantages for response speed over\nquality. These findings reveal how behaviors emerge across market conditions,\ninforming the design of fair and efficient agentic marketplaces.",
      "upvotes": 4,
      "discussionId": "69042d902c556835fa67f0f1",
      "githubRepo": "https://github.com/microsoft/multi-agent-marketplace",
      "githubStars": 9,
      "organization": {
        "_id": "68151d0f51add3813f3f7d1b",
        "name": "MicrosoftResearch",
        "fullname": "Microsoft Research",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"
      }
    },
    "publishedAt": "2025-10-27T14:35:59.000Z",
    "title": "Magentic Marketplace: An Open-Source Environment for Studying Agentic\n  Markets",
    "summary": "As LLM agents advance, they are increasingly mediating economic decisions,\nranging from product discovery to transactions, on behalf of users. Such\napplications promise benefits but also raise many questions about agent\naccountability and value for users. Addressing these questions requires\nunderstanding how agents behave in realistic market conditions. However,\nprevious research has largely evaluated agents in constrained settings, such as\nsingle-task marketplaces (e.g., negotiation) or structured two-agent\ninteractions. Real-world markets are fundamentally different: they require\nagents to handle diverse economic activities and coordinate within large,\ndynamic ecosystems where multiple agents with opaque behaviors may engage in\nopen-ended dialogues. To bridge this gap, we investigate two-sided agentic\nmarketplaces where Assistant agents represent consumers and Service agents\nrepresent competing businesses. To study these interactions safely, we develop\nMagentic-Marketplace-- a simulated environment where Assistants and Services\ncan operate. This environment enables us to study key market dynamics: the\nutility agents achieve, behavioral biases, vulnerability to manipulation, and\nhow search mechanisms shape market outcomes. Our experiments show that frontier\nmodels can approach optimal welfare-- but only under ideal search conditions.\nPerformance degrades sharply with scale, and all models exhibit severe\nfirst-proposal bias, creating 10-30x advantages for response speed over\nquality. These findings reveal how behaviors emerge across market conditions,\ninforming the design of fair and efficient agentic marketplaces.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.25779.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64aba383fddf117e6e5ba818",
      "avatarUrl": "/avatars/ee7d25d865b34be5902872d060ad9153.svg",
      "fullname": "Akshay  Nambi",
      "name": "akshaynambi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "organization": {
      "_id": "68151d0f51add3813f3f7d1b",
      "name": "MicrosoftResearch",
      "fullname": "Microsoft Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.26298",
      "authors": [
        {
          "_id": "690414fd2c556835fa67efce",
          "name": "Jingran Zhang",
          "hidden": false
        },
        {
          "_id": "690414fd2c556835fa67efcf",
          "name": "Ning Li",
          "hidden": false
        },
        {
          "_id": "690414fd2c556835fa67efd0",
          "name": "Justin Cui",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/XQsuSRT7f95axrkHL3DnO.mp4"
      ],
      "publishedAt": "2025-10-30T09:35:51.000Z",
      "submittedOnDailyAt": "2025-10-31T00:16:51.181Z",
      "title": "Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in\n  Web Games",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "OpenAI's ChatGPT Atlas introduces new capabilities for web interaction,\nenabling the model to analyze webpages, process user intents, and execute\ncursor and keyboard inputs directly within the browser. While its capacity for\ninformation retrieval tasks has been demonstrated, its performance in dynamic,\ninteractive environments remains less explored. In this study, we conduct an\nearly evaluation of Atlas's web interaction capabilities using browser-based\ngames as test scenarios, including Google's T-Rex Runner, Sudoku, Flappy Bird,\nand Stein.world. We employ in-game performance scores as quantitative metrics\nto assess performance across different task types. Our results show that Atlas\nperforms strongly in logical reasoning tasks like Sudoku, completing puzzles\nsignificantly faster than human baselines, but struggles substantially in\nreal-time games requiring precise timing and motor control, often failing to\nprogress beyond initial obstacles. These findings suggest that while Atlas\ndemonstrates capable analytical processing, there remain notable limitations in\ndynamic web environments requiring real-time interaction. The website of our\nproject can be found at https://atlas-game-eval.github.io.",
      "upvotes": 3,
      "discussionId": "690414fe2c556835fa67efd1",
      "projectPage": "https://atlas-game-eval.github.io/"
    },
    "publishedAt": "2025-10-30T05:35:51.000Z",
    "title": "Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in\n  Web Games",
    "summary": "OpenAI's ChatGPT Atlas introduces new capabilities for web interaction,\nenabling the model to analyze webpages, process user intents, and execute\ncursor and keyboard inputs directly within the browser. While its capacity for\ninformation retrieval tasks has been demonstrated, its performance in dynamic,\ninteractive environments remains less explored. In this study, we conduct an\nearly evaluation of Atlas's web interaction capabilities using browser-based\ngames as test scenarios, including Google's T-Rex Runner, Sudoku, Flappy Bird,\nand Stein.world. We employ in-game performance scores as quantitative metrics\nto assess performance across different task types. Our results show that Atlas\nperforms strongly in logical reasoning tasks like Sudoku, completing puzzles\nsignificantly faster than human baselines, but struggles substantially in\nreal-time games requiring precise timing and motor control, often failing to\nprogress beyond initial obstacles. These findings suggest that while Atlas\ndemonstrates capable analytical processing, there remain notable limitations in\ndynamic web environments requiring real-time interaction. The website of our\nproject can be found at https://atlas-game-eval.github.io.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/XQsuSRT7f95axrkHL3DnO.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26298.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 152
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.25628",
      "authors": [
        {
          "_id": "69042c562c556835fa67f0c8",
          "name": "Yusheng Liao",
          "hidden": false
        },
        {
          "_id": "69042c562c556835fa67f0c9",
          "name": "Chaoyi Wu",
          "hidden": false
        },
        {
          "_id": "69042c562c556835fa67f0ca",
          "name": "Junwei Liu",
          "hidden": false
        },
        {
          "_id": "69042c562c556835fa67f0cb",
          "name": "Shuyang Jiang",
          "hidden": false
        },
        {
          "_id": "69042c562c556835fa67f0cc",
          "name": "Pengcheng Qiu",
          "hidden": false
        },
        {
          "_id": "69042c562c556835fa67f0cd",
          "name": "Haowen Wang",
          "hidden": false
        },
        {
          "_id": "69042c562c556835fa67f0ce",
          "name": "Yun Yue",
          "hidden": false
        },
        {
          "_id": "69042c562c556835fa67f0cf",
          "name": "Shuai Zhen",
          "hidden": false
        },
        {
          "_id": "69042c562c556835fa67f0d0",
          "name": "Jian Wang",
          "hidden": false
        },
        {
          "_id": "69042c562c556835fa67f0d1",
          "name": "Qianrui Fan",
          "hidden": false
        },
        {
          "_id": "69042c562c556835fa67f0d2",
          "name": "Jinjie Gu",
          "hidden": false
        },
        {
          "_id": "69042c562c556835fa67f0d3",
          "name": "Ya Zhang",
          "hidden": false
        },
        {
          "_id": "69042c562c556835fa67f0d4",
          "name": "Yanfeng Wang",
          "hidden": false
        },
        {
          "_id": "69042c562c556835fa67f0d5",
          "name": "Yu Wang",
          "hidden": false
        },
        {
          "_id": "69042c562c556835fa67f0d6",
          "name": "Weidi Xie",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-29T15:32:47.000Z",
      "submittedOnDailyAt": "2025-10-31T01:57:51.700Z",
      "title": "EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic\n  Health Record Analysis",
      "submittedOnDailyBy": {
        "_id": "6436aaaa0c77d7c5036abdbd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6436aaaa0c77d7c5036abdbd/C9A276yEeAPkKLEqUcjl_.jpeg",
        "isPro": false,
        "fullname": "Chaoyi Wu",
        "user": "chaoyi-wu",
        "type": "user"
      },
      "summary": "Electronic Health Records (EHRs) contain rich yet complex information, and\ntheir automated analysis is critical for clinical decision-making. Despite\nrecent advances of large language models (LLMs) in clinical workflows, their\nability to analyze EHRs remains limited due to narrow task coverage and lack of\nEHR-oriented reasoning capabilities. This paper aims to bridge the gap,\nspecifically, we present EHR-Ins, a large-scale, comprehensive EHR reasoning\ninstruction dataset, comprising 300k high-quality reasoning cases and 4M\nnon-reasoning cases across 42 distinct EHR tasks. Its core innovation is a\nthinking-graph-driven framework that enables to generate high-quality reasoning\ndata at scale. Based on it, we develop EHR-R1, a series of reasoning-enhanced\nLLMs with up to 72B parameters tailored for EHR analysis. Through a multi-stage\ntraining paradigm, including domain adaptation, reasoning enhancement, and\nreinforcement learning, EHR-R1 systematically acquires domain knowledge and\ndiverse reasoning capabilities, enabling accurate and robust EHR analysis.\nLastly, we introduce EHR-Bench, a new benchmark curated from MIMIC-IV, spanning\n42 tasks, to comprehensively assess reasoning and prediction across EHR\nscenarios. In experiments, we show that the resulting EHR-R1 consistently\noutperforms state-of-the-art commercial and open-source LLMs (including\nDeepSeek-V3 and GPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and\nachieving a 10\\% higher zero-shot AUROC on EHRSHOT. Collectively, EHR-Ins,\nEHR-R1, and EHR-Bench have significantly advanced the development for more\nreliable and clinically relevant EHR analysis.",
      "upvotes": 3,
      "discussionId": "69042c562c556835fa67f0d7",
      "githubRepo": "https://github.com/MAGIC-AI4Med/EHR-R1",
      "githubStars": 4,
      "organization": {
        "_id": "63e5ef7bf2e9a8f22c515654",
        "name": "SJTU",
        "fullname": "Shanghai Jiao Tong University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"
      }
    },
    "publishedAt": "2025-10-29T11:32:47.000Z",
    "title": "EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic\n  Health Record Analysis",
    "summary": "Electronic Health Records (EHRs) contain rich yet complex information, and\ntheir automated analysis is critical for clinical decision-making. Despite\nrecent advances of large language models (LLMs) in clinical workflows, their\nability to analyze EHRs remains limited due to narrow task coverage and lack of\nEHR-oriented reasoning capabilities. This paper aims to bridge the gap,\nspecifically, we present EHR-Ins, a large-scale, comprehensive EHR reasoning\ninstruction dataset, comprising 300k high-quality reasoning cases and 4M\nnon-reasoning cases across 42 distinct EHR tasks. Its core innovation is a\nthinking-graph-driven framework that enables to generate high-quality reasoning\ndata at scale. Based on it, we develop EHR-R1, a series of reasoning-enhanced\nLLMs with up to 72B parameters tailored for EHR analysis. Through a multi-stage\ntraining paradigm, including domain adaptation, reasoning enhancement, and\nreinforcement learning, EHR-R1 systematically acquires domain knowledge and\ndiverse reasoning capabilities, enabling accurate and robust EHR analysis.\nLastly, we introduce EHR-Bench, a new benchmark curated from MIMIC-IV, spanning\n42 tasks, to comprehensively assess reasoning and prediction across EHR\nscenarios. In experiments, we show that the resulting EHR-R1 consistently\noutperforms state-of-the-art commercial and open-source LLMs (including\nDeepSeek-V3 and GPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and\nachieving a 10\\% higher zero-shot AUROC on EHRSHOT. Collectively, EHR-Ins,\nEHR-R1, and EHR-Bench have significantly advanced the development for more\nreliable and clinically relevant EHR analysis.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.25628.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6436aaaa0c77d7c5036abdbd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6436aaaa0c77d7c5036abdbd/C9A276yEeAPkKLEqUcjl_.jpeg",
      "fullname": "Chaoyi Wu",
      "name": "chaoyi-wu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 32
    },
    "organization": {
      "_id": "63e5ef7bf2e9a8f22c515654",
      "name": "SJTU",
      "fullname": "Shanghai Jiao Tong University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.26787",
      "authors": [
        {
          "_id": "690415cf2c556835fa67efd3",
          "name": "Mantas Mazeika",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efd4",
          "name": "Alice Gatti",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efd5",
          "name": "Cristina Menghini",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efd6",
          "name": "Udari Madhushani Sehwag",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efd7",
          "name": "Shivam Singhal",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efd8",
          "name": "Yury Orlovskiy",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efd9",
          "name": "Steven Basart",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efda",
          "name": "Manasi Sharma",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efdb",
          "name": "Denis Peskoff",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efdc",
          "name": "Elaine Lau",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efdd",
          "name": "Jaehyuk Lim",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efde",
          "name": "Lachlan Carroll",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efdf",
          "name": "Alice Blair",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efe0",
          "name": "Vinaya Sivakumar",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efe1",
          "name": "Sumana Basu",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efe2",
          "name": "Brad Kenstler",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efe3",
          "name": "Yuntao Ma",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efe4",
          "name": "Julian Michael",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efe5",
          "name": "Xiaoke Li",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efe6",
          "name": "Oliver Ingebretsen",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efe7",
          "name": "Aditya Mehta",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efe8",
          "name": "Jean Mottola",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efe9",
          "name": "John Teichmann",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efea",
          "name": "Kevin Yu",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efeb",
          "name": "Zaina Shaik",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efec",
          "name": "Adam Khoja",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efed",
          "name": "Richard Ren",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efee",
          "name": "Jason Hausenloy",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efef",
          "name": "Long Phan",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67eff0",
          "name": "Ye Htet",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67eff1",
          "name": "Ankit Aich",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67eff2",
          "name": "Tahseen Rabbani",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67eff3",
          "name": "Vivswan Shah",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67eff4",
          "name": "Andriy Novykov",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67eff5",
          "name": "Felix Binder",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67eff6",
          "name": "Kirill Chugunov",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67eff7",
          "name": "Luis Ramirez",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67eff8",
          "name": "Matias Geralnik",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67eff9",
          "name": "Hernán Mesura",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67effa",
          "name": "Dean Lee",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67effb",
          "name": "Ed-Yeremai Hernandez Cardona",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67effc",
          "name": "Annette Diamond",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67effd",
          "name": "Summer Yue",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67effe",
          "name": "Alexandr Wang",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67efff",
          "name": "Bing Liu",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67f000",
          "name": "Ernesto Hernandez",
          "hidden": false
        },
        {
          "_id": "690415cf2c556835fa67f001",
          "name": "Dan Hendrycks",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-30T17:58:04.000Z",
      "submittedOnDailyAt": "2025-10-31T00:20:15.697Z",
      "title": "Remote Labor Index: Measuring AI Automation of Remote Work",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "AIs have made rapid progress on research-oriented benchmarks of knowledge and\nreasoning, but it remains unclear how these gains translate into economic value\nand automation. To measure this, we introduce the Remote Labor Index (RLI), a\nbroadly multi-sector benchmark comprising real-world, economically valuable\nprojects designed to evaluate end-to-end agent performance in practical\nsettings. AI agents perform near the floor on RLI, with the highest-performing\nagent achieving an automation rate of 2.5%. These results help ground\ndiscussions of AI automation in empirical evidence, setting a common basis for\ntracking AI impacts and enabling stakeholders to proactively navigate AI-driven\nlabor automation.",
      "upvotes": 2,
      "discussionId": "690415cf2c556835fa67f002"
    },
    "publishedAt": "2025-10-30T13:58:04.000Z",
    "title": "Remote Labor Index: Measuring AI Automation of Remote Work",
    "summary": "AIs have made rapid progress on research-oriented benchmarks of knowledge and\nreasoning, but it remains unclear how these gains translate into economic value\nand automation. To measure this, we introduce the Remote Labor Index (RLI), a\nbroadly multi-sector benchmark comprising real-world, economically valuable\nprojects designed to evaluate end-to-end agent performance in practical\nsettings. AI agents perform near the floor on RLI, with the highest-performing\nagent achieving an automation rate of 2.5%. These results help ground\ndiscussions of AI automation in empirical evidence, setting a common basis for\ntracking AI impacts and enabling stakeholders to proactively navigate AI-driven\nlabor automation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26787.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 152
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.26140",
      "authors": [
        {
          "_id": "69041ae72c556835fa67f06c",
          "name": "Lihe Ding",
          "hidden": false
        },
        {
          "_id": "69041ae72c556835fa67f06d",
          "name": "Shaocong Dong",
          "hidden": false
        },
        {
          "_id": "69041ae72c556835fa67f06e",
          "name": "Yaokun Li",
          "hidden": false
        },
        {
          "_id": "69041ae72c556835fa67f06f",
          "name": "Chenjian Gao",
          "hidden": false
        },
        {
          "_id": "69041ae72c556835fa67f070",
          "name": "Xiao Chen",
          "hidden": false
        },
        {
          "_id": "69041ae72c556835fa67f071",
          "name": "Rui Han",
          "hidden": false
        },
        {
          "_id": "69041ae72c556835fa67f072",
          "name": "Yihao Kuang",
          "hidden": false
        },
        {
          "_id": "69041ae72c556835fa67f073",
          "name": "Hong Zhang",
          "hidden": false
        },
        {
          "_id": "69041ae72c556835fa67f074",
          "name": "Bo Huang",
          "hidden": false
        },
        {
          "_id": "69041ae72c556835fa67f075",
          "name": "Zhanpeng Huang",
          "hidden": false
        },
        {
          "_id": "69041ae72c556835fa67f076",
          "name": "Zibin Wang",
          "hidden": false
        },
        {
          "_id": "69041ae72c556835fa67f077",
          "name": "Dan Xu",
          "hidden": false
        },
        {
          "_id": "69041ae72c556835fa67f078",
          "name": "Tianfan Xue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-30T04:51:05.000Z",
      "submittedOnDailyAt": "2025-10-31T00:46:19.339Z",
      "title": "FullPart: Generating each 3D Part at Full Resolution",
      "submittedOnDailyBy": {
        "_id": "63ae91af2314b93f9e6dde42",
        "avatarUrl": "/avatars/792ce138cbee85b8754fdcec7fb1ff52.svg",
        "isPro": true,
        "fullname": "Shaocong Dong",
        "user": "dscdyc",
        "type": "user"
      },
      "summary": "Part-based 3D generation holds great potential for various applications.\nPrevious part generators that represent parts using implicit vector-set tokens\noften suffer from insufficient geometric details. Another line of work adopts\nan explicit voxel representation but shares a global voxel grid among all\nparts; this often causes small parts to occupy too few voxels, leading to\ndegraded quality. In this paper, we propose FullPart, a novel framework that\ncombines both implicit and explicit paradigms. It first derives the bounding\nbox layout through an implicit box vector-set diffusion process, a task that\nimplicit diffusion handles effectively since box tokens contain little\ngeometric detail. Then, it generates detailed parts, each within its own fixed\nfull-resolution voxel grid. Instead of sharing a global low-resolution space,\neach part in our method - even small ones - is generated at full resolution,\nenabling the synthesis of intricate details. We further introduce a\ncenter-point encoding strategy to address the misalignment issue when\nexchanging information between parts of different actual sizes, thereby\nmaintaining global coherence. Moreover, to tackle the scarcity of reliable part\ndata, we present PartVerse-XL, the largest human-annotated 3D part dataset to\ndate with 40K objects and 320K parts. Extensive experiments demonstrate that\nFullPart achieves state-of-the-art results in 3D part generation. We will\nrelease all code, data, and model to benefit future research in 3D part\ngeneration.",
      "upvotes": 2,
      "discussionId": "69041ae72c556835fa67f079",
      "projectPage": "https://fullpart3d.github.io/",
      "githubRepo": "https://github.com/hkdsc/fullpart",
      "githubStars": 3
    },
    "publishedAt": "2025-10-30T00:51:05.000Z",
    "title": "FullPart: Generating each 3D Part at Full Resolution",
    "summary": "Part-based 3D generation holds great potential for various applications.\nPrevious part generators that represent parts using implicit vector-set tokens\noften suffer from insufficient geometric details. Another line of work adopts\nan explicit voxel representation but shares a global voxel grid among all\nparts; this often causes small parts to occupy too few voxels, leading to\ndegraded quality. In this paper, we propose FullPart, a novel framework that\ncombines both implicit and explicit paradigms. It first derives the bounding\nbox layout through an implicit box vector-set diffusion process, a task that\nimplicit diffusion handles effectively since box tokens contain little\ngeometric detail. Then, it generates detailed parts, each within its own fixed\nfull-resolution voxel grid. Instead of sharing a global low-resolution space,\neach part in our method - even small ones - is generated at full resolution,\nenabling the synthesis of intricate details. We further introduce a\ncenter-point encoding strategy to address the misalignment issue when\nexchanging information between parts of different actual sizes, thereby\nmaintaining global coherence. Moreover, to tackle the scarcity of reliable part\ndata, we present PartVerse-XL, the largest human-annotated 3D part dataset to\ndate with 40K objects and 320K parts. Extensive experiments demonstrate that\nFullPart achieves state-of-the-art results in 3D part generation. We will\nrelease all code, data, and model to benefit future research in 3D part\ngeneration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26140.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63ae91af2314b93f9e6dde42",
      "avatarUrl": "/avatars/792ce138cbee85b8754fdcec7fb1ff52.svg",
      "fullname": "Shaocong Dong",
      "name": "dscdyc",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.26474",
      "authors": [
        {
          "_id": "6904182a2c556835fa67f02d",
          "name": "Xin Guo",
          "hidden": false
        },
        {
          "_id": "6904182a2c556835fa67f02e",
          "name": "Zhiheng Xi",
          "hidden": false
        },
        {
          "_id": "6904182a2c556835fa67f02f",
          "name": "Yiwen Ding",
          "hidden": false
        },
        {
          "_id": "6904182a2c556835fa67f030",
          "name": "Yitao Zhai",
          "hidden": false
        },
        {
          "_id": "6904182a2c556835fa67f031",
          "name": "Xiaowei Shi",
          "hidden": false
        },
        {
          "_id": "6904182a2c556835fa67f032",
          "name": "Xunliang Cai",
          "hidden": false
        },
        {
          "_id": "6904182a2c556835fa67f033",
          "name": "Tao Gui",
          "hidden": false
        },
        {
          "_id": "6904182a2c556835fa67f034",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "6904182a2c556835fa67f035",
          "name": "Xuanjing Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-30T13:26:58.000Z",
      "submittedOnDailyAt": "2025-10-31T00:30:19.141Z",
      "title": "Counteracting Matthew Effect in Self-Improvement of LVLMs through\n  Head-Tail Re-balancing",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Self-improvement has emerged as a mainstream paradigm for advancing the\nreasoning capabilities of large vision-language models (LVLMs), where models\nexplore and learn from successful trajectories iteratively. However, we\nidentify a critical issue during this process: the model excels at generating\nhigh-quality trajectories for simple queries (i.e., head data) but struggles\nwith more complex ones (i.e., tail data). This leads to an imbalanced\noptimization that drives the model to prioritize simple reasoning skills, while\nhindering its ability to tackle more complex reasoning tasks. Over iterations,\nthis imbalance becomes increasingly pronounced--a dynamic we term the \"Matthew\neffect\"--which ultimately hinders further model improvement and leads to\nperformance bottlenecks. To counteract this challenge, we introduce four\nefficient strategies from two perspectives: distribution-reshaping and\ntrajectory-resampling, to achieve head-tail re-balancing during the\nexploration-and-learning self-improvement process. Extensive experiments on\nQwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks\ndemonstrate that our methods consistently improve visual reasoning\ncapabilities, outperforming vanilla self-improvement by 3.86 points on average.",
      "upvotes": 1,
      "discussionId": "6904182a2c556835fa67f036"
    },
    "publishedAt": "2025-10-30T09:26:58.000Z",
    "title": "Counteracting Matthew Effect in Self-Improvement of LVLMs through\n  Head-Tail Re-balancing",
    "summary": "Self-improvement has emerged as a mainstream paradigm for advancing the\nreasoning capabilities of large vision-language models (LVLMs), where models\nexplore and learn from successful trajectories iteratively. However, we\nidentify a critical issue during this process: the model excels at generating\nhigh-quality trajectories for simple queries (i.e., head data) but struggles\nwith more complex ones (i.e., tail data). This leads to an imbalanced\noptimization that drives the model to prioritize simple reasoning skills, while\nhindering its ability to tackle more complex reasoning tasks. Over iterations,\nthis imbalance becomes increasingly pronounced--a dynamic we term the \"Matthew\neffect\"--which ultimately hinders further model improvement and leads to\nperformance bottlenecks. To counteract this challenge, we introduce four\nefficient strategies from two perspectives: distribution-reshaping and\ntrajectory-resampling, to achieve head-tail re-balancing during the\nexploration-and-learning self-improvement process. Extensive experiments on\nQwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks\ndemonstrate that our methods consistently improve visual reasoning\ncapabilities, outperforming vanilla self-improvement by 3.86 points on average.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26474.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 152
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.26160",
      "authors": [
        {
          "_id": "690419172c556835fa67f038",
          "name": "Jiaqi Wang",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f039",
          "name": "Xiao Yang",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f03a",
          "name": "Kai Sun",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f03b",
          "name": "Parth Suresh",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f03c",
          "name": "Sanat Sharma",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f03d",
          "name": "Adam Czyzewski",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f03e",
          "name": "Derek Andersen",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f03f",
          "name": "Surya Appini",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f040",
          "name": "Arkav Banerjee",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f041",
          "name": "Sajal Choudhary",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f042",
          "name": "Shervin Ghasemlou",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f043",
          "name": "Ziqiang Guan",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f044",
          "name": "Akil Iyer",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f045",
          "name": "Haidar Khan",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f046",
          "name": "Lingkun Kong",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f047",
          "name": "Roy Luo",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f048",
          "name": "Tiffany Ma",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f049",
          "name": "Zhen Qiao",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f04a",
          "name": "David Tran",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f04b",
          "name": "Wenfang Xu",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f04c",
          "name": "Skyler Yeatman",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f04d",
          "name": "Chen Zhou",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f04e",
          "name": "Gunveer Gujral",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f04f",
          "name": "Yinglong Xia",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f050",
          "name": "Shane Moon",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f051",
          "name": "Nicolas Scheffer",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f052",
          "name": "Nirav Shah",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f053",
          "name": "Eun Chang",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f054",
          "name": "Yue Liu",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f055",
          "name": "Florian Metze",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f056",
          "name": "Tammy Stark",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f057",
          "name": "Zhaleh Feizollahi",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f058",
          "name": "Andrea Jessee",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f059",
          "name": "Mangesh Pujari",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f05a",
          "name": "Ahmed Aly",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f05b",
          "name": "Babak Damavandi",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f05c",
          "name": "Rakesh Wanga",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f05d",
          "name": "Anuj Kumar",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f05e",
          "name": "Rohit Patel",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f05f",
          "name": "Wen-tau Yih",
          "hidden": false
        },
        {
          "_id": "690419172c556835fa67f060",
          "name": "Xin Luna Dong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-30T05:50:48.000Z",
      "submittedOnDailyAt": "2025-10-31T00:34:25.527Z",
      "title": "CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Wearable devices such as smart glasses are transforming the way people\ninteract with their surroundings, enabling users to seek information regarding\nentities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG)\nplays a key role in supporting such questions, yet there is still no\ncomprehensive benchmark for this task, especially regarding wearables\nscenarios. To fill this gap, we present CRAG-MM -- a Comprehensive RAG\nbenchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse\nset of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn\nconversations across 13 domains, including 6.2K egocentric images designed to\nmimic captures from wearable devices. We carefully constructed the questions to\nreflect real-world scenarios and challenges, including five types of\nimage-quality issues, six question types, varying entity popularity, differing\ninformation dynamism, and different conversation turns. We design three tasks:\nsingle-source augmentation, multi-source augmentation, and multi-turn\nconversations -- each paired with an associated retrieval corpus and APIs for\nboth image-KG retrieval and webpage retrieval. Our evaluation shows that\nstraightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM\nsingle- and multi-turn QA, respectively, whereas state-of-the-art industry\nsolutions have similar quality (32%/45%), underscoring ample room for\nimprovement. The benchmark has hosted KDD Cup 2025, attracting about 1K\nparticipants and 5K submissions, with winning solutions improving baseline\nperformance by 28%, highlighting its early impact on advancing the field.",
      "upvotes": 1,
      "discussionId": "690419172c556835fa67f061",
      "organization": {
        "_id": "5e63d8713071d5be688861b8",
        "name": "facebook",
        "fullname": "AI at Meta",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
      }
    },
    "publishedAt": "2025-10-30T01:50:48.000Z",
    "title": "CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark",
    "summary": "Wearable devices such as smart glasses are transforming the way people\ninteract with their surroundings, enabling users to seek information regarding\nentities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG)\nplays a key role in supporting such questions, yet there is still no\ncomprehensive benchmark for this task, especially regarding wearables\nscenarios. To fill this gap, we present CRAG-MM -- a Comprehensive RAG\nbenchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse\nset of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn\nconversations across 13 domains, including 6.2K egocentric images designed to\nmimic captures from wearable devices. We carefully constructed the questions to\nreflect real-world scenarios and challenges, including five types of\nimage-quality issues, six question types, varying entity popularity, differing\ninformation dynamism, and different conversation turns. We design three tasks:\nsingle-source augmentation, multi-source augmentation, and multi-turn\nconversations -- each paired with an associated retrieval corpus and APIs for\nboth image-KG retrieval and webpage retrieval. Our evaluation shows that\nstraightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM\nsingle- and multi-turn QA, respectively, whereas state-of-the-art industry\nsolutions have similar quality (32%/45%), underscoring ample room for\nimprovement. The benchmark has hosted KDD Cup 2025, attracting about 1K\nparticipants and 5K submissions, with winning solutions improving baseline\nperformance by 28%, highlighting its early impact on advancing the field.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26160.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 152
    },
    "organization": {
      "_id": "5e63d8713071d5be688861b8",
      "name": "facebook",
      "fullname": "AI at Meta",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.25132",
      "authors": [
        {
          "_id": "6904224b2c556835fa67f0a5",
          "name": "Chao Song",
          "hidden": false
        },
        {
          "_id": "6904224b2c556835fa67f0a6",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "6904224b2c556835fa67f0a7",
          "name": "Han Huang",
          "hidden": false
        },
        {
          "_id": "6904224b2c556835fa67f0a8",
          "name": "Liang Wang",
          "hidden": false
        },
        {
          "_id": "6904224b2c556835fa67f0a9",
          "name": "Qiong Wang",
          "hidden": false
        },
        {
          "_id": "6904224b2c556835fa67f0aa",
          "name": "Jianyu Shi",
          "hidden": false
        },
        {
          "_id": "6904224b2c556835fa67f0ab",
          "name": "Hui Yu",
          "hidden": false
        },
        {
          "_id": "6904224b2c556835fa67f0ac",
          "name": "Yihang Zhou",
          "hidden": false
        },
        {
          "_id": "6904224b2c556835fa67f0ad",
          "name": "Yang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-29T03:22:32.000Z",
      "submittedOnDailyAt": "2025-10-31T01:15:01.124Z",
      "title": "EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme\n  Backbone Generation",
      "submittedOnDailyBy": {
        "_id": "6310a3cd531cc21f9e06de6a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6310a3cd531cc21f9e06de6a/aTGMx3O41lUARK9s3dAik.jpeg",
        "isPro": false,
        "fullname": "Zhiyuan Liu",
        "user": "acharkq",
        "type": "user"
      },
      "summary": "Designing enzyme backbones with substrate-specific functionality is a\ncritical challenge in computational protein engineering. Current generative\nmodels excel in protein design but face limitations in binding data,\nsubstrate-specific control, and flexibility for de novo enzyme backbone\ngeneration. To address this, we introduce EnzyBind, a dataset with 11,100\nexperimentally validated enzyme-substrate pairs specifically curated from\nPDBbind. Building on this, we propose EnzyControl, a method that enables\nfunctional and substrate-specific control in enzyme backbone generation. Our\napproach generates enzyme backbones conditioned on MSA-annotated catalytic\nsites and their corresponding substrates, which are automatically extracted\nfrom curated enzyme-substrate data. At the core of EnzyControl is EnzyAdapter,\na lightweight, modular component integrated into a pretrained motif-scaffolding\nmodel, allowing it to become substrate-aware. A two-stage training paradigm\nfurther refines the model's ability to generate accurate and functional enzyme\nstructures. Experiments show that our EnzyControl achieves the best performance\nacross structural and functional metrics on EnzyBind and EnzyBench benchmarks,\nwith particularly notable improvements of 13\\% in designability and 13\\% in\ncatalytic efficiency compared to the baseline models. The code is released at\nhttps://github.com/Vecteur-libre/EnzyControl.",
      "upvotes": 1,
      "discussionId": "6904224b2c556835fa67f0ae",
      "organization": {
        "_id": "6508ab2b349930913196378b",
        "name": "NationalUniversityofSingapore",
        "fullname": "National University of Singapore",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"
      }
    },
    "publishedAt": "2025-10-28T23:22:32.000Z",
    "title": "EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme\n  Backbone Generation",
    "summary": "Designing enzyme backbones with substrate-specific functionality is a\ncritical challenge in computational protein engineering. Current generative\nmodels excel in protein design but face limitations in binding data,\nsubstrate-specific control, and flexibility for de novo enzyme backbone\ngeneration. To address this, we introduce EnzyBind, a dataset with 11,100\nexperimentally validated enzyme-substrate pairs specifically curated from\nPDBbind. Building on this, we propose EnzyControl, a method that enables\nfunctional and substrate-specific control in enzyme backbone generation. Our\napproach generates enzyme backbones conditioned on MSA-annotated catalytic\nsites and their corresponding substrates, which are automatically extracted\nfrom curated enzyme-substrate data. At the core of EnzyControl is EnzyAdapter,\na lightweight, modular component integrated into a pretrained motif-scaffolding\nmodel, allowing it to become substrate-aware. A two-stage training paradigm\nfurther refines the model's ability to generate accurate and functional enzyme\nstructures. Experiments show that our EnzyControl achieves the best performance\nacross structural and functional metrics on EnzyBind and EnzyBench benchmarks,\nwith particularly notable improvements of 13\\% in designability and 13\\% in\ncatalytic efficiency compared to the baseline models. The code is released at\nhttps://github.com/Vecteur-libre/EnzyControl.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.25132.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6310a3cd531cc21f9e06de6a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6310a3cd531cc21f9e06de6a/aTGMx3O41lUARK9s3dAik.jpeg",
      "fullname": "Zhiyuan Liu",
      "name": "acharkq",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "6508ab2b349930913196378b",
      "name": "NationalUniversityofSingapore",
      "fullname": "National University of Singapore",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.26781",
      "authors": [
        {
          "_id": "69045abf2c556835fa67f163",
          "name": "Aniruddh Bansal",
          "hidden": false
        },
        {
          "_id": "69045abf2c556835fa67f164",
          "name": "Davit Soselia",
          "hidden": false
        },
        {
          "_id": "69045abf2c556835fa67f165",
          "name": "Dang Nguyen",
          "hidden": false
        },
        {
          "_id": "69045abf2c556835fa67f166",
          "name": "Tianyi Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-30T17:56:31.000Z",
      "submittedOnDailyAt": "2025-10-31T05:16:49.401Z",
      "title": "ChartAB: A Benchmark for Chart Grounding & Dense Alignment",
      "submittedOnDailyBy": {
        "_id": "647f5af5b0e96764589f3b2a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
        "isPro": false,
        "fullname": "Tianyi Zhou",
        "user": "zhoutianyi",
        "type": "user"
      },
      "summary": "Charts play an important role in visualization, reasoning, data analysis, and\nthe exchange of ideas among humans. However, existing vision-language models\n(VLMs) still lack accurate perception of details and struggle to extract\nfine-grained structures from charts. Such limitations in chart grounding also\nhinder their ability to compare multiple charts and reason over them. In this\npaper, we introduce a novel \"ChartAlign Benchmark (ChartAB)\" to provide a\ncomprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting\ntabular data, localizing visualization elements, and recognizing various\nattributes from charts of diverse types and complexities. We design a JSON\ntemplate to facilitate the calculation of evaluation metrics specifically\ntailored for each grounding task. By incorporating a novel two-stage inference\nworkflow, the benchmark can further evaluate VLMs' capability to align and\ncompare elements/attributes across two charts. Our analysis of evaluations on\nseveral recent VLMs reveals new insights into their perception biases,\nweaknesses, robustness, and hallucinations in chart understanding. These\nfindings highlight the fine-grained discrepancies among VLMs in chart\nunderstanding tasks and point to specific skills that need to be strengthened\nin current models.",
      "upvotes": 0,
      "discussionId": "69045abf2c556835fa67f167",
      "projectPage": "https://huggingface.co/datasets/umd-zhou-lab/ChartAlignBench",
      "githubRepo": "https://github.com/tianyi-lab/ChartAlignBench",
      "githubStars": 0,
      "organization": {
        "_id": "68b3c3bbc375e05b059370b2",
        "name": "UMCP",
        "fullname": "University of Maryland College Park",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68b3c2c3a4ea236d1a97871a/bji3nI5ZWm2r4JX_-HLo0.png"
      }
    },
    "publishedAt": "2025-10-30T13:56:31.000Z",
    "title": "ChartAB: A Benchmark for Chart Grounding & Dense Alignment",
    "summary": "Charts play an important role in visualization, reasoning, data analysis, and\nthe exchange of ideas among humans. However, existing vision-language models\n(VLMs) still lack accurate perception of details and struggle to extract\nfine-grained structures from charts. Such limitations in chart grounding also\nhinder their ability to compare multiple charts and reason over them. In this\npaper, we introduce a novel \"ChartAlign Benchmark (ChartAB)\" to provide a\ncomprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting\ntabular data, localizing visualization elements, and recognizing various\nattributes from charts of diverse types and complexities. We design a JSON\ntemplate to facilitate the calculation of evaluation metrics specifically\ntailored for each grounding task. By incorporating a novel two-stage inference\nworkflow, the benchmark can further evaluate VLMs' capability to align and\ncompare elements/attributes across two charts. Our analysis of evaluations on\nseveral recent VLMs reveals new insights into their perception biases,\nweaknesses, robustness, and hallucinations in chart understanding. These\nfindings highlight the fine-grained discrepancies among VLMs in chart\nunderstanding tasks and point to specific skills that need to be strengthened\nin current models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.26781.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647f5af5b0e96764589f3b2a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
      "fullname": "Tianyi Zhou",
      "name": "zhoutianyi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "organization": {
      "_id": "68b3c3bbc375e05b059370b2",
      "name": "UMCP",
      "fullname": "University of Maryland College Park",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68b3c2c3a4ea236d1a97871a/bji3nI5ZWm2r4JX_-HLo0.png"
    },
    "isAuthorParticipating": false
  }
]