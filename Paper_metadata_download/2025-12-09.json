[
  {
    "paper": {
      "id": "2512.07525",
      "authors": [
        {
          "_id": "693794d319d912300c34a291",
          "name": "Xiaoran Liu",
          "hidden": false
        },
        {
          "_id": "693794d319d912300c34a292",
          "name": "Yuerong Song",
          "hidden": false
        },
        {
          "_id": "693794d319d912300c34a293",
          "name": "Zhigeng Liu",
          "hidden": false
        },
        {
          "_id": "693794d319d912300c34a294",
          "name": "Zengfeng Huang",
          "hidden": false
        },
        {
          "_id": "693794d319d912300c34a295",
          "name": "Qipeng Guo",
          "hidden": false
        },
        {
          "_id": "693794d319d912300c34a296",
          "name": "Zhaoxiang Liu",
          "hidden": false
        },
        {
          "_id": "693794d319d912300c34a297",
          "name": "Shiguo Lian",
          "hidden": false
        },
        {
          "_id": "693794d319d912300c34a298",
          "name": "Ziwei He",
          "hidden": false
        },
        {
          "_id": "693794d319d912300c34a299",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-08T12:59:54.000Z",
      "submittedOnDailyAt": "2025-12-09T00:49:29.234Z",
      "title": "Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs",
      "submittedOnDailyBy": {
        "_id": "64f033ef82c6eea604c4da8b",
        "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
        "isPro": false,
        "fullname": "Xiaoran Liu (SII)",
        "user": "SII-xrliu",
        "type": "user"
      },
      "summary": "Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp.",
      "upvotes": 31,
      "discussionId": "693794d419d912300c34a29a",
      "githubRepo": "https://github.com/OpenMOSS/rope_pp",
      "ai_summary": "The paper proposes a method to enhance Rotary Position Embeddings by utilizing both the real and imaginary components of the complex-valued dot product, improving long-context modeling in Large Language Models.",
      "ai_keywords": [
        "Rotary Position Embeddings",
        "Large Language Models",
        "complex-valued dot product",
        "attention score",
        "long-context dependencies",
        "positional information"
      ],
      "githubStars": 5,
      "organization": {
        "_id": "613b0dee83ec35d460684607",
        "name": "OpenMOSS-Team",
        "fullname": "OpenMOSS",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"
      }
    },
    "publishedAt": "2025-12-08T07:59:54.000Z",
    "title": "Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs",
    "summary": "Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07525.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f033ef82c6eea604c4da8b",
      "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
      "fullname": "Xiaoran Liu (SII)",
      "name": "SII-xrliu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "organization": {
      "_id": "613b0dee83ec35d460684607",
      "name": "OpenMOSS-Team",
      "fullname": "OpenMOSS",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.07461",
      "authors": [
        {
          "_id": "6937b96219d912300c34a398",
          "name": "Tong Wu",
          "hidden": false
        },
        {
          "_id": "6937b96219d912300c34a399",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "6937b96219d912300c34a39a",
          "name": "Jun Bai",
          "hidden": false
        },
        {
          "_id": "6937b96219d912300c34a39b",
          "name": "Zixia Jia",
          "hidden": false
        },
        {
          "_id": "6937b96219d912300c34a39c",
          "name": "Shuyi Zhang",
          "hidden": false
        },
        {
          "_id": "6937b96219d912300c34a39d",
          "name": "Ziyong Lin",
          "hidden": false
        },
        {
          "_id": "6937b96219d912300c34a39e",
          "name": "Yanting Wang",
          "hidden": false
        },
        {
          "_id": "6937b96219d912300c34a39f",
          "name": "Song-Chun Zhu",
          "hidden": false
        },
        {
          "_id": "6937b96219d912300c34a3a0",
          "name": "Zilong Zheng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-08T11:39:43.000Z",
      "submittedOnDailyAt": "2025-12-09T04:12:55.960Z",
      "title": "Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "63a95a6a7930fa8c7dd63d4e",
        "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg",
        "isPro": false,
        "fullname": "Zilong Zheng",
        "user": "zlzheng",
        "type": "user"
      },
      "summary": "We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.",
      "upvotes": 27,
      "discussionId": "6937b96219d912300c34a3a1",
      "ai_summary": "NPR, a teacher-free framework, enhances Large Language Models with native parallel reasoning capabilities through self-distilled training, Parallel-Aware Policy Optimization, and a robust NPR Engine, achieving substantial performance and speed improvements.",
      "ai_keywords": [
        "Native Parallel Reasoner",
        "Large Language Models",
        "self-evolve",
        "parallel reasoning",
        "self-distilled progressive training",
        "cold-start format discovery",
        "topological constraints",
        "Parallel-Aware Policy Optimization",
        "branching policies",
        "execution graph",
        "adaptive decomposition",
        "trial and error",
        "NPR Engine",
        "memory management",
        "flow control",
        "parallel RL training",
        "reasoning benchmarks",
        "Qwen3-4B",
        "genuine parallel execution",
        "autoregressive decoding",
        "agentic reasoning"
      ],
      "organization": {
        "_id": "63a95ac93453852ef5399a77",
        "name": "bigai",
        "fullname": "Beijing Institute for General Artificial Intelligence",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1672043197974-63a95a6a7930fa8c7dd63d4e.png"
      }
    },
    "publishedAt": "2025-12-08T06:39:43.000Z",
    "title": "Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning",
    "summary": "We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07461.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a95a6a7930fa8c7dd63d4e",
      "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg",
      "fullname": "Zilong Zheng",
      "name": "zlzheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "organization": {
      "_id": "63a95ac93453852ef5399a77",
      "name": "bigai",
      "fullname": "Beijing Institute for General Artificial Intelligence",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1672043197974-63a95a6a7930fa8c7dd63d4e.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.07469",
      "authors": [
        {
          "_id": "69379e0319d912300c34a2fb",
          "name": "Xiangpeng Yang",
          "hidden": false
        },
        {
          "_id": "69379e0319d912300c34a2fc",
          "name": "Ji Xie",
          "hidden": false
        },
        {
          "_id": "69379e0319d912300c34a2fd",
          "name": "Yiyuan Yang",
          "hidden": false
        },
        {
          "_id": "69379e0319d912300c34a2fe",
          "name": "Yan Huang",
          "hidden": false
        },
        {
          "_id": "69379e0319d912300c34a2ff",
          "name": "Min Xu",
          "hidden": false
        },
        {
          "_id": "69379e0319d912300c34a300",
          "name": "Qiang Wu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6486df66373f79a52913e017/iKPp57sIku0K9HVBgHjuu.mp4"
      ],
      "publishedAt": "2025-12-08T11:50:18.000Z",
      "submittedOnDailyAt": "2025-12-09T01:34:28.853Z",
      "title": "Unified Video Editing with Temporal Reasoner",
      "submittedOnDailyBy": {
        "_id": "6486df66373f79a52913e017",
        "avatarUrl": "/avatars/4741683fbbcec3a615d0a8df62bc6fec.svg",
        "isPro": false,
        "fullname": "Xiangpeng Yang",
        "user": "XiangpengYang",
        "type": "user"
      },
      "summary": "Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit\" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.",
      "upvotes": 24,
      "discussionId": "69379e0319d912300c34a301",
      "projectPage": "https://videocof.github.io/",
      "githubRepo": "https://github.com/knightyxp/VideoCoF",
      "ai_summary": "VideoCoF, a Chain-of-Frames approach, improves video editing precision and instruction-to-region mapping by using reasoning tokens without requiring user-provided masks.",
      "ai_keywords": [
        "chain-of-frames",
        "chain-of-thought reasoning",
        "video diffusion model",
        "reasoning tokens",
        "edit-region latents",
        "target video tokens",
        "instruction-to-region alignment",
        "fine-grained video editing",
        "RoPE alignment",
        "motion alignment",
        "length extrapolation"
      ],
      "githubStars": 8,
      "organization": {
        "_id": "67c4a2574f5d0005fd418d85",
        "name": "staraj3",
        "fullname": "University of Technology Sydney",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67c4a1ab71e55dc41e273b5a/8l4iiw2XH5nbIlLURt7ep.png"
      }
    },
    "publishedAt": "2025-12-08T06:50:18.000Z",
    "title": "Unified Video Editing with Temporal Reasoner",
    "summary": "Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit\" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6486df66373f79a52913e017/iKPp57sIku0K9HVBgHjuu.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07469.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6486df66373f79a52913e017",
      "avatarUrl": "/avatars/4741683fbbcec3a615d0a8df62bc6fec.svg",
      "fullname": "Xiangpeng Yang",
      "name": "XiangpengYang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "organization": {
      "_id": "67c4a2574f5d0005fd418d85",
      "name": "staraj3",
      "fullname": "University of Technology Sydney",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67c4a1ab71e55dc41e273b5a/8l4iiw2XH5nbIlLURt7ep.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.06749",
      "authors": [
        {
          "_id": "693791a319d912300c34a271",
          "name": "Ming Ma",
          "hidden": false
        },
        {
          "_id": "693791a319d912300c34a272",
          "name": "Jue Zhang",
          "hidden": false
        },
        {
          "_id": "693791a319d912300c34a273",
          "name": "Fangkai Yang",
          "hidden": false
        },
        {
          "_id": "693791a319d912300c34a274",
          "name": "Yu Kang",
          "hidden": false
        },
        {
          "_id": "693791a319d912300c34a275",
          "name": "Qingwei Lin",
          "hidden": false
        },
        {
          "_id": "693791a319d912300c34a276",
          "name": "Saravan Rajmohan",
          "hidden": false
        },
        {
          "_id": "693791a319d912300c34a277",
          "name": "Dongmei Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-07T09:23:48.000Z",
      "submittedOnDailyAt": "2025-12-09T00:43:03.031Z",
      "title": "DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems",
      "submittedOnDailyBy": {
        "_id": "65f40e43653c231cbaf7d1e4",
        "avatarUrl": "/avatars/a42ac5454cbe175f04c3420fce90cad2.svg",
        "isPro": false,
        "fullname": "Jue Zhang",
        "user": "JueZhang",
        "type": "user"
      },
      "summary": "Large language model (LLM)-based multi-agent systems are challenging to debug because failures often arise from long, branching interaction traces. The prevailing practice is to leverage LLMs for log-based failure localization, attributing errors to a specific agent and step. However, this paradigm has two key limitations: (i) log-only debugging lacks validation, producing untested hypotheses, and (ii) single-step or single-agent attribution is often ill-posed, as we find that multiple distinct interventions can independently repair the failed task. To address the first limitation, we introduce DoVer, an intervention-driven debugging framework, which augments hypothesis generation with active verification through targeted interventions (e.g., editing messages, altering plans). For the second limitation, rather than evaluating on attribution accuracy, we focus on measuring whether the system resolves the failure or makes quantifiable progress toward task success, reflecting a more outcome-oriented view of debugging. Within the Magnetic-One agent framework, on the datasets derived from GAIA and AssistantBench, DoVer flips 18-28% of failed trials into successes, achieves up to 16% milestone progress, and validates or refutes 30-60% of failure hypotheses. DoVer also performs effectively on a different dataset (GSMPlus) and agent framework (AG2), where it recovers 49% of failed trials. These results highlight intervention as a practical mechanism for improving reliability in agentic systems and open opportunities for more robust, scalable debugging methods for LLM-based multi-agent systems. Project website and code will be available at https://aka.ms/DoVer.",
      "upvotes": 20,
      "discussionId": "693791a319d912300c34a278",
      "projectPage": "https://aka.ms/DoVer",
      "ai_summary": "DoVer, an intervention-driven debugging framework, enhances reliability in LLM-based multi-agent systems by actively validating failure hypotheses and measuring task progress through targeted interventions.",
      "ai_keywords": [
        "DoVer",
        "intervention-driven debugging",
        "LLM-based multi-agent systems",
        "failure localization",
        "targeted interventions",
        "outcome-oriented debugging",
        "Magnetic-One agent framework",
        "GAIA",
        "AssistantBench",
        "GSMPlus",
        "AG2"
      ],
      "organization": {
        "_id": "5e6485f787403103f9f1055e",
        "name": "microsoft",
        "fullname": "Microsoft",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
      }
    },
    "publishedAt": "2025-12-07T04:23:48.000Z",
    "title": "DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems",
    "summary": "Large language model (LLM)-based multi-agent systems are challenging to debug because failures often arise from long, branching interaction traces. The prevailing practice is to leverage LLMs for log-based failure localization, attributing errors to a specific agent and step. However, this paradigm has two key limitations: (i) log-only debugging lacks validation, producing untested hypotheses, and (ii) single-step or single-agent attribution is often ill-posed, as we find that multiple distinct interventions can independently repair the failed task. To address the first limitation, we introduce DoVer, an intervention-driven debugging framework, which augments hypothesis generation with active verification through targeted interventions (e.g., editing messages, altering plans). For the second limitation, rather than evaluating on attribution accuracy, we focus on measuring whether the system resolves the failure or makes quantifiable progress toward task success, reflecting a more outcome-oriented view of debugging. Within the Magnetic-One agent framework, on the datasets derived from GAIA and AssistantBench, DoVer flips 18-28% of failed trials into successes, achieves up to 16% milestone progress, and validates or refutes 30-60% of failure hypotheses. DoVer also performs effectively on a different dataset (GSMPlus) and agent framework (AG2), where it recovers 49% of failed trials. These results highlight intervention as a practical mechanism for improving reliability in agentic systems and open opportunities for more robust, scalable debugging methods for LLM-based multi-agent systems. Project website and code will be available at https://aka.ms/DoVer.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.06749.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "65f40e43653c231cbaf7d1e4",
      "avatarUrl": "/avatars/a42ac5454cbe175f04c3420fce90cad2.svg",
      "fullname": "Jue Zhang",
      "name": "JueZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "5e6485f787403103f9f1055e",
      "name": "microsoft",
      "fullname": "Microsoft",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.07834",
      "authors": [
        {
          "_id": "69379f8019d912300c34a30d",
          "name": "Yi-Chuan Huang",
          "hidden": false
        },
        {
          "_id": "69379f8019d912300c34a30e",
          "name": "Jiewen Chan",
          "hidden": false
        },
        {
          "_id": "69379f8019d912300c34a30f",
          "name": "Hao-Jen Chien",
          "hidden": false
        },
        {
          "_id": "69379f8019d912300c34a310",
          "name": "Yu-Lun Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/xmWIqFounUj-kosdkGMIN.mp4"
      ],
      "publishedAt": "2025-12-08T18:59:58.000Z",
      "submittedOnDailyAt": "2025-12-09T01:33:21.818Z",
      "title": "Voxify3D: Pixel Art Meets Volumetric Rendering",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Voxel art is a distinctive stylization widely used in games and digital media, yet automated generation from 3D meshes remains challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve the pixel-precise, palette-constrained aesthetics of voxel art. We introduce Voxify3D, a differentiable two-stage framework bridging 3D mesh optimization with 2D pixel art supervision. Our core innovation lies in the synergistic integration of three components: (1) orthographic pixel art supervision that eliminates perspective distortion for precise voxel-pixel alignment; (2) patch-based CLIP alignment that preserves semantics across discretization levels; (3) palette-constrained Gumbel-Softmax quantization enabling differentiable optimization over discrete color spaces with controllable palette strategies. This integration addresses fundamental challenges: semantic preservation under extreme discretization, pixel-art aesthetics through volumetric rendering, and end-to-end discrete optimization. Experiments show superior performance (37.12 CLIP-IQA, 77.90\\% user preference) across diverse characters and controllable abstraction (2-8 colors, 20x-50x resolutions). Project page: https://yichuanh.github.io/Voxify-3D/",
      "upvotes": 14,
      "discussionId": "69379f8019d912300c34a311",
      "projectPage": "https://yichuanh.github.io/Voxify-3D/",
      "ai_summary": "Voxify3D is a two-stage framework that combines 3D mesh optimization with 2D pixel art supervision to generate high-quality voxel art with semantic preservation, pixel-art aesthetics, and discrete color coherence.",
      "ai_keywords": [
        "orthographic pixel art supervision",
        "patch-based CLIP alignment",
        "Gumbel-Softmax quantization",
        "volumetric rendering",
        "CLIP-IQA",
        "user preference"
      ]
    },
    "publishedAt": "2025-12-08T13:59:58.000Z",
    "title": "Voxify3D: Pixel Art Meets Volumetric Rendering",
    "summary": "Voxel art is a distinctive stylization widely used in games and digital media, yet automated generation from 3D meshes remains challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve the pixel-precise, palette-constrained aesthetics of voxel art. We introduce Voxify3D, a differentiable two-stage framework bridging 3D mesh optimization with 2D pixel art supervision. Our core innovation lies in the synergistic integration of three components: (1) orthographic pixel art supervision that eliminates perspective distortion for precise voxel-pixel alignment; (2) patch-based CLIP alignment that preserves semantics across discretization levels; (3) palette-constrained Gumbel-Softmax quantization enabling differentiable optimization over discrete color spaces with controllable palette strategies. This integration addresses fundamental challenges: semantic preservation under extreme discretization, pixel-art aesthetics through volumetric rendering, and end-to-end discrete optimization. Experiments show superior performance (37.12 CLIP-IQA, 77.90\\% user preference) across diverse characters and controllable abstraction (2-8 colors, 20x-50x resolutions). Project page: https://yichuanh.github.io/Voxify-3D/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/xmWIqFounUj-kosdkGMIN.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07834.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 180
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.06065",
      "authors": [
        {
          "_id": "69379c1c19d912300c34a2bc",
          "name": "Runjia Li",
          "hidden": false
        },
        {
          "_id": "69379c1c19d912300c34a2bd",
          "name": "Moayed Haji-Ali",
          "hidden": false
        },
        {
          "_id": "69379c1c19d912300c34a2be",
          "name": "Ashkan Mirzaei",
          "hidden": false
        },
        {
          "_id": "69379c1c19d912300c34a2bf",
          "name": "Chaoyang Wang",
          "hidden": false
        },
        {
          "_id": "69379c1c19d912300c34a2c0",
          "name": "Arpit Sahni",
          "hidden": false
        },
        {
          "_id": "69379c1c19d912300c34a2c1",
          "name": "Ivan Skorokhodov",
          "hidden": false
        },
        {
          "_id": "69379c1c19d912300c34a2c2",
          "name": "Aliaksandr Siarohin",
          "hidden": false
        },
        {
          "_id": "69379c1c19d912300c34a2c3",
          "name": "Tomas Jakab",
          "hidden": false
        },
        {
          "_id": "69379c1c19d912300c34a2c4",
          "name": "Junlin Han",
          "hidden": false
        },
        {
          "_id": "69379c1c19d912300c34a2c5",
          "name": "Sergey Tulyakov",
          "hidden": false
        },
        {
          "_id": "69379c1c19d912300c34a2c6",
          "name": "Philip Torr",
          "hidden": false
        },
        {
          "_id": "69379c1c19d912300c34a2c7",
          "name": "Willi Menapace",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/638e29cf319f9c746b87ad4b/wD9Ka0q9FOeTD72oLpCEw.mp4"
      ],
      "publishedAt": "2025-12-05T18:57:05.000Z",
      "submittedOnDailyAt": "2025-12-09T01:19:46.582Z",
      "title": "EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing",
      "submittedOnDailyBy": {
        "_id": "638e29cf319f9c746b87ad4b",
        "avatarUrl": "/avatars/70cac8d47847c389eb0393051a64c4a4.svg",
        "isPro": true,
        "fullname": "Runjia Li",
        "user": "liguang0115",
        "type": "user"
      },
      "summary": "We study instruction-guided editing of egocentric videos for interactive AR applications. While recent AI video editors perform well on third-person footage, egocentric views present unique challenges - including rapid egomotion and frequent hand-object interactions - that create a significant domain gap. Moreover, existing offline editing pipelines suffer from high latency, limiting real-time interaction. To address these issues, we present a complete ecosystem for egocentric video editing. First, we construct EgoEditData, a carefully designed and manually curated dataset specifically designed for egocentric editing scenarios, featuring rich hand-object interactions, while explicitly preserving hands. Second, we develop EgoEdit, an instruction-following egocentric video editor that supports real-time streaming inference on a single GPU. Finally, we introduce EgoEditBench, an evaluation suite targeting instruction faithfulness, hand and interaction preservation, and temporal stability under egomotion. Across both egocentric and general editing tasks, EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks-where existing methods struggle-while maintaining performance comparable to the strongest baselines on general editing tasks. EgoEditData and EgoEditBench will be made public for the research community. See our website at https://snap-research.github.io/EgoEdit",
      "upvotes": 13,
      "discussionId": "69379c1c19d912300c34a2c8",
      "projectPage": "https://snap-research.github.io/EgoEdit/",
      "githubRepo": "https://github.com/snap-research/EgoEdit",
      "ai_summary": "EgoEdit is a real-time, instruction-following egocentric video editor that addresses challenges in handling egomotion and hand-object interactions, outperforming existing methods on egocentric editing tasks.",
      "ai_keywords": [
        "EgoEditData",
        "EgoEdit",
        "EgoEditBench",
        "egocentric video editing",
        "instruction-following",
        "real-time streaming inference",
        "hand-object interactions",
        "egomotion",
        "instruction faithfulness",
        "hand preservation",
        "interaction preservation",
        "temporal stability"
      ],
      "githubStars": 2,
      "organization": {
        "_id": "63c87c41cd6a490608ce31d1",
        "name": "snap-research",
        "fullname": "Snap Research",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1674083325534-61f19829233c91cbd2f79e70.png"
      }
    },
    "publishedAt": "2025-12-05T13:57:05.000Z",
    "title": "EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing",
    "summary": "We study instruction-guided editing of egocentric videos for interactive AR applications. While recent AI video editors perform well on third-person footage, egocentric views present unique challenges - including rapid egomotion and frequent hand-object interactions - that create a significant domain gap. Moreover, existing offline editing pipelines suffer from high latency, limiting real-time interaction. To address these issues, we present a complete ecosystem for egocentric video editing. First, we construct EgoEditData, a carefully designed and manually curated dataset specifically designed for egocentric editing scenarios, featuring rich hand-object interactions, while explicitly preserving hands. Second, we develop EgoEdit, an instruction-following egocentric video editor that supports real-time streaming inference on a single GPU. Finally, we introduce EgoEditBench, an evaluation suite targeting instruction faithfulness, hand and interaction preservation, and temporal stability under egomotion. Across both egocentric and general editing tasks, EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks-where existing methods struggle-while maintaining performance comparable to the strongest baselines on general editing tasks. EgoEditData and EgoEditBench will be made public for the research community. See our website at https://snap-research.github.io/EgoEdit",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/638e29cf319f9c746b87ad4b/wD9Ka0q9FOeTD72oLpCEw.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.06065.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638e29cf319f9c746b87ad4b",
      "avatarUrl": "/avatars/70cac8d47847c389eb0393051a64c4a4.svg",
      "fullname": "Runjia Li",
      "name": "liguang0115",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "organization": {
      "_id": "63c87c41cd6a490608ce31d1",
      "name": "snap-research",
      "fullname": "Snap Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1674083325534-61f19829233c91cbd2f79e70.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.07778",
      "authors": [
        {
          "_id": "69379c6119d912300c34a2d0",
          "name": "Sen Ye",
          "hidden": false
        },
        {
          "_id": "69379c6119d912300c34a2d1",
          "name": "Jianning Pei",
          "hidden": false
        },
        {
          "_id": "69379c6119d912300c34a2d2",
          "name": "Mengde Xu",
          "hidden": false
        },
        {
          "_id": "69379c6119d912300c34a2d3",
          "name": "Shuyang Gu",
          "hidden": false
        },
        {
          "_id": "69379c6119d912300c34a2d4",
          "name": "Chunyu Wang",
          "hidden": false
        },
        {
          "_id": "69379c6119d912300c34a2d5",
          "name": "Liwei Wang",
          "hidden": false
        },
        {
          "_id": "69379c6119d912300c34a2d6",
          "name": "Han Hu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-08T17:59:47.000Z",
      "submittedOnDailyAt": "2025-12-09T04:42:46.811Z",
      "title": "Distribution Matching Variational AutoEncoder",
      "submittedOnDailyBy": {
        "_id": "64c38fcf573c5a427e12cd37",
        "avatarUrl": "/avatars/2b9de06f29147ed2c212e920afba0eaf.svg",
        "isPro": false,
        "fullname": "cientgu",
        "user": "cientgu",
        "type": "user"
      },
      "summary": "Most visual generative models compress images into a latent space before applying diffusion or autoregressive modelling. Yet, existing approaches such as VAEs and foundation model aligned encoders implicitly constrain the latent space without explicitly shaping its distribution, making it unclear which types of distributions are optimal for modeling. We introduce Distribution-Matching VAE (DMVAE), which explicitly aligns the encoder's latent distribution with an arbitrary reference distribution via a distribution matching constraint. This generalizes beyond the Gaussian prior of conventional VAEs, enabling alignment with distributions derived from self-supervised features, diffusion noise, or other prior distributions. With DMVAE, we can systematically investigate which latent distributions are more conducive to modeling, and we find that SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, reaching gFID equals 3.2 on ImageNet with only 64 training epochs. Our results suggest that choosing a suitable latent distribution structure (achieved via distribution-level alignment), rather than relying on fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis. Code is avaliable at https://github.com/sen-ye/dmvae.",
      "upvotes": 10,
      "discussionId": "69379c6119d912300c34a2d7",
      "ai_summary": "DMVAE explicitly aligns the encoder's latent distribution with a reference distribution, improving modeling efficiency and image synthesis fidelity compared to conventional VAEs.",
      "ai_keywords": [
        "VAEs",
        "foundation model aligned encoders",
        "latent space",
        "distribution matching constraint",
        "Distribution-Matching VAE (DMVAE)",
        "Gaussian prior",
        "SSL-derived distributions",
        "gFID",
        "ImageNet",
        "distribution-level alignment"
      ],
      "organization": {
        "_id": "66543b6e420092799d2f625c",
        "name": "tencent",
        "fullname": "Tencent",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
      }
    },
    "publishedAt": "2025-12-08T12:59:47.000Z",
    "title": "Distribution Matching Variational AutoEncoder",
    "summary": "Most visual generative models compress images into a latent space before applying diffusion or autoregressive modelling. Yet, existing approaches such as VAEs and foundation model aligned encoders implicitly constrain the latent space without explicitly shaping its distribution, making it unclear which types of distributions are optimal for modeling. We introduce Distribution-Matching VAE (DMVAE), which explicitly aligns the encoder's latent distribution with an arbitrary reference distribution via a distribution matching constraint. This generalizes beyond the Gaussian prior of conventional VAEs, enabling alignment with distributions derived from self-supervised features, diffusion noise, or other prior distributions. With DMVAE, we can systematically investigate which latent distributions are more conducive to modeling, and we find that SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, reaching gFID equals 3.2 on ImageNet with only 64 training epochs. Our results suggest that choosing a suitable latent distribution structure (achieved via distribution-level alignment), rather than relying on fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis. Code is avaliable at https://github.com/sen-ye/dmvae.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07778.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c38fcf573c5a427e12cd37",
      "avatarUrl": "/avatars/2b9de06f29147ed2c212e920afba0eaf.svg",
      "fullname": "cientgu",
      "name": "cientgu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "66543b6e420092799d2f625c",
      "name": "tencent",
      "fullname": "Tencent",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.07806",
      "authors": [
        {
          "_id": "6937ca7819d912300c34a468",
          "name": "Gyeongjin Kang",
          "hidden": false
        },
        {
          "_id": "6937ca7819d912300c34a469",
          "name": "Seungkwon Yang",
          "hidden": false
        },
        {
          "_id": "6937ca7819d912300c34a46a",
          "name": "Seungtae Nam",
          "hidden": false
        },
        {
          "_id": "6937ca7819d912300c34a46b",
          "name": "Younggeun Lee",
          "hidden": false
        },
        {
          "_id": "6937ca7819d912300c34a46c",
          "name": "Jungwoo Kim",
          "hidden": false
        },
        {
          "_id": "6937ca7819d912300c34a46d",
          "name": "Eunbyung Park",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-08T18:39:27.000Z",
      "submittedOnDailyAt": "2025-12-09T04:42:14.526Z",
      "title": "Multi-view Pyramid Transformer: Look Coarser to See Broader",
      "submittedOnDailyBy": {
        "_id": "64aa93e799639cc312795ea8",
        "avatarUrl": "/avatars/904453c39c09aa51d3f6aa9fafb4a47d.svg",
        "isPro": false,
        "fullname": "Gyeongjin Kang",
        "user": "Gynjn",
        "type": "user"
      },
      "summary": "We propose Multi-view Pyramid Transformer (MVP), a scalable multi-view transformer architecture that directly reconstructs large 3D scenes from tens to hundreds of images in a single forward pass. Drawing on the idea of ``looking broader to see the whole, looking finer to see the details,\" MVP is built on two core design principles: 1) a local-to-global inter-view hierarchy that gradually broadens the model's perspective from local views to groups and ultimately the full scene, and 2) a fine-to-coarse intra-view hierarchy that starts from detailed spatial representations and progressively aggregates them into compact, information-dense tokens. This dual hierarchy achieves both computational efficiency and representational richness, enabling fast reconstruction of large and complex scenes. We validate MVP on diverse datasets and show that, when coupled with 3D Gaussian Splatting as the underlying 3D representation, it achieves state-of-the-art generalizable reconstruction quality while maintaining high efficiency and scalability across a wide range of view configurations.",
      "upvotes": 9,
      "discussionId": "6937ca7819d912300c34a46e",
      "projectPage": "https://gynjn.github.io/MVP/",
      "githubRepo": "https://github.com/Gynjn/MVP",
      "ai_summary": "MVP, a scalable multi-view transformer architecture, efficiently reconstructs large 3D scenes from multiple images using dual hierarchies and achieves state-of-the-art quality.",
      "ai_keywords": [
        "Multi-view Pyramid Transformer",
        "MVP",
        "local-to-global inter-view hierarchy",
        "fine-to-coarse intra-view hierarchy",
        "3D Gaussian Splatting",
        "3D scenes",
        "generalizable reconstruction quality",
        "scalability",
        "view configurations"
      ]
    },
    "publishedAt": "2025-12-08T13:39:27.000Z",
    "title": "Multi-view Pyramid Transformer: Look Coarser to See Broader",
    "summary": "We propose Multi-view Pyramid Transformer (MVP), a scalable multi-view transformer architecture that directly reconstructs large 3D scenes from tens to hundreds of images in a single forward pass. Drawing on the idea of ``looking broader to see the whole, looking finer to see the details,\" MVP is built on two core design principles: 1) a local-to-global inter-view hierarchy that gradually broadens the model's perspective from local views to groups and ultimately the full scene, and 2) a fine-to-coarse intra-view hierarchy that starts from detailed spatial representations and progressively aggregates them into compact, information-dense tokens. This dual hierarchy achieves both computational efficiency and representational richness, enabling fast reconstruction of large and complex scenes. We validate MVP on diverse datasets and show that, when coupled with 3D Gaussian Splatting as the underlying 3D representation, it achieves state-of-the-art generalizable reconstruction quality while maintaining high efficiency and scalability across a wide range of view configurations.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07806.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64aa93e799639cc312795ea8",
      "avatarUrl": "/avatars/904453c39c09aa51d3f6aa9fafb4a47d.svg",
      "fullname": "Gyeongjin Kang",
      "name": "Gynjn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.07584",
      "authors": [
        {
          "_id": "69379ae419d912300c34a2ad",
          "name": "Meituan LongCat Team",
          "hidden": false
        },
        {
          "_id": "69379ae419d912300c34a2ae",
          "name": "Hanghang Ma",
          "hidden": false
        },
        {
          "_id": "69379ae419d912300c34a2af",
          "name": "Haoxian Tan",
          "hidden": false
        },
        {
          "_id": "69379ae419d912300c34a2b0",
          "name": "Jiale Huang",
          "hidden": false
        },
        {
          "_id": "69379ae419d912300c34a2b1",
          "name": "Junqiang Wu",
          "hidden": false
        },
        {
          "_id": "69379ae419d912300c34a2b2",
          "name": "Jun-Yan He",
          "hidden": false
        },
        {
          "_id": "69379ae419d912300c34a2b3",
          "name": "Lishuai Gao",
          "hidden": false
        },
        {
          "_id": "69379ae419d912300c34a2b4",
          "name": "Songlin Xiao",
          "hidden": false
        },
        {
          "_id": "69379ae419d912300c34a2b5",
          "name": "Xiaoming Wei",
          "hidden": false
        },
        {
          "_id": "69379ae419d912300c34a2b6",
          "name": "Xiaoqi Ma",
          "hidden": false
        },
        {
          "_id": "69379ae419d912300c34a2b7",
          "name": "Xunliang Cai",
          "hidden": false
        },
        {
          "_id": "69379ae419d912300c34a2b8",
          "name": "Yayong Guan",
          "hidden": false
        },
        {
          "_id": "69379ae419d912300c34a2b9",
          "name": "Jie Hu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-08T14:26:40.000Z",
      "submittedOnDailyAt": "2025-12-09T01:13:45.545Z",
      "title": "LongCat-Image Technical Report",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as a new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets a new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With a core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the openness of LongCat-Image will provide robust support for developers and researchers, pushing the frontiers of visual content creation.",
      "upvotes": 9,
      "discussionId": "69379ae519d912300c34a2ba",
      "projectPage": "https://longcat.chat/",
      "githubRepo": "https://github.com/meituan-longcat/LongCat-Image",
      "ai_summary": "LongCat-Image is a bilingual open-source foundation model for image generation that addresses multilingual text rendering, photorealism, and deployment efficiency through rigorous data curation, compact design, and comprehensive open-source support.",
      "ai_keywords": [
        "foundation model",
        "image generation",
        "multilingual text rendering",
        "photorealism",
        "deployment efficiency",
        "data curation",
        "reward models",
        "diffusion model",
        "parameters",
        "Mixture-of-Experts",
        "VRAM usage",
        "inference",
        "open-source ecosystem",
        "text-to-image",
        "image editing",
        "training procedure"
      ],
      "githubStars": 259,
      "organization": {
        "_id": "68b28d79a176a9beb30d2049",
        "name": "meituan-longcat",
        "fullname": "LongCat",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"
      }
    },
    "publishedAt": "2025-12-08T09:26:40.000Z",
    "title": "LongCat-Image Technical Report",
    "summary": "We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as a new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets a new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With a core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the openness of LongCat-Image will provide robust support for developers and researchers, pushing the frontiers of visual content creation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07584.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 180
    },
    "organization": {
      "_id": "68b28d79a176a9beb30d2049",
      "name": "meituan-longcat",
      "fullname": "LongCat",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.07833",
      "authors": [
        {
          "_id": "69379fe819d912300c34a313",
          "name": "Thao Nguyen",
          "hidden": false
        },
        {
          "_id": "69379fe819d912300c34a314",
          "name": "Sicheng Mo",
          "hidden": false
        },
        {
          "_id": "69379fe819d912300c34a315",
          "name": "Krishna Kumar Singh",
          "hidden": false
        },
        {
          "_id": "69379fe819d912300c34a316",
          "name": "Yilin Wang",
          "hidden": false
        },
        {
          "_id": "69379fe819d912300c34a317",
          "name": "Jing Shi",
          "hidden": false
        },
        {
          "_id": "69379fe819d912300c34a318",
          "name": "Nicholas Kolkin",
          "hidden": false
        },
        {
          "_id": "69379fe819d912300c34a319",
          "name": "Eli Shechtman",
          "hidden": false
        },
        {
          "_id": "69379fe819d912300c34a31a",
          "name": "Yong Jae Lee",
          "hidden": false
        },
        {
          "_id": "69379fe819d912300c34a31b",
          "name": "Yuheng Li",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Jqivw4Bd5tlTCrwgFwhkq.mp4"
      ],
      "publishedAt": "2025-12-08T18:59:56.000Z",
      "submittedOnDailyAt": "2025-12-09T01:35:18.706Z",
      "title": "Relational Visual Similarity",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.",
      "upvotes": 6,
      "discussionId": "69379fe819d912300c34a31c",
      "projectPage": "https://thaoshibe.github.io/relsim/",
      "githubRepo": "https://github.com/thaoshibe/relsim",
      "ai_summary": "Vision-Language models fine-tuned on anonymized image captions can capture relational similarity between images, a capability lacking in current visual similarity metrics.",
      "ai_keywords": [
        "relational similarity",
        "perceptual attribute similarity",
        "LPIPS",
        "CLIP",
        "DINO",
        "Vision-Language model",
        "image-caption dataset",
        "relational logic",
        "representation space"
      ],
      "githubStars": 4
    },
    "publishedAt": "2025-12-08T13:59:56.000Z",
    "title": "Relational Visual Similarity",
    "summary": "Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Jqivw4Bd5tlTCrwgFwhkq.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07833.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 180
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.07783",
      "authors": [
        {
          "_id": "6937ab4e19d912300c34a343",
          "name": "Charlie Zhang",
          "hidden": false
        },
        {
          "_id": "6937ab4e19d912300c34a344",
          "name": "Graham Neubig",
          "hidden": false
        },
        {
          "_id": "6937ab4e19d912300c34a345",
          "name": "Xiang Yue",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-08T18:12:10.000Z",
      "submittedOnDailyAt": "2025-12-09T02:25:01.518Z",
      "title": "On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models",
      "submittedOnDailyBy": {
        "_id": "6230d750d93e84e233882dbc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6230d750d93e84e233882dbc/4MGEekLW3oWzqeFWDWvIK.jpeg",
        "isPro": false,
        "fullname": "Xiang Yue",
        "user": "yuexiang96",
        "type": "user"
      },
      "summary": "Recent reinforcement learning (RL) techniques have yielded impressive reasoning improvements in language models, yet it remains unclear whether post-training truly extends a model's reasoning ability beyond what it acquires during pre-training. A central challenge is the lack of control in modern training pipelines: large-scale pre-training corpora are opaque, mid-training is often underexamined, and RL objectives interact with unknown prior knowledge in complex ways. To resolve this ambiguity, we develop a fully controlled experimental framework that isolates the causal contributions of pre-training, mid-training, and RL-based post-training. Our approach employs synthetic reasoning tasks with explicit atomic operations, parseable step-by-step reasoning traces, and systematic manipulation of training distributions. We evaluate models along two axes: extrapolative generalization to more complex compositions and contextual generalization across surface contexts. Using this framework, we reconcile competing views on RL's effectiveness. We show that: 1) RL produces true capability gains (pass@128) only when pre-training leaves sufficient headroom and when RL data target the model's edge of competence, tasks at the boundary that are difficult but not yet out of reach. 2) Contextual generalization requires minimal yet sufficient pre-training exposure, after which RL can reliably transfer. 3) Mid-training significantly enhances performance under fixed compute compared with RL only, demonstrating its central but underexplored role in training pipelines. 4) Process-level rewards reduce reward hacking and improve reasoning fidelity. Together, these results clarify the interplay between pre-training, mid-training, and RL, offering a foundation for understanding and improving reasoning LM training strategies.",
      "upvotes": 4,
      "discussionId": "6937ab4e19d912300c34a346",
      "githubRepo": "https://github.com/Interplay-LM-Reasoning/Interplay-LM-Reasoning",
      "ai_summary": "A controlled experimental framework isolates and evaluates the contributions of pre-training, mid-training, and reinforcement learning in improving language model reasoning, demonstrating the necessity of each phase and the role of process-level rewards.",
      "ai_keywords": [
        "reinforcement learning",
        "reasoning improvements",
        "language models",
        "pre-training",
        "mid-training",
        "RL-based post-training",
        "synthetic reasoning tasks",
        "parseable step-by-step reasoning traces",
        "extrapolative generalization",
        "contextual generalization",
        "reward hacking",
        "reasoning fidelity"
      ],
      "githubStars": 2,
      "organization": {
        "_id": "6474ab086d4dda6f7c6beaee",
        "name": "cmu-lti",
        "fullname": "CMU-LTI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63a4d079658851481f7e4394/HOFKzjMXriLBOGJw6S8uI.png"
      }
    },
    "publishedAt": "2025-12-08T13:12:10.000Z",
    "title": "On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models",
    "summary": "Recent reinforcement learning (RL) techniques have yielded impressive reasoning improvements in language models, yet it remains unclear whether post-training truly extends a model's reasoning ability beyond what it acquires during pre-training. A central challenge is the lack of control in modern training pipelines: large-scale pre-training corpora are opaque, mid-training is often underexamined, and RL objectives interact with unknown prior knowledge in complex ways. To resolve this ambiguity, we develop a fully controlled experimental framework that isolates the causal contributions of pre-training, mid-training, and RL-based post-training. Our approach employs synthetic reasoning tasks with explicit atomic operations, parseable step-by-step reasoning traces, and systematic manipulation of training distributions. We evaluate models along two axes: extrapolative generalization to more complex compositions and contextual generalization across surface contexts. Using this framework, we reconcile competing views on RL's effectiveness. We show that: 1) RL produces true capability gains (pass@128) only when pre-training leaves sufficient headroom and when RL data target the model's edge of competence, tasks at the boundary that are difficult but not yet out of reach. 2) Contextual generalization requires minimal yet sufficient pre-training exposure, after which RL can reliably transfer. 3) Mid-training significantly enhances performance under fixed compute compared with RL only, demonstrating its central but underexplored role in training pipelines. 4) Process-level rewards reduce reward hacking and improve reasoning fidelity. Together, these results clarify the interplay between pre-training, mid-training, and RL, offering a foundation for understanding and improving reasoning LM training strategies.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07783.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6230d750d93e84e233882dbc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6230d750d93e84e233882dbc/4MGEekLW3oWzqeFWDWvIK.jpeg",
      "fullname": "Xiang Yue",
      "name": "yuexiang96",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 40
    },
    "organization": {
      "_id": "6474ab086d4dda6f7c6beaee",
      "name": "cmu-lti",
      "fullname": "CMU-LTI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63a4d079658851481f7e4394/HOFKzjMXriLBOGJw6S8uI.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.07831",
      "authors": [
        {
          "_id": "6937cb9719d912300c34a470",
          "name": "Jiehui Huang",
          "hidden": false
        },
        {
          "_id": "6937cb9719d912300c34a471",
          "name": "Yuechen Zhang",
          "hidden": false
        },
        {
          "_id": "6937cb9719d912300c34a472",
          "name": "Xu He",
          "hidden": false
        },
        {
          "_id": "6937cb9719d912300c34a473",
          "name": "Yuan Gao",
          "hidden": false
        },
        {
          "_id": "6937cb9719d912300c34a474",
          "name": "Zhi Cen",
          "hidden": false
        },
        {
          "_id": "6937cb9719d912300c34a475",
          "name": "Bin Xia",
          "hidden": false
        },
        {
          "_id": "6937cb9719d912300c34a476",
          "name": "Yan Zhou",
          "hidden": false
        },
        {
          "_id": "6937cb9719d912300c34a477",
          "name": "Xin Tao",
          "hidden": false
        },
        {
          "_id": "6937cb9719d912300c34a478",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "6937cb9719d912300c34a479",
          "name": "Jiaya Jia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-08T18:59:01.000Z",
      "submittedOnDailyAt": "2025-12-09T04:43:12.738Z",
      "title": "UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation",
      "submittedOnDailyBy": {
        "_id": "64d1efef4a204a4d125fd4fc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d1efef4a204a4d125fd4fc/mOjMzPUkAR-SmzYwofazX.jpeg",
        "isPro": false,
        "fullname": "JackAILab",
        "user": "JackAILab",
        "type": "user"
      },
      "summary": "Recent video generation models demonstrate impressive synthesis capabilities but remain limited by single-modality conditioning, constraining their holistic world understanding. This stems from insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation. To address these limitations, we introduce UnityVideo, a unified framework for world-aware video generation that jointly learns across multiple modalities (segmentation masks, human skeletons, DensePose, optical flow, and depth maps) and training paradigms. Our approach features two core components: (1) dynamic noising to unify heterogeneous training paradigms, and (2) a modality switcher with an in-context learner that enables unified processing via modular parameters and contextual learning. We contribute a large-scale unified dataset with 1.3M samples. Through joint optimization, UnityVideo accelerates convergence and significantly enhances zero-shot generalization to unseen data. We demonstrate that UnityVideo achieves superior video quality, consistency, and improved alignment with physical world constraints. Code and data can be found at: https://github.com/dvlab-research/UnityVideo",
      "upvotes": 3,
      "discussionId": "6937cb9719d912300c34a47a",
      "projectPage": "https://jackailab.github.io/Projects/UnityVideo/",
      "githubRepo": "https://github.com/dvlab-research/UnityVideo",
      "ai_summary": "UnityVideo, a unified framework, enhances video generation by integrating multiple modalities and paradigms, leading to improved quality and alignment with real-world constraints.",
      "ai_keywords": [
        "UnityVideo",
        "dynamic noising",
        "modality switcher",
        "in-context learner",
        "segmentation masks",
        "human skeletons",
        "DensePose",
        "optical flow",
        "depth maps",
        "joint optimization",
        "zero-shot generalization"
      ],
      "organization": {
        "_id": "662c559b322afcbae51b3c8b",
        "name": "KlingTeam",
        "fullname": "Kling Team",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"
      }
    },
    "publishedAt": "2025-12-08T13:59:01.000Z",
    "title": "UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation",
    "summary": "Recent video generation models demonstrate impressive synthesis capabilities but remain limited by single-modality conditioning, constraining their holistic world understanding. This stems from insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation. To address these limitations, we introduce UnityVideo, a unified framework for world-aware video generation that jointly learns across multiple modalities (segmentation masks, human skeletons, DensePose, optical flow, and depth maps) and training paradigms. Our approach features two core components: (1) dynamic noising to unify heterogeneous training paradigms, and (2) a modality switcher with an in-context learner that enables unified processing via modular parameters and contextual learning. We contribute a large-scale unified dataset with 1.3M samples. Through joint optimization, UnityVideo accelerates convergence and significantly enhances zero-shot generalization to unseen data. We demonstrate that UnityVideo achieves superior video quality, consistency, and improved alignment with physical world constraints. Code and data can be found at: https://github.com/dvlab-research/UnityVideo",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07831.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d1efef4a204a4d125fd4fc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d1efef4a204a4d125fd4fc/mOjMzPUkAR-SmzYwofazX.jpeg",
      "fullname": "JackAILab",
      "name": "JackAILab",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "organization": {
      "_id": "662c559b322afcbae51b3c8b",
      "name": "KlingTeam",
      "fullname": "Kling Team",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.07805",
      "authors": [
        {
          "_id": "6937a83519d912300c34a326",
          "name": "Yifan Zhang",
          "hidden": false
        },
        {
          "_id": "6937a83519d912300c34a327",
          "name": "Zixiang Chen",
          "hidden": false
        },
        {
          "_id": "6937a83519d912300c34a328",
          "name": "Yifeng Liu",
          "hidden": false
        },
        {
          "_id": "6937a83519d912300c34a329",
          "name": "Zhen Qin",
          "hidden": false
        },
        {
          "_id": "6937a83519d912300c34a32a",
          "name": "Huizhuo Yuan",
          "hidden": false
        },
        {
          "_id": "6937a83519d912300c34a32b",
          "name": "Kangping Xu",
          "hidden": false
        },
        {
          "_id": "6937a83519d912300c34a32c",
          "name": "Yang Yuan",
          "hidden": false
        },
        {
          "_id": "6937a83519d912300c34a32d",
          "name": "Quanquan Gu",
          "hidden": false
        },
        {
          "_id": "6937a83519d912300c34a32e",
          "name": "Andrew Chi-Chih Yao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-08T18:39:13.000Z",
      "submittedOnDailyAt": "2025-12-09T02:11:10.987Z",
      "title": "Group Representational Position Encoding",
      "submittedOnDailyBy": {
        "_id": "647bf082aba7062fe5c51ca9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
        "isPro": false,
        "fullname": "Yifan Zhang",
        "user": "yifAI",
        "type": "user"
      },
      "summary": "We present GRAPE (Group RepresentAtional Position Encoding), a unified framework for positional encoding based on group actions. GRAPE brings together two families of mechanisms: (i) multiplicative rotations (Multiplicative GRAPE) in SO(d) and (ii) additive logit biases (Additive GRAPE) arising from unipotent actions in the general linear group GL. In Multiplicative GRAPE, a position n in Z (or t in R) acts as G(n)=exp(n,,L) with a rank-2 skew generator L in R^{d times d}, yielding a relative, compositional, norm-preserving map with a closed-form matrix exponential. RoPE is recovered exactly when the d/2 planes are the canonical coordinate pairs with log-uniform spectrum. Learned commuting subspaces and compact non-commuting mixtures strictly extend this geometry to capture cross-subspace feature coupling at O(d) and O(r d) cost per head, respectively. In Additive GRAPE, additive logits arise as rank-1 (or low-rank) unipotent actions, recovering ALiBi and the Forgetting Transformer (FoX) as exact special cases while preserving an exact relative law and streaming cacheability. Altogether, GRAPE supplies a principled design space for positional geometry in long-context models, subsuming RoPE and ALiBi as special cases. Project Page: https://github.com/model-architectures/GRAPE.",
      "upvotes": 3,
      "discussionId": "6937a83519d912300c34a32f",
      "ai_summary": "GRAPE is a unified positional encoding framework that combines multiplicative rotations and additive logit biases, extending existing methods like RoPE and ALiBi.",
      "ai_keywords": [
        "multiplicative rotations",
        "additive logit biases",
        "group actions",
        "SO(d)",
        "GL",
        "rank-2 skew generator",
        "matrix exponential",
        "unipotent actions",
        "learned commuting subspaces",
        "compact non-commuting mixtures",
        "relative law",
        "streaming cacheability",
        "GRAPE",
        "RoPE",
        "ALiBi",
        "Forgetting Transformer (FoX)"
      ],
      "organization": {
        "_id": "6779cfba64085a93fd039c0c",
        "name": "model-architectures",
        "fullname": "Model Architectures"
      }
    },
    "publishedAt": "2025-12-08T13:39:13.000Z",
    "title": "Group Representational Position Encoding",
    "summary": "We present GRAPE (Group RepresentAtional Position Encoding), a unified framework for positional encoding based on group actions. GRAPE brings together two families of mechanisms: (i) multiplicative rotations (Multiplicative GRAPE) in SO(d) and (ii) additive logit biases (Additive GRAPE) arising from unipotent actions in the general linear group GL. In Multiplicative GRAPE, a position n in Z (or t in R) acts as G(n)=exp(n,,L) with a rank-2 skew generator L in R^{d times d}, yielding a relative, compositional, norm-preserving map with a closed-form matrix exponential. RoPE is recovered exactly when the d/2 planes are the canonical coordinate pairs with log-uniform spectrum. Learned commuting subspaces and compact non-commuting mixtures strictly extend this geometry to capture cross-subspace feature coupling at O(d) and O(r d) cost per head, respectively. In Additive GRAPE, additive logits arise as rank-1 (or low-rank) unipotent actions, recovering ALiBi and the Forgetting Transformer (FoX) as exact special cases while preserving an exact relative law and streaming cacheability. Altogether, GRAPE supplies a principled design space for positional geometry in long-context models, subsuming RoPE and ALiBi as special cases. Project Page: https://github.com/model-architectures/GRAPE.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07805.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "647bf082aba7062fe5c51ca9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
      "fullname": "Yifan Zhang",
      "name": "yifAI",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 17
    },
    "organization": {
      "_id": "6779cfba64085a93fd039c0c",
      "name": "model-architectures",
      "fullname": "Model Architectures"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.06905",
      "authors": [
        {
          "_id": "69379d8b19d912300c34a2de",
          "name": "Zijian Zhou",
          "hidden": false
        },
        {
          "_id": "69379d8b19d912300c34a2df",
          "name": "Shikun Liu",
          "hidden": false
        },
        {
          "_id": "69379d8b19d912300c34a2e0",
          "name": "Haozhe Liu",
          "hidden": false
        },
        {
          "_id": "69379d8b19d912300c34a2e1",
          "name": "Haonan Qiu",
          "hidden": false
        },
        {
          "_id": "69379d8b19d912300c34a2e2",
          "name": "Zhaochong An",
          "hidden": false
        },
        {
          "_id": "69379d8b19d912300c34a2e3",
          "name": "Weiming Ren",
          "hidden": false
        },
        {
          "_id": "69379d8b19d912300c34a2e4",
          "name": "Zhiheng Liu",
          "hidden": false
        },
        {
          "_id": "69379d8b19d912300c34a2e5",
          "name": "Xiaoke Huang",
          "hidden": false
        },
        {
          "_id": "69379d8b19d912300c34a2e6",
          "name": "Kam Woh Ng",
          "hidden": false
        },
        {
          "_id": "69379d8b19d912300c34a2e7",
          "name": "Tian Xie",
          "hidden": false
        },
        {
          "_id": "69379d8b19d912300c34a2e8",
          "name": "Xiao Han",
          "hidden": false
        },
        {
          "_id": "69379d8b19d912300c34a2e9",
          "name": "Yuren Cong",
          "hidden": false
        },
        {
          "_id": "69379d8b19d912300c34a2ea",
          "name": "Hang Li",
          "hidden": false
        },
        {
          "_id": "69379d8b19d912300c34a2eb",
          "name": "Chuyan Zhu",
          "hidden": false
        },
        {
          "_id": "69379d8b19d912300c34a2ec",
          "name": "Aditya Patel",
          "hidden": false
        },
        {
          "_id": "69379d8b19d912300c34a2ed",
          "name": "Tao Xiang",
          "hidden": false
        },
        {
          "_id": "69379d8b19d912300c34a2ee",
          "name": "Sen He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-07T16:10:25.000Z",
      "submittedOnDailyAt": "2025-12-09T01:25:03.574Z",
      "title": "Scaling Zero-Shot Reference-to-Video Generation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Reference-to-video (R2V) generation aims to synthesize videos that align with a text prompt while preserving the subject identity from reference images. However, current R2V methods are hindered by the reliance on explicit reference image-video-text triplets, whose construction is highly expensive and difficult to scale. We bypass this bottleneck by introducing Saber, a scalable zero-shot framework that requires no explicit R2V data. Trained exclusively on video-text pairs, Saber employs a masked training strategy and a tailored attention-based model design to learn identity-consistent and reference-aware representations. Mask augmentation techniques are further integrated to mitigate copy-paste artifacts common in reference-to-video generation. Moreover, Saber demonstrates remarkable generalization capabilities across a varying number of references and achieves superior performance on the OpenS2V-Eval benchmark compared to methods trained with R2V data.",
      "upvotes": 2,
      "discussionId": "69379d8b19d912300c34a2ef",
      "projectPage": "https://franciszzj.github.io/Saber/",
      "ai_summary": "Saber is a scalable zero-shot framework for reference-to-video generation that uses video-text pairs to learn identity-consistent representations and outperforms models trained with explicit reference data.",
      "ai_keywords": [
        "reference-to-video (R2V) generation",
        "masked training strategy",
        "attention-based model design",
        "mask augmentation",
        "OpenS2V-Eval benchmark"
      ],
      "organization": {
        "_id": "5e63d8713071d5be688861b8",
        "name": "facebook",
        "fullname": "AI at Meta",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
      }
    },
    "publishedAt": "2025-12-07T11:10:25.000Z",
    "title": "Scaling Zero-Shot Reference-to-Video Generation",
    "summary": "Reference-to-video (R2V) generation aims to synthesize videos that align with a text prompt while preserving the subject identity from reference images. However, current R2V methods are hindered by the reliance on explicit reference image-video-text triplets, whose construction is highly expensive and difficult to scale. We bypass this bottleneck by introducing Saber, a scalable zero-shot framework that requires no explicit R2V data. Trained exclusively on video-text pairs, Saber employs a masked training strategy and a tailored attention-based model design to learn identity-consistent and reference-aware representations. Mask augmentation techniques are further integrated to mitigate copy-paste artifacts common in reference-to-video generation. Moreover, Saber demonstrates remarkable generalization capabilities across a varying number of references and achieves superior performance on the OpenS2V-Eval benchmark compared to methods trained with R2V data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.06905.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 180
    },
    "organization": {
      "_id": "5e63d8713071d5be688861b8",
      "name": "facebook",
      "fullname": "AI at Meta",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.06421",
      "authors": [
        {
          "_id": "6937987e19d912300c34a2a6",
          "name": "Gengze Zhou",
          "hidden": false
        },
        {
          "_id": "6937987e19d912300c34a2a7",
          "name": "Chongjian Ge",
          "hidden": false
        },
        {
          "_id": "6937987e19d912300c34a2a8",
          "name": "Hao Tan",
          "hidden": false
        },
        {
          "_id": "6937987e19d912300c34a2a9",
          "name": "Feng Liu",
          "hidden": false
        },
        {
          "_id": "6937987e19d912300c34a2aa",
          "name": "Yicong Hong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-06T12:41:42.000Z",
      "submittedOnDailyAt": "2025-12-09T01:04:22.710Z",
      "title": "Rethinking Training Dynamics in Scale-wise Autoregressive Generation",
      "submittedOnDailyBy": {
        "_id": "6450bcd3673b2bcfaf8681af",
        "avatarUrl": "/avatars/f5f93d780562d0772ec5dc1728945fcf.svg",
        "isPro": false,
        "fullname": "Gengze Zhou",
        "user": "ZGZzz",
        "type": "user"
      },
      "summary": "Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.",
      "upvotes": 2,
      "discussionId": "6937987f19d912300c34a2ab",
      "projectPage": "https://gengzezhou.github.io/SAR/",
      "ai_summary": "Self-Autoregressive Refinement (SAR) improves the quality of autoregressive generative models by addressing exposure bias through Stagger-Scale Rollout and Contrastive Student-Forcing Loss, leading to consistent improvements with minimal computational overhead.",
      "ai_keywords": [
        "autoregressive generative models",
        "next-scale prediction",
        "exposure bias",
        "train-test mismatch",
        "scale-wise learning difficulty",
        "training dynamics",
        "Self-Autoregressive Refinement",
        "Stagger-Scale Rollout",
        "Contrastive Student-Forcing Loss",
        "FlexVAR-d16",
        "ImageNet 256",
        "FID"
      ],
      "organization": {
        "_id": "637b318856db0404b7c5a0c2",
        "name": "adobe-research",
        "fullname": "Adobe Research",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1669033410364-624bebf604abc7ebb01789af.png"
      }
    },
    "publishedAt": "2025-12-06T07:41:42.000Z",
    "title": "Rethinking Training Dynamics in Scale-wise Autoregressive Generation",
    "summary": "Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.06421.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6450bcd3673b2bcfaf8681af",
      "avatarUrl": "/avatars/f5f93d780562d0772ec5dc1728945fcf.svg",
      "fullname": "Gengze Zhou",
      "name": "ZGZzz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "organization": {
      "_id": "637b318856db0404b7c5a0c2",
      "name": "adobe-research",
      "fullname": "Adobe Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1669033410364-624bebf604abc7ebb01789af.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.06609",
      "authors": [
        {
          "_id": "6937a8ab19d912300c34a331",
          "name": "Tongda Xu",
          "hidden": false
        },
        {
          "_id": "6937a8ab19d912300c34a332",
          "name": "Wendi Zheng",
          "hidden": false
        },
        {
          "_id": "6937a8ab19d912300c34a333",
          "name": "Jiajun He",
          "hidden": false
        },
        {
          "_id": "6937a8ab19d912300c34a334",
          "name": "Jose Miguel Hernandez-Lobato",
          "hidden": false
        },
        {
          "_id": "6937a8ab19d912300c34a335",
          "name": "Yan Wang",
          "hidden": false
        },
        {
          "_id": "6937a8ab19d912300c34a336",
          "name": "Ya-Qin Zhang",
          "hidden": false
        },
        {
          "_id": "6937a8ab19d912300c34a337",
          "name": "Jie Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-07T00:57:58.000Z",
      "submittedOnDailyAt": "2025-12-09T02:15:57.408Z",
      "title": "Vector Quantization using Gaussian Variational Autoencoder",
      "submittedOnDailyBy": {
        "_id": "64e805afd884b768fc610880",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e805afd884b768fc610880/VMfKuaXF6JLp2I2DDv3zH.png",
        "isPro": false,
        "fullname": "tongda xu",
        "user": "xutongda",
        "type": "user"
      },
      "summary": "Vector quantized variational autoencoder (VQ-VAE) is a discrete auto-encoder that compresses images into discrete tokens. It is difficult to train due to discretization. In this paper, we propose a simple yet effective technique, dubbed Gaussian Quant (GQ), that converts a Gaussian VAE with certain constraint into a VQ-VAE without training. GQ generates random Gaussian noise as a codebook and finds the closest noise to the posterior mean. Theoretically, we prove that when the logarithm of the codebook size exceeds the bits-back coding rate of the Gaussian VAE, a small quantization error is guaranteed. Practically, we propose a heuristic to train Gaussian VAE for effective GQ, named target divergence constraint (TDC). Empirically, we show that GQ outperforms previous VQ-VAEs, such as VQGAN, FSQ, LFQ, and BSQ, on both UNet and ViT architectures. Furthermore, TDC also improves upon previous Gaussian VAE discretization methods, such as TokenBridge. The source code is provided in https://github.com/tongdaxu/VQ-VAE-from-Gaussian-VAE.",
      "upvotes": 1,
      "discussionId": "6937a8ab19d912300c34a338",
      "projectPage": "https://tongdaxu.github.io/pages/gq.html",
      "githubRepo": "https://github.com/tongdaxu/VQ-VAE-from-Gaussian-VAE",
      "ai_summary": "Gaussian Quant (GQ) converts Gaussian VAE to VQ-VAE without training, outperforming previous VQ-VAEs and Gaussian VAE discretization methods across different architectures.",
      "ai_keywords": [
        "Vector quantized variational autoencoder",
        "VQ-VAE",
        "Gaussian VAE",
        "Gaussian Quant",
        "GQ",
        "codebook",
        "posterior mean",
        "bits-back coding rate",
        "target divergence constraint",
        "TDC",
        "VQGAN",
        "FSQ",
        "LFQ",
        "BSQ",
        "TokenBridge",
        "UNet",
        "ViT"
      ],
      "githubStars": 9
    },
    "publishedAt": "2025-12-06T19:57:58.000Z",
    "title": "Vector Quantization using Gaussian Variational Autoencoder",
    "summary": "Vector quantized variational autoencoder (VQ-VAE) is a discrete auto-encoder that compresses images into discrete tokens. It is difficult to train due to discretization. In this paper, we propose a simple yet effective technique, dubbed Gaussian Quant (GQ), that converts a Gaussian VAE with certain constraint into a VQ-VAE without training. GQ generates random Gaussian noise as a codebook and finds the closest noise to the posterior mean. Theoretically, we prove that when the logarithm of the codebook size exceeds the bits-back coding rate of the Gaussian VAE, a small quantization error is guaranteed. Practically, we propose a heuristic to train Gaussian VAE for effective GQ, named target divergence constraint (TDC). Empirically, we show that GQ outperforms previous VQ-VAEs, such as VQGAN, FSQ, LFQ, and BSQ, on both UNet and ViT architectures. Furthermore, TDC also improves upon previous Gaussian VAE discretization methods, such as TokenBridge. The source code is provided in https://github.com/tongdaxu/VQ-VAE-from-Gaussian-VAE.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.06609.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e805afd884b768fc610880",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e805afd884b768fc610880/VMfKuaXF6JLp2I2DDv3zH.png",
      "fullname": "tongda xu",
      "name": "xutongda",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.06963",
      "authors": [
        {
          "_id": "69379eec19d912300c34a303",
          "name": "Yichao Shen",
          "hidden": false
        },
        {
          "_id": "69379eec19d912300c34a304",
          "name": "Fangyun Wei",
          "hidden": false
        },
        {
          "_id": "69379eec19d912300c34a305",
          "name": "Zhiying Du",
          "hidden": false
        },
        {
          "_id": "69379eec19d912300c34a306",
          "name": "Yaobo Liang",
          "hidden": false
        },
        {
          "_id": "69379eec19d912300c34a307",
          "name": "Yan Lu",
          "hidden": false
        },
        {
          "_id": "69379eec19d912300c34a308",
          "name": "Jiaolong Yang",
          "hidden": false
        },
        {
          "_id": "69379eec19d912300c34a309",
          "name": "Nanning Zheng",
          "hidden": false
        },
        {
          "_id": "69379eec19d912300c34a30a",
          "name": "Baining Guo",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/fhX9UW-Xy5xRJq0gpihLx.png",
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/RXOLUo-OquUUN4pUL3Vxi.qt"
      ],
      "publishedAt": "2025-12-07T18:57:15.000Z",
      "submittedOnDailyAt": "2025-12-09T01:32:28.300Z",
      "title": "VideoVLA: Video Generators Can Be Generalizable Robot Manipulators",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Generalization in robot manipulation is essential for deploying robots in open-world environments and advancing toward artificial general intelligence. While recent Vision-Language-Action (VLA) models leverage large pre-trained understanding models for perception and instruction following, their ability to generalize to novel tasks, objects, and settings remains limited. In this work, we present VideoVLA, a simple approach that explores the potential of transforming large video generation models into robotic VLA manipulators. Given a language instruction and an image, VideoVLA predicts an action sequence as well as the future visual outcomes. Built on a multi-modal Diffusion Transformer, VideoVLA jointly models video, language, and action modalities, using pre-trained video generative models for joint visual and action forecasting. Our experiments show that high-quality imagined futures correlate with reliable action predictions and task success, highlighting the importance of visual imagination in manipulation. VideoVLA demonstrates strong generalization, including imitating other embodiments' skills and handling novel objects. This dual-prediction strategy - forecasting both actions and their visual consequences - explores a paradigm shift in robot learning and unlocks generalization capabilities in manipulation systems.",
      "upvotes": 0,
      "discussionId": "69379eec19d912300c34a30b",
      "projectPage": "https://videovla-nips2025.github.io/",
      "ai_summary": "VideoVLA uses a multi-modal Diffusion Transformer to predict actions and visual outcomes from language and image inputs, enabling strong generalization in robotic manipulation tasks.",
      "ai_keywords": [
        "VideoVLA",
        "multi-modal Diffusion Transformer",
        "visual imagination",
        "manipulation",
        "action sequence",
        "future visual outcomes",
        "generalization",
        "imitation",
        "novel objects"
      ]
    },
    "publishedAt": "2025-12-07T13:57:15.000Z",
    "title": "VideoVLA: Video Generators Can Be Generalizable Robot Manipulators",
    "summary": "Generalization in robot manipulation is essential for deploying robots in open-world environments and advancing toward artificial general intelligence. While recent Vision-Language-Action (VLA) models leverage large pre-trained understanding models for perception and instruction following, their ability to generalize to novel tasks, objects, and settings remains limited. In this work, we present VideoVLA, a simple approach that explores the potential of transforming large video generation models into robotic VLA manipulators. Given a language instruction and an image, VideoVLA predicts an action sequence as well as the future visual outcomes. Built on a multi-modal Diffusion Transformer, VideoVLA jointly models video, language, and action modalities, using pre-trained video generative models for joint visual and action forecasting. Our experiments show that high-quality imagined futures correlate with reliable action predictions and task success, highlighting the importance of visual imagination in manipulation. VideoVLA demonstrates strong generalization, including imitating other embodiments' skills and handling novel objects. This dual-prediction strategy - forecasting both actions and their visual consequences - explores a paradigm shift in robot learning and unlocks generalization capabilities in manipulation systems.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/fhX9UW-Xy5xRJq0gpihLx.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/RXOLUo-OquUUN4pUL3Vxi.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.06963.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 180
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2512.06558",
      "authors": [
        {
          "_id": "6937981c19d912300c34a29c",
          "name": "Md Mofijul Islam",
          "hidden": false
        },
        {
          "_id": "6937981c19d912300c34a29d",
          "name": "Alexi Gladstone",
          "hidden": false
        },
        {
          "_id": "6937981c19d912300c34a29e",
          "name": "Sujan Sarker",
          "hidden": false
        },
        {
          "_id": "6937981c19d912300c34a29f",
          "name": "Ganesh Nanduru",
          "hidden": false
        },
        {
          "_id": "6937981c19d912300c34a2a0",
          "name": "Md Fahim",
          "hidden": false
        },
        {
          "_id": "6937981c19d912300c34a2a1",
          "name": "Keyan Du",
          "hidden": false
        },
        {
          "_id": "6937981c19d912300c34a2a2",
          "name": "Aman Chadha",
          "hidden": false
        },
        {
          "_id": "6937981c19d912300c34a2a3",
          "name": "Tariq Iqbal",
          "hidden": false
        }
      ],
      "publishedAt": "2025-12-06T20:10:06.000Z",
      "submittedOnDailyAt": "2025-12-09T01:03:58.907Z",
      "title": "Embodied Referring Expression Comprehension in Human-Robot Interaction",
      "submittedOnDailyBy": {
        "_id": "63a4754927f1f64ed7238dac",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
        "isPro": false,
        "fullname": "Aman Chadha",
        "user": "amanchadha",
        "type": "user"
      },
      "summary": "As robots enter human workspaces, there is a crucial need for them to comprehend embodied human instructions, enabling intuitive and fluent human-robot interaction (HRI). However, accurate comprehension is challenging due to a lack of large-scale datasets that capture natural embodied interactions in diverse HRI settings. Existing datasets suffer from perspective bias, single-view collection, inadequate coverage of nonverbal gestures, and a predominant focus on indoor environments. To address these issues, we present the Refer360 dataset, a large-scale dataset of embodied verbal and nonverbal interactions collected across diverse viewpoints in both indoor and outdoor settings. Additionally, we introduce MuRes, a multimodal guided residual module designed to improve embodied referring expression comprehension. MuRes acts as an information bottleneck, extracting salient modality-specific signals and reinforcing them into pre-trained representations to form complementary features for downstream tasks. We conduct extensive experiments on four HRI datasets, including the Refer360 dataset, and demonstrate that current multimodal models fail to capture embodied interactions comprehensively; however, augmenting them with MuRes consistently improves performance. These findings establish Refer360 as a valuable benchmark and exhibit the potential of guided residual learning to advance embodied referring expression comprehension in robots operating within human environments.",
      "upvotes": 0,
      "discussionId": "6937981c19d912300c34a2a4",
      "ai_summary": "A large-scale dataset and multimodal model improve embodied interaction comprehension in robots by addressing perspective bias and enhancing multimodal signal integration.",
      "ai_keywords": [
        "Refer360",
        "MuRes",
        "multimodal guided residual module",
        "embodied referring expression comprehension",
        "information bottleneck",
        "salient modality-specific signals",
        "pre-trained representations",
        "HRI datasets"
      ]
    },
    "publishedAt": "2025-12-06T15:10:06.000Z",
    "title": "Embodied Referring Expression Comprehension in Human-Robot Interaction",
    "summary": "As robots enter human workspaces, there is a crucial need for them to comprehend embodied human instructions, enabling intuitive and fluent human-robot interaction (HRI). However, accurate comprehension is challenging due to a lack of large-scale datasets that capture natural embodied interactions in diverse HRI settings. Existing datasets suffer from perspective bias, single-view collection, inadequate coverage of nonverbal gestures, and a predominant focus on indoor environments. To address these issues, we present the Refer360 dataset, a large-scale dataset of embodied verbal and nonverbal interactions collected across diverse viewpoints in both indoor and outdoor settings. Additionally, we introduce MuRes, a multimodal guided residual module designed to improve embodied referring expression comprehension. MuRes acts as an information bottleneck, extracting salient modality-specific signals and reinforcing them into pre-trained representations to form complementary features for downstream tasks. We conduct extensive experiments on four HRI datasets, including the Refer360 dataset, and demonstrate that current multimodal models fail to capture embodied interactions comprehensively; however, augmenting them with MuRes consistently improves performance. These findings establish Refer360 as a valuable benchmark and exhibit the potential of guided residual learning to advance embodied referring expression comprehension in robots operating within human environments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.06558.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4754927f1f64ed7238dac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
      "fullname": "Aman Chadha",
      "name": "amanchadha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  }
]