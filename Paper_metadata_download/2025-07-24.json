[
  {
    "paper": {
      "id": "2507.17744",
      "authors": [
        {
          "_id": "6881b7d1df7c5aafaf37f0d5",
          "name": "Xiaofeng Mao",
          "hidden": false
        },
        {
          "_id": "6881b7d1df7c5aafaf37f0d6",
          "name": "Shaoheng Lin",
          "hidden": false
        },
        {
          "_id": "6881b7d1df7c5aafaf37f0d7",
          "name": "Zhen Li",
          "hidden": false
        },
        {
          "_id": "6881b7d1df7c5aafaf37f0d8",
          "name": "Chuanhao Li",
          "hidden": false
        },
        {
          "_id": "6881b7d1df7c5aafaf37f0d9",
          "name": "Wenshuo Peng",
          "hidden": false
        },
        {
          "_id": "6881b7d1df7c5aafaf37f0da",
          "name": "Tong He",
          "hidden": false
        },
        {
          "_id": "6881b7d1df7c5aafaf37f0db",
          "name": "Jiangmiao Pang",
          "hidden": false
        },
        {
          "_id": "6881b7d1df7c5aafaf37f0dc",
          "name": "Mingmin Chi",
          "hidden": false
        },
        {
          "_id": "6881b7d1df7c5aafaf37f0dd",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "6881b7d1df7c5aafaf37f0de",
          "name": "Kaipeng Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/l3pzwBZRLJ2_T_7faBEK6.mp4"
      ],
      "publishedAt": "2025-07-23T17:57:09.000Z",
      "submittedOnDailyAt": "2025-07-24T04:38:47.770Z",
      "title": "Yume: An Interactive World Generation Model",
      "submittedOnDailyBy": {
        "_id": "65f1713552c38a91e0a445e8",
        "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
        "isPro": false,
        "fullname": "kaipeng",
        "user": "kpzhang996",
        "type": "user"
      },
      "summary": "Yume aims to use images, text, or videos to create an interactive, realistic,\nand dynamic world, which allows exploration and control using peripheral\ndevices or neural signals. In this report, we present a preview version of\n\\method, which creates a dynamic world from an input image and allows\nexploration of the world using keyboard actions. To achieve this high-fidelity\nand interactive video world generation, we introduce a well-designed framework,\nwhich consists of four main components, including camera motion quantization,\nvideo generation architecture, advanced sampler, and model acceleration. First,\nwe quantize camera motions for stable training and user-friendly interaction\nusing keyboard inputs. Then, we introduce the Masked Video Diffusion\nTransformer~(MVDT) with a memory module for infinite video generation in an\nautoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)\nand Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)\nare introduced to the sampler for better visual quality and more precise\ncontrol. Moreover, we investigate model acceleration by synergistic\noptimization of adversarial distillation and caching mechanisms. We use the\nhigh-quality world exploration dataset \\sekai to train \\method, and it achieves\nremarkable results in diverse scenes and applications. All data, codebase, and\nmodel weights are available on https://github.com/stdstu12/YUME. Yume will\nupdate monthly to achieve its original goal. Project page:\nhttps://stdstu12.github.io/YUME-Project/.",
      "upvotes": 22,
      "discussionId": "6881b7d1df7c5aafaf37f0df",
      "projectPage": "https://stdstu12.github.io/YUME-Project/",
      "githubRepo": "https://github.com/stdstu12/YUME",
      "ai_summary": "A framework for generating and exploring interactive video worlds from images using Masked Video Diffusion Transformer, Anti-Artifact Mechanism, Time Travel Sampling, and model acceleration techniques.",
      "ai_keywords": [
        "camera motion quantization",
        "Masked Video Diffusion Transformer",
        "MVDT",
        "memory module",
        "infinite video generation",
        "autoregressive",
        "Anti-Artifact Mechanism",
        "AAM",
        "Time Travel Sampling",
        "TTS-SDE",
        "stochastic differential equations",
        "model acceleration",
        "adversarial distillation",
        "caching mechanisms"
      ],
      "githubStars": 22
    },
    "publishedAt": "2025-07-23T13:57:09.000Z",
    "title": "Yume: An Interactive World Generation Model",
    "summary": "Yume aims to use images, text, or videos to create an interactive, realistic,\nand dynamic world, which allows exploration and control using peripheral\ndevices or neural signals. In this report, we present a preview version of\n\\method, which creates a dynamic world from an input image and allows\nexploration of the world using keyboard actions. To achieve this high-fidelity\nand interactive video world generation, we introduce a well-designed framework,\nwhich consists of four main components, including camera motion quantization,\nvideo generation architecture, advanced sampler, and model acceleration. First,\nwe quantize camera motions for stable training and user-friendly interaction\nusing keyboard inputs. Then, we introduce the Masked Video Diffusion\nTransformer~(MVDT) with a memory module for infinite video generation in an\nautoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)\nand Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)\nare introduced to the sampler for better visual quality and more precise\ncontrol. Moreover, we investigate model acceleration by synergistic\noptimization of adversarial distillation and caching mechanisms. We use the\nhigh-quality world exploration dataset \\sekai to train \\method, and it achieves\nremarkable results in diverse scenes and applications. All data, codebase, and\nmodel weights are available on https://github.com/stdstu12/YUME. Yume will\nupdate monthly to achieve its original goal. Project page:\nhttps://stdstu12.github.io/YUME-Project/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/l3pzwBZRLJ2_T_7faBEK6.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.17744.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65f1713552c38a91e0a445e8",
      "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
      "fullname": "kaipeng",
      "name": "kpzhang996",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.17512",
      "authors": [
        {
          "_id": "6881a669df7c5aafaf37f0bc",
          "name": "Yu Li",
          "hidden": false
        },
        {
          "_id": "6881a669df7c5aafaf37f0bd",
          "name": "Zhuoshi Pan",
          "hidden": false
        },
        {
          "_id": "6881a669df7c5aafaf37f0be",
          "name": "Honglin Lin",
          "hidden": false
        },
        {
          "_id": "6881a669df7c5aafaf37f0bf",
          "name": "Mengyuan Sun",
          "hidden": false
        },
        {
          "_id": "6881a669df7c5aafaf37f0c0",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "6881a669df7c5aafaf37f0c1",
          "name": "Lijun Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-23T13:51:04.000Z",
      "submittedOnDailyAt": "2025-07-24T01:51:17.747Z",
      "title": "Can One Domain Help Others? A Data-Centric Study on Multi-Domain\n  Reasoning via Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "671b852aa4fa4f8f5fb5404c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/671b852aa4fa4f8f5fb5404c/TDLsgP8WgKW-qaA8Ys-iJ.jpeg",
        "isPro": false,
        "fullname": "YU LI",
        "user": "yu0226",
        "type": "user"
      },
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful paradigm for enhancing the reasoning capabilities of LLMs. Existing\nresearch has predominantly concentrated on isolated reasoning domains such as\nmathematical problem-solving, coding tasks, or logical reasoning. However, real\nworld reasoning scenarios inherently demand an integrated application of\nmultiple cognitive skills. Despite this, the interplay among these reasoning\nskills under reinforcement learning remains poorly understood. To bridge this\ngap, we present a systematic investigation of multi-domain reasoning within the\nRLVR framework, explicitly focusing on three primary domains: mathematical\nreasoning, code generation, and logical puzzle solving. We conduct a\ncomprehensive study comprising four key components: (1) Leveraging the GRPO\nalgorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the\nmodels' in-domain improvements and cross-domain generalization capabilities\nwhen trained on single-domain datasets. (2) Additionally, we examine the\nintricate interactions including mutual enhancements and conflicts that emerge\nduring combined cross-domain training. (3) To further understand the influence\nof SFT on RL, we also analyze and compare performance differences between base\nand instruct models under identical RL configurations. (4) Furthermore, we\ndelve into critical RL training details, systematically exploring the impacts\nof curriculum learning strategies, variations in reward design, and\nlanguage-specific factors. Through extensive experiments, our results offer\nsignificant insights into the dynamics governing domain interactions, revealing\nkey factors influencing both specialized and generalizable reasoning\nperformance. These findings provide valuable guidance for optimizing RL\nmethodologies to foster comprehensive, multi-domain reasoning capabilities in\nLLMs.",
      "upvotes": 20,
      "discussionId": "6881a669df7c5aafaf37f0c2"
    },
    "publishedAt": "2025-07-23T09:51:04.000Z",
    "title": "Can One Domain Help Others? A Data-Centric Study on Multi-Domain\n  Reasoning via Reinforcement Learning",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful paradigm for enhancing the reasoning capabilities of LLMs. Existing\nresearch has predominantly concentrated on isolated reasoning domains such as\nmathematical problem-solving, coding tasks, or logical reasoning. However, real\nworld reasoning scenarios inherently demand an integrated application of\nmultiple cognitive skills. Despite this, the interplay among these reasoning\nskills under reinforcement learning remains poorly understood. To bridge this\ngap, we present a systematic investigation of multi-domain reasoning within the\nRLVR framework, explicitly focusing on three primary domains: mathematical\nreasoning, code generation, and logical puzzle solving. We conduct a\ncomprehensive study comprising four key components: (1) Leveraging the GRPO\nalgorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the\nmodels' in-domain improvements and cross-domain generalization capabilities\nwhen trained on single-domain datasets. (2) Additionally, we examine the\nintricate interactions including mutual enhancements and conflicts that emerge\nduring combined cross-domain training. (3) To further understand the influence\nof SFT on RL, we also analyze and compare performance differences between base\nand instruct models under identical RL configurations. (4) Furthermore, we\ndelve into critical RL training details, systematically exploring the impacts\nof curriculum learning strategies, variations in reward design, and\nlanguage-specific factors. Through extensive experiments, our results offer\nsignificant insights into the dynamics governing domain interactions, revealing\nkey factors influencing both specialized and generalizable reasoning\nperformance. These findings provide valuable guidance for optimizing RL\nmethodologies to foster comprehensive, multi-domain reasoning capabilities in\nLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.17512.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "671b852aa4fa4f8f5fb5404c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/671b852aa4fa4f8f5fb5404c/TDLsgP8WgKW-qaA8Ys-iJ.jpeg",
      "fullname": "YU LI",
      "name": "yu0226",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.17202",
      "authors": [
        {
          "_id": "6881cb17df7c5aafaf37f0f1",
          "name": "Jooyeol Yun",
          "hidden": false
        },
        {
          "_id": "6881cb17df7c5aafaf37f0f2",
          "name": "Heng Wang",
          "hidden": false
        },
        {
          "_id": "6881cb17df7c5aafaf37f0f3",
          "name": "Yotaro Shimose",
          "hidden": false
        },
        {
          "_id": "6881cb17df7c5aafaf37f0f4",
          "name": "Jaegul Choo",
          "hidden": false
        },
        {
          "_id": "6881cb17df7c5aafaf37f0f5",
          "name": "Shingo Takamatsu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-23T04:49:48.000Z",
      "submittedOnDailyAt": "2025-07-24T04:33:49.250Z",
      "title": "DesignLab: Designing Slides Through Iterative Detection and Correction",
      "submittedOnDailyBy": {
        "_id": "6369f693bf21b20c5692937b",
        "avatarUrl": "/avatars/e937dc8234b3e456149882bfce34841f.svg",
        "isPro": false,
        "fullname": "Jooyeol Yun",
        "user": "YeolJoo",
        "type": "user"
      },
      "summary": "Designing high-quality presentation slides can be challenging for non-experts\ndue to the complexity involved in navigating various design choices. Numerous\nautomated tools can suggest layouts and color schemes, yet often lack the\nability to refine their own output, which is a key aspect in real-world\nworkflows. We propose DesignLab, which separates the design process into two\nroles, the design reviewer, who identifies design-related issues, and the\ndesign contributor who corrects them. This decomposition enables an iterative\nloop where the reviewer continuously detects issues and the contributor\ncorrects them, allowing a draft to be further polished with each iteration,\nreaching qualities that were unattainable. We fine-tune large language models\nfor these roles and simulate intermediate drafts by introducing controlled\nperturbations, enabling the design reviewer learn design errors and the\ncontributor learn how to fix them. Our experiments show that DesignLab\noutperforms existing design-generation methods, including a commercial tool, by\nembracing the iterative nature of designing which can result in polished,\nprofessional slides.",
      "upvotes": 19,
      "discussionId": "6881cb17df7c5aafaf37f0f6",
      "projectPage": "https://yeolj00.github.io/personal-projects/designlab/",
      "ai_summary": "DesignLab uses fine-tuned large language models to iteratively improve presentation slides through a design reviewer and contributor system, outperforming existing tools.",
      "ai_keywords": [
        "large language models",
        "design reviewer",
        "design contributor",
        "iterative loop",
        "controlled perturbations",
        "design errors"
      ]
    },
    "publishedAt": "2025-07-23T00:49:48.000Z",
    "title": "DesignLab: Designing Slides Through Iterative Detection and Correction",
    "summary": "Designing high-quality presentation slides can be challenging for non-experts\ndue to the complexity involved in navigating various design choices. Numerous\nautomated tools can suggest layouts and color schemes, yet often lack the\nability to refine their own output, which is a key aspect in real-world\nworkflows. We propose DesignLab, which separates the design process into two\nroles, the design reviewer, who identifies design-related issues, and the\ndesign contributor who corrects them. This decomposition enables an iterative\nloop where the reviewer continuously detects issues and the contributor\ncorrects them, allowing a draft to be further polished with each iteration,\nreaching qualities that were unattainable. We fine-tune large language models\nfor these roles and simulate intermediate drafts by introducing controlled\nperturbations, enabling the design reviewer learn design errors and the\ncontributor learn how to fix them. Our experiments show that DesignLab\noutperforms existing design-generation methods, including a commercial tool, by\nembracing the iterative nature of designing which can result in polished,\nprofessional slides.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.17202.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6369f693bf21b20c5692937b",
      "avatarUrl": "/avatars/e937dc8234b3e456149882bfce34841f.svg",
      "fullname": "Jooyeol Yun",
      "name": "YeolJoo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.16863",
      "authors": [
        {
          "_id": "6881be07df7c5aafaf37f0e1",
          "name": "Hongcheng Gao",
          "hidden": false
        },
        {
          "_id": "6881be07df7c5aafaf37f0e2",
          "name": "Zihao Huang",
          "hidden": false
        },
        {
          "_id": "6881be07df7c5aafaf37f0e3",
          "name": "Lin Xu",
          "hidden": false
        },
        {
          "_id": "6881be07df7c5aafaf37f0e4",
          "name": "Jingyi Tang",
          "hidden": false
        },
        {
          "_id": "6881be07df7c5aafaf37f0e5",
          "name": "Xinhao Li",
          "hidden": false
        },
        {
          "_id": "6881be07df7c5aafaf37f0e6",
          "name": "Yue Liu",
          "hidden": false
        },
        {
          "_id": "6881be07df7c5aafaf37f0e7",
          "name": "Haoyang Li",
          "hidden": false
        },
        {
          "_id": "6881be07df7c5aafaf37f0e8",
          "name": "Taihang Hu",
          "hidden": false
        },
        {
          "_id": "6881be07df7c5aafaf37f0e9",
          "name": "Minhua Lin",
          "hidden": false
        },
        {
          "_id": "6881be07df7c5aafaf37f0ea",
          "name": "Xinlong Yang",
          "hidden": false
        },
        {
          "_id": "6881be07df7c5aafaf37f0eb",
          "name": "Ge Wu",
          "hidden": false
        },
        {
          "_id": "6881be07df7c5aafaf37f0ec",
          "name": "Balong Bi",
          "hidden": false
        },
        {
          "_id": "6881be07df7c5aafaf37f0ed",
          "name": "Hongyu Chen",
          "hidden": false
        },
        {
          "_id": "6881be07df7c5aafaf37f0ee",
          "name": "Wentao Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-21T21:50:16.000Z",
      "submittedOnDailyAt": "2025-07-24T03:31:48.243Z",
      "title": "Pixels, Patterns, but No Poetry: To See The World like Humans",
      "submittedOnDailyBy": {
        "_id": "62728f4f6253fe2068da1021",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62728f4f6253fe2068da1021/KZ65X0EH98AF3zXemPiap.jpeg",
        "isPro": false,
        "fullname": "Hongcheng Gao",
        "user": "HongchengGao",
        "type": "user"
      },
      "summary": "Achieving human-like perception and reasoning in Multimodal Large Language\nModels (MLLMs) remains a central challenge in artificial intelligence. While\nrecent research has primarily focused on enhancing reasoning capabilities in\nMLLMs, a fundamental question persists: Can Multimodal Large Language Models\ntruly perceive the world as humans do? This paper shifts focus from reasoning\nto perception. Rather than constructing benchmarks specifically for reasoning,\nwe introduce the Turing Eye Test (TET), a challenging perception-oriented\nbenchmark comprising four diagnostic tasks that evaluate MLLMs' performance on\nsynthetic images that humans process intuitively. Our findings reveal that\nstate-of-the-art MLLMs exhibit catastrophic failures on our perceptual tasks\ntrivial for humans. Both in-context learning and training on language\nbackbone-effective for previous benchmarks-fail to improve performance on our\ntasks, while fine-tuning the vision tower enables rapid adaptation, suggesting\nthat our benchmark poses challenges for vision tower generalization rather than\nfor the knowledge and reasoning capabilities of the language backbone-a key gap\nbetween current MLLMs and human perception. We release a representative subset\nof TET tasks in this version, and will introduce more diverse tasks and methods\nto enhance visual generalization in future work.",
      "upvotes": 16,
      "discussionId": "6881be08df7c5aafaf37f0ef",
      "projectPage": "https://turingeyetest.github.io/",
      "ai_summary": "The Turing Eye Test evaluates MLLMs' perceptual abilities through synthetic images, revealing that vision tower generalization is a significant gap compared to human perception.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "Turing Eye Test",
        "in-context learning",
        "fine-tuning",
        "vision tower",
        "visual generalization"
      ]
    },
    "publishedAt": "2025-07-21T17:50:16.000Z",
    "title": "Pixels, Patterns, but No Poetry: To See The World like Humans",
    "summary": "Achieving human-like perception and reasoning in Multimodal Large Language\nModels (MLLMs) remains a central challenge in artificial intelligence. While\nrecent research has primarily focused on enhancing reasoning capabilities in\nMLLMs, a fundamental question persists: Can Multimodal Large Language Models\ntruly perceive the world as humans do? This paper shifts focus from reasoning\nto perception. Rather than constructing benchmarks specifically for reasoning,\nwe introduce the Turing Eye Test (TET), a challenging perception-oriented\nbenchmark comprising four diagnostic tasks that evaluate MLLMs' performance on\nsynthetic images that humans process intuitively. Our findings reveal that\nstate-of-the-art MLLMs exhibit catastrophic failures on our perceptual tasks\ntrivial for humans. Both in-context learning and training on language\nbackbone-effective for previous benchmarks-fail to improve performance on our\ntasks, while fine-tuning the vision tower enables rapid adaptation, suggesting\nthat our benchmark poses challenges for vision tower generalization rather than\nfor the knowledge and reasoning capabilities of the language backbone-a key gap\nbetween current MLLMs and human perception. We release a representative subset\nof TET tasks in this version, and will introduce more diverse tasks and methods\nto enhance visual generalization in future work.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.16863.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "62728f4f6253fe2068da1021",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62728f4f6253fe2068da1021/KZ65X0EH98AF3zXemPiap.jpeg",
      "fullname": "Hongcheng Gao",
      "name": "HongchengGao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.16331",
      "authors": [
        {
          "_id": "6881d9d8df7c5aafaf37f117",
          "name": "Chuanhao Yan",
          "hidden": false
        },
        {
          "_id": "6881d9d8df7c5aafaf37f118",
          "name": "Fengdi Che",
          "hidden": false
        },
        {
          "_id": "6881d9d8df7c5aafaf37f119",
          "name": "Xuhan Huang",
          "hidden": false
        },
        {
          "_id": "6881d9d8df7c5aafaf37f11a",
          "name": "Xu Xu",
          "hidden": false
        },
        {
          "_id": "6881d9d8df7c5aafaf37f11b",
          "name": "Xin Li",
          "hidden": false
        },
        {
          "_id": "6881d9d8df7c5aafaf37f11c",
          "name": "Yizhi Li",
          "hidden": false
        },
        {
          "_id": "6881d9d8df7c5aafaf37f11d",
          "name": "Xingwei Qu",
          "hidden": false
        },
        {
          "_id": "6881d9d8df7c5aafaf37f11e",
          "name": "Jingzhe Shi",
          "hidden": false
        },
        {
          "_id": "6881d9d8df7c5aafaf37f11f",
          "name": "Zhuangzhuang He",
          "hidden": false
        },
        {
          "_id": "6881d9d8df7c5aafaf37f120",
          "name": "Chenghua Lin",
          "hidden": false
        },
        {
          "_id": "6881d9d8df7c5aafaf37f121",
          "name": "Yaodong Yang",
          "hidden": false
        },
        {
          "_id": "6881d9d8df7c5aafaf37f122",
          "name": "Binhang Yuan",
          "hidden": false
        },
        {
          "_id": "6881d9d8df7c5aafaf37f123",
          "name": "Hang Zhao",
          "hidden": false
        },
        {
          "_id": "6881d9d8df7c5aafaf37f124",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "6881d9d8df7c5aafaf37f125",
          "name": "Bowen Zhou",
          "hidden": false
        },
        {
          "_id": "6881d9d8df7c5aafaf37f126",
          "name": "Jie Fu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-22T08:13:01.000Z",
      "submittedOnDailyAt": "2025-07-24T05:31:08.422Z",
      "title": "Re:Form -- Reducing Human Priors in Scalable Formal Software\n  Verification with RL in LLMs: A Preliminary Study on Dafny",
      "submittedOnDailyBy": {
        "_id": "641a6895fb5ffff5ac79d593",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641a6895fb5ffff5ac79d593/vxvwsto3llOEWGqQKGMYx.jpeg",
        "isPro": false,
        "fullname": "Jie Fu",
        "user": "bigaidream",
        "type": "user"
      },
      "summary": "Existing informal language-based (e.g., human language) Large Language Models\n(LLMs) trained with Reinforcement Learning (RL) face a significant challenge:\ntheir verification processes, which provide crucial training signals, are\nneither reliable nor scalable. In fact, the prevalent large proprietary models\ncould hardly generate verifiable programs. A promising yet largely uncharted\nalternative is formal language-based reasoning. Grounding LLMs in rigorous\nformal systems where generative models operate in formal language spaces (e.g.,\nDafny) enables the automatic and mathematically provable verification of their\nreasoning processes and outcomes. This capability is pivotal for achieving\nlarge-scale, reliable formal software verification. It is a common practice to\nemploy human-annotated chain-of-thought and other human priors to induce the\nreasoning and coding capabilities of LLMs. Unfortunately, it becomes\nunacceptably all-consuming to provide such priors for supervising complex\nprogramming tasks. In this work, we systematically explore ways to reduce human\npriors with the formal language, Dafny, as the main environment for our pilot\nstudy. Our pipeline mainly relies on introducing an automatic and scalable data\ncuration pipeline, and careful RL designs integrated with feedback from the\nformal language verifier. We introduce DafnyComp, a benchmark of compositional\nformal programs with auto-formalized specifications for specification\nreasoning. Our supervised fine-tuning (SFT) stage enables even small models\n(e.g., 0.5B) to generate syntactically valid and verifiable Dafny code,\nsurpassing proprietary models. RL with regularization further improves\nperformance, achieving stronger generalization to out-of-domain tasks and\noutperforming all strong baselines on the challenging DafnyComp benchmark.",
      "upvotes": 9,
      "discussionId": "6881d9d8df7c5aafaf37f127",
      "ai_summary": "Formal language-based reasoning and automatic verification improve the reliability and scalability of Large Language Models for generating verifiable programs.",
      "ai_keywords": [
        "Reinforcement Learning",
        "formal language-based reasoning",
        "formal systems",
        "formal language spaces",
        "generative models",
        "automatic verification",
        "chain-of-thought",
        "human priors",
        "data curation pipeline",
        "supervised fine-tuning",
        "DafnyComp",
        "compositional formal programs",
        "auto-formalized specifications",
        "specification reasoning",
        "syntactically valid",
        "verifiable Dafny code",
        "regularization",
        "generalization",
        "out-of-domain tasks"
      ]
    },
    "publishedAt": "2025-07-22T04:13:01.000Z",
    "title": "Re:Form -- Reducing Human Priors in Scalable Formal Software\n  Verification with RL in LLMs: A Preliminary Study on Dafny",
    "summary": "Existing informal language-based (e.g., human language) Large Language Models\n(LLMs) trained with Reinforcement Learning (RL) face a significant challenge:\ntheir verification processes, which provide crucial training signals, are\nneither reliable nor scalable. In fact, the prevalent large proprietary models\ncould hardly generate verifiable programs. A promising yet largely uncharted\nalternative is formal language-based reasoning. Grounding LLMs in rigorous\nformal systems where generative models operate in formal language spaces (e.g.,\nDafny) enables the automatic and mathematically provable verification of their\nreasoning processes and outcomes. This capability is pivotal for achieving\nlarge-scale, reliable formal software verification. It is a common practice to\nemploy human-annotated chain-of-thought and other human priors to induce the\nreasoning and coding capabilities of LLMs. Unfortunately, it becomes\nunacceptably all-consuming to provide such priors for supervising complex\nprogramming tasks. In this work, we systematically explore ways to reduce human\npriors with the formal language, Dafny, as the main environment for our pilot\nstudy. Our pipeline mainly relies on introducing an automatic and scalable data\ncuration pipeline, and careful RL designs integrated with feedback from the\nformal language verifier. We introduce DafnyComp, a benchmark of compositional\nformal programs with auto-formalized specifications for specification\nreasoning. Our supervised fine-tuning (SFT) stage enables even small models\n(e.g., 0.5B) to generate syntactically valid and verifiable Dafny code,\nsurpassing proprietary models. RL with regularization further improves\nperformance, achieving stronger generalization to out-of-domain tasks and\noutperforming all strong baselines on the challenging DafnyComp benchmark.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.16331.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "641a6895fb5ffff5ac79d593",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641a6895fb5ffff5ac79d593/vxvwsto3llOEWGqQKGMYx.jpeg",
      "fullname": "Jie Fu",
      "name": "bigaidream",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.17745",
      "authors": [
        {
          "_id": "6881b243df7c5aafaf37f0c4",
          "name": "Yiwen Chen",
          "hidden": false
        },
        {
          "_id": "6881b243df7c5aafaf37f0c5",
          "name": "Zhihao Li",
          "hidden": false
        },
        {
          "_id": "6881b243df7c5aafaf37f0c6",
          "name": "Yikai Wang",
          "hidden": false
        },
        {
          "_id": "6881b243df7c5aafaf37f0c7",
          "name": "Hu Zhang",
          "hidden": false
        },
        {
          "_id": "6881b243df7c5aafaf37f0c8",
          "name": "Qin Li",
          "hidden": false
        },
        {
          "_id": "6881b243df7c5aafaf37f0c9",
          "name": "Chi Zhang",
          "hidden": false
        },
        {
          "_id": "6881b243df7c5aafaf37f0ca",
          "name": "Guosheng Lin",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64e45b1c2fbf570ac70dc6e8/pEcsL-6nuzyxL0kQxvdFJ.png"
      ],
      "publishedAt": "2025-07-23T17:57:16.000Z",
      "submittedOnDailyAt": "2025-07-24T02:42:32.620Z",
      "title": "Ultra3D: Efficient and High-Fidelity 3D Generation with Part Attention",
      "submittedOnDailyBy": {
        "_id": "64e45b1c2fbf570ac70dc6e8",
        "avatarUrl": "/avatars/8dd65f75f2c1b92abf2f189f6d8466a2.svg",
        "isPro": true,
        "fullname": "Chen Yiwen",
        "user": "Yiwen-ntu",
        "type": "user"
      },
      "summary": "Recent advances in sparse voxel representations have significantly improved\nthe quality of 3D content generation, enabling high-resolution modeling with\nfine-grained geometry. However, existing frameworks suffer from severe\ncomputational inefficiencies due to the quadratic complexity of attention\nmechanisms in their two-stage diffusion pipelines. In this work, we propose\nUltra3D, an efficient 3D generation framework that significantly accelerates\nsparse voxel modeling without compromising quality. Our method leverages the\ncompact VecSet representation to efficiently generate a coarse object layout in\nthe first stage, reducing token count and accelerating voxel coordinate\nprediction. To refine per-voxel latent features in the second stage, we\nintroduce Part Attention, a geometry-aware localized attention mechanism that\nrestricts attention computation within semantically consistent part regions.\nThis design preserves structural continuity while avoiding unnecessary global\nattention, achieving up to 6.7x speed-up in latent generation. To support this\nmechanism, we construct a scalable part annotation pipeline that converts raw\nmeshes into part-labeled sparse voxels. Extensive experiments demonstrate that\nUltra3D supports high-resolution 3D generation at 1024 resolution and achieves\nstate-of-the-art performance in both visual fidelity and user preference.",
      "upvotes": 6,
      "discussionId": "6881b243df7c5aafaf37f0cb",
      "ai_summary": "Ultra3D uses VecSet and Part Attention to accelerate 3D voxel generation while maintaining high quality and resolution.",
      "ai_keywords": [
        "sparse voxel representations",
        "diffusion pipelines",
        "VecSet",
        "Part Attention",
        "geometry-aware localized attention",
        "semantic consistency",
        "part-labeled sparse voxels"
      ]
    },
    "publishedAt": "2025-07-23T13:57:16.000Z",
    "title": "Ultra3D: Efficient and High-Fidelity 3D Generation with Part Attention",
    "summary": "Recent advances in sparse voxel representations have significantly improved\nthe quality of 3D content generation, enabling high-resolution modeling with\nfine-grained geometry. However, existing frameworks suffer from severe\ncomputational inefficiencies due to the quadratic complexity of attention\nmechanisms in their two-stage diffusion pipelines. In this work, we propose\nUltra3D, an efficient 3D generation framework that significantly accelerates\nsparse voxel modeling without compromising quality. Our method leverages the\ncompact VecSet representation to efficiently generate a coarse object layout in\nthe first stage, reducing token count and accelerating voxel coordinate\nprediction. To refine per-voxel latent features in the second stage, we\nintroduce Part Attention, a geometry-aware localized attention mechanism that\nrestricts attention computation within semantically consistent part regions.\nThis design preserves structural continuity while avoiding unnecessary global\nattention, achieving up to 6.7x speed-up in latent generation. To support this\nmechanism, we construct a scalable part annotation pipeline that converts raw\nmeshes into part-labeled sparse voxels. Extensive experiments demonstrate that\nUltra3D supports high-resolution 3D generation at 1024 resolution and achieves\nstate-of-the-art performance in both visual fidelity and user preference.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64e45b1c2fbf570ac70dc6e8/pEcsL-6nuzyxL0kQxvdFJ.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.17745.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e45b1c2fbf570ac70dc6e8",
      "avatarUrl": "/avatars/8dd65f75f2c1b92abf2f189f6d8466a2.svg",
      "fullname": "Chen Yiwen",
      "name": "Yiwen-ntu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.11465",
      "authors": [
        {
          "_id": "6881b78bdf7c5aafaf37f0cd",
          "name": "Nuri Ryu",
          "hidden": false
        },
        {
          "_id": "6881b78bdf7c5aafaf37f0ce",
          "name": "Jiyun Won",
          "hidden": false
        },
        {
          "_id": "6881b78bdf7c5aafaf37f0cf",
          "name": "Jooeun Son",
          "hidden": false
        },
        {
          "_id": "6881b78bdf7c5aafaf37f0d0",
          "name": "Minsu Gong",
          "hidden": false
        },
        {
          "_id": "6881b78bdf7c5aafaf37f0d1",
          "name": "Joo-Haeng Lee",
          "hidden": false
        },
        {
          "_id": "6881b78bdf7c5aafaf37f0d2",
          "name": "Sunghyun Cho",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-15T16:36:20.000Z",
      "submittedOnDailyAt": "2025-07-24T03:03:53.801Z",
      "title": "Elevating 3D Models: High-Quality Texture and Geometry Refinement from a\n  Low-Quality Model",
      "submittedOnDailyBy": {
        "_id": "63106aac95c34b954078ef67",
        "avatarUrl": "/avatars/0e627c74d301d65f3b364e8229dd78ab.svg",
        "isPro": false,
        "fullname": "Nuri Ryu",
        "user": "terryryu",
        "type": "user"
      },
      "summary": "High-quality 3D assets are essential for various applications in computer\ngraphics and 3D vision but remain scarce due to significant acquisition costs.\nTo address this shortage, we introduce Elevate3D, a novel framework that\ntransforms readily accessible low-quality 3D assets into higher quality. At the\ncore of Elevate3D is HFS-SDEdit, a specialized texture enhancement method that\nsignificantly improves texture quality while preserving the appearance and\ngeometry while fixing its degradations. Furthermore, Elevate3D operates in a\nview-by-view manner, alternating between texture and geometry refinement.\nUnlike previous methods that have largely overlooked geometry refinement, our\nframework leverages geometric cues from images refined with HFS-SDEdit by\nemploying state-of-the-art monocular geometry predictors. This approach ensures\ndetailed and accurate geometry that aligns seamlessly with the enhanced\ntexture. Elevate3D outperforms recent competitors by achieving state-of-the-art\nquality in 3D model refinement, effectively addressing the scarcity of\nhigh-quality open-source 3D assets.",
      "upvotes": 6,
      "discussionId": "6881b78cdf7c5aafaf37f0d3",
      "projectPage": "https://cg.postech.ac.kr/research/Elevate3D/",
      "githubRepo": "https://github.com/ryunuri/Elevate3D",
      "ai_summary": "Elevate3D enhances both texture and geometry of low-quality 3D assets using HFS-SDEdit and monocular geometry predictors, achieving superior refinement quality.",
      "ai_keywords": [
        "HFS-SDEdit",
        "monocular geometry predictors"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-07-15T12:36:20.000Z",
    "title": "Elevating 3D Models: High-Quality Texture and Geometry Refinement from a\n  Low-Quality Model",
    "summary": "High-quality 3D assets are essential for various applications in computer\ngraphics and 3D vision but remain scarce due to significant acquisition costs.\nTo address this shortage, we introduce Elevate3D, a novel framework that\ntransforms readily accessible low-quality 3D assets into higher quality. At the\ncore of Elevate3D is HFS-SDEdit, a specialized texture enhancement method that\nsignificantly improves texture quality while preserving the appearance and\ngeometry while fixing its degradations. Furthermore, Elevate3D operates in a\nview-by-view manner, alternating between texture and geometry refinement.\nUnlike previous methods that have largely overlooked geometry refinement, our\nframework leverages geometric cues from images refined with HFS-SDEdit by\nemploying state-of-the-art monocular geometry predictors. This approach ensures\ndetailed and accurate geometry that aligns seamlessly with the enhanced\ntexture. Elevate3D outperforms recent competitors by achieving state-of-the-art\nquality in 3D model refinement, effectively addressing the scarcity of\nhigh-quality open-source 3D assets.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.11465.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63106aac95c34b954078ef67",
      "avatarUrl": "/avatars/0e627c74d301d65f3b364e8229dd78ab.svg",
      "fullname": "Nuri Ryu",
      "name": "terryryu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  }
]