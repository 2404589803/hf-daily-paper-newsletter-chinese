[
  {
    "paper": {
      "id": "2505.04620",
      "authors": [
        {
          "_id": "681c6c1817fc8222eff39a1a",
          "user": {
            "_id": "647773a1168cb428e00e9a8f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
            "isPro": false,
            "fullname": "Hao Fei",
            "user": "scofield7419",
            "type": "user"
          },
          "name": "Hao Fei",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-08T09:58:07.591Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a1b",
          "name": "Yuan Zhou",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a1c",
          "name": "Juncheng Li",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a1d",
          "name": "Xiangtai Li",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a1e",
          "name": "Qingshan Xu",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a1f",
          "name": "Bobo Li",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a20",
          "user": {
            "_id": "64c139d867eff857ea51caa8",
            "avatarUrl": "/avatars/4b7b3f41c2e2cfa21dd43bbac6e081ae.svg",
            "isPro": false,
            "fullname": "Shengqiong Wu",
            "user": "ChocoWu",
            "type": "user"
          },
          "name": "Shengqiong Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:22:39.333Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a21",
          "name": "Yaoting Wang",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a22",
          "name": "Junbao Zhou",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a23",
          "name": "Jiahao Meng",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a24",
          "user": {
            "_id": "656724074f6ec72017754d33",
            "avatarUrl": "/avatars/e61de248f6f53719b2375077340dd033.svg",
            "isPro": false,
            "fullname": "QingyuShi",
            "user": "QingyuShi",
            "type": "user"
          },
          "name": "Qingyu Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:22:34.088Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a25",
          "name": "Zhiyuan Zhou",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a26",
          "name": "Liangtao Shi",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a27",
          "name": "Minghe Gao",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a28",
          "name": "Daoan Zhang",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a29",
          "name": "Zhiqi Ge",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a2a",
          "name": "Weiming Wu",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a2b",
          "name": "Siliang Tang",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a2c",
          "name": "Kaihang Pan",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a2d",
          "name": "Yaobo Ye",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a2e",
          "name": "Haobo Yuan",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a2f",
          "name": "Tao Zhang",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a30",
          "name": "Tianjie Ju",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a31",
          "name": "Zixiang Meng",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a32",
          "name": "Shilin Xu",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a33",
          "name": "Liyu Jia",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a34",
          "name": "Wentao Hu",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a35",
          "user": {
            "_id": "64ad1c0bad6218d51a07b54e",
            "avatarUrl": "/avatars/0f84d9a51c6ca9bcef44de2d7c707d9b.svg",
            "isPro": false,
            "fullname": "LUO MENG",
            "user": "Eureka-Leo",
            "type": "user"
          },
          "name": "Meng Luo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:22:37.235Z",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a36",
          "name": "Jiebo Luo",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a37",
          "name": "Tat-Seng Chua",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a38",
          "name": "Shuicheng Yan",
          "hidden": false
        },
        {
          "_id": "681c6c1817fc8222eff39a39",
          "name": "Hanwang Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/TqzNcdmo0rwc-0JkEc0-i.mp4",
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/HBVHtWBiagedRLXD-lWZc.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/X02B6xk6CZDywtG8tGliK.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/Xnc89DOmtr5j2hST5Um17.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/Awl6jj9MX38cMRkgXpxHp.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/HT_9Y1ponvvqUREjMTMBB.png",
        "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/sG3FBiQCx3EONF25cEbED.png"
      ],
      "publishedAt": "2025-05-07T17:59:32.000Z",
      "submittedOnDailyAt": "2025-05-09T01:19:17.510Z",
      "title": "On Path to Multimodal Generalist: General-Level and General-Bench",
      "submittedOnDailyBy": {
        "_id": "647773a1168cb428e00e9a8f",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
        "isPro": false,
        "fullname": "Hao Fei",
        "user": "scofield7419",
        "type": "user"
      },
      "summary": "The Multimodal Large Language Model (MLLM) is currently experiencing rapid\ngrowth, driven by the advanced capabilities of LLMs. Unlike earlier\nspecialists, existing MLLMs are evolving towards a Multimodal Generalist\nparadigm. Initially limited to understanding multiple modalities, these models\nhave advanced to not only comprehend but also generate across modalities. Their\ncapabilities have expanded from coarse-grained to fine-grained multimodal\nunderstanding and from supporting limited modalities to arbitrary ones. While\nmany benchmarks exist to assess MLLMs, a critical question arises: Can we\nsimply assume that higher performance across tasks indicates a stronger MLLM\ncapability, bringing us closer to human-level AI? We argue that the answer is\nnot as straightforward as it seems. This project introduces General-Level, an\nevaluation framework that defines 5-scale levels of MLLM performance and\ngenerality, offering a methodology to compare MLLMs and gauge the progress of\nexisting systems towards more robust multimodal generalists and, ultimately,\ntowards AGI. At the core of the framework is the concept of Synergy, which\nmeasures whether models maintain consistent capabilities across comprehension\nand generation, and across multiple modalities. To support this evaluation, we\npresent General-Bench, which encompasses a broader spectrum of skills,\nmodalities, formats, and capabilities, including over 700 tasks and 325,800\ninstances. The evaluation results that involve over 100 existing\nstate-of-the-art MLLMs uncover the capability rankings of generalists,\nhighlighting the challenges in reaching genuine AI. We expect this project to\npave the way for future research on next-generation multimodal foundation\nmodels, providing a robust infrastructure to accelerate the realization of AGI.\nProject page: https://generalist.top/",
      "upvotes": 38,
      "discussionId": "681c6c1d17fc8222eff39b45",
      "projectPage": "https://generalist.top/",
      "githubRepo": "https://github.com/path2generalist/General-Level",
      "ai_keywords": [
        "Multimodal Large Language Model (MLLM)",
        "Multimodal Generalist",
        "multimodal understanding",
        "comprehension",
        "generation",
        "General-Level",
        "Synergy",
        "General-Bench",
        "AGI (Artificial General Intelligence)"
      ]
    },
    "publishedAt": "2025-05-07T13:59:32.000Z",
    "title": "On Path to Multimodal Generalist: General-Level and General-Bench",
    "summary": "The Multimodal Large Language Model (MLLM) is currently experiencing rapid\ngrowth, driven by the advanced capabilities of LLMs. Unlike earlier\nspecialists, existing MLLMs are evolving towards a Multimodal Generalist\nparadigm. Initially limited to understanding multiple modalities, these models\nhave advanced to not only comprehend but also generate across modalities. Their\ncapabilities have expanded from coarse-grained to fine-grained multimodal\nunderstanding and from supporting limited modalities to arbitrary ones. While\nmany benchmarks exist to assess MLLMs, a critical question arises: Can we\nsimply assume that higher performance across tasks indicates a stronger MLLM\ncapability, bringing us closer to human-level AI? We argue that the answer is\nnot as straightforward as it seems. This project introduces General-Level, an\nevaluation framework that defines 5-scale levels of MLLM performance and\ngenerality, offering a methodology to compare MLLMs and gauge the progress of\nexisting systems towards more robust multimodal generalists and, ultimately,\ntowards AGI. At the core of the framework is the concept of Synergy, which\nmeasures whether models maintain consistent capabilities across comprehension\nand generation, and across multiple modalities. To support this evaluation, we\npresent General-Bench, which encompasses a broader spectrum of skills,\nmodalities, formats, and capabilities, including over 700 tasks and 325,800\ninstances. The evaluation results that involve over 100 existing\nstate-of-the-art MLLMs uncover the capability rankings of generalists,\nhighlighting the challenges in reaching genuine AI. We expect this project to\npave the way for future research on next-generation multimodal foundation\nmodels, providing a robust infrastructure to accelerate the realization of AGI.\nProject page: https://generalist.top/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/TqzNcdmo0rwc-0JkEc0-i.mp4",
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/HBVHtWBiagedRLXD-lWZc.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/X02B6xk6CZDywtG8tGliK.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/Xnc89DOmtr5j2hST5Um17.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/Awl6jj9MX38cMRkgXpxHp.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/HT_9Y1ponvvqUREjMTMBB.png",
      "https://cdn-uploads.huggingface.co/production/uploads/647773a1168cb428e00e9a8f/sG3FBiQCx3EONF25cEbED.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.04620.png",
    "numComments": 5,
    "submittedBy": {
      "_id": "647773a1168cb428e00e9a8f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647773a1168cb428e00e9a8f/NiRR3ScY6Plzjibfwy1hC.jpeg",
      "fullname": "Hao Fei",
      "name": "scofield7419",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05470",
      "authors": [
        {
          "_id": "681d9829edf34a77aab565eb",
          "name": "Jie Liu",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565ec",
          "name": "Gongye Liu",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565ed",
          "name": "Jiajun Liang",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565ee",
          "name": "Yangguang Li",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565ef",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565f0",
          "name": "Xintao Wang",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565f1",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565f2",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "681d9829edf34a77aab565f3",
          "name": "Wanli Ouyang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T17:58:45.000Z",
      "submittedOnDailyAt": "2025-05-09T05:45:53.355Z",
      "title": "Flow-GRPO: Training Flow Matching Models via Online RL",
      "submittedOnDailyBy": {
        "_id": "64d71083a787c9bc7b9f1238",
        "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
        "isPro": false,
        "fullname": "Yangguang Li",
        "user": "Lp256",
        "type": "user"
      },
      "summary": "We propose Flow-GRPO, the first method integrating online reinforcement\nlearning (RL) into flow matching models. Our approach uses two key strategies:\n(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary\nDifferential Equation (ODE) into an equivalent Stochastic Differential Equation\n(SDE) that matches the original model's marginal distribution at all timesteps,\nenabling statistical sampling for RL exploration; and (2) a Denoising Reduction\nstrategy that reduces training denoising steps while retaining the original\ninference timestep number, significantly improving sampling efficiency without\nperformance degradation. Empirically, Flow-GRPO is effective across multiple\ntext-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly\nperfect object counts, spatial relations, and fine-grained attributes, boosting\nGenEval accuracy from 63% to 95%. In visual text rendering, its accuracy\nimproves from 59% to 92%, significantly enhancing text generation.\nFlow-GRPO also achieves substantial gains in human preference alignment.\nNotably, little to no reward hacking occurred, meaning rewards did not increase\nat the cost of image quality or diversity, and both remained stable in our\nexperiments.",
      "upvotes": 14,
      "discussionId": "681d982aedf34a77aab56635",
      "ai_keywords": [
        "Flow-GRPO",
        "reinforcement learning (RL)",
        "flow matching models",
        "ODE-to-SDE conversion",
        "Ordinary Differential Equation (ODE)",
        "Stochastic Differential Equation (SDE)",
        "Denoising Reduction strategy",
        "GenEval accuracy",
        "text-to-image tasks",
        "SD3.5",
        "visual text rendering",
        "human preference alignment",
        "reward hacking"
      ]
    },
    "publishedAt": "2025-05-08T13:58:45.000Z",
    "title": "Flow-GRPO: Training Flow Matching Models via Online RL",
    "summary": "We propose Flow-GRPO, the first method integrating online reinforcement\nlearning (RL) into flow matching models. Our approach uses two key strategies:\n(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary\nDifferential Equation (ODE) into an equivalent Stochastic Differential Equation\n(SDE) that matches the original model's marginal distribution at all timesteps,\nenabling statistical sampling for RL exploration; and (2) a Denoising Reduction\nstrategy that reduces training denoising steps while retaining the original\ninference timestep number, significantly improving sampling efficiency without\nperformance degradation. Empirically, Flow-GRPO is effective across multiple\ntext-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly\nperfect object counts, spatial relations, and fine-grained attributes, boosting\nGenEval accuracy from 63% to 95%. In visual text rendering, its accuracy\nimproves from 59% to 92%, significantly enhancing text generation.\nFlow-GRPO also achieves substantial gains in human preference alignment.\nNotably, little to no reward hacking occurred, meaning rewards did not increase\nat the cost of image quality or diversity, and both remained stable in our\nexperiments.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05470.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64d71083a787c9bc7b9f1238",
      "avatarUrl": "/avatars/d0b0546dec7fc5792921154bec41385a.svg",
      "fullname": "Yangguang Li",
      "name": "Lp256",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.02847",
      "authors": [
        {
          "_id": "681d7031e9969eecfcb4eb81",
          "name": "Bang Zhang",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb82",
          "user": {
            "_id": "648294b2eb4befee378951c1",
            "avatarUrl": "/avatars/da5d8bf9d8662cc2ffa2c0de49bd66a3.svg",
            "isPro": false,
            "fullname": "Ruotian Ma",
            "user": "vvibt",
            "type": "user"
          },
          "name": "Ruotian Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:30.886Z",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb83",
          "name": "Qingxuan Jiang",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb84",
          "name": "Peisong Wang",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb85",
          "name": "Jiaqi Chen",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb86",
          "name": "Zheng Xie",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb87",
          "name": "Xingyu Chen",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb88",
          "name": "Yue Wang",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb89",
          "name": "Fanghua Ye",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb8a",
          "name": "Jian Li",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb8b",
          "name": "Yifan Yang",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb8c",
          "user": {
            "_id": "67485743561b1e6f9579389f",
            "avatarUrl": "/avatars/8a4cc63bd7be388010bc329bb74582a1.svg",
            "isPro": false,
            "fullname": "Zhaopeng Tu",
            "user": "zptu",
            "type": "user"
          },
          "name": "Zhaopeng Tu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:28.468Z",
          "hidden": false
        },
        {
          "_id": "681d7031e9969eecfcb4eb8d",
          "name": "Xiaolong Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-01T19:06:10.000Z",
      "submittedOnDailyAt": "2025-05-09T01:37:10.548Z",
      "title": "Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in\n  Large Language Models",
      "submittedOnDailyBy": {
        "_id": "648294b2eb4befee378951c1",
        "avatarUrl": "/avatars/da5d8bf9d8662cc2ffa2c0de49bd66a3.svg",
        "isPro": false,
        "fullname": "Ruotian Ma",
        "user": "vvibt",
        "type": "user"
      },
      "summary": "Assessing how well a large language model (LLM) understands human, rather\nthan merely text, remains an open challenge. To bridge the gap, we introduce\nSentient Agent as a Judge (SAGE), an automated evaluation framework that\nmeasures an LLM's higher-order social cognition. SAGE instantiates a Sentient\nAgent that simulates human-like emotional changes and inner thoughts during\ninteraction, providing a more realistic evaluation of the tested model in\nmulti-turn conversations. At every turn, the agent reasons about (i) how its\nemotion changes, (ii) how it feels, and (iii) how it should reply, yielding a\nnumerical emotion trajectory and interpretable inner thoughts. Experiments on\n100 supportive-dialogue scenarios show that the final Sentient emotion score\ncorrelates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings\nand utterance-level empathy metrics, validating psychological fidelity. We also\nbuild a public Sentient Leaderboard covering 18 commercial and open-source\nmodels that uncovers substantial gaps (up to 4x) between frontier systems\n(GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in\nconventional leaderboards (e.g., Arena). SAGE thus provides a principled,\nscalable and interpretable tool for tracking progress toward genuinely\nempathetic and socially adept language agents.",
      "upvotes": 13,
      "discussionId": "681d7033e9969eecfcb4ec2d",
      "ai_keywords": [
        "Sentient Agent as a Judge (SAGE)",
        "higher-order social cognition",
        "emotional changes",
        "inner thoughts",
        "multi-turn conversations",
        "numerical emotion trajectory",
        "Barrett-Lennard Relationship Inventory (BLRI)",
        "utterance-level empathy metrics",
        "psychological fidelity",
        "Sentient Leaderboard",
        "empathetic",
        "socially adept language agents"
      ]
    },
    "publishedAt": "2025-05-01T15:06:10.000Z",
    "title": "Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in\n  Large Language Models",
    "summary": "Assessing how well a large language model (LLM) understands human, rather\nthan merely text, remains an open challenge. To bridge the gap, we introduce\nSentient Agent as a Judge (SAGE), an automated evaluation framework that\nmeasures an LLM's higher-order social cognition. SAGE instantiates a Sentient\nAgent that simulates human-like emotional changes and inner thoughts during\ninteraction, providing a more realistic evaluation of the tested model in\nmulti-turn conversations. At every turn, the agent reasons about (i) how its\nemotion changes, (ii) how it feels, and (iii) how it should reply, yielding a\nnumerical emotion trajectory and interpretable inner thoughts. Experiments on\n100 supportive-dialogue scenarios show that the final Sentient emotion score\ncorrelates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings\nand utterance-level empathy metrics, validating psychological fidelity. We also\nbuild a public Sentient Leaderboard covering 18 commercial and open-source\nmodels that uncovers substantial gaps (up to 4x) between frontier systems\n(GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in\nconventional leaderboards (e.g., Arena). SAGE thus provides a principled,\nscalable and interpretable tool for tracking progress toward genuinely\nempathetic and socially adept language agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02847.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "648294b2eb4befee378951c1",
      "avatarUrl": "/avatars/da5d8bf9d8662cc2ffa2c0de49bd66a3.svg",
      "fullname": "Ruotian Ma",
      "name": "vvibt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05315",
      "authors": [
        {
          "_id": "681d7ccb572e742b3f42d1f3",
          "name": "Yuhui Xu",
          "hidden": false
        },
        {
          "_id": "681d7ccb572e742b3f42d1f4",
          "name": "Hanze Dong",
          "hidden": false
        },
        {
          "_id": "681d7ccb572e742b3f42d1f5",
          "name": "Lei Wang",
          "hidden": false
        },
        {
          "_id": "681d7ccb572e742b3f42d1f6",
          "name": "Doyen Sahoo",
          "hidden": false
        },
        {
          "_id": "681d7ccb572e742b3f42d1f7",
          "name": "Junnan Li",
          "hidden": false
        },
        {
          "_id": "681d7ccb572e742b3f42d1f8",
          "name": "Caiming Xiong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T15:01:06.000Z",
      "submittedOnDailyAt": "2025-05-09T02:31:21.542Z",
      "title": "Scalable Chain of Thoughts via Elastic Reasoning",
      "submittedOnDailyBy": {
        "_id": "6602869253a0518b2a98cafd",
        "avatarUrl": "/avatars/c14b5953a716f42c83ad28147f8308ae.svg",
        "isPro": false,
        "fullname": "Yuhui Xu",
        "user": "yuhuixu",
        "type": "user"
      },
      "summary": "Large reasoning models (LRMs) have achieved remarkable progress on complex\ntasks by generating extended chains of thought (CoT). However, their\nuncontrolled output lengths pose significant challenges for real-world\ndeployment, where inference-time budgets on tokens, latency, or compute are\nstrictly constrained. We propose Elastic Reasoning, a novel framework for\nscalable chain of thoughts that explicitly separates reasoning into two\nphases--thinking and solution--with independently allocated budgets. At test\ntime, Elastic Reasoning prioritize that completeness of solution segments,\nsignificantly improving reliability under tight resource constraints. To train\nmodels that are robust to truncated thinking, we introduce a lightweight\nbudget-constrained rollout strategy, integrated into GRPO, which teaches the\nmodel to reason adaptively when the thinking process is cut short and\ngeneralizes effectively to unseen budget constraints without additional\ntraining. Empirical results on mathematical (AIME, MATH500) and programming\n(LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning\nperforms robustly under strict budget constraints, while incurring\nsignificantly lower training cost than baseline methods. Remarkably, our\napproach also produces more concise and efficient reasoning even in\nunconstrained settings. Elastic Reasoning offers a principled and practical\nsolution to the pressing challenge of controllable reasoning at scale.",
      "upvotes": 11,
      "discussionId": "681d7ccc572e742b3f42d21a",
      "ai_keywords": [
        "Large reasoning models (LRMs)",
        "chain of thought (CoT)",
        "inference-time budgets",
        "tokens",
        "latency",
        "compute",
        "Elastic Reasoning",
        "scalable chain of thoughts",
        "thinking phase",
        "solution phase",
        "independently allocated budgets",
        "completeness of solution segments",
        "reliability",
        "resource constraints",
        "lightweight budget-constrained rollout strategy",
        "GRPO",
        "adaptive reasoning",
        "unseen budget constraints",
        "mathematical benchmarks (AIME, MATH500)",
        "programming benchmarks (LiveCodeBench, Codeforces)",
        "unconstrained settings",
        "principled solution"
      ]
    },
    "publishedAt": "2025-05-08T11:01:06.000Z",
    "title": "Scalable Chain of Thoughts via Elastic Reasoning",
    "summary": "Large reasoning models (LRMs) have achieved remarkable progress on complex\ntasks by generating extended chains of thought (CoT). However, their\nuncontrolled output lengths pose significant challenges for real-world\ndeployment, where inference-time budgets on tokens, latency, or compute are\nstrictly constrained. We propose Elastic Reasoning, a novel framework for\nscalable chain of thoughts that explicitly separates reasoning into two\nphases--thinking and solution--with independently allocated budgets. At test\ntime, Elastic Reasoning prioritize that completeness of solution segments,\nsignificantly improving reliability under tight resource constraints. To train\nmodels that are robust to truncated thinking, we introduce a lightweight\nbudget-constrained rollout strategy, integrated into GRPO, which teaches the\nmodel to reason adaptively when the thinking process is cut short and\ngeneralizes effectively to unseen budget constraints without additional\ntraining. Empirical results on mathematical (AIME, MATH500) and programming\n(LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning\nperforms robustly under strict budget constraints, while incurring\nsignificantly lower training cost than baseline methods. Remarkably, our\napproach also produces more concise and efficient reasoning even in\nunconstrained settings. Elastic Reasoning offers a principled and practical\nsolution to the pressing challenge of controllable reasoning at scale.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05315.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6602869253a0518b2a98cafd",
      "avatarUrl": "/avatars/c14b5953a716f42c83ad28147f8308ae.svg",
      "fullname": "Yuhui Xu",
      "name": "yuhuixu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.05474",
      "authors": [
        {
          "_id": "681d7b5ae27a030c96a28bde",
          "name": "Beichen Wen",
          "hidden": false
        },
        {
          "_id": "681d7b5ae27a030c96a28bdf",
          "name": "Haozhe Xie",
          "hidden": false
        },
        {
          "_id": "681d7b5ae27a030c96a28be0",
          "name": "Zhaoxi Chen",
          "hidden": false
        },
        {
          "_id": "681d7b5ae27a030c96a28be1",
          "name": "Fangzhou Hong",
          "hidden": false
        },
        {
          "_id": "681d7b5ae27a030c96a28be2",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63f47b5321eb234ab739e91a/PqIosa0nVWamlNxCmd6w1.webp"
      ],
      "publishedAt": "2025-05-08T17:59:54.000Z",
      "submittedOnDailyAt": "2025-05-09T02:21:54.023Z",
      "title": "3D Scene Generation: A Survey",
      "submittedOnDailyBy": {
        "_id": "63f47b5321eb234ab739e91a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg",
        "isPro": false,
        "fullname": "Haozhe Xie",
        "user": "hzxie",
        "type": "user"
      },
      "summary": "3D scene generation seeks to synthesize spatially structured, semantically\nmeaningful, and photorealistic environments for applications such as immersive\nmedia, robotics, autonomous driving, and embodied AI. Early methods based on\nprocedural rules offered scalability but limited diversity. Recent advances in\ndeep generative models (e.g., GANs, diffusion models) and 3D representations\n(e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene\ndistributions, improving fidelity, diversity, and view consistency. Recent\nadvances like diffusion models bridge 3D scene synthesis and photorealism by\nreframing generation as image or video synthesis problems. This survey provides\na systematic overview of state-of-the-art approaches, organizing them into four\nparadigms: procedural generation, neural 3D-based generation, image-based\ngeneration, and video-based generation. We analyze their technical foundations,\ntrade-offs, and representative results, and review commonly used datasets,\nevaluation protocols, and downstream applications. We conclude by discussing\nkey challenges in generation capacity, 3D representation, data and annotations,\nand evaluation, and outline promising directions including higher fidelity,\nphysics-aware and interactive generation, and unified perception-generation\nmodels. This review organizes recent advances in 3D scene generation and\nhighlights promising directions at the intersection of generative AI, 3D\nvision, and embodied intelligence. To track ongoing developments, we maintain\nan up-to-date project page:\nhttps://github.com/hzxie/Awesome-3D-Scene-Generation.",
      "upvotes": 5,
      "discussionId": "681d7b5be27a030c96a28c29",
      "githubRepo": "https://github.com/hzxie/Awesome-3D-Scene-Generation",
      "ai_keywords": [
        "deep generative models",
        "GANs",
        "diffusion models",
        "NeRF",
        "3D Gaussians",
        "procedural generation",
        "neural 3D-based generation",
        "image-based generation",
        "video-based generation"
      ]
    },
    "publishedAt": "2025-05-08T13:59:54.000Z",
    "title": "3D Scene Generation: A Survey",
    "summary": "3D scene generation seeks to synthesize spatially structured, semantically\nmeaningful, and photorealistic environments for applications such as immersive\nmedia, robotics, autonomous driving, and embodied AI. Early methods based on\nprocedural rules offered scalability but limited diversity. Recent advances in\ndeep generative models (e.g., GANs, diffusion models) and 3D representations\n(e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene\ndistributions, improving fidelity, diversity, and view consistency. Recent\nadvances like diffusion models bridge 3D scene synthesis and photorealism by\nreframing generation as image or video synthesis problems. This survey provides\na systematic overview of state-of-the-art approaches, organizing them into four\nparadigms: procedural generation, neural 3D-based generation, image-based\ngeneration, and video-based generation. We analyze their technical foundations,\ntrade-offs, and representative results, and review commonly used datasets,\nevaluation protocols, and downstream applications. We conclude by discussing\nkey challenges in generation capacity, 3D representation, data and annotations,\nand evaluation, and outline promising directions including higher fidelity,\nphysics-aware and interactive generation, and unified perception-generation\nmodels. This review organizes recent advances in 3D scene generation and\nhighlights promising directions at the intersection of generative AI, 3D\nvision, and embodied intelligence. To track ongoing developments, we maintain\nan up-to-date project page:\nhttps://github.com/hzxie/Awesome-3D-Scene-Generation.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63f47b5321eb234ab739e91a/PqIosa0nVWamlNxCmd6w1.webp"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05474.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63f47b5321eb234ab739e91a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg",
      "fullname": "Haozhe Xie",
      "name": "hzxie",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.03981",
      "authors": [
        {
          "_id": "681d7ee755699177c7fb636a",
          "name": "Qianchu Liu",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb636b",
          "user": {
            "_id": "6234c11b7d5de9839bc44163",
            "avatarUrl": "/avatars/3ca569596f9c7134e8d4b560a06ee1e7.svg",
            "isPro": false,
            "fullname": "Sheng Zhang",
            "user": "shengz",
            "type": "user"
          },
          "name": "Sheng Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:22.974Z",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb636c",
          "name": "Guanghui Qin",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb636d",
          "name": "Timothy Ossowski",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb636e",
          "name": "Yu Gu",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb636f",
          "name": "Ying Jin",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb6370",
          "name": "Sid Kiblawi",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb6371",
          "name": "Sam Preston",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb6372",
          "name": "Mu Wei",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb6373",
          "name": "Paul Vozila",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb6374",
          "name": "Tristan Naumann",
          "hidden": false
        },
        {
          "_id": "681d7ee755699177c7fb6375",
          "name": "Hoifung Poon",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-06T21:08:27.000Z",
      "submittedOnDailyAt": "2025-05-09T02:41:23.534Z",
      "title": "X-Reasoner: Towards Generalizable Reasoning Across Modalities and\n  Domains",
      "submittedOnDailyBy": {
        "_id": "6234c11b7d5de9839bc44163",
        "avatarUrl": "/avatars/3ca569596f9c7134e8d4b560a06ee1e7.svg",
        "isPro": false,
        "fullname": "Sheng Zhang",
        "user": "shengz",
        "type": "user"
      },
      "summary": "Recent proprietary models (e.g., o3) have begun to demonstrate strong\nmultimodal reasoning capabilities. Yet, most existing open-source research\nconcentrates on training text-only reasoning models, with evaluations limited\nto mainly mathematical and general-domain tasks. Therefore, it remains unclear\nhow to effectively extend reasoning capabilities beyond text input and general\ndomains. This paper explores a fundamental research question: Is reasoning\ngeneralizable across modalities and domains? Our findings support an\naffirmative answer: General-domain text-based post-training can enable such\nstrong generalizable reasoning. Leveraging this finding, we introduce\nX-Reasoner, a vision-language model post-trained solely on general-domain text\nfor generalizable reasoning, using a two-stage approach: an initial supervised\nfine-tuning phase with distilled long chain-of-thoughts, followed by\nreinforcement learning with verifiable rewards. Experiments show that\nX-Reasoner successfully transfers reasoning capabilities to both multimodal and\nout-of-domain settings, outperforming existing state-of-the-art models trained\nwith in-domain and multimodal data across various general and medical\nbenchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in\nspecialized domains can be further enhanced through continued training on\ndomain-specific text-only data. Building upon this, we introduce\nX-Reasoner-Med, a medical-specialized variant that achieves new state of the\nart on numerous text-only and multimodal medical benchmarks.",
      "upvotes": 4,
      "discussionId": "681d7ee855699177c7fb63b7",
      "projectPage": "https://github.com/microsoft/x-reasoner",
      "ai_keywords": [
        "multimodal reasoning",
        "vision-language model",
        "post-training",
        "long chain-of-thoughts",
        "reinforcement learning",
        "verifiable rewards",
        "X-Reasoner",
        "out-of-domain settings",
        "X-Reasoner-Med"
      ]
    },
    "publishedAt": "2025-05-06T17:08:27.000Z",
    "title": "X-Reasoner: Towards Generalizable Reasoning Across Modalities and\n  Domains",
    "summary": "Recent proprietary models (e.g., o3) have begun to demonstrate strong\nmultimodal reasoning capabilities. Yet, most existing open-source research\nconcentrates on training text-only reasoning models, with evaluations limited\nto mainly mathematical and general-domain tasks. Therefore, it remains unclear\nhow to effectively extend reasoning capabilities beyond text input and general\ndomains. This paper explores a fundamental research question: Is reasoning\ngeneralizable across modalities and domains? Our findings support an\naffirmative answer: General-domain text-based post-training can enable such\nstrong generalizable reasoning. Leveraging this finding, we introduce\nX-Reasoner, a vision-language model post-trained solely on general-domain text\nfor generalizable reasoning, using a two-stage approach: an initial supervised\nfine-tuning phase with distilled long chain-of-thoughts, followed by\nreinforcement learning with verifiable rewards. Experiments show that\nX-Reasoner successfully transfers reasoning capabilities to both multimodal and\nout-of-domain settings, outperforming existing state-of-the-art models trained\nwith in-domain and multimodal data across various general and medical\nbenchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in\nspecialized domains can be further enhanced through continued training on\ndomain-specific text-only data. Building upon this, we introduce\nX-Reasoner-Med, a medical-specialized variant that achieves new state of the\nart on numerous text-only and multimodal medical benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.03981.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6234c11b7d5de9839bc44163",
      "avatarUrl": "/avatars/3ca569596f9c7134e8d4b560a06ee1e7.svg",
      "fullname": "Sheng Zhang",
      "name": "shengz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05071",
      "authors": [
        {
          "_id": "681da6375f701833274a0d21",
          "user": {
            "_id": "6621e591c50869c1e91a1639",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621e591c50869c1e91a1639/L_PoEn2BRAJcnWZX-JebR.jpeg",
            "isPro": false,
            "fullname": "Chunyu Xie",
            "user": "xiechunyu",
            "type": "user"
          },
          "name": "Chunyu Xie",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:03.296Z",
          "hidden": false
        },
        {
          "_id": "681da6375f701833274a0d22",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "681da6375f701833274a0d23",
          "name": "Fanjing Kong",
          "hidden": false
        },
        {
          "_id": "681da6375f701833274a0d24",
          "name": "Jincheng Li",
          "hidden": false
        },
        {
          "_id": "681da6375f701833274a0d25",
          "name": "Dawei Liang",
          "hidden": false
        },
        {
          "_id": "681da6375f701833274a0d26",
          "name": "Gengshen Zhang",
          "hidden": false
        },
        {
          "_id": "681da6375f701833274a0d27",
          "user": {
            "_id": "649935abbe8fd92c27ab1ed8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649935abbe8fd92c27ab1ed8/ueWnaZtJa-oWpzupP6FV8.png",
            "isPro": false,
            "fullname": "David Leon",
            "user": "DavidLeon",
            "type": "user"
          },
          "name": "Dawei Leng",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-09T07:00:10.249Z",
          "hidden": false
        },
        {
          "_id": "681da6375f701833274a0d28",
          "name": "Yuhui Yin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T09:06:53.000Z",
      "submittedOnDailyAt": "2025-05-09T05:27:38.509Z",
      "title": "FG-CLIP: Fine-Grained Visual and Textual Alignment",
      "submittedOnDailyBy": {
        "_id": "649935abbe8fd92c27ab1ed8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649935abbe8fd92c27ab1ed8/ueWnaZtJa-oWpzupP6FV8.png",
        "isPro": false,
        "fullname": "David Leon",
        "user": "DavidLeon",
        "type": "user"
      },
      "summary": "Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks\nsuch as image-text retrieval and zero-shot classification but struggles with\nfine-grained understanding due to its focus on coarse-grained short captions.\nTo address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances\nfine-grained understanding through three key innovations. First, we leverage\nlarge multimodal models to generate 1.6 billion long caption-image pairs for\ncapturing global-level semantic details. Second, a high-quality dataset is\nconstructed with 12 million images and 40 million region-specific bounding\nboxes aligned with detailed captions to ensure precise, context-rich\nrepresentations. Third, 10 million hard fine-grained negative samples are\nincorporated to improve the model's ability to distinguish subtle semantic\ndifferences. Corresponding training methods are meticulously designed for these\ndata. Extensive experiments demonstrate that FG-CLIP outperforms the original\nCLIP and other state-of-the-art methods across various downstream tasks,\nincluding fine-grained understanding, open-vocabulary object detection,\nimage-text retrieval, and general multimodal benchmarks. These results\nhighlight FG-CLIP's effectiveness in capturing fine-grained image details and\nimproving overall model performance. The related data, code, and models are\navailable at https://github.com/360CVGroup/FG-CLIP.",
      "upvotes": 3,
      "discussionId": "681da6385f701833274a0d8a",
      "githubRepo": "https://github.com/360CVGroup/FG-CLIP",
      "ai_keywords": [
        "Contrastive Language-Image Pre-training (CLIP)",
        "image-text retrieval",
        "zero-shot classification",
        "fine-grained understanding",
        "coarse-grained short captions",
        "multimodal models",
        "1.6 billion long caption-image pairs",
        "high-quality dataset",
        "12 million images",
        "40 million region-specific bounding boxes",
        "detailed captions",
        "10 million hard fine-grained negative samples",
        "fine-grained understanding",
        "open-vocabulary object detection",
        "general multimodal benchmarks",
        "FG-CLIP"
      ]
    },
    "publishedAt": "2025-05-08T05:06:53.000Z",
    "title": "FG-CLIP: Fine-Grained Visual and Textual Alignment",
    "summary": "Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks\nsuch as image-text retrieval and zero-shot classification but struggles with\nfine-grained understanding due to its focus on coarse-grained short captions.\nTo address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances\nfine-grained understanding through three key innovations. First, we leverage\nlarge multimodal models to generate 1.6 billion long caption-image pairs for\ncapturing global-level semantic details. Second, a high-quality dataset is\nconstructed with 12 million images and 40 million region-specific bounding\nboxes aligned with detailed captions to ensure precise, context-rich\nrepresentations. Third, 10 million hard fine-grained negative samples are\nincorporated to improve the model's ability to distinguish subtle semantic\ndifferences. Corresponding training methods are meticulously designed for these\ndata. Extensive experiments demonstrate that FG-CLIP outperforms the original\nCLIP and other state-of-the-art methods across various downstream tasks,\nincluding fine-grained understanding, open-vocabulary object detection,\nimage-text retrieval, and general multimodal benchmarks. These results\nhighlight FG-CLIP's effectiveness in capturing fine-grained image details and\nimproving overall model performance. The related data, code, and models are\navailable at https://github.com/360CVGroup/FG-CLIP.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05071.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649935abbe8fd92c27ab1ed8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649935abbe8fd92c27ab1ed8/ueWnaZtJa-oWpzupP6FV8.png",
      "fullname": "David Leon",
      "name": "DavidLeon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2504.19314",
      "authors": [
        {
          "_id": "681d89e6d025518b321f67ce",
          "name": "Peilin Zhou",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67cf",
          "name": "Bruce Leon",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d0",
          "user": {
            "_id": "64489ca21d52a633c8f55aba",
            "avatarUrl": "/avatars/5199a5e93161c61d14ec13f79dd8c2c5.svg",
            "isPro": false,
            "fullname": "Xiang Ying",
            "user": "MindYing",
            "type": "user"
          },
          "name": "Xiang Ying",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-09T04:51:51.507Z",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d1",
          "name": "Can Zhang",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d2",
          "name": "Yifan Shao",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d3",
          "name": "Qichen Ye",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d4",
          "name": "Dading Chong",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d5",
          "name": "Zhiling Jin",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d6",
          "name": "Chenxuan Xie",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d7",
          "name": "Meng Cao",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d8",
          "name": "Yuxin Gu",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67d9",
          "name": "Sixin Hong",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67da",
          "name": "Jing Ren",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67db",
          "name": "Jian Chen",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67dc",
          "name": "Chao Liu",
          "hidden": false
        },
        {
          "_id": "681d89e6d025518b321f67dd",
          "name": "Yining Hua",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6673cf668d570d59b83511cc/NJsFys1PlInj9W3E7pD8t.png",
        "https://cdn-uploads.huggingface.co/production/uploads/6673cf668d570d59b83511cc/gJJ8b6rc6njgCz7Fepbbd.png"
      ],
      "publishedAt": "2025-04-27T17:32:43.000Z",
      "submittedOnDailyAt": "2025-05-09T03:24:22.739Z",
      "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language\n  Models in Chinese",
      "submittedOnDailyBy": {
        "_id": "6673cf668d570d59b83511cc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/g9sJViXr1Cmx8A_is7Zgq.jpeg",
        "isPro": false,
        "fullname": "Peilin Zhou",
        "user": "PALIN2018",
        "type": "user"
      },
      "summary": "As large language models (LLMs) evolve into tool-using agents, the ability to\nbrowse the web in real-time has become a critical yardstick for measuring their\nreasoning and retrieval competence. Existing benchmarks such as BrowseComp\nconcentrate on English and overlook the linguistic, infrastructural, and\ncensorship-related complexities of other major information ecosystems -- most\nnotably Chinese. To address this gap, we introduce BrowseComp-ZH, a\nhigh-difficulty benchmark purpose-built to comprehensively evaluate LLM agents\non the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning\n11 diverse domains. Each question is reverse-engineered from a short,\nobjective, and easily verifiable answer (e.g., a date, number, or proper noun).\nA two-stage quality control protocol is applied to strive for high question\ndifficulty and answer uniqueness. We benchmark over 20 state-of-the-art\nlanguage models and agentic search systems on our proposed BrowseComp-ZH.\nDespite their strong conversational and retrieval capabilities, most models\nstruggle severely: a large number achieve accuracy rates below 10%, and only a\nhandful exceed 20%. Even the best-performing system, OpenAI's DeepResearch,\nreaches just 42.9%. These results demonstrate the considerable difficulty of\nBrowseComp-ZH, where success demands not only effective retrieval strategies,\nbut also sophisticated reasoning and information reconciliation -- capabilities\nthat current models still struggle to master. Our dataset, construction\nguidelines, and benchmark results have been publicly released at\nhttps://github.com/PALIN2018/BrowseComp-ZH.",
      "upvotes": 2,
      "discussionId": "681d89e7d025518b321f6807",
      "githubRepo": "https://github.com/PALIN2018/BrowseComp-ZH",
      "ai_keywords": [
        "tool-using agents",
        "multihop questions",
        "information ecosystems",
        "quality control protocol",
        "reasoning and information reconciliation"
      ]
    },
    "publishedAt": "2025-04-27T13:32:43.000Z",
    "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language\n  Models in Chinese",
    "summary": "As large language models (LLMs) evolve into tool-using agents, the ability to\nbrowse the web in real-time has become a critical yardstick for measuring their\nreasoning and retrieval competence. Existing benchmarks such as BrowseComp\nconcentrate on English and overlook the linguistic, infrastructural, and\ncensorship-related complexities of other major information ecosystems -- most\nnotably Chinese. To address this gap, we introduce BrowseComp-ZH, a\nhigh-difficulty benchmark purpose-built to comprehensively evaluate LLM agents\non the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning\n11 diverse domains. Each question is reverse-engineered from a short,\nobjective, and easily verifiable answer (e.g., a date, number, or proper noun).\nA two-stage quality control protocol is applied to strive for high question\ndifficulty and answer uniqueness. We benchmark over 20 state-of-the-art\nlanguage models and agentic search systems on our proposed BrowseComp-ZH.\nDespite their strong conversational and retrieval capabilities, most models\nstruggle severely: a large number achieve accuracy rates below 10%, and only a\nhandful exceed 20%. Even the best-performing system, OpenAI's DeepResearch,\nreaches just 42.9%. These results demonstrate the considerable difficulty of\nBrowseComp-ZH, where success demands not only effective retrieval strategies,\nbut also sophisticated reasoning and information reconciliation -- capabilities\nthat current models still struggle to master. Our dataset, construction\nguidelines, and benchmark results have been publicly released at\nhttps://github.com/PALIN2018/BrowseComp-ZH.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6673cf668d570d59b83511cc/NJsFys1PlInj9W3E7pD8t.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6673cf668d570d59b83511cc/gJJ8b6rc6njgCz7Fepbbd.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.19314.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6673cf668d570d59b83511cc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/g9sJViXr1Cmx8A_is7Zgq.jpeg",
      "fullname": "Peilin Zhou",
      "name": "PALIN2018",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.05288",
      "authors": [
        {
          "_id": "681d9dd229119d666079b275",
          "user": {
            "_id": "63a3170f8c0c89dcae316858",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a3170f8c0c89dcae316858/aZjVQl1jwnagTDGxalREE.jpeg",
            "isPro": false,
            "fullname": "Ahmed Abdelreheem",
            "user": "Samir55",
            "type": "user"
          },
          "name": "Ahmed Abdelreheem",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:06.667Z",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b276",
          "name": "Filippo Aleotti",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b277",
          "name": "Jamie Watson",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b278",
          "name": "Zawar Qureshi",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b279",
          "name": "Abdelrahman Eldesokey",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b27a",
          "name": "Peter Wonka",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b27b",
          "name": "Gabriel Brostow",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b27c",
          "name": "Sara Vicente",
          "hidden": false
        },
        {
          "_id": "681d9dd229119d666079b27d",
          "name": "Guillermo Garcia-Hernando",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T14:29:11.000Z",
      "submittedOnDailyAt": "2025-05-09T04:50:18.526Z",
      "title": "PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes",
      "submittedOnDailyBy": {
        "_id": "63a3170f8c0c89dcae316858",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a3170f8c0c89dcae316858/aZjVQl1jwnagTDGxalREE.jpeg",
        "isPro": false,
        "fullname": "Ahmed Abdelreheem",
        "user": "Samir55",
        "type": "user"
      },
      "summary": "We introduce the novel task of Language-Guided Object Placement in Real 3D\nScenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual\nprompt broadly describing where the 3D asset should be placed. The task here is\nto find a valid placement for the 3D asset that respects the prompt. Compared\nwith other language-guided localization tasks in 3D scenes such as grounding,\nthis task has specific challenges: it is ambiguous because it has multiple\nvalid solutions, and it requires reasoning about 3D geometric relationships and\nfree space. We inaugurate this task by proposing a new benchmark and evaluation\nprotocol. We also introduce a new dataset for training 3D LLMs on this task, as\nwell as the first method to serve as a non-trivial baseline. We believe that\nthis challenging task and our new benchmark could become part of the suite of\nbenchmarks used to evaluate and compare generalist 3D LLM models.",
      "upvotes": 1,
      "discussionId": "681d9dd529119d666079b348",
      "projectPage": "https://nianticlabs.github.io/placeit3d/",
      "githubRepo": "https://github.com/nianticlabs/placeit3d",
      "ai_keywords": [
        "point cloud",
        "3D asset",
        "textual prompt",
        "3D LLMs",
        "bounding",
        "grounding",
        "3D geometric relationships",
        "free space",
        "evaluation protocol",
        "benchmark"
      ]
    },
    "publishedAt": "2025-05-08T10:29:11.000Z",
    "title": "PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes",
    "summary": "We introduce the novel task of Language-Guided Object Placement in Real 3D\nScenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual\nprompt broadly describing where the 3D asset should be placed. The task here is\nto find a valid placement for the 3D asset that respects the prompt. Compared\nwith other language-guided localization tasks in 3D scenes such as grounding,\nthis task has specific challenges: it is ambiguous because it has multiple\nvalid solutions, and it requires reasoning about 3D geometric relationships and\nfree space. We inaugurate this task by proposing a new benchmark and evaluation\nprotocol. We also introduce a new dataset for training 3D LLMs on this task, as\nwell as the first method to serve as a non-trivial baseline. We believe that\nthis challenging task and our new benchmark could become part of the suite of\nbenchmarks used to evaluate and compare generalist 3D LLM models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05288.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a3170f8c0c89dcae316858",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a3170f8c0c89dcae316858/aZjVQl1jwnagTDGxalREE.jpeg",
      "fullname": "Ahmed Abdelreheem",
      "name": "Samir55",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05467",
      "authors": [
        {
          "_id": "681d95b5c7ae5f65b0e55ff9",
          "user": {
            "_id": "63fee47352441fe3e87b5088",
            "avatarUrl": "/avatars/c1df1899e3925aa6fdfc8ee0049fa8a7.svg",
            "isPro": false,
            "fullname": "WANG HAIBO",
            "user": "WHB139426",
            "type": "user"
          },
          "name": "Haibo Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:21:20.829Z",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e55ffa",
          "name": "Bo Feng",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e55ffb",
          "name": "Zhengfeng Lai",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e55ffc",
          "name": "Mingze Xu",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e55ffd",
          "name": "Shiyu Li",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e55ffe",
          "name": "Weifeng Ge",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e55fff",
          "name": "Afshin Dehghan",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e56000",
          "name": "Meng Cao",
          "hidden": false
        },
        {
          "_id": "681d95b5c7ae5f65b0e56001",
          "name": "Ping Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T17:57:40.000Z",
      "submittedOnDailyAt": "2025-05-09T04:13:07.003Z",
      "title": "StreamBridge: Turning Your Offline Video Large Language Model into a\n  Proactive Streaming Assistant",
      "submittedOnDailyBy": {
        "_id": "63fee47352441fe3e87b5088",
        "avatarUrl": "/avatars/c1df1899e3925aa6fdfc8ee0049fa8a7.svg",
        "isPro": false,
        "fullname": "WANG HAIBO",
        "user": "WHB139426",
        "type": "user"
      },
      "summary": "We present StreamBridge, a simple yet effective framework that seamlessly\ntransforms offline Video-LLMs into streaming-capable models. It addresses two\nfundamental challenges in adapting existing models into online scenarios: (1)\nlimited capability for multi-turn real-time understanding, and (2) lack of\nproactive response mechanisms. Specifically, StreamBridge incorporates (1) a\nmemory buffer combined with a round-decayed compression strategy, supporting\nlong-context multi-turn interactions, and (2) a decoupled, lightweight\nactivation model that can be effortlessly integrated into existing Video-LLMs,\nenabling continuous proactive responses. To further support StreamBridge, we\nconstruct Stream-IT, a large-scale dataset tailored for streaming video\nunderstanding, featuring interleaved video-text sequences and diverse\ninstruction formats. Extensive experiments show that StreamBridge significantly\nimproves the streaming understanding capabilities of offline Video-LLMs across\nvarious tasks, outperforming even proprietary models such as GPT-4o and Gemini\n1.5 Pro. Simultaneously, it achieves competitive or superior performance on\nstandard video understanding benchmarks.",
      "upvotes": 0,
      "discussionId": "681d95b6c7ae5f65b0e5606c",
      "ai_keywords": [
        "Video-LLMs",
        "streaming-capable models",
        "multi-turn real-time understanding",
        "proactive response mechanisms",
        "memory buffer",
        "round-decayed compression strategy",
        "long-context multi-turn interactions",
        "decoupled activation model",
        "Stream-IT",
        "interleaved video-text sequences",
        "standard video understanding benchmarks",
        "GPT-4o",
        "Gemini 1.5 Pro"
      ]
    },
    "publishedAt": "2025-05-08T13:57:40.000Z",
    "title": "StreamBridge: Turning Your Offline Video Large Language Model into a\n  Proactive Streaming Assistant",
    "summary": "We present StreamBridge, a simple yet effective framework that seamlessly\ntransforms offline Video-LLMs into streaming-capable models. It addresses two\nfundamental challenges in adapting existing models into online scenarios: (1)\nlimited capability for multi-turn real-time understanding, and (2) lack of\nproactive response mechanisms. Specifically, StreamBridge incorporates (1) a\nmemory buffer combined with a round-decayed compression strategy, supporting\nlong-context multi-turn interactions, and (2) a decoupled, lightweight\nactivation model that can be effortlessly integrated into existing Video-LLMs,\nenabling continuous proactive responses. To further support StreamBridge, we\nconstruct Stream-IT, a large-scale dataset tailored for streaming video\nunderstanding, featuring interleaved video-text sequences and diverse\ninstruction formats. Extensive experiments show that StreamBridge significantly\nimproves the streaming understanding capabilities of offline Video-LLMs across\nvarious tasks, outperforming even proprietary models such as GPT-4o and Gemini\n1.5 Pro. Simultaneously, it achieves competitive or superior performance on\nstandard video understanding benchmarks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05467.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63fee47352441fe3e87b5088",
      "avatarUrl": "/avatars/c1df1899e3925aa6fdfc8ee0049fa8a7.svg",
      "fullname": "WANG HAIBO",
      "name": "WHB139426",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.05408",
      "authors": [
        {
          "_id": "681daba2e3775056736651ce",
          "user": {
            "_id": "61424bf4f0d914a5f606a823",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61424bf4f0d914a5f606a823/0td8lR4elBaVvJUD9Pojh.png",
            "isPro": false,
            "fullname": "Yong Zheng-Xin",
            "user": "yongzx",
            "type": "user"
          },
          "name": "Zheng-Xin Yong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-05-09T07:20:45.630Z",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651cf",
          "name": "M. Farid Adilazuarda",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d0",
          "name": "Jonibek Mansurov",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d1",
          "name": "Ruochen Zhang",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d2",
          "name": "Niklas Muennighoff",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d3",
          "name": "Carsten Eickhoff",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d4",
          "name": "Genta Indra Winata",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d5",
          "name": "Julia Kreutzer",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d6",
          "name": "Stephen H. Bach",
          "hidden": false
        },
        {
          "_id": "681daba2e3775056736651d7",
          "name": "Alham Fikri Aji",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-08T16:50:06.000Z",
      "submittedOnDailyAt": "2025-05-09T05:46:57.523Z",
      "title": "Crosslingual Reasoning through Test-Time Scaling",
      "submittedOnDailyBy": {
        "_id": "61424bf4f0d914a5f606a823",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61424bf4f0d914a5f606a823/0td8lR4elBaVvJUD9Pojh.png",
        "isPro": false,
        "fullname": "Yong Zheng-Xin",
        "user": "yongzx",
        "type": "user"
      },
      "summary": "Reasoning capabilities of large language models are primarily studied for\nEnglish, even when pretrained models are multilingual. In this work, we\ninvestigate to what extent English reasoning finetuning with long\nchain-of-thoughts (CoTs) can generalize across languages. First, we find that\nscaling up inference compute for English-centric reasoning language models\n(RLMs) improves multilingual mathematical reasoning across many languages\nincluding low-resource languages, to an extent where they outperform models\ntwice their size. Second, we reveal that while English-centric RLM's CoTs are\nnaturally predominantly English, they consistently follow a quote-and-think\npattern to reason about quoted non-English inputs. Third, we discover an\neffective strategy to control the language of long CoT reasoning, and we\nobserve that models reason better and more efficiently in high-resource\nlanguages. Finally, we observe poor out-of-domain reasoning generalization, in\nparticular from STEM to cultural commonsense knowledge, even for English.\nOverall, we demonstrate the potentials, study the mechanisms and outline the\nlimitations of crosslingual generalization of English reasoning test-time\nscaling. We conclude that practitioners should let English-centric RLMs reason\nin high-resource languages, while further work is needed to improve reasoning\nin low-resource languages and out-of-domain contexts.",
      "upvotes": 0,
      "discussionId": "681daba2e3775056736651f9",
      "ai_keywords": [
        "reasoning language models (RLMs)",
        "long chain-of-thoughts (CoTs)",
        "multilingual mathematical reasoning",
        "low-resource languages",
        "quote-and-think pattern",
        "scaling up inference compute",
        "high-resource languages",
        "out-of-domain reasoning generalization",
        "STEM",
        "cultural commonsense knowledge",
        "crosslingual generalization",
        "test-time scaling"
      ]
    },
    "publishedAt": "2025-05-08T12:50:06.000Z",
    "title": "Crosslingual Reasoning through Test-Time Scaling",
    "summary": "Reasoning capabilities of large language models are primarily studied for\nEnglish, even when pretrained models are multilingual. In this work, we\ninvestigate to what extent English reasoning finetuning with long\nchain-of-thoughts (CoTs) can generalize across languages. First, we find that\nscaling up inference compute for English-centric reasoning language models\n(RLMs) improves multilingual mathematical reasoning across many languages\nincluding low-resource languages, to an extent where they outperform models\ntwice their size. Second, we reveal that while English-centric RLM's CoTs are\nnaturally predominantly English, they consistently follow a quote-and-think\npattern to reason about quoted non-English inputs. Third, we discover an\neffective strategy to control the language of long CoT reasoning, and we\nobserve that models reason better and more efficiently in high-resource\nlanguages. Finally, we observe poor out-of-domain reasoning generalization, in\nparticular from STEM to cultural commonsense knowledge, even for English.\nOverall, we demonstrate the potentials, study the mechanisms and outline the\nlimitations of crosslingual generalization of English reasoning test-time\nscaling. We conclude that practitioners should let English-centric RLMs reason\nin high-resource languages, while further work is needed to improve reasoning\nin low-resource languages and out-of-domain contexts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.05408.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61424bf4f0d914a5f606a823",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61424bf4f0d914a5f606a823/0td8lR4elBaVvJUD9Pojh.png",
      "fullname": "Yong Zheng-Xin",
      "name": "yongzx",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  }
]