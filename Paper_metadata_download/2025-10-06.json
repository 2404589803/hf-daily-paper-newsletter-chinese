[
  {
    "paper": {
      "id": "2510.00515",
      "authors": [
        {
          "_id": "68e34f7173e20ab5778420bb",
          "name": "Zichen Wen",
          "hidden": false
        },
        {
          "_id": "68e34f7173e20ab5778420bc",
          "name": "Shaobo Wang",
          "hidden": false
        },
        {
          "_id": "68e34f7173e20ab5778420bd",
          "name": "Yufa Zhou",
          "hidden": false
        },
        {
          "_id": "68e34f7173e20ab5778420be",
          "name": "Junyuan Zhang",
          "hidden": false
        },
        {
          "_id": "68e34f7173e20ab5778420bf",
          "name": "Qintong Zhang",
          "hidden": false
        },
        {
          "_id": "68e34f7173e20ab5778420c0",
          "name": "Yifeng Gao",
          "hidden": false
        },
        {
          "_id": "68e34f7173e20ab5778420c1",
          "name": "Zhaorun Chen",
          "hidden": false
        },
        {
          "_id": "68e34f7173e20ab5778420c2",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "68e34f7173e20ab5778420c3",
          "name": "Weijia Li",
          "hidden": false
        },
        {
          "_id": "68e34f7173e20ab5778420c4",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "68e34f7173e20ab5778420c5",
          "name": "Linfeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-01T04:56:40.000Z",
      "submittedOnDailyAt": "2025-10-06T03:44:13.823Z",
      "title": "Efficient Multi-modal Large Language Models via Progressive Consistency\n  Distillation",
      "submittedOnDailyBy": {
        "_id": "653b8c3e97a4d71d950e2f20",
        "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
        "isPro": false,
        "fullname": "Zichen Wen",
        "user": "zichenwen",
        "type": "user"
      },
      "summary": "Visual tokens consume substantial computational resources in multi-modal\nlarge models (MLLMs), significantly compromising their efficiency. Recent works\nhave attempted to improve efficiency by compressing visual tokens during\ntraining, either through modifications to model components or by introducing\nadditional parameters. However, they often overlook the increased learning\ndifficulty caused by such compression, as the model's parameter space struggles\nto quickly adapt to the substantial perturbations in the feature space induced\nby token compression. In this work, we propose to develop Efficient MLLMs via\nProgressive Consistency Distillation (EPIC), a progressive learning framework.\nSpecifically, by decomposing the feature space perturbations introduced by\ntoken compression along the token-wise and layer-wise dimensions, we introduce\ntoken consistency distillation and layer consistency distillation,\nrespectively, aiming to reduce the training difficulty by leveraging guidance\nfrom a teacher model and following a progressive learning trajectory. Extensive\nexperiments demonstrate the superior effectiveness, robustness, and\ngeneralization capabilities of our proposed framework.",
      "upvotes": 16,
      "discussionId": "68e34f7173e20ab5778420c6",
      "projectPage": "https://zichenwen1.github.io/EPIC",
      "githubRepo": "https://github.com/ZichenWen1/EPIC",
      "ai_summary": "EPIC, a progressive learning framework, improves the efficiency of multi-modal large models by reducing training difficulty through token and layer consistency distillation during visual token compression.",
      "ai_keywords": [
        "visual tokens",
        "multi-modal large models",
        "MLLMs",
        "token compression",
        "parameter space",
        "feature space",
        "progressive learning framework",
        "token consistency distillation",
        "layer consistency distillation",
        "teacher model",
        "progressive learning trajectory"
      ],
      "githubStars": 4,
      "organization": {
        "_id": "63e5ef7bf2e9a8f22c515654",
        "name": "SJTU",
        "fullname": "Shanghai Jiao Tong University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"
      }
    },
    "publishedAt": "2025-10-01T00:56:40.000Z",
    "title": "Efficient Multi-modal Large Language Models via Progressive Consistency\n  Distillation",
    "summary": "Visual tokens consume substantial computational resources in multi-modal\nlarge models (MLLMs), significantly compromising their efficiency. Recent works\nhave attempted to improve efficiency by compressing visual tokens during\ntraining, either through modifications to model components or by introducing\nadditional parameters. However, they often overlook the increased learning\ndifficulty caused by such compression, as the model's parameter space struggles\nto quickly adapt to the substantial perturbations in the feature space induced\nby token compression. In this work, we propose to develop Efficient MLLMs via\nProgressive Consistency Distillation (EPIC), a progressive learning framework.\nSpecifically, by decomposing the feature space perturbations introduced by\ntoken compression along the token-wise and layer-wise dimensions, we introduce\ntoken consistency distillation and layer consistency distillation,\nrespectively, aiming to reduce the training difficulty by leveraging guidance\nfrom a teacher model and following a progressive learning trajectory. Extensive\nexperiments demonstrate the superior effectiveness, robustness, and\ngeneralization capabilities of our proposed framework.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00515.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653b8c3e97a4d71d950e2f20",
      "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
      "fullname": "Zichen Wen",
      "name": "zichenwen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "organization": {
      "_id": "63e5ef7bf2e9a8f22c515654",
      "name": "SJTU",
      "fullname": "Shanghai Jiao Tong University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.01068",
      "authors": [
        {
          "_id": "68de0d466024653e8a3ed16f",
          "user": {
            "_id": "64780ba6f32a4117fd182b81",
            "avatarUrl": "/avatars/85f01f4c6c745a04f04805462f9fe9c2.svg",
            "isPro": false,
            "fullname": "CAO",
            "user": "SAGE2000",
            "type": "user"
          },
          "name": "Jiahang Cao",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-06T03:21:22.930Z",
          "hidden": false
        },
        {
          "_id": "68de0d466024653e8a3ed170",
          "name": "Yize Huang",
          "hidden": false
        },
        {
          "_id": "68de0d466024653e8a3ed171",
          "name": "Hanzhong Guo",
          "hidden": false
        },
        {
          "_id": "68de0d466024653e8a3ed172",
          "name": "Rui Zhang",
          "hidden": false
        },
        {
          "_id": "68de0d466024653e8a3ed173",
          "name": "Mu Nan",
          "hidden": false
        },
        {
          "_id": "68de0d466024653e8a3ed174",
          "name": "Weijian Mai",
          "hidden": false
        },
        {
          "_id": "68de0d466024653e8a3ed175",
          "name": "Jiaxu Wang",
          "hidden": false
        },
        {
          "_id": "68de0d466024653e8a3ed176",
          "name": "Hao Cheng",
          "hidden": false
        },
        {
          "_id": "68de0d466024653e8a3ed177",
          "name": "Jingkai Sun",
          "hidden": false
        },
        {
          "_id": "68de0d466024653e8a3ed178",
          "name": "Gang Han",
          "hidden": false
        },
        {
          "_id": "68de0d466024653e8a3ed179",
          "name": "Wen Zhao",
          "hidden": false
        },
        {
          "_id": "68de0d466024653e8a3ed17a",
          "name": "Qiang Zhang",
          "hidden": false
        },
        {
          "_id": "68de0d466024653e8a3ed17b",
          "name": "Yijie Guo",
          "hidden": false
        },
        {
          "_id": "68de0d466024653e8a3ed17c",
          "name": "Qihao Zheng",
          "hidden": false
        },
        {
          "_id": "68de0d466024653e8a3ed17d",
          "name": "Chunfeng Song",
          "hidden": false
        },
        {
          "_id": "68de0d466024653e8a3ed17e",
          "name": "Xiao Li",
          "hidden": false
        },
        {
          "_id": "68de0d466024653e8a3ed17f",
          "name": "Ping Luo",
          "hidden": false
        },
        {
          "_id": "68de0d466024653e8a3ed180",
          "user": {
            "_id": "64b6ce23dbbd1f2cdb624d56",
            "avatarUrl": "/avatars/022738ce8f76fa9545b3e363d7264b53.svg",
            "isPro": false,
            "fullname": "Andrew Luo",
            "user": "aluo-x",
            "type": "user"
          },
          "name": "Andrew F. Luo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-06T03:21:20.592Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64780ba6f32a4117fd182b81/bsb_536O8r19Ocb6K03Ww.png"
      ],
      "publishedAt": "2025-10-01T16:05:53.000Z",
      "submittedOnDailyAt": "2025-10-06T00:35:20.824Z",
      "title": "Compose Your Policies! Improving Diffusion-based or Flow-based Robot\n  Policies via Test-time Distribution-level Composition",
      "submittedOnDailyBy": {
        "_id": "64780ba6f32a4117fd182b81",
        "avatarUrl": "/avatars/85f01f4c6c745a04f04805462f9fe9c2.svg",
        "isPro": false,
        "fullname": "CAO",
        "user": "SAGE2000",
        "type": "user"
      },
      "summary": "Diffusion-based models for robotic control, including vision-language-action\n(VLA) and vision-action (VA) policies, have demonstrated significant\ncapabilities. Yet their advancement is constrained by the high cost of\nacquiring large-scale interaction datasets. This work introduces an alternative\nparadigm for enhancing policy performance without additional model training.\nPerhaps surprisingly, we demonstrate that the composed policies can exceed the\nperformance of either parent policy. Our contribution is threefold. First, we\nestablish a theoretical foundation showing that the convex composition of\ndistributional scores from multiple diffusion models can yield a superior\none-step functional objective compared to any individual score. A\nGr\\\"onwall-type bound is then used to show that this single-step improvement\npropagates through entire generation trajectories, leading to systemic\nperformance gains. Second, motivated by these results, we propose General\nPolicy Composition (GPC), a training-free method that enhances performance by\ncombining the distributional scores of multiple pre-trained policies via a\nconvex combination and test-time search. GPC is versatile, allowing for the\nplug-and-play composition of heterogeneous policies, including VA and VLA\nmodels, as well as those based on diffusion or flow-matching, irrespective of\ntheir input visual modalities. Third, we provide extensive empirical\nvalidation. Experiments on Robomimic, PushT, and RoboTwin benchmarks, alongside\nreal-world robotic evaluations, confirm that GPC consistently improves\nperformance and adaptability across a diverse set of tasks. Further analysis of\nalternative composition operators and weighting strategies offers insights into\nthe mechanisms underlying the success of GPC. These results establish GPC as a\nsimple yet effective method for improving control performance by leveraging\nexisting policies.",
      "upvotes": 10,
      "discussionId": "68de0d476024653e8a3ed181",
      "projectPage": "https://sagecao1125.github.io/GPC-Site/",
      "githubRepo": "https://github.com/SageCao1125/GPC",
      "ai_summary": "General Policy Composition (GPC) enhances robotic control performance by combining pre-trained diffusion-based policies without additional training, leading to superior results across various benchmarks.",
      "ai_keywords": [
        "diffusion-based models",
        "vision-language-action",
        "vision-action",
        "distributional scores",
        "convex composition",
        "Gr\\\"onwall-type bound",
        "General Policy Composition",
        "GPC",
        "pre-trained policies",
        "Robomimic",
        "PushT",
        "RoboTwin",
        "real-world robotic evaluations"
      ],
      "githubStars": 5,
      "organization": {
        "_id": "67ea9ecfc234715db8dbf339",
        "name": "hkuhk",
        "fullname": "The University of Hong Kong",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"
      }
    },
    "publishedAt": "2025-10-01T12:05:53.000Z",
    "title": "Compose Your Policies! Improving Diffusion-based or Flow-based Robot\n  Policies via Test-time Distribution-level Composition",
    "summary": "Diffusion-based models for robotic control, including vision-language-action\n(VLA) and vision-action (VA) policies, have demonstrated significant\ncapabilities. Yet their advancement is constrained by the high cost of\nacquiring large-scale interaction datasets. This work introduces an alternative\nparadigm for enhancing policy performance without additional model training.\nPerhaps surprisingly, we demonstrate that the composed policies can exceed the\nperformance of either parent policy. Our contribution is threefold. First, we\nestablish a theoretical foundation showing that the convex composition of\ndistributional scores from multiple diffusion models can yield a superior\none-step functional objective compared to any individual score. A\nGr\\\"onwall-type bound is then used to show that this single-step improvement\npropagates through entire generation trajectories, leading to systemic\nperformance gains. Second, motivated by these results, we propose General\nPolicy Composition (GPC), a training-free method that enhances performance by\ncombining the distributional scores of multiple pre-trained policies via a\nconvex combination and test-time search. GPC is versatile, allowing for the\nplug-and-play composition of heterogeneous policies, including VA and VLA\nmodels, as well as those based on diffusion or flow-matching, irrespective of\ntheir input visual modalities. Third, we provide extensive empirical\nvalidation. Experiments on Robomimic, PushT, and RoboTwin benchmarks, alongside\nreal-world robotic evaluations, confirm that GPC consistently improves\nperformance and adaptability across a diverse set of tasks. Further analysis of\nalternative composition operators and weighting strategies offers insights into\nthe mechanisms underlying the success of GPC. These results establish GPC as a\nsimple yet effective method for improving control performance by leveraging\nexisting policies.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64780ba6f32a4117fd182b81/bsb_536O8r19Ocb6K03Ww.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01068.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64780ba6f32a4117fd182b81",
      "avatarUrl": "/avatars/85f01f4c6c745a04f04805462f9fe9c2.svg",
      "fullname": "CAO",
      "name": "SAGE2000",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "67ea9ecfc234715db8dbf339",
      "name": "hkuhk",
      "fullname": "The University of Hong Kong",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.02665",
      "authors": [
        {
          "_id": "68e31d5073e20ab577841f93",
          "name": "Shijian Deng",
          "hidden": false
        },
        {
          "_id": "68e31d5073e20ab577841f94",
          "name": "Kai Wang",
          "hidden": false
        },
        {
          "_id": "68e31d5073e20ab577841f95",
          "name": "Tianyu Yang",
          "hidden": false
        },
        {
          "_id": "68e31d5073e20ab577841f96",
          "name": "Harsh Singh",
          "hidden": false
        },
        {
          "_id": "68e31d5073e20ab577841f97",
          "name": "Yapeng Tian",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-03T01:48:26.000Z",
      "submittedOnDailyAt": "2025-10-06T00:10:23.995Z",
      "title": "Self-Improvement in Multimodal Large Language Models: A Survey",
      "submittedOnDailyBy": {
        "_id": "668bafa02c3897266dcbacd0",
        "avatarUrl": "/avatars/ef41cd4a855ebbe6efc79f34c38ef136.svg",
        "isPro": false,
        "fullname": "Shijian Deng",
        "user": "ShijianDeng",
        "type": "user"
      },
      "summary": "Recent advancements in self-improvement for Large Language Models (LLMs) have\nefficiently enhanced model capabilities without significantly increasing costs,\nparticularly in terms of human effort. While this area is still relatively\nyoung, its extension to the multimodal domain holds immense potential for\nleveraging diverse data sources and developing more general self-improving\nmodels. This survey is the first to provide a comprehensive overview of\nself-improvement in Multimodal LLMs (MLLMs). We provide a structured overview\nof the current literature and discuss methods from three perspectives: 1) data\ncollection, 2) data organization, and 3) model optimization, to facilitate the\nfurther development of self-improvement in MLLMs. We also include commonly used\nevaluations and downstream applications. Finally, we conclude by outlining open\nchallenges and future research directions.",
      "upvotes": 8,
      "discussionId": "68e31d5073e20ab577841f98",
      "ai_summary": "A survey of self-improvement methods in Multimodal Large Language Models (MLLMs) from data collection, organization, and model optimization perspectives.",
      "ai_keywords": [
        "Large Language Models",
        "Multimodal LLMs",
        "self-improvement",
        "data collection",
        "data organization",
        "model optimization",
        "evaluations",
        "downstream applications",
        "open challenges",
        "future research directions"
      ]
    },
    "publishedAt": "2025-10-02T21:48:26.000Z",
    "title": "Self-Improvement in Multimodal Large Language Models: A Survey",
    "summary": "Recent advancements in self-improvement for Large Language Models (LLMs) have\nefficiently enhanced model capabilities without significantly increasing costs,\nparticularly in terms of human effort. While this area is still relatively\nyoung, its extension to the multimodal domain holds immense potential for\nleveraging diverse data sources and developing more general self-improving\nmodels. This survey is the first to provide a comprehensive overview of\nself-improvement in Multimodal LLMs (MLLMs). We provide a structured overview\nof the current literature and discuss methods from three perspectives: 1) data\ncollection, 2) data organization, and 3) model optimization, to facilitate the\nfurther development of self-improvement in MLLMs. We also include commonly used\nevaluations and downstream applications. Finally, we conclude by outlining open\nchallenges and future research directions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02665.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "668bafa02c3897266dcbacd0",
      "avatarUrl": "/avatars/ef41cd4a855ebbe6efc79f34c38ef136.svg",
      "fullname": "Shijian Deng",
      "name": "ShijianDeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.03120",
      "authors": [
        {
          "_id": "68e322c073e20ab577841fbb",
          "name": "Zhaojun Sun",
          "hidden": false
        },
        {
          "_id": "68e322c073e20ab577841fbc",
          "name": "Xuzhou Zhu",
          "hidden": false
        },
        {
          "_id": "68e322c073e20ab577841fbd",
          "name": "Xuanhe Zhou",
          "hidden": false
        },
        {
          "_id": "68e322c073e20ab577841fbe",
          "name": "Xin Tong",
          "hidden": false
        },
        {
          "_id": "68e322c073e20ab577841fbf",
          "name": "Shuo Wang",
          "hidden": false
        },
        {
          "_id": "68e322c073e20ab577841fc0",
          "name": "Jie Fu",
          "hidden": false
        },
        {
          "_id": "68e322c073e20ab577841fc1",
          "name": "Guoliang Li",
          "hidden": false
        },
        {
          "_id": "68e322c073e20ab577841fc2",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "68e322c073e20ab577841fc3",
          "name": "Fan Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-03T15:49:09.000Z",
      "submittedOnDailyAt": "2025-10-06T00:30:41.651Z",
      "title": "SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Academic survey writing, which distills vast literature into a coherent and\ninsightful narrative, remains a labor-intensive and intellectually demanding\ntask. While recent approaches, such as general DeepResearch agents and\nsurvey-specialized methods, can generate surveys automatically (a.k.a.\nLLM4Survey), their outputs often fall short of human standards and there lacks\na rigorous, reader-aligned benchmark for thoroughly revealing their\ndeficiencies. To fill the gap, we propose a fine-grained, quiz-driven\nevaluation framework SurveyBench, featuring (1) typical survey topics source\nfrom recent 11,343 arXiv papers and corresponding 4,947 high-quality surveys;\n(2) a multifaceted metric hierarchy that assesses the outline quality (e.g.,\ncoverage breadth, logical coherence), content quality (e.g., synthesis\ngranularity, clarity of insights), and non-textual richness; and (3) a\ndual-mode evaluation protocol that includes content-based and quiz-based\nanswerability tests, explicitly aligned with readers' informational needs.\nResults show SurveyBench effectively challenges existing LLM4Survey approaches\n(e.g., on average 21% lower than human in content-based evaluation).",
      "upvotes": 5,
      "discussionId": "68e322c073e20ab577841fc4",
      "ai_summary": "A new evaluation framework, SurveyBench, assesses the quality of automatically generated academic surveys using a quiz-driven approach, revealing deficiencies in current LLM4Survey methods.",
      "ai_keywords": [
        "DeepResearch agents",
        "LLM4Survey",
        "SurveyBench",
        "arXiv papers",
        "high-quality surveys",
        "outline quality",
        "content quality",
        "synthesis granularity",
        "logical coherence",
        "non-textual richness",
        "content-based evaluation",
        "quiz-based evaluation"
      ]
    },
    "publishedAt": "2025-10-03T11:49:09.000Z",
    "title": "SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?",
    "summary": "Academic survey writing, which distills vast literature into a coherent and\ninsightful narrative, remains a labor-intensive and intellectually demanding\ntask. While recent approaches, such as general DeepResearch agents and\nsurvey-specialized methods, can generate surveys automatically (a.k.a.\nLLM4Survey), their outputs often fall short of human standards and there lacks\na rigorous, reader-aligned benchmark for thoroughly revealing their\ndeficiencies. To fill the gap, we propose a fine-grained, quiz-driven\nevaluation framework SurveyBench, featuring (1) typical survey topics source\nfrom recent 11,343 arXiv papers and corresponding 4,947 high-quality surveys;\n(2) a multifaceted metric hierarchy that assesses the outline quality (e.g.,\ncoverage breadth, logical coherence), content quality (e.g., synthesis\ngranularity, clarity of insights), and non-textual richness; and (3) a\ndual-mode evaluation protocol that includes content-based and quiz-based\nanswerability tests, explicitly aligned with readers' informational needs.\nResults show SurveyBench effectively challenges existing LLM4Survey approaches\n(e.g., on average 21% lower than human in content-based evaluation).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.03120.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 119
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.26354",
      "authors": [
        {
          "_id": "68dfe50f73e20ab5778419f0",
          "name": "Shuai Shao",
          "hidden": false
        },
        {
          "_id": "68dfe50f73e20ab5778419f1",
          "name": "Qihan Ren",
          "hidden": false
        },
        {
          "_id": "68dfe50f73e20ab5778419f2",
          "name": "Chen Qian",
          "hidden": false
        },
        {
          "_id": "68dfe50f73e20ab5778419f3",
          "name": "Boyi Wei",
          "hidden": false
        },
        {
          "_id": "68dfe50f73e20ab5778419f4",
          "name": "Dadi Guo",
          "hidden": false
        },
        {
          "_id": "68dfe50f73e20ab5778419f5",
          "user": {
            "_id": "64f73a44102fbfb26410962e",
            "avatarUrl": "/avatars/328302a495de6a4418be835456d1d3c6.svg",
            "isPro": false,
            "fullname": "jingyi Yang",
            "user": "JY-Young",
            "type": "user"
          },
          "name": "Jingyi Yang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-05T12:46:26.913Z",
          "hidden": false
        },
        {
          "_id": "68dfe50f73e20ab5778419f6",
          "name": "Xinhao Song",
          "hidden": false
        },
        {
          "_id": "68dfe50f73e20ab5778419f7",
          "name": "Linfeng Zhang",
          "hidden": false
        },
        {
          "_id": "68dfe50f73e20ab5778419f8",
          "name": "Weinan Zhang",
          "hidden": false
        },
        {
          "_id": "68dfe50f73e20ab5778419f9",
          "name": "Dongrui Liu",
          "hidden": false
        },
        {
          "_id": "68dfe50f73e20ab5778419fa",
          "name": "Jing Shao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-30T14:55:55.000Z",
      "submittedOnDailyAt": "2025-10-06T02:44:58.885Z",
      "title": "Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents",
      "submittedOnDailyBy": {
        "_id": "66e2624a436a1798365e4581",
        "avatarUrl": "/avatars/6c605807d34faa8fb505e135a4b47776.svg",
        "isPro": false,
        "fullname": "Qihan Ren",
        "user": "jasonrqh",
        "type": "user"
      },
      "summary": "Advances in Large Language Models (LLMs) have enabled a new class of\nself-evolving agents that autonomously improve through interaction with the\nenvironment, demonstrating strong capabilities. However, self-evolution also\nintroduces novel risks overlooked by current safety research. In this work, we\nstudy the case where an agent's self-evolution deviates in unintended ways,\nleading to undesirable or even harmful outcomes. We refer to this as\nMisevolution. To provide a systematic investigation, we evaluate misevolution\nalong four key evolutionary pathways: model, memory, tool, and workflow. Our\nempirical findings reveal that misevolution is a widespread risk, affecting\nagents built even on top-tier LLMs (e.g., Gemini-2.5-Pro). Different emergent\nrisks are observed in the self-evolutionary process, such as the degradation of\nsafety alignment after memory accumulation, or the unintended introduction of\nvulnerabilities in tool creation and reuse. To our knowledge, this is the first\nstudy to systematically conceptualize misevolution and provide empirical\nevidence of its occurrence, highlighting an urgent need for new safety\nparadigms for self-evolving agents. Finally, we discuss potential mitigation\nstrategies to inspire further research on building safer and more trustworthy\nself-evolving agents. Our code and data are available at\nhttps://github.com/ShaoShuai0605/Misevolution . Warning: this paper includes\nexamples that may be offensive or harmful in nature.",
      "upvotes": 5,
      "discussionId": "68dfe51073e20ab5778419fb",
      "githubRepo": "https://github.com/ShaoShuai0605/Misevolution",
      "ai_summary": "Self-evolving agents based on Large Language Models can deviate in unintended ways, leading to various risks such as safety misalignment and vulnerability introduction, necessitating new safety paradigms.",
      "ai_keywords": [
        "Large Language Models",
        "self-evolving agents",
        "Misevolution",
        "evolutionary pathways",
        "safety alignment",
        "memory accumulation",
        "tool creation",
        "vulnerability introduction"
      ],
      "githubStars": 6
    },
    "publishedAt": "2025-09-30T10:55:55.000Z",
    "title": "Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents",
    "summary": "Advances in Large Language Models (LLMs) have enabled a new class of\nself-evolving agents that autonomously improve through interaction with the\nenvironment, demonstrating strong capabilities. However, self-evolution also\nintroduces novel risks overlooked by current safety research. In this work, we\nstudy the case where an agent's self-evolution deviates in unintended ways,\nleading to undesirable or even harmful outcomes. We refer to this as\nMisevolution. To provide a systematic investigation, we evaluate misevolution\nalong four key evolutionary pathways: model, memory, tool, and workflow. Our\nempirical findings reveal that misevolution is a widespread risk, affecting\nagents built even on top-tier LLMs (e.g., Gemini-2.5-Pro). Different emergent\nrisks are observed in the self-evolutionary process, such as the degradation of\nsafety alignment after memory accumulation, or the unintended introduction of\nvulnerabilities in tool creation and reuse. To our knowledge, this is the first\nstudy to systematically conceptualize misevolution and provide empirical\nevidence of its occurrence, highlighting an urgent need for new safety\nparadigms for self-evolving agents. Finally, we discuss potential mitigation\nstrategies to inspire further research on building safer and more trustworthy\nself-evolving agents. Our code and data are available at\nhttps://github.com/ShaoShuai0605/Misevolution . Warning: this paper includes\nexamples that may be offensive or harmful in nature.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.26354.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66e2624a436a1798365e4581",
      "avatarUrl": "/avatars/6c605807d34faa8fb505e135a4b47776.svg",
      "fullname": "Qihan Ren",
      "name": "jasonrqh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.01879",
      "authors": [
        {
          "_id": "68e322f873e20ab577841fc6",
          "name": "Yisu Wang",
          "hidden": false
        },
        {
          "_id": "68e322f873e20ab577841fc7",
          "name": "Ming Wang",
          "hidden": false
        },
        {
          "_id": "68e322f873e20ab577841fc8",
          "name": "Haoyuan Song",
          "hidden": false
        },
        {
          "_id": "68e322f873e20ab577841fc9",
          "name": "Wenjie Huang",
          "hidden": false
        },
        {
          "_id": "68e322f873e20ab577841fca",
          "name": "Chaozheng Wang",
          "hidden": false
        },
        {
          "_id": "68e322f873e20ab577841fcb",
          "name": "Yi Xie",
          "hidden": false
        },
        {
          "_id": "68e322f873e20ab577841fcc",
          "name": "Xuming Ran",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-02T10:35:39.000Z",
      "submittedOnDailyAt": "2025-10-06T00:50:36.906Z",
      "title": "REPAIR: Robust Editing via Progressive Adaptive Intervention and\n  Reintegration",
      "submittedOnDailyBy": {
        "_id": "644378a96cea0db46dc96b39",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644378a96cea0db46dc96b39/wjGNaXjfhSdWr8kM6Amai.jpeg",
        "isPro": false,
        "fullname": "Ming Wang",
        "user": "sci-m-wang",
        "type": "user"
      },
      "summary": "Post-training for large language models (LLMs) is constrained by the high\ncost of acquiring new knowledge or correcting errors and by the unintended side\neffects that frequently arise from retraining. To address these issues, we\nintroduce REPAIR (Robust Editing via Progressive Adaptive Intervention and\nReintegration), a lifelong editing framework designed to support precise and\nlow-cost model updates while preserving non-target knowledge. REPAIR mitigates\nthe instability and conflicts of large-scale sequential edits through a\nclosed-loop feedback mechanism coupled with dynamic memory management.\nFurthermore, by incorporating frequent knowledge fusion and enforcing strong\nlocality guards, REPAIR effectively addresses the shortcomings of traditional\ndistribution-agnostic approaches that often overlook unintended ripple effects.\nOur experiments demonstrate that REPAIR boosts editing accuracy by 10%-30%\nacross multiple model families and significantly reduces knowledge forgetting.\nThis work introduces a robust framework for developing reliable, scalable, and\ncontinually evolving LLMs.",
      "upvotes": 4,
      "discussionId": "68e322f973e20ab577841fcd",
      "ai_summary": "REPAIR is a lifelong editing framework for large language models that enhances editing accuracy and reduces knowledge forgetting through progressive adaptive intervention and reintegration.",
      "ai_keywords": [
        "REPAIR",
        "Robust Editing",
        "Progressive Adaptive Intervention",
        "Reintegration",
        "lifelong editing framework",
        "large language models",
        "closed-loop feedback mechanism",
        "dynamic memory management",
        "knowledge fusion",
        "locality guards",
        "knowledge forgetting"
      ],
      "organization": {
        "_id": "68e32652529937e426ebee7c",
        "name": "ContiAI",
        "fullname": "ContiAI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e325231de2c8d44ab61237/ixA4ZfwwaFekwiLlMzWUy.png"
      }
    },
    "publishedAt": "2025-10-02T06:35:39.000Z",
    "title": "REPAIR: Robust Editing via Progressive Adaptive Intervention and\n  Reintegration",
    "summary": "Post-training for large language models (LLMs) is constrained by the high\ncost of acquiring new knowledge or correcting errors and by the unintended side\neffects that frequently arise from retraining. To address these issues, we\nintroduce REPAIR (Robust Editing via Progressive Adaptive Intervention and\nReintegration), a lifelong editing framework designed to support precise and\nlow-cost model updates while preserving non-target knowledge. REPAIR mitigates\nthe instability and conflicts of large-scale sequential edits through a\nclosed-loop feedback mechanism coupled with dynamic memory management.\nFurthermore, by incorporating frequent knowledge fusion and enforcing strong\nlocality guards, REPAIR effectively addresses the shortcomings of traditional\ndistribution-agnostic approaches that often overlook unintended ripple effects.\nOur experiments demonstrate that REPAIR boosts editing accuracy by 10%-30%\nacross multiple model families and significantly reduces knowledge forgetting.\nThis work introduces a robust framework for developing reliable, scalable, and\ncontinually evolving LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01879.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644378a96cea0db46dc96b39",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644378a96cea0db46dc96b39/wjGNaXjfhSdWr8kM6Amai.jpeg",
      "fullname": "Ming Wang",
      "name": "sci-m-wang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "68e32652529937e426ebee7c",
      "name": "ContiAI",
      "fullname": "ContiAI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e325231de2c8d44ab61237/ixA4ZfwwaFekwiLlMzWUy.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.03204",
      "authors": [
        {
          "_id": "68e323b473e20ab577841fcf",
          "name": "Imene Kerboua",
          "hidden": false
        },
        {
          "_id": "68e323b473e20ab577841fd0",
          "name": "Sahar Omidi Shayegan",
          "hidden": false
        },
        {
          "_id": "68e323b473e20ab577841fd1",
          "name": "Megh Thakkar",
          "hidden": false
        },
        {
          "_id": "68e323b473e20ab577841fd2",
          "name": "Xing Han Lù",
          "hidden": false
        },
        {
          "_id": "68e323b473e20ab577841fd3",
          "name": "Léo Boisvert",
          "hidden": false
        },
        {
          "_id": "68e323b473e20ab577841fd4",
          "name": "Massimo Caccia",
          "hidden": false
        },
        {
          "_id": "68e323b473e20ab577841fd5",
          "name": "Jérémy Espinas",
          "hidden": false
        },
        {
          "_id": "68e323b473e20ab577841fd6",
          "name": "Alexandre Aussem",
          "hidden": false
        },
        {
          "_id": "68e323b473e20ab577841fd7",
          "name": "Véronique Eglin",
          "hidden": false
        },
        {
          "_id": "68e323b473e20ab577841fd8",
          "name": "Alexandre Lacoste",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-03T17:41:30.000Z",
      "submittedOnDailyAt": "2025-10-06T00:34:46.420Z",
      "title": "FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of\n  Web Agents",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Web agents powered by large language models (LLMs) must process lengthy web\npage observations to complete user goals; these pages often exceed tens of\nthousands of tokens. This saturates context limits and increases computational\ncost processing; moreover, processing full pages exposes agents to security\nrisks such as prompt injection. Existing pruning strategies either discard\nrelevant content or retain irrelevant context, leading to suboptimal action\nprediction. We introduce FocusAgent, a simple yet effective approach that\nleverages a lightweight LLM retriever to extract the most relevant lines from\naccessibility tree (AxTree) observations, guided by task goals. By pruning\nnoisy and irrelevant content, FocusAgent enables efficient reasoning while\nreducing vulnerability to injection attacks. Experiments on WorkArena and\nWebArena benchmarks show that FocusAgent matches the performance of strong\nbaselines, while reducing observation size by over 50%. Furthermore, a variant\nof FocusAgent significantly reduces the success rate of prompt-injection\nattacks, including banner and pop-up attacks, while maintaining task success\nperformance in attack-free settings. Our results highlight that targeted\nLLM-based retrieval is a practical and robust strategy for building web agents\nthat are efficient, effective, and secure.",
      "upvotes": 3,
      "discussionId": "68e323b473e20ab577841fd9",
      "ai_summary": "FocusAgent uses a lightweight LLM retriever to extract relevant content from web page observations, improving efficiency and security in web agents.",
      "ai_keywords": [
        "large language models",
        "LLM",
        "accessibility tree",
        "AxTree",
        "prompt injection",
        "WorkArena",
        "WebArena",
        "targeted LLM-based retrieval"
      ]
    },
    "publishedAt": "2025-10-03T13:41:30.000Z",
    "title": "FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of\n  Web Agents",
    "summary": "Web agents powered by large language models (LLMs) must process lengthy web\npage observations to complete user goals; these pages often exceed tens of\nthousands of tokens. This saturates context limits and increases computational\ncost processing; moreover, processing full pages exposes agents to security\nrisks such as prompt injection. Existing pruning strategies either discard\nrelevant content or retain irrelevant context, leading to suboptimal action\nprediction. We introduce FocusAgent, a simple yet effective approach that\nleverages a lightweight LLM retriever to extract the most relevant lines from\naccessibility tree (AxTree) observations, guided by task goals. By pruning\nnoisy and irrelevant content, FocusAgent enables efficient reasoning while\nreducing vulnerability to injection attacks. Experiments on WorkArena and\nWebArena benchmarks show that FocusAgent matches the performance of strong\nbaselines, while reducing observation size by over 50%. Furthermore, a variant\nof FocusAgent significantly reduces the success rate of prompt-injection\nattacks, including banner and pop-up attacks, while maintaining task success\nperformance in attack-free settings. Our results highlight that targeted\nLLM-based retrieval is a practical and robust strategy for building web agents\nthat are efficient, effective, and secure.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.03204.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 119
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.03194",
      "authors": [
        {
          "_id": "68e33f3573e20ab577842038",
          "name": "Zichen Chen",
          "hidden": false
        },
        {
          "_id": "68e33f3573e20ab577842039",
          "name": "Jiefeng Chen",
          "hidden": false
        },
        {
          "_id": "68e33f3573e20ab57784203a",
          "name": "Sercan Ö. Arik",
          "hidden": false
        },
        {
          "_id": "68e33f3573e20ab57784203b",
          "name": "Misha Sra",
          "hidden": false
        },
        {
          "_id": "68e33f3573e20ab57784203c",
          "name": "Tomas Pfister",
          "hidden": false
        },
        {
          "_id": "68e33f3573e20ab57784203d",
          "name": "Jinsung Yoon",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-03T17:30:16.000Z",
      "submittedOnDailyAt": "2025-10-06T04:36:21.820Z",
      "title": "CoDA: Agentic Systems for Collaborative Data Visualization",
      "submittedOnDailyBy": {
        "_id": "653df1323479e9ebbe3eb6cc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
        "isPro": true,
        "fullname": "Zhangchen Xu",
        "user": "zhangchenxu",
        "type": "user"
      },
      "summary": "Deep research has revolutionized data analysis, yet data scientists still\ndevote substantial time to manually crafting visualizations, highlighting the\nneed for robust automation from natural language queries. However, current\nsystems struggle with complex datasets containing multiple files and iterative\nrefinement. Existing approaches, including simple single- or multi-agent\nsystems, often oversimplify the task, focusing on initial query parsing while\nfailing to robustly manage data complexity, code errors, or final visualization\nquality. In this paper, we reframe this challenge as a collaborative\nmulti-agent problem. We introduce CoDA, a multi-agent system that employs\nspecialized LLM agents for metadata analysis, task planning, code generation,\nand self-reflection. We formalize this pipeline, demonstrating how\nmetadata-focused analysis bypasses token limits and quality-driven refinement\nensures robustness. Extensive evaluations show CoDA achieves substantial gains\nin the overall score, outperforming competitive baselines by up to 41.5%. This\nwork demonstrates that the future of visualization automation lies not in\nisolated code generation but in integrated, collaborative agentic workflows.",
      "upvotes": 3,
      "discussionId": "68e33f3573e20ab57784203e",
      "ai_summary": "CoDA, a multi-agent system using specialized LLM agents, enhances visualization automation by managing data complexity and ensuring high-quality visualizations through collaborative workflows.",
      "ai_keywords": [
        "LLM agents",
        "metadata analysis",
        "task planning",
        "code generation",
        "self-reflection",
        "token limits",
        "quality-driven refinement"
      ],
      "organization": {
        "_id": "5e6aca39878b8b2bf9806447",
        "name": "google",
        "fullname": "Google",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
      }
    },
    "publishedAt": "2025-10-03T13:30:16.000Z",
    "title": "CoDA: Agentic Systems for Collaborative Data Visualization",
    "summary": "Deep research has revolutionized data analysis, yet data scientists still\ndevote substantial time to manually crafting visualizations, highlighting the\nneed for robust automation from natural language queries. However, current\nsystems struggle with complex datasets containing multiple files and iterative\nrefinement. Existing approaches, including simple single- or multi-agent\nsystems, often oversimplify the task, focusing on initial query parsing while\nfailing to robustly manage data complexity, code errors, or final visualization\nquality. In this paper, we reframe this challenge as a collaborative\nmulti-agent problem. We introduce CoDA, a multi-agent system that employs\nspecialized LLM agents for metadata analysis, task planning, code generation,\nand self-reflection. We formalize this pipeline, demonstrating how\nmetadata-focused analysis bypasses token limits and quality-driven refinement\nensures robustness. Extensive evaluations show CoDA achieves substantial gains\nin the overall score, outperforming competitive baselines by up to 41.5%. This\nwork demonstrates that the future of visualization automation lies not in\nisolated code generation but in integrated, collaborative agentic workflows.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.03194.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653df1323479e9ebbe3eb6cc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653df1323479e9ebbe3eb6cc/K_g-r1iMRNKj99LXPuYF3.jpeg",
      "fullname": "Zhangchen Xu",
      "name": "zhangchenxu",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 20
    },
    "organization": {
      "_id": "5e6aca39878b8b2bf9806447",
      "name": "google",
      "fullname": "Google",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.03230",
      "authors": [
        {
          "_id": "68e31e7c73e20ab577841f9f",
          "name": "Suyuchen Wang",
          "hidden": false
        },
        {
          "_id": "68e31e7c73e20ab577841fa0",
          "name": "Tianyu Zhang",
          "hidden": false
        },
        {
          "_id": "68e31e7c73e20ab577841fa1",
          "name": "Ahmed Masry",
          "hidden": false
        },
        {
          "_id": "68e31e7c73e20ab577841fa2",
          "name": "Christopher Pal",
          "hidden": false
        },
        {
          "_id": "68e31e7c73e20ab577841fa3",
          "name": "Spandana Gella",
          "hidden": false
        },
        {
          "_id": "68e31e7c73e20ab577841fa4",
          "name": "Bang Liu",
          "hidden": false
        },
        {
          "_id": "68e31e7c73e20ab577841fa5",
          "name": "Perouz Taslakian",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-03T17:59:34.000Z",
      "submittedOnDailyAt": "2025-10-06T00:12:29.606Z",
      "title": "Improving GUI Grounding with Explicit Position-to-Coordinate Mapping",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "GUI grounding, the task of mapping natural-language instructions to pixel\ncoordinates, is crucial for autonomous agents, yet remains difficult for\ncurrent VLMs. The core bottleneck is reliable patch-to-pixel mapping, which\nbreaks when extrapolating to high-resolution displays unseen during training.\nCurrent approaches generate coordinates as text tokens directly from visual\nfeatures, forcing the model to infer complex position-to-pixel mappings\nimplicitly; as a result, accuracy degrades and failures proliferate on new\nresolutions. We address this with two complementary innovations. First, RULER\ntokens serve as explicit coordinate markers, letting the model reference\npositions similar to gridlines on a map and adjust rather than generate\ncoordinates from scratch. Second, Interleaved MRoPE (I-MRoPE) improves spatial\nencoding by ensuring that width and height dimensions are represented equally,\naddressing the asymmetry of standard positional schemes. Experiments on\nScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in\ngrounding accuracy, with the largest improvements on high-resolution\ninterfaces. By providing explicit spatial guidance rather than relying on\nimplicit learning, our approach enables more reliable GUI automation across\ndiverse resolutions and platforms.",
      "upvotes": 2,
      "discussionId": "68e31e7c73e20ab577841fa6",
      "ai_summary": "Explicit coordinate markers and improved spatial encoding enhance GUI grounding accuracy across diverse resolutions and platforms.",
      "ai_keywords": [
        "RULER tokens",
        "Interleaved MRoPE",
        "I-MRoPE",
        "spatial encoding",
        "GUI grounding",
        "ScreenSpot",
        "ScreenSpot-V2",
        "ScreenSpot-Pro"
      ]
    },
    "publishedAt": "2025-10-03T13:59:34.000Z",
    "title": "Improving GUI Grounding with Explicit Position-to-Coordinate Mapping",
    "summary": "GUI grounding, the task of mapping natural-language instructions to pixel\ncoordinates, is crucial for autonomous agents, yet remains difficult for\ncurrent VLMs. The core bottleneck is reliable patch-to-pixel mapping, which\nbreaks when extrapolating to high-resolution displays unseen during training.\nCurrent approaches generate coordinates as text tokens directly from visual\nfeatures, forcing the model to infer complex position-to-pixel mappings\nimplicitly; as a result, accuracy degrades and failures proliferate on new\nresolutions. We address this with two complementary innovations. First, RULER\ntokens serve as explicit coordinate markers, letting the model reference\npositions similar to gridlines on a map and adjust rather than generate\ncoordinates from scratch. Second, Interleaved MRoPE (I-MRoPE) improves spatial\nencoding by ensuring that width and height dimensions are represented equally,\naddressing the asymmetry of standard positional schemes. Experiments on\nScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in\ngrounding accuracy, with the largest improvements on high-resolution\ninterfaces. By providing explicit spatial guidance rather than relying on\nimplicit learning, our approach enables more reliable GUI automation across\ndiverse resolutions and platforms.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.03230.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 119
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.01459",
      "authors": [
        {
          "_id": "68e3449a73e20ab57784206e",
          "name": "Weizhe Chen",
          "hidden": false
        },
        {
          "_id": "68e3449a73e20ab57784206f",
          "name": "Sven Koenig",
          "hidden": false
        },
        {
          "_id": "68e3449a73e20ab577842070",
          "name": "Bistra Dilkina",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-01T20:57:22.000Z",
      "submittedOnDailyAt": "2025-10-06T03:06:08.194Z",
      "title": "LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM\n  Reasoning",
      "submittedOnDailyBy": {
        "_id": "649c99644cc14193293d522b",
        "avatarUrl": "/avatars/e7e0052d310a0b3a1c4b303150d4f914.svg",
        "isPro": false,
        "fullname": "weizhech",
        "user": "weizhech",
        "type": "user"
      },
      "summary": "Since the release of Deepseek-R1, reinforcement learning with verifiable\nrewards (RLVR) has become a central approach for training large language models\n(LLMs) on reasoning tasks. Recent work has largely focused on modifying loss\nfunctions to make RLVR more efficient and effective. In this paper, motivated\nby studies of overthinking in LLMs, we propose Length-aware Sampling for Policy\nOptimization (LSPO), a novel meta-RLVR algorithm that dynamically selects\ntraining data at each step based on the average response length. We evaluate\nLSPO across multiple base models and datasets, demonstrating that it\nconsistently improves learning effectiveness. In addition, we conduct a\ndetailed ablation study to examine alternative ways of incorporating length\nsignals into dynamic sampling, offering further insights and highlighting\npromising directions for future research.",
      "upvotes": 2,
      "discussionId": "68e3449a73e20ab577842071",
      "githubRepo": "https://github.com/laonahongchen/LSPO",
      "ai_summary": "Length-aware Sampling for Policy Optimization (LSPO) is a meta-RLVR algorithm that dynamically selects training data based on response length, improving learning effectiveness in large language models.",
      "ai_keywords": [
        "reinforcement learning with verifiable rewards",
        "RLVR",
        "large language models",
        "LLMs",
        "meta-RLVR",
        "Length-aware Sampling for Policy Optimization",
        "LSPO",
        "dynamic sampling",
        "length signals",
        "ablation study"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "66a403d0dcb5bbc6e98bb7d0",
        "name": "UniversityofSouthernCalifornia",
        "fullname": "University of Southern California",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a403728069e3c30e0d8524/tkYCfeIJfF1FxtYiRZ8bf.png"
      }
    },
    "publishedAt": "2025-10-01T16:57:22.000Z",
    "title": "LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM\n  Reasoning",
    "summary": "Since the release of Deepseek-R1, reinforcement learning with verifiable\nrewards (RLVR) has become a central approach for training large language models\n(LLMs) on reasoning tasks. Recent work has largely focused on modifying loss\nfunctions to make RLVR more efficient and effective. In this paper, motivated\nby studies of overthinking in LLMs, we propose Length-aware Sampling for Policy\nOptimization (LSPO), a novel meta-RLVR algorithm that dynamically selects\ntraining data at each step based on the average response length. We evaluate\nLSPO across multiple base models and datasets, demonstrating that it\nconsistently improves learning effectiveness. In addition, we conduct a\ndetailed ablation study to examine alternative ways of incorporating length\nsignals into dynamic sampling, offering further insights and highlighting\npromising directions for future research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01459.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "649c99644cc14193293d522b",
      "avatarUrl": "/avatars/e7e0052d310a0b3a1c4b303150d4f914.svg",
      "fullname": "weizhech",
      "name": "weizhech",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "66a403d0dcb5bbc6e98bb7d0",
      "name": "UniversityofSouthernCalifornia",
      "fullname": "University of Southern California",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a403728069e3c30e0d8524/tkYCfeIJfF1FxtYiRZ8bf.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2509.25771",
      "authors": [
        {
          "_id": "68e336df73e20ab577842024",
          "name": "Jia Jun Cheng Xian",
          "hidden": false
        },
        {
          "_id": "68e336df73e20ab577842025",
          "name": "Muchen Li",
          "hidden": false
        },
        {
          "_id": "68e336df73e20ab577842026",
          "name": "Haotian Yang",
          "hidden": false
        },
        {
          "_id": "68e336df73e20ab577842027",
          "name": "Xin Tao",
          "hidden": false
        },
        {
          "_id": "68e336df73e20ab577842028",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "68e336df73e20ab577842029",
          "name": "Leonid Sigal",
          "hidden": false
        },
        {
          "_id": "68e336df73e20ab57784202a",
          "name": "Renjie Liao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-09-30T04:32:34.000Z",
      "submittedOnDailyAt": "2025-10-06T01:59:44.420Z",
      "title": "Free Lunch Alignment of Text-to-Image Diffusion Models without\n  Preference Image Pairs",
      "submittedOnDailyBy": {
        "_id": "62be41bacf4665894c7d6950",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62be41bacf4665894c7d6950/XdQy0NZpNxkvLt816CXKh.jpeg",
        "isPro": false,
        "fullname": "Muchen Li",
        "user": "jojo23333",
        "type": "user"
      },
      "summary": "Recent advances in diffusion-based text-to-image (T2I) models have led to\nremarkable success in generating high-quality images from textual prompts.\nHowever, ensuring accurate alignment between the text and the generated image\nremains a significant challenge for state-of-the-art diffusion models. To\naddress this, existing studies employ reinforcement learning with human\nfeedback (RLHF) to align T2I outputs with human preferences. These methods,\nhowever, either rely directly on paired image preference data or require a\nlearned reward function, both of which depend heavily on costly, high-quality\nhuman annotations and thus face scalability limitations. In this work, we\nintroduce Text Preference Optimization (TPO), a framework that enables\n\"free-lunch\" alignment of T2I models, achieving alignment without the need for\npaired image preference data. TPO works by training the model to prefer matched\nprompts over mismatched prompts, which are constructed by perturbing original\ncaptions using a large language model. Our framework is general and compatible\nwith existing preference-based algorithms. We extend both DPO and KTO to our\nsetting, resulting in TDPO and TKTO. Quantitative and qualitative evaluations\nacross multiple benchmarks show that our methods consistently outperform their\noriginal counterparts, delivering better human preference scores and improved\ntext-to-image alignment. Our Open-source code is available at\nhttps://github.com/DSL-Lab/T2I-Free-Lunch-Alignment.",
      "upvotes": 2,
      "discussionId": "68e336e073e20ab57784202b",
      "ai_summary": "A new framework, Text Preference Optimization (TPO), aligns text-to-image models with human preferences without requiring paired image preference data, improving text-to-image alignment and human preference scores.",
      "ai_keywords": [
        "diffusion-based text-to-image",
        "T2I models",
        "reinforcement learning with human feedback",
        "RLHF",
        "Text Preference Optimization",
        "TPO",
        "matched prompts",
        "mismatched prompts",
        "large language model",
        "DPO",
        "KTO",
        "TDPO",
        "TKTO"
      ],
      "organization": {
        "_id": "67481e994bcfe97d0e7f242d",
        "name": "UBC-V",
        "fullname": "University of British Columbia",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66c75a8a060b6c3335d28ee7/5qUvu4_6J60ULUQaMYZ2C.png"
      }
    },
    "publishedAt": "2025-09-30T00:32:34.000Z",
    "title": "Free Lunch Alignment of Text-to-Image Diffusion Models without\n  Preference Image Pairs",
    "summary": "Recent advances in diffusion-based text-to-image (T2I) models have led to\nremarkable success in generating high-quality images from textual prompts.\nHowever, ensuring accurate alignment between the text and the generated image\nremains a significant challenge for state-of-the-art diffusion models. To\naddress this, existing studies employ reinforcement learning with human\nfeedback (RLHF) to align T2I outputs with human preferences. These methods,\nhowever, either rely directly on paired image preference data or require a\nlearned reward function, both of which depend heavily on costly, high-quality\nhuman annotations and thus face scalability limitations. In this work, we\nintroduce Text Preference Optimization (TPO), a framework that enables\n\"free-lunch\" alignment of T2I models, achieving alignment without the need for\npaired image preference data. TPO works by training the model to prefer matched\nprompts over mismatched prompts, which are constructed by perturbing original\ncaptions using a large language model. Our framework is general and compatible\nwith existing preference-based algorithms. We extend both DPO and KTO to our\nsetting, resulting in TDPO and TKTO. Quantitative and qualitative evaluations\nacross multiple benchmarks show that our methods consistently outperform their\noriginal counterparts, delivering better human preference scores and improved\ntext-to-image alignment. Our Open-source code is available at\nhttps://github.com/DSL-Lab/T2I-Free-Lunch-Alignment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.25771.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62be41bacf4665894c7d6950",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62be41bacf4665894c7d6950/XdQy0NZpNxkvLt816CXKh.jpeg",
      "fullname": "Muchen Li",
      "name": "jojo23333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "67481e994bcfe97d0e7f242d",
      "name": "UBC-V",
      "fullname": "University of British Columbia",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66c75a8a060b6c3335d28ee7/5qUvu4_6J60ULUQaMYZ2C.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.03160",
      "authors": [
        {
          "_id": "68e3295d73e20ab577841fde",
          "name": "Ming Zhao",
          "hidden": false
        },
        {
          "_id": "68e3295d73e20ab577841fdf",
          "name": "Wenhui Dong",
          "hidden": false
        },
        {
          "_id": "68e3295d73e20ab577841fe0",
          "name": "Yang Zhang",
          "hidden": false
        },
        {
          "_id": "68e3295d73e20ab577841fe1",
          "name": "Xiang Zheng",
          "hidden": false
        },
        {
          "_id": "68e3295d73e20ab577841fe2",
          "name": "Zhonghao Zhang",
          "hidden": false
        },
        {
          "_id": "68e3295d73e20ab577841fe3",
          "name": "Zian Zhou",
          "hidden": false
        },
        {
          "_id": "68e3295d73e20ab577841fe4",
          "name": "Yunzhi Guan",
          "hidden": false
        },
        {
          "_id": "68e3295d73e20ab577841fe5",
          "name": "Liukun Xu",
          "hidden": false
        },
        {
          "_id": "68e3295d73e20ab577841fe6",
          "name": "Wei Peng",
          "hidden": false
        },
        {
          "_id": "68e3295d73e20ab577841fe7",
          "name": "Zhaoyang Gong",
          "hidden": false
        },
        {
          "_id": "68e3295d73e20ab577841fe8",
          "name": "Zhicheng Zhang",
          "hidden": false
        },
        {
          "_id": "68e3295d73e20ab577841fe9",
          "name": "Dachuan Li",
          "hidden": false
        },
        {
          "_id": "68e3295d73e20ab577841fea",
          "name": "Xiaosheng Ma",
          "hidden": false
        },
        {
          "_id": "68e3295d73e20ab577841feb",
          "name": "Yuli Ma",
          "hidden": false
        },
        {
          "_id": "68e3295d73e20ab577841fec",
          "name": "Jianing Ni",
          "hidden": false
        },
        {
          "_id": "68e3295d73e20ab577841fed",
          "name": "Changjiang Jiang",
          "hidden": false
        },
        {
          "_id": "68e3295d73e20ab577841fee",
          "name": "Lixia Tian",
          "hidden": false
        },
        {
          "_id": "68e3295d73e20ab577841fef",
          "name": "Qixin Chen",
          "hidden": false
        },
        {
          "_id": "68e3295d73e20ab577841ff0",
          "name": "Kaishun Xia",
          "hidden": false
        },
        {
          "_id": "68e3295d73e20ab577841ff1",
          "name": "Pingping Liu",
          "hidden": false
        },
        {
          "_id": "68e3295d73e20ab577841ff2",
          "name": "Tongshun Zhang",
          "hidden": false
        },
        {
          "_id": "68e3295d73e20ab577841ff3",
          "name": "Zhiqiang Liu",
          "hidden": false
        },
        {
          "_id": "68e3295d73e20ab577841ff4",
          "name": "Zhongan Bi",
          "hidden": false
        },
        {
          "_id": "68e3295d73e20ab577841ff5",
          "name": "Chenyang Si",
          "hidden": false
        },
        {
          "_id": "68e3295d73e20ab577841ff6",
          "name": "Tiansheng Sun",
          "hidden": false
        },
        {
          "_id": "68e3295d73e20ab577841ff7",
          "name": "Caifeng Shan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-03T16:32:02.000Z",
      "submittedOnDailyAt": "2025-10-06T00:58:52.442Z",
      "title": "SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the\n  SpineMed-450k Corpus",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Spine disorders affect 619 million people globally and are a leading cause of\ndisability, yet AI-assisted diagnosis remains limited by the lack of\nlevel-aware, multimodal datasets. Clinical decision-making for spine disorders\nrequires sophisticated reasoning across X-ray, CT, and MRI at specific\nvertebral levels. However, progress has been constrained by the absence of\ntraceable, clinically-grounded instruction data and standardized,\nspine-specific benchmarks. To address this, we introduce SpineMed, an ecosystem\nco-designed with practicing spine surgeons. It features SpineMed-450k, the\nfirst large-scale dataset explicitly designed for vertebral-level reasoning\nacross imaging modalities with over 450,000 instruction instances, and\nSpineBench, a clinically-grounded evaluation framework. SpineMed-450k is\ncurated from diverse sources, including textbooks, guidelines, open datasets,\nand ~1,000 de-identified hospital cases, using a clinician-in-the-loop pipeline\nwith a two-stage LLM generation method (draft and revision) to ensure\nhigh-quality, traceable data for question-answering, multi-turn consultations,\nand report generation. SpineBench evaluates models on clinically salient axes,\nincluding level identification, pathology assessment, and surgical planning.\nOur comprehensive evaluation of several recently advanced large vision-language\nmodels (LVLMs) on SpineBench reveals systematic weaknesses in fine-grained,\nlevel-specific reasoning. In contrast, our model fine-tuned on SpineMed-450k\ndemonstrates consistent and significant improvements across all tasks.\nClinician assessments confirm the diagnostic clarity and practical utility of\nour model's outputs.",
      "upvotes": 1,
      "discussionId": "68e3295d73e20ab577841ff8",
      "ai_summary": "SpineMed, an ecosystem with SpineMed-450k and SpineBench, addresses the lack of level-aware, multimodal datasets and benchmarks for AI-assisted diagnosis of spine disorders, improving model performance through fine-grained, level-specific reasoning.",
      "ai_keywords": [
        "vertebral-level reasoning",
        "X-ray",
        "CT",
        "MRI",
        "instruction data",
        "spine-specific benchmarks",
        "SpineMed",
        "SpineMed-450k",
        "SpineBench",
        "clinician-in-the-loop pipeline",
        "LLM generation method",
        "large vision-language models (LVLMs)",
        "fine-grained",
        "level-specific reasoning"
      ]
    },
    "publishedAt": "2025-10-03T12:32:02.000Z",
    "title": "SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the\n  SpineMed-450k Corpus",
    "summary": "Spine disorders affect 619 million people globally and are a leading cause of\ndisability, yet AI-assisted diagnosis remains limited by the lack of\nlevel-aware, multimodal datasets. Clinical decision-making for spine disorders\nrequires sophisticated reasoning across X-ray, CT, and MRI at specific\nvertebral levels. However, progress has been constrained by the absence of\ntraceable, clinically-grounded instruction data and standardized,\nspine-specific benchmarks. To address this, we introduce SpineMed, an ecosystem\nco-designed with practicing spine surgeons. It features SpineMed-450k, the\nfirst large-scale dataset explicitly designed for vertebral-level reasoning\nacross imaging modalities with over 450,000 instruction instances, and\nSpineBench, a clinically-grounded evaluation framework. SpineMed-450k is\ncurated from diverse sources, including textbooks, guidelines, open datasets,\nand ~1,000 de-identified hospital cases, using a clinician-in-the-loop pipeline\nwith a two-stage LLM generation method (draft and revision) to ensure\nhigh-quality, traceable data for question-answering, multi-turn consultations,\nand report generation. SpineBench evaluates models on clinically salient axes,\nincluding level identification, pathology assessment, and surgical planning.\nOur comprehensive evaluation of several recently advanced large vision-language\nmodels (LVLMs) on SpineBench reveals systematic weaknesses in fine-grained,\nlevel-specific reasoning. In contrast, our model fine-tuned on SpineMed-450k\ndemonstrates consistent and significant improvements across all tasks.\nClinician assessments confirm the diagnostic clarity and practical utility of\nour model's outputs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.03160.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 119
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.02571",
      "authors": [
        {
          "_id": "68e31f7073e20ab577841fb6",
          "name": "Zhiting Mei",
          "hidden": false
        },
        {
          "_id": "68e31f7073e20ab577841fb7",
          "name": "Ola Shorinwa",
          "hidden": false
        },
        {
          "_id": "68e31f7073e20ab577841fb8",
          "name": "Anirudha Majumdar",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-02T21:20:41.000Z",
      "submittedOnDailyAt": "2025-10-06T00:16:32.486Z",
      "title": "How Confident are Video Models? Empowering Video Models to Express their\n  Uncertainty",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Generative video models demonstrate impressive text-to-video capabilities,\nspurring widespread adoption in many real-world applications. However, like\nlarge language models (LLMs), video generation models tend to hallucinate,\nproducing plausible videos even when they are factually wrong. Although\nuncertainty quantification (UQ) of LLMs has been extensively studied in prior\nwork, no UQ method for video models exists, raising critical safety concerns.\nTo our knowledge, this paper represents the first work towards quantifying the\nuncertainty of video models. We present a framework for uncertainty\nquantification of generative video models, consisting of: (i) a metric for\nevaluating the calibration of video models based on robust rank correlation\nestimation with no stringent modeling assumptions; (ii) a black-box UQ method\nfor video models (termed S-QUBED), which leverages latent modeling to\nrigorously decompose predictive uncertainty into its aleatoric and epistemic\ncomponents; and (iii) a UQ dataset to facilitate benchmarking calibration in\nvideo models. By conditioning the generation task in the latent space, we\ndisentangle uncertainty arising due to vague task specifications from that\narising from lack of knowledge. Through extensive experiments on benchmark\nvideo datasets, we demonstrate that S-QUBED computes calibrated total\nuncertainty estimates that are negatively correlated with the task accuracy and\neffectively computes the aleatoric and epistemic constituents.",
      "upvotes": 1,
      "discussionId": "68e31f7073e20ab577841fb9",
      "projectPage": "https://s-qubed.github.io/",
      "githubRepo": "https://github.com/irom-princeton/s-qubed",
      "ai_summary": "A framework for uncertainty quantification in generative video models is introduced, including a metric for calibration, a black-box method called S-QUBED, and a benchmark dataset, demonstrating improved uncertainty estimates and task accuracy.",
      "ai_keywords": [
        "generative video models",
        "text-to-video",
        "uncertainty quantification",
        "UQ",
        "large language models",
        "LLMs",
        "robust rank correlation estimation",
        "latent modeling",
        "aleatoric uncertainty",
        "epistemic uncertainty",
        "latent space",
        "benchmark video datasets"
      ],
      "githubStars": 1
    },
    "publishedAt": "2025-10-02T17:20:41.000Z",
    "title": "How Confident are Video Models? Empowering Video Models to Express their\n  Uncertainty",
    "summary": "Generative video models demonstrate impressive text-to-video capabilities,\nspurring widespread adoption in many real-world applications. However, like\nlarge language models (LLMs), video generation models tend to hallucinate,\nproducing plausible videos even when they are factually wrong. Although\nuncertainty quantification (UQ) of LLMs has been extensively studied in prior\nwork, no UQ method for video models exists, raising critical safety concerns.\nTo our knowledge, this paper represents the first work towards quantifying the\nuncertainty of video models. We present a framework for uncertainty\nquantification of generative video models, consisting of: (i) a metric for\nevaluating the calibration of video models based on robust rank correlation\nestimation with no stringent modeling assumptions; (ii) a black-box UQ method\nfor video models (termed S-QUBED), which leverages latent modeling to\nrigorously decompose predictive uncertainty into its aleatoric and epistemic\ncomponents; and (iii) a UQ dataset to facilitate benchmarking calibration in\nvideo models. By conditioning the generation task in the latent space, we\ndisentangle uncertainty arising due to vague task specifications from that\narising from lack of knowledge. Through extensive experiments on benchmark\nvideo datasets, we demonstrate that S-QUBED computes calibrated total\nuncertainty estimates that are negatively correlated with the task accuracy and\neffectively computes the aleatoric and epistemic constituents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.02571.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 119
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.01354",
      "authors": [
        {
          "_id": "68e35b7c73e20ab5778420c8",
          "name": "Yinuo Liu",
          "hidden": false
        },
        {
          "_id": "68e35b7c73e20ab5778420c9",
          "name": "Ruohan Xu",
          "hidden": false
        },
        {
          "_id": "68e35b7c73e20ab5778420ca",
          "name": "Xilong Wang",
          "hidden": false
        },
        {
          "_id": "68e35b7c73e20ab5778420cb",
          "name": "Yuqi Jia",
          "hidden": false
        },
        {
          "_id": "68e35b7c73e20ab5778420cc",
          "name": "Neil Zhenqiang Gong",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-01T18:34:06.000Z",
      "submittedOnDailyAt": "2025-10-06T04:39:23.341Z",
      "title": "WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents",
      "submittedOnDailyBy": {
        "_id": "65daeafd27be2e8c651b46b5",
        "avatarUrl": "/avatars/fb67d528ee3487698789bdd8b0562707.svg",
        "isPro": false,
        "fullname": "Yinuo Liu",
        "user": "Norrrrrrr",
        "type": "user"
      },
      "summary": "Multiple prompt injection attacks have been proposed against web agents. At\nthe same time, various methods have been developed to detect general prompt\ninjection attacks, but none have been systematically evaluated for web agents.\nIn this work, we bridge this gap by presenting the first comprehensive\nbenchmark study on detecting prompt injection attacks targeting web agents. We\nbegin by introducing a fine-grained categorization of such attacks based on the\nthreat model. We then construct datasets containing both malicious and benign\nsamples: malicious text segments generated by different attacks, benign text\nsegments from four categories, malicious images produced by attacks, and benign\nimages from two categories. Next, we systematize both text-based and\nimage-based detection methods. Finally, we evaluate their performance across\nmultiple scenarios. Our key findings show that while some detectors can\nidentify attacks that rely on explicit textual instructions or visible image\nperturbations with moderate to high accuracy, they largely fail against attacks\nthat omit explicit instructions or employ imperceptible perturbations. Our\ndatasets and code are released at:\nhttps://github.com/Norrrrrrr-lyn/WAInjectBench.",
      "upvotes": 1,
      "discussionId": "68e35b7c73e20ab5778420cd",
      "ai_summary": "A comprehensive benchmark study evaluates the detection of prompt injection attacks against web agents, revealing that current detectors perform well against explicit attacks but struggle with subtle ones.",
      "ai_keywords": [
        "prompt injection attacks",
        "web agents",
        "threat model",
        "malicious text segments",
        "benign text segments",
        "malicious images",
        "benign images",
        "text-based detection",
        "image-based detection"
      ]
    },
    "publishedAt": "2025-10-01T14:34:06.000Z",
    "title": "WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents",
    "summary": "Multiple prompt injection attacks have been proposed against web agents. At\nthe same time, various methods have been developed to detect general prompt\ninjection attacks, but none have been systematically evaluated for web agents.\nIn this work, we bridge this gap by presenting the first comprehensive\nbenchmark study on detecting prompt injection attacks targeting web agents. We\nbegin by introducing a fine-grained categorization of such attacks based on the\nthreat model. We then construct datasets containing both malicious and benign\nsamples: malicious text segments generated by different attacks, benign text\nsegments from four categories, malicious images produced by attacks, and benign\nimages from two categories. Next, we systematize both text-based and\nimage-based detection methods. Finally, we evaluate their performance across\nmultiple scenarios. Our key findings show that while some detectors can\nidentify attacks that rely on explicit textual instructions or visible image\nperturbations with moderate to high accuracy, they largely fail against attacks\nthat omit explicit instructions or employ imperceptible perturbations. Our\ndatasets and code are released at:\nhttps://github.com/Norrrrrrr-lyn/WAInjectBench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01354.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65daeafd27be2e8c651b46b5",
      "avatarUrl": "/avatars/fb67d528ee3487698789bdd8b0562707.svg",
      "fullname": "Yinuo Liu",
      "name": "Norrrrrrr",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.01132",
      "authors": [
        {
          "_id": "68e36caf73e20ab5778420dc",
          "name": "Ruiyi Wang",
          "hidden": false
        },
        {
          "_id": "68e36caf73e20ab5778420dd",
          "name": "Prithviraj Ammanabrolu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-01T17:23:04.000Z",
      "submittedOnDailyAt": "2025-10-06T05:47:23.219Z",
      "title": "A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "631fec6e5ba8c026340fdda0",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Dn8NzKCYNkBiiinFEyQz5.png",
        "isPro": false,
        "fullname": "Ruiyi Wang",
        "user": "Pamela153",
        "type": "user"
      },
      "summary": "We study what actually works and what doesn't for training large language\nmodels as agents via multi-turn reinforcement learning. Despite rapid progress,\nexisting frameworks and definitions are fragmented, and there is no systematic\nformulation or analysis of which design choices matter across tasks. We address\nthis gap by first breaking down the design space into three inter-related\npillars -- environment, reward, and policy -- and empirically derive a recipe\nfor training LLM agents in situated textual domains. In particular, we test\nTextWorld and ALFWorld, popular domains for testing situated embodied\nreasoning, as well as SWE-Gym for more software engineering style tasks. (i)\nFor the environment, we analyze the impacts of task complexity in terms of\nsizes of the state and action spaces as well as optimal solution length,\nfinding that even simple environments within a domain can provide signal on how\nwell an agent can generalize to more complex tasks. (ii) For the reward, we\nablate relative reward sparsity, observing that while dense turn-level rewards\naccelerate training, performance and stability is highly dependent on the\nchoice of RL algorithm. (iii) And for the agent's policy, we explore the\ninterplay between reward sparsity and biased (PPO, GRPO) and unbiased (RLOO)\npolicy gradient methods in addition to showing how to find the optimal\nSupervised Fine-tuning (SFT) to RL training ratio given a fixed budget. We\ndistill these findings into a training recipe that guides co-design across the\nthree pillars, facilitating research and practical efforts in multi-turn\nagentic RL. Code: https://github.com/pearls-lab/meow-tea-taro",
      "upvotes": 1,
      "discussionId": "68e36caf73e20ab5778420de",
      "ai_summary": "Research identifies key design choices for training large language models as agents via multi-turn reinforcement learning, focusing on environment complexity, reward sparsity, and policy methods.",
      "ai_keywords": [
        "multi-turn reinforcement learning",
        "large language models",
        "TextWorld",
        "ALFWorld",
        "SWE-Gym",
        "task complexity",
        "state space",
        "action space",
        "optimal solution length",
        "reward sparsity",
        "dense turn-level rewards",
        "RL algorithms",
        "PPO",
        "GRPO",
        "RLOO",
        "policy gradient methods",
        "Supervised Fine-tuning",
        "SFT",
        "RL training ratio"
      ],
      "organization": {
        "_id": "66b51d6d778c98d29426ed8e",
        "name": "PEARLS-Lab",
        "fullname": "PEARLS Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/632d2f986bcb864974d68cb1/oFKNoU8q9eIvtNHFEB7IN.png"
      }
    },
    "publishedAt": "2025-10-01T13:23:04.000Z",
    "title": "A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning",
    "summary": "We study what actually works and what doesn't for training large language\nmodels as agents via multi-turn reinforcement learning. Despite rapid progress,\nexisting frameworks and definitions are fragmented, and there is no systematic\nformulation or analysis of which design choices matter across tasks. We address\nthis gap by first breaking down the design space into three inter-related\npillars -- environment, reward, and policy -- and empirically derive a recipe\nfor training LLM agents in situated textual domains. In particular, we test\nTextWorld and ALFWorld, popular domains for testing situated embodied\nreasoning, as well as SWE-Gym for more software engineering style tasks. (i)\nFor the environment, we analyze the impacts of task complexity in terms of\nsizes of the state and action spaces as well as optimal solution length,\nfinding that even simple environments within a domain can provide signal on how\nwell an agent can generalize to more complex tasks. (ii) For the reward, we\nablate relative reward sparsity, observing that while dense turn-level rewards\naccelerate training, performance and stability is highly dependent on the\nchoice of RL algorithm. (iii) And for the agent's policy, we explore the\ninterplay between reward sparsity and biased (PPO, GRPO) and unbiased (RLOO)\npolicy gradient methods in addition to showing how to find the optimal\nSupervised Fine-tuning (SFT) to RL training ratio given a fixed budget. We\ndistill these findings into a training recipe that guides co-design across the\nthree pillars, facilitating research and practical efforts in multi-turn\nagentic RL. Code: https://github.com/pearls-lab/meow-tea-taro",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.01132.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631fec6e5ba8c026340fdda0",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Dn8NzKCYNkBiiinFEyQz5.png",
      "fullname": "Ruiyi Wang",
      "name": "Pamela153",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "organization": {
      "_id": "66b51d6d778c98d29426ed8e",
      "name": "PEARLS-Lab",
      "fullname": "PEARLS Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/632d2f986bcb864974d68cb1/oFKNoU8q9eIvtNHFEB7IN.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.00658",
      "authors": [
        {
          "_id": "68e3605973e20ab5778420cf",
          "name": "Beomsu Kim",
          "hidden": false
        },
        {
          "_id": "68e3605973e20ab5778420d0",
          "name": "Byunghee Cha",
          "hidden": false
        },
        {
          "_id": "68e3605973e20ab5778420d1",
          "name": "Jong Chul Ye",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-01T08:35:18.000Z",
      "submittedOnDailyAt": "2025-10-06T04:54:58.646Z",
      "title": "Align Your Tangent: Training Better Consistency Models via\n  Manifold-Aligned Tangents",
      "submittedOnDailyBy": {
        "_id": "63db3a610cc3bc12bc0cfe2f",
        "avatarUrl": "/avatars/9926d709f2505c4758f9f66fc7a2c8e2.svg",
        "isPro": false,
        "fullname": "ByungHeeCha",
        "user": "paulcha1025",
        "type": "user"
      },
      "summary": "With diffusion and flow matching models achieving state-of-the-art generating\nperformance, the interest of the community now turned to reducing the inference\ntime without sacrificing sample quality. Consistency Models (CMs), which are\ntrained to be consistent on diffusion or probability flow ordinary differential\nequation (PF-ODE) trajectories, enable one or two-step flow or diffusion\nsampling. However, CMs typically require prolonged training with large batch\nsizes to obtain competitive sample quality. In this paper, we examine the\ntraining dynamics of CMs near convergence and discover that CM tangents -- CM\noutput update directions -- are quite oscillatory, in the sense that they move\nparallel to the data manifold, not towards the manifold. To mitigate\noscillatory tangents, we propose a new loss function, called the manifold\nfeature distance (MFD), which provides manifold-aligned tangents that point\ntoward the data manifold. Consequently, our method -- dubbed Align Your Tangent\n(AYT) -- can accelerate CM training by orders of magnitude and even out-perform\nthe learned perceptual image patch similarity metric (LPIPS). Furthermore, we\nfind that our loss enables training with extremely small batch sizes without\ncompromising sample quality. Code: https://github.com/1202kbs/AYT",
      "upvotes": 0,
      "discussionId": "68e3605a73e20ab5778420d2",
      "ai_summary": "Align Your Tangent (AYT) improves Consistency Model training by reducing oscillatory tangents and enabling faster convergence with small batch sizes.",
      "ai_keywords": [
        "diffusion models",
        "flow matching models",
        "Consistency Models",
        "CMs",
        "diffusion sampling",
        "probability flow ordinary differential equation",
        "PF-ODE",
        "training dynamics",
        "CM tangents",
        "manifold feature distance",
        "MFD",
        "Align Your Tangent",
        "AYT",
        "learned perceptual image patch similarity metric",
        "LPIPS"
      ]
    },
    "publishedAt": "2025-10-01T04:35:18.000Z",
    "title": "Align Your Tangent: Training Better Consistency Models via\n  Manifold-Aligned Tangents",
    "summary": "With diffusion and flow matching models achieving state-of-the-art generating\nperformance, the interest of the community now turned to reducing the inference\ntime without sacrificing sample quality. Consistency Models (CMs), which are\ntrained to be consistent on diffusion or probability flow ordinary differential\nequation (PF-ODE) trajectories, enable one or two-step flow or diffusion\nsampling. However, CMs typically require prolonged training with large batch\nsizes to obtain competitive sample quality. In this paper, we examine the\ntraining dynamics of CMs near convergence and discover that CM tangents -- CM\noutput update directions -- are quite oscillatory, in the sense that they move\nparallel to the data manifold, not towards the manifold. To mitigate\noscillatory tangents, we propose a new loss function, called the manifold\nfeature distance (MFD), which provides manifold-aligned tangents that point\ntoward the data manifold. Consequently, our method -- dubbed Align Your Tangent\n(AYT) -- can accelerate CM training by orders of magnitude and even out-perform\nthe learned perceptual image patch similarity metric (LPIPS). Furthermore, we\nfind that our loss enables training with extremely small batch sizes without\ncompromising sample quality. Code: https://github.com/1202kbs/AYT",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.00658.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63db3a610cc3bc12bc0cfe2f",
      "avatarUrl": "/avatars/9926d709f2505c4758f9f66fc7a2c8e2.svg",
      "fullname": "ByungHeeCha",
      "name": "paulcha1025",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]