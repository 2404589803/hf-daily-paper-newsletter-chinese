[
    {
        "paper": {
            "id": "2410.02740",
            "authors": [
                {
                    "_id": "66ff559f1e0b212adc0da561",
                    "user": {
                        "_id": "66b5295f83425904fa7a1a6a",
                        "avatarUrl": "/avatars/a35568fb933ceef7451bd88fb3d5ab17.svg",
                        "isPro": false,
                        "fullname": "Zhengfeng Lai",
                        "user": "jefflai",
                        "type": "user"
                    },
                    "name": "Zhengfeng Lai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-04T07:31:00.892Z",
                    "hidden": false
                },
                {
                    "_id": "66ff559f1e0b212adc0da562",
                    "name": "Vasileios Saveris",
                    "hidden": false
                },
                {
                    "_id": "66ff559f1e0b212adc0da563",
                    "name": "Chen Chen",
                    "hidden": false
                },
                {
                    "_id": "66ff559f1e0b212adc0da564",
                    "user": {
                        "_id": "66fb920d63d612c629bfea36",
                        "avatarUrl": "/avatars/86f5ccd4a15e8394792b422fefa64b69.svg",
                        "isPro": false,
                        "fullname": "Chen",
                        "user": "Hong-You",
                        "type": "user"
                    },
                    "name": "Hong-You Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-04T07:31:04.373Z",
                    "hidden": false
                },
                {
                    "_id": "66ff559f1e0b212adc0da565",
                    "user": {
                        "_id": "631516348d85ad332fa47b2c",
                        "avatarUrl": "/avatars/100f5ae3cf3c52faaecdaecd5d8f2881.svg",
                        "isPro": false,
                        "fullname": "Haotian Zhang",
                        "user": "haotiz",
                        "type": "user"
                    },
                    "name": "Haotian Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-04T07:31:02.875Z",
                    "hidden": false
                },
                {
                    "_id": "66ff559f1e0b212adc0da566",
                    "name": "Bowen Zhang",
                    "hidden": false
                },
                {
                    "_id": "66ff559f1e0b212adc0da567",
                    "name": "Juan Lao Tebar",
                    "hidden": false
                },
                {
                    "_id": "66ff559f1e0b212adc0da568",
                    "name": "Wenze Hu",
                    "hidden": false
                },
                {
                    "_id": "66ff559f1e0b212adc0da569",
                    "name": "Zhe Gan",
                    "hidden": false
                },
                {
                    "_id": "66ff559f1e0b212adc0da56a",
                    "name": "Peter Grasch",
                    "hidden": false
                },
                {
                    "_id": "66ff559f1e0b212adc0da56b",
                    "name": "Meng Cao",
                    "hidden": false
                },
                {
                    "_id": "66ff559f1e0b212adc0da56c",
                    "name": "Yinfei Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-03T17:54:52.000Z",
            "title": "Revisit Large-Scale Image-Caption Data in Pre-training Multimodal\n  Foundation Models",
            "summary": "Recent advancements in multimodal models highlight the value of rewritten\ncaptions for improving performance, yet key challenges remain. For example,\nwhile synthetic captions often provide superior quality and image-text\nalignment, it is not clear whether they can fully replace AltTexts: the role of\nsynthetic captions and their interaction with original web-crawled AltTexts in\npre-training is still not well understood. Moreover, different multimodal\nfoundation models may have unique preferences for specific caption formats, but\nefforts to identify the optimal captions for each model remain limited. In this\nwork, we propose a novel, controllable, and scalable captioning pipeline\ndesigned to generate diverse caption formats tailored to various multimodal\nmodels. By examining Short Synthetic Captions (SSC) towards Dense Synthetic\nCaptions (DSC+) as case studies, we systematically explore their effects and\ninteractions with AltTexts across models such as CLIP, multimodal LLMs, and\ndiffusion models. Our findings reveal that a hybrid approach that keeps both\nsynthetic captions and AltTexts can outperform the use of synthetic captions\nalone, improving both alignment and performance, with each model demonstrating\npreferences for particular caption formats. This comprehensive analysis\nprovides valuable insights into optimizing captioning strategies, thereby\nadvancing the pre-training of multimodal foundation models.",
            "upvotes": 34,
            "discussionId": "66ff55a21e0b212adc0da6bb"
        },
        "publishedAt": "2024-10-04T01:11:19.304Z",
        "title": "Revisit Large-Scale Image-Caption Data in Pre-training Multimodal Foundation Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02740.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/a35568fb933ceef7451bd88fb3d5ab17.svg",
            "fullname": "Zhengfeng Lai",
            "name": "jefflai",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.02757",
            "authors": [
                {
                    "_id": "66ff590f1687036580ad3899",
                    "user": {
                        "_id": "63ea23b9dedfeebe54d02bdf",
                        "avatarUrl": "/avatars/4d9f9a546aa8c63e277161ea700075c4.svg",
                        "isPro": false,
                        "fullname": "Yuqing Wang",
                        "user": "Epiphqny",
                        "type": "user"
                    },
                    "name": "Yuqing Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-04T07:30:54.287Z",
                    "hidden": false
                },
                {
                    "_id": "66ff590f1687036580ad389a",
                    "name": "Tianwei Xiong",
                    "hidden": false
                },
                {
                    "_id": "66ff590f1687036580ad389b",
                    "name": "Daquan Zhou",
                    "hidden": false
                },
                {
                    "_id": "66ff590f1687036580ad389c",
                    "user": {
                        "_id": "64415957bd0c9726529802f6",
                        "avatarUrl": "/avatars/1132d1ee68fb58ec635d57c8175caacd.svg",
                        "isPro": false,
                        "fullname": "Zhijie Lin",
                        "user": "Ikuinen",
                        "type": "user"
                    },
                    "name": "Zhijie Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-04T08:35:48.411Z",
                    "hidden": false
                },
                {
                    "_id": "66ff590f1687036580ad389d",
                    "name": "Yang Zhao",
                    "hidden": false
                },
                {
                    "_id": "66ff590f1687036580ad389e",
                    "user": {
                        "_id": "647b5fef6a79fbf5e996c47c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647b5fef6a79fbf5e996c47c/IkSMnDsCY_CyEFCiMDuxe.jpeg",
                        "isPro": false,
                        "fullname": "Bingyi Kang",
                        "user": "bykang",
                        "type": "user"
                    },
                    "name": "Bingyi Kang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-04T08:36:06.799Z",
                    "hidden": false
                },
                {
                    "_id": "66ff590f1687036580ad389f",
                    "name": "Jiashi Feng",
                    "hidden": false
                },
                {
                    "_id": "66ff590f1687036580ad38a0",
                    "user": {
                        "_id": "65d5ec74cd05bc1eaa125040",
                        "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg",
                        "isPro": false,
                        "fullname": "Xihui Liu",
                        "user": "XihuiLiu",
                        "type": "user"
                    },
                    "name": "Xihui Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-04T08:26:25.925Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-03T17:59:02.000Z",
            "title": "Loong: Generating Minute-level Long Videos with Autoregressive Language\n  Models",
            "summary": "It is desirable but challenging to generate content-rich long videos in the\nscale of minutes. Autoregressive large language models (LLMs) have achieved\ngreat success in generating coherent and long sequences of tokens in the domain\nof natural language processing, while the exploration of autoregressive LLMs\nfor video generation is limited to generating short videos of several seconds.\nIn this work, we conduct a deep analysis of the challenges that prevent\nautoregressive LLM-based video generators from generating long videos. Based on\nthe observations and analysis, we propose Loong, a new autoregressive LLM-based\nvideo generator that can generate minute-long videos. Specifically, we model\nthe text tokens and video tokens as a unified sequence for autoregressive LLMs\nand train the model from scratch. We propose progressive short-to-long training\nwith a loss re-weighting scheme to mitigate the loss imbalance problem for long\nvideo training. We further investigate inference strategies, including video\ntoken re-encoding and sampling strategies, to diminish error accumulation\nduring inference. Our proposed Loong can be trained on 10-second videos and be\nextended to generate minute-level long videos conditioned on text prompts, as\ndemonstrated by the results. More samples are available at:\nhttps://epiphqny.github.io/Loong-video.",
            "upvotes": 26,
            "discussionId": "66ff59141687036580ad3b19"
        },
        "publishedAt": "2024-10-04T01:25:31.407Z",
        "title": "Loong: Generating Minute-level Long Videos with Autoregressive Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02757.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.02713",
            "authors": [
                {
                    "_id": "66ff4f5e5a359c1af1169664",
                    "user": {
                        "_id": "62a993d80472c0b7f94027df",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a993d80472c0b7f94027df/j5vp-IwLA2YBexylUHiQU.png",
                        "isPro": false,
                        "fullname": "Zhang Yuanhan",
                        "user": "ZhangYuanhan",
                        "type": "user"
                    },
                    "name": "Yuanhan Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-04T08:39:48.736Z",
                    "hidden": false
                },
                {
                    "_id": "66ff4f5e5a359c1af1169665",
                    "user": {
                        "_id": "652fbe8cb2acab0b82f855a6",
                        "avatarUrl": "/avatars/c35672b2229bb4f986ef01c61211c3f2.svg",
                        "isPro": false,
                        "fullname": "Jinming Wu",
                        "user": "kimingng",
                        "type": "user"
                    },
                    "name": "Jinming Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-04T08:40:01.301Z",
                    "hidden": false
                },
                {
                    "_id": "66ff4f5e5a359c1af1169666",
                    "name": "Wei Li",
                    "hidden": false
                },
                {
                    "_id": "66ff4f5e5a359c1af1169667",
                    "name": "Bo Li",
                    "hidden": false
                },
                {
                    "_id": "66ff4f5e5a359c1af1169668",
                    "name": "Zejun Ma",
                    "hidden": false
                },
                {
                    "_id": "66ff4f5e5a359c1af1169669",
                    "user": {
                        "_id": "62ab1ac1d48b4d8b048a3473",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png",
                        "isPro": false,
                        "fullname": "Ziwei Liu",
                        "user": "liuziwei7",
                        "type": "user"
                    },
                    "name": "Ziwei Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-04T08:40:14.432Z",
                    "hidden": false
                },
                {
                    "_id": "66ff4f5e5a359c1af116966a",
                    "user": {
                        "_id": "62aba526cae4462c0c6caa0f",
                        "avatarUrl": "/avatars/430560ec2c2547f819225769ab432f30.svg",
                        "isPro": false,
                        "fullname": "Chunyuan Li",
                        "user": "Chunyuan24",
                        "type": "user"
                    },
                    "name": "Chunyuan Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-04T08:40:08.501Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-03T17:36:49.000Z",
            "title": "Video Instruction Tuning With Synthetic Data",
            "summary": "The development of video large multimodal models (LMMs) has been hindered by\nthe difficulty of curating large amounts of high-quality raw data from the web.\nTo address this, we propose an alternative approach by creating a high-quality\nsynthetic dataset specifically for video instruction-following, namely\nLLaVA-Video-178K. This dataset includes key tasks such as detailed captioning,\nopen-ended question-answering (QA), and multiple-choice QA. By training on this\ndataset, in combination with existing visual instruction tuning data, we\nintroduce LLaVA-Video, a new video LMM. Our experiments demonstrate that\nLLaVA-Video achieves strong performance across various video benchmarks,\nhighlighting the effectiveness of our dataset. We plan to release the dataset,\nits generation pipeline, and the model checkpoints.",
            "upvotes": 26,
            "discussionId": "66ff4f625a359c1af116974f"
        },
        "publishedAt": "2024-10-04T00:45:02.023Z",
        "title": "Video Instruction Tuning With Synthetic Data",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02713.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a993d80472c0b7f94027df/j5vp-IwLA2YBexylUHiQU.png",
            "fullname": "Zhang Yuanhan",
            "name": "ZhangYuanhan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.02712",
            "authors": [
                {
                    "_id": "66ff51ed9e1143bff207d587",
                    "user": {
                        "_id": "6570977f87a92b76922c9950",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6570977f87a92b76922c9950/AQGto1w6ugBvH2yCV46YU.jpeg",
                        "isPro": false,
                        "fullname": "Tianyi Xiong",
                        "user": "txiong23",
                        "type": "user"
                    },
                    "name": "Tianyi Xiong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-04T08:41:36.661Z",
                    "hidden": false
                },
                {
                    "_id": "66ff51ed9e1143bff207d588",
                    "user": {
                        "_id": "655fed9fdef5905d38b84af3",
                        "avatarUrl": "/avatars/2cda4182dfd11a1e94743639e62328ea.svg",
                        "isPro": false,
                        "fullname": "Xiyao Wang",
                        "user": "russwang",
                        "type": "user"
                    },
                    "name": "Xiyao Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-04T08:44:04.812Z",
                    "hidden": false
                },
                {
                    "_id": "66ff51ed9e1143bff207d589",
                    "name": "Dong Guo",
                    "hidden": false
                },
                {
                    "_id": "66ff51ed9e1143bff207d58a",
                    "name": "Qinghao Ye",
                    "hidden": false
                },
                {
                    "_id": "66ff51ed9e1143bff207d58b",
                    "name": "Haoqi Fan",
                    "hidden": false
                },
                {
                    "_id": "66ff51ed9e1143bff207d58c",
                    "user": {
                        "_id": "64c039128e2612254356bba5",
                        "avatarUrl": "/avatars/06cc76feebba0cc80ebb8f4ff86f6d9b.svg",
                        "isPro": false,
                        "fullname": "Quanquan Gu",
                        "user": "thughost",
                        "type": "user"
                    },
                    "name": "Quanquan Gu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-04T08:41:11.820Z",
                    "hidden": false
                },
                {
                    "_id": "66ff51ed9e1143bff207d58d",
                    "user": {
                        "_id": "6527bc6b34bf5ece73da426d",
                        "avatarUrl": "/avatars/120739a9ac84e7319d9ea157a63dc547.svg",
                        "isPro": false,
                        "fullname": "henghuang",
                        "user": "henghuang",
                        "type": "user"
                    },
                    "name": "Heng Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-04T08:43:29.377Z",
                    "hidden": false
                },
                {
                    "_id": "66ff51ed9e1143bff207d58e",
                    "user": {
                        "_id": "62aba526cae4462c0c6caa0f",
                        "avatarUrl": "/avatars/430560ec2c2547f819225769ab432f30.svg",
                        "isPro": false,
                        "fullname": "Chunyuan Li",
                        "user": "Chunyuan24",
                        "type": "user"
                    },
                    "name": "Chunyuan Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-04T08:43:01.990Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-03T17:36:33.000Z",
            "title": "LLaVA-Critic: Learning to Evaluate Multimodal Models",
            "summary": "We introduce LLaVA-Critic, the first open-source large multimodal model (LMM)\ndesigned as a generalist evaluator to assess performance across a wide range of\nmultimodal tasks. LLaVA-Critic is trained using a high-quality critic\ninstruction-following dataset that incorporates diverse evaluation criteria and\nscenarios. Our experiments demonstrate the model's effectiveness in two key\nareas: (1) LMM-as-a-Judge, where LLaVA-Critic provides reliable evaluation\nscores, performing on par with or surpassing GPT models on multiple evaluation\nbenchmarks; and (2) Preference Learning, where it generates reward signals for\npreference learning, enhancing model alignment capabilities. This work\nunderscores the potential of open-source LMMs in self-critique and evaluation,\nsetting the stage for future research into scalable, superhuman alignment\nfeedback mechanisms for LMMs.",
            "upvotes": 22,
            "discussionId": "66ff51ee9e1143bff207d5d8"
        },
        "publishedAt": "2024-10-04T00:55:11.521Z",
        "title": "LLaVA-Critic: Learning to Evaluate Multimodal Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02712.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/06cc76feebba0cc80ebb8f4ff86f6d9b.svg",
            "fullname": "Quanquan Gu",
            "name": "thughost",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.02746",
            "authors": [
                {
                    "_id": "66ff590b39485e454983f18e",
                    "user": {
                        "_id": "66fb920d63d612c629bfea36",
                        "avatarUrl": "/avatars/86f5ccd4a15e8394792b422fefa64b69.svg",
                        "isPro": false,
                        "fullname": "Chen",
                        "user": "Hong-You",
                        "type": "user"
                    },
                    "name": "Hong-You Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-04T07:30:57.578Z",
                    "hidden": false
                },
                {
                    "_id": "66ff590b39485e454983f18f",
                    "user": {
                        "_id": "66b5295f83425904fa7a1a6a",
                        "avatarUrl": "/avatars/a35568fb933ceef7451bd88fb3d5ab17.svg",
                        "isPro": false,
                        "fullname": "Zhengfeng Lai",
                        "user": "jefflai",
                        "type": "user"
                    },
                    "name": "Zhengfeng Lai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-04T07:30:55.894Z",
                    "hidden": false
                },
                {
                    "_id": "66ff590b39485e454983f190",
                    "user": {
                        "_id": "631516348d85ad332fa47b2c",
                        "avatarUrl": "/avatars/100f5ae3cf3c52faaecdaecd5d8f2881.svg",
                        "isPro": false,
                        "fullname": "Haotian Zhang",
                        "user": "haotiz",
                        "type": "user"
                    },
                    "name": "Haotian Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-04T07:30:59.170Z",
                    "hidden": false
                },
                {
                    "_id": "66ff590b39485e454983f191",
                    "name": "Xinze Wang",
                    "hidden": false
                },
                {
                    "_id": "66ff590b39485e454983f192",
                    "name": "Marcin Eichner",
                    "hidden": false
                },
                {
                    "_id": "66ff590b39485e454983f193",
                    "name": "Keen You",
                    "hidden": false
                },
                {
                    "_id": "66ff590b39485e454983f194",
                    "name": "Meng Cao",
                    "hidden": false
                },
                {
                    "_id": "66ff590b39485e454983f195",
                    "name": "Bowen Zhang",
                    "hidden": false
                },
                {
                    "_id": "66ff590b39485e454983f196",
                    "name": "Yinfei Yang",
                    "hidden": false
                },
                {
                    "_id": "66ff590b39485e454983f197",
                    "name": "Zhe Gan",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-03T17:56:09.000Z",
            "title": "Contrastive Localized Language-Image Pre-Training",
            "summary": "Contrastive Language-Image Pre-training (CLIP) has been a celebrated method\nfor training vision encoders to generate image/text representations\nfacilitating various applications. Recently, CLIP has been widely adopted as\nthe vision backbone of multimodal large language models (MLLMs) to connect\nimage inputs for language interactions. The success of CLIP as a\nvision-language foundation model relies on aligning web-crawled noisy text\nannotations at image levels. Nevertheless, such criteria may become\ninsufficient for downstream tasks in need of fine-grained vision\nrepresentations, especially when region-level understanding is demanding for\nMLLMs. In this paper, we improve the localization capability of CLIP with\nseveral advances. We propose a pre-training method called Contrastive Localized\nLanguage-Image Pre-training (CLOC) by complementing CLIP with region-text\ncontrastive loss and modules. We formulate a new concept, promptable\nembeddings, of which the encoder produces image embeddings easy to transform\ninto region representations given spatial hints. To support large-scale\npre-training, we design a visually-enriched and spatially-localized captioning\nframework to effectively generate region-text pseudo-labels at scale. By\nscaling up to billions of annotated images, CLOC enables high-quality regional\nembeddings for image region recognition and retrieval tasks, and can be a\ndrop-in replacement of CLIP to enhance MLLMs, especially on referring and\ngrounding tasks.",
            "upvotes": 18,
            "discussionId": "66ff590c39485e454983f258"
        },
        "publishedAt": "2024-10-04T01:25:38.775Z",
        "title": "Contrastive Localized Language-Image Pre-Training",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02746.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/100f5ae3cf3c52faaecdaecd5d8f2881.svg",
            "fullname": "Haotian Zhang",
            "name": "haotiz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.01679",
            "authors": [
                {
                    "_id": "66fe2d6da77ec468fff0bdf7",
                    "user": {
                        "_id": "63458f12d54fb141dedac508",
                        "avatarUrl": "/avatars/3946fb9c23d1cd24037770cc0a3489bf.svg",
                        "isPro": false,
                        "fullname": "Amirhossein Kazemnejad",
                        "user": "kazemnejad",
                        "type": "user"
                    },
                    "name": "Amirhossein Kazemnejad",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-04T07:31:53.800Z",
                    "hidden": false
                },
                {
                    "_id": "66fe2d6da77ec468fff0bdf8",
                    "name": "Milad Aghajohari",
                    "hidden": false
                },
                {
                    "_id": "66fe2d6da77ec468fff0bdf9",
                    "name": "Eva Portelance",
                    "hidden": false
                },
                {
                    "_id": "66fe2d6da77ec468fff0bdfa",
                    "name": "Alessandro Sordoni",
                    "hidden": false
                },
                {
                    "_id": "66fe2d6da77ec468fff0bdfb",
                    "name": "Siva Reddy",
                    "hidden": false
                },
                {
                    "_id": "66fe2d6da77ec468fff0bdfc",
                    "name": "Aaron Courville",
                    "hidden": false
                },
                {
                    "_id": "66fe2d6da77ec468fff0bdfd",
                    "name": "Nicolas Le Roux",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-02T15:49:30.000Z",
            "title": "VinePPO: Unlocking RL Potential For LLM Reasoning Through Refined Credit\n  Assignment",
            "summary": "Large language models (LLMs) are increasingly applied to complex reasoning\ntasks that require executing several complex steps before receiving any reward.\nProperly assigning credit to these steps is essential for enhancing model\nperformance. Proximal Policy Optimization (PPO), a state-of-the-art\nreinforcement learning (RL) algorithm used for LLM finetuning, employs value\nnetworks to tackle credit assignment. However, value networks face challenges\nin predicting the expected cumulative rewards accurately in complex reasoning\ntasks, often leading to high-variance updates and suboptimal performance. In\nthis work, we systematically evaluate the efficacy of value networks and reveal\ntheir significant shortcomings in reasoning-heavy LLM tasks, showing that they\nbarely outperform a random baseline when comparing alternative steps. To\naddress this, we propose VinePPO, a straightforward approach that leverages the\nflexibility of language environments to compute unbiased Monte Carlo-based\nestimates, bypassing the need for large value networks. Our method consistently\noutperforms PPO and other RL-free baselines across MATH and GSM8K datasets with\nfewer gradient updates (up to 9x), less wall-clock time (up to 3.0x). These\nresults emphasize the importance of accurate credit assignment in RL finetuning\nof LLM and demonstrate VinePPO's potential as a superior alternative.",
            "upvotes": 13,
            "discussionId": "66fe2d6fa77ec468fff0be82"
        },
        "publishedAt": "2024-10-04T12:05:54.914Z",
        "title": "VinePPO: Unlocking RL Potential For LLM Reasoning Through Refined Credit Assignment",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.01679.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/3946fb9c23d1cd24037770cc0a3489bf.svg",
            "fullname": "Amirhossein Kazemnejad",
            "name": "kazemnejad",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.02073",
            "authors": [
                {
                    "_id": "66ff615b81382f012c08a66f",
                    "name": "Aleksei Bochkovskii",
                    "hidden": false
                },
                {
                    "_id": "66ff615b81382f012c08a670",
                    "user": {
                        "_id": "665ebfb7a57b277d7a202fb1",
                        "avatarUrl": "/avatars/15538783a4fbc4688db85d1b49cd24e2.svg",
                        "isPro": false,
                        "fullname": "Amael Delaunoy",
                        "user": "amael-apple",
                        "type": "user"
                    },
                    "name": "Amaël Delaunoy",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-04T04:41:38.750Z",
                    "hidden": false
                },
                {
                    "_id": "66ff615b81382f012c08a671",
                    "name": "Hugo Germain",
                    "hidden": false
                },
                {
                    "_id": "66ff615b81382f012c08a672",
                    "user": {
                        "_id": "65986fd3f0102bce68e15717",
                        "avatarUrl": "/avatars/da161214fb43d8bc3c07f98df0554e73.svg",
                        "isPro": false,
                        "fullname": "Marcel Santoso",
                        "user": "msantoso98",
                        "type": "user"
                    },
                    "name": "Marcel Santos",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-04T08:46:54.308Z",
                    "hidden": false
                },
                {
                    "_id": "66ff615b81382f012c08a673",
                    "name": "Yichao Zhou",
                    "hidden": false
                },
                {
                    "_id": "66ff615b81382f012c08a674",
                    "user": {
                        "_id": "66ff713ea98cf0a69f8d5cd0",
                        "avatarUrl": "/avatars/4b77834fce727f3af99f8fb77e646609.svg",
                        "isPro": false,
                        "fullname": "Stephan Richter",
                        "user": "srrichter",
                        "type": "user"
                    },
                    "name": "Stephan R. Richter",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-04T07:23:22.709Z",
                    "hidden": false
                },
                {
                    "_id": "66ff615b81382f012c08a675",
                    "name": "Vladlen Koltun",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-02T22:42:20.000Z",
            "title": "Depth Pro: Sharp Monocular Metric Depth in Less Than a Second",
            "summary": "We present a foundation model for zero-shot metric monocular depth\nestimation. Our model, Depth Pro, synthesizes high-resolution depth maps with\nunparalleled sharpness and high-frequency details. The predictions are metric,\nwith absolute scale, without relying on the availability of metadata such as\ncamera intrinsics. And the model is fast, producing a 2.25-megapixel depth map\nin 0.3 seconds on a standard GPU. These characteristics are enabled by a number\nof technical contributions, including an efficient multi-scale vision\ntransformer for dense prediction, a training protocol that combines real and\nsynthetic datasets to achieve high metric accuracy alongside fine boundary\ntracing, dedicated evaluation metrics for boundary accuracy in estimated depth\nmaps, and state-of-the-art focal length estimation from a single image.\nExtensive experiments analyze specific design choices and demonstrate that\nDepth Pro outperforms prior work along multiple dimensions. We release code and\nweights at https://github.com/apple/ml-depth-pro",
            "upvotes": 13,
            "discussionId": "66ff616181382f012c08a85d"
        },
        "publishedAt": "2024-10-04T02:00:44.143Z",
        "title": "Depth Pro: Sharp Monocular Metric Depth in Less Than a Second",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02073.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.02724",
            "authors": [
                {
                    "_id": "66ffa1073187ada66f619f7d",
                    "name": "Oussama Zekri",
                    "hidden": false
                },
                {
                    "_id": "66ffa1073187ada66f619f7e",
                    "user": {
                        "_id": "6661f79ac36ae4c83f3213e4",
                        "avatarUrl": "/avatars/2e48f052fdf37b5b06d101a6a3232eea.svg",
                        "isPro": false,
                        "fullname": "Ambroise Odonnat ",
                        "user": "ambroiseodt",
                        "type": "user"
                    },
                    "name": "Ambroise Odonnat",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-10-04T08:03:48.194Z",
                    "hidden": false
                },
                {
                    "_id": "66ffa1073187ada66f619f7f",
                    "name": "Abdelhakim Benechehab",
                    "hidden": false
                },
                {
                    "_id": "66ffa1073187ada66f619f80",
                    "name": "Linus Bleistein",
                    "hidden": false
                },
                {
                    "_id": "66ffa1073187ada66f619f81",
                    "user": {
                        "_id": "66717cf956b2f9c4d0db149d",
                        "avatarUrl": "/avatars/57646a3209c75c4b1ff5f4d24a2b0bba.svg",
                        "isPro": false,
                        "fullname": "Nicolas Boulle",
                        "user": "NBoulle",
                        "type": "user"
                    },
                    "name": "Nicolas Boullé",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-04T09:02:34.994Z",
                    "hidden": false
                },
                {
                    "_id": "66ffa1073187ada66f619f82",
                    "user": {
                        "_id": "643d3c9c043254ce1092d195",
                        "avatarUrl": "/avatars/d44f504d62888e1e8b58f5723147ae9b.svg",
                        "isPro": false,
                        "fullname": "Ievgen R",
                        "user": "ievred",
                        "type": "user"
                    },
                    "name": "Ievgen Redko",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-04T09:02:36.915Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-03T17:45:31.000Z",
            "title": "Large Language Models as Markov Chains",
            "summary": "Large language models (LLMs) have proven to be remarkably efficient, both\nacross a wide range of natural language processing tasks and well beyond them.\nHowever, a comprehensive theoretical analysis of the origins of their\nimpressive performance remains elusive. In this paper, we approach this\nchallenging task by drawing an equivalence between generic autoregressive\nlanguage models with vocabulary of size T and context window of size K and\nMarkov chains defined on a finite state space of size O(T^K). We\nderive several surprising findings related to the existence of a stationary\ndistribution of Markov chains that capture the inference power of LLMs, their\nspeed of convergence to it, and the influence of the temperature on the latter.\nWe then prove pre-training and in-context generalization bounds and show how\nthe drawn equivalence allows us to enrich their interpretation. Finally, we\nillustrate our theoretical guarantees with experiments on several recent LLMs\nto highlight how they capture the behavior observed in practice.",
            "upvotes": 11,
            "discussionId": "66ffa1083187ada66f619fec"
        },
        "publishedAt": "2024-10-04T07:26:26.926Z",
        "title": "Large Language Models as Markov Chains",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02724.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/2e48f052fdf37b5b06d101a6a3232eea.svg",
            "fullname": "Ambroise Odonnat ",
            "name": "ambroiseodt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.02416",
            "authors": [
                {
                    "_id": "66ff8020001816b29eb1082d",
                    "user": {
                        "_id": "63b4b02a103617b0a5b0ee2e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
                        "isPro": false,
                        "fullname": "Seyedmorteza Sadat",
                        "user": "msadat97",
                        "type": "user"
                    },
                    "name": "Seyedmorteza Sadat",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-04T07:23:11.988Z",
                    "hidden": false
                },
                {
                    "_id": "66ff8020001816b29eb1082e",
                    "name": "Otmar Hilliges",
                    "hidden": false
                },
                {
                    "_id": "66ff8020001816b29eb1082f",
                    "user": {
                        "_id": "630f7646197cd3f24e7f8e9f",
                        "avatarUrl": "/avatars/59bbd4ed38277b313051aac78f6808ac.svg",
                        "isPro": false,
                        "fullname": "Romann Weber",
                        "user": "RMW",
                        "type": "user"
                    },
                    "name": "Romann M. Weber",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-04T08:35:04.385Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-03T12:06:29.000Z",
            "title": "Eliminating Oversaturation and Artifacts of High Guidance Scales in\n  Diffusion Models",
            "summary": "Classifier-free guidance (CFG) is crucial for improving both generation\nquality and alignment between the input condition and final output in diffusion\nmodels. While a high guidance scale is generally required to enhance these\naspects, it also causes oversaturation and unrealistic artifacts. In this\npaper, we revisit the CFG update rule and introduce modifications to address\nthis issue. We first decompose the update term in CFG into parallel and\northogonal components with respect to the conditional model prediction and\nobserve that the parallel component primarily causes oversaturation, while the\northogonal component enhances image quality. Accordingly, we propose\ndown-weighting the parallel component to achieve high-quality generations\nwithout oversaturation. Additionally, we draw a connection between CFG and\ngradient ascent and introduce a new rescaling and momentum method for the CFG\nupdate rule based on this insight. Our approach, termed adaptive projected\nguidance (APG), retains the quality-boosting advantages of CFG while enabling\nthe use of higher guidance scales without oversaturation. APG is easy to\nimplement and introduces practically no additional computational overhead to\nthe sampling process. Through extensive experiments, we demonstrate that APG is\ncompatible with various conditional diffusion models and samplers, leading to\nimproved FID, recall, and saturation scores while maintaining precision\ncomparable to CFG, making our method a superior plug-and-play alternative to\nstandard classifier-free guidance.",
            "upvotes": 11,
            "discussionId": "66ff8027001816b29eb10aa6"
        },
        "publishedAt": "2024-10-04T04:16:57.373Z",
        "title": "Eliminating Oversaturation and Artifacts of High Guidance Scales in Diffusion Models",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63b4b02a103617b0a5b0ee2e/wSBsasK8XwCkybi4kAAyp.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02416.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
            "fullname": "Seyedmorteza Sadat",
            "name": "msadat97",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2409.19291",
            "authors": [
                {
                    "_id": "66ffab22dbdb4b0b9f759af9",
                    "name": "Jihai Zhang",
                    "hidden": false
                },
                {
                    "_id": "66ffab22dbdb4b0b9f759afa",
                    "user": {
                        "_id": "64cb54da1af278541d663708",
                        "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg",
                        "isPro": false,
                        "fullname": "Xiaoye Qu",
                        "user": "Xiaoye08",
                        "type": "user"
                    },
                    "name": "Xiaoye Qu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-04T08:46:35.159Z",
                    "hidden": false
                },
                {
                    "_id": "66ffab22dbdb4b0b9f759afb",
                    "user": {
                        "_id": "629454301ae2138079f7ff31",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629454301ae2138079f7ff31/rVtbF-j06gDiYzomTeVTc.jpeg",
                        "isPro": false,
                        "fullname": "Tong Zhu",
                        "user": "Spico",
                        "type": "user"
                    },
                    "name": "Tong Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-04T09:02:32.963Z",
                    "hidden": false
                },
                {
                    "_id": "66ffab22dbdb4b0b9f759afc",
                    "name": "Yu Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-09-28T09:28:51.000Z",
            "title": "CLIP-MoE: Towards Building Mixture of Experts for CLIP with Diversified\n  Multiplet Upcycling",
            "summary": "In recent years, Contrastive Language-Image Pre-training (CLIP) has become a\ncornerstone in multimodal intelligence. However, recent studies have identified\nthat the information loss in the CLIP encoding process is substantial, and CLIP\ntends to capture only coarse-grained features from the input. This deficiency\nsignificantly limits the ability of a single CLIP model to handle images rich\nin visual detail. In this work, we propose a simple yet effective\nmodel-agnostic strategy, Diversified Multiplet Upcycling (DMU), for CLIP. DMU\nefficiently fine-tunes a series of CLIP models that capture different feature\nspaces, from a dense pre-trained CLIP checkpoint, sharing parameters except for\nthe Feed-Forward Network (FFN). These models can then be transformed into a\nCLIP-MoE with a larger model capacity, leading to significantly enhanced\nperformance with minimal computational overhead. To the best of our knowledge,\nDiversified Multiplet Upcycling is the first approach to introduce sparsely\nactivated MoE into CLIP foundation models. Extensive experiments demonstrate\nthe significant performance of CLIP-MoE across various zero-shot retrieval,\nzero-shot image classification tasks, and downstream Multimodal Large Language\nModel (MLLM) benchmarks by serving as a vision encoder. Furthermore,\nDiversified Multiplet Upcycling enables the conversion of any dense CLIP model\ninto CLIP-MoEs, which can seamlessly replace CLIP in a plug-and-play manner\nwithout requiring further adaptation in downstream frameworks. Through\nDiversified Multiplet Upcycling, we aim to provide valuable insights for future\nresearch on developing more efficient and effective multimodal learning\nsystems.",
            "upvotes": 10,
            "discussionId": "66ffab23dbdb4b0b9f759b71"
        },
        "publishedAt": "2024-10-04T07:19:06.939Z",
        "title": "CLIP-MoE: Towards Building Mixture of Experts for CLIP with Diversified Multiplet Upcycling",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2409.19291.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/c44507cc92bb2e83154bad31b90ce6dd.svg",
            "fullname": "Xiaoye Qu",
            "name": "Xiaoye08",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.02678",
            "authors": [
                {
                    "_id": "66ff775e7d722f087907e902",
                    "user": {
                        "_id": "632116accafe12f481a473cb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1666207676653-632116accafe12f481a473cb.jpeg",
                        "isPro": true,
                        "fullname": "Will Held",
                        "user": "WillHeld",
                        "type": "user"
                    },
                    "name": "William Held",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-04T09:22:47.551Z",
                    "hidden": false
                },
                {
                    "_id": "66ff775e7d722f087907e903",
                    "name": "Ella Li",
                    "hidden": false
                },
                {
                    "_id": "66ff775e7d722f087907e904",
                    "name": "Michael Ryan",
                    "hidden": false
                },
                {
                    "_id": "66ff775e7d722f087907e905",
                    "user": {
                        "_id": "65a37e6681a46e7dd96535d1",
                        "avatarUrl": "/avatars/182ae8a06c7fddefacd4ecdcdeb44201.svg",
                        "isPro": false,
                        "fullname": "Weiyan Shi",
                        "user": "missblanchett",
                        "type": "user"
                    },
                    "name": "Weiyan Shi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-04T09:23:55.783Z",
                    "hidden": false
                },
                {
                    "_id": "66ff775e7d722f087907e906",
                    "user": {
                        "_id": "61362307df63bbd1c2459c4c",
                        "avatarUrl": "/avatars/28d3ba4b0fa00acb1eed623ab3620789.svg",
                        "isPro": false,
                        "fullname": "Yanzhe Zhang",
                        "user": "zyanzhe",
                        "type": "user"
                    },
                    "name": "Yanzhe Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-04T09:23:50.229Z",
                    "hidden": false
                },
                {
                    "_id": "66ff775e7d722f087907e907",
                    "name": "Diyi Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-03T17:04:48.000Z",
            "title": "Distilling an End-to-End Voice Assistant Without Instruction Training\n  Data",
            "summary": "Voice assistants, such as Siri and Google Assistant, typically model audio\nand text separately, resulting in lost speech information and increased\ncomplexity. Recent efforts to address this with end-to-end Speech Large\nLanguage Models (LLMs) trained with supervised finetuning (SFT)\n  have led to models ``forgetting\" capabilities from text-only LLMs. Our work\nproposes an alternative paradigm for training Speech LLMs without instruction\ndata, using the response of a text-only LLM to transcripts as self-supervision.\nImportantly, this process can be performed without annotated responses. We show\nthat our Distilled Voice Assistant (DiVA) generalizes to Spoken Question\nAnswering, Classification, and Translation. Furthermore, we show that DiVA\nbetter meets user preferences, achieving a 72\\% win rate compared with\nstate-of-the-art models like Qwen 2 Audio, despite using >100x less training\ncompute.",
            "upvotes": 9,
            "discussionId": "66ff775f7d722f087907e987"
        },
        "publishedAt": "2024-10-04T03:45:46.188Z",
        "title": "Distilling an End-to-End Voice Assistant Without Instruction Training Data",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02678.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1666207676653-632116accafe12f481a473cb.jpeg",
            "fullname": "Will Held",
            "name": "WillHeld",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.02749",
            "authors": [
                {
                    "_id": "66ff7b3d43510cc1bd347770",
                    "user": {
                        "_id": "6337054e0267ebcf02637018",
                        "avatarUrl": "/avatars/89ad12d6643f18706050a92efccd92e6.svg",
                        "isPro": false,
                        "fullname": "Ulyana Piterbarg",
                        "user": "upiter",
                        "type": "user"
                    },
                    "name": "Ulyana Piterbarg",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-04T09:16:39.075Z",
                    "hidden": false
                },
                {
                    "_id": "66ff7b3d43510cc1bd347771",
                    "user": {
                        "_id": "6388cdb6685efaedd15dc0f3",
                        "avatarUrl": "/avatars/b2b74454bf86bf61fd90ba088644f547.svg",
                        "isPro": false,
                        "fullname": "Lerrel Pinto",
                        "user": "lerrel",
                        "type": "user"
                    },
                    "name": "Lerrel Pinto",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-04T09:16:45.575Z",
                    "hidden": false
                },
                {
                    "_id": "66ff7b3d43510cc1bd347772",
                    "name": "Rob Fergus",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-03T17:57:22.000Z",
            "title": "Training Language Models on Synthetic Edit Sequences Improves Code\n  Synthesis",
            "summary": "Software engineers mainly write code by editing existing programs. In\ncontrast, large language models (LLMs) autoregressively synthesize programs in\na single pass. One explanation for this is the scarcity of open-sourced edit\ndata. While high-quality instruction data for code synthesis is already scarce,\nhigh-quality edit data is even scarcer. To fill this gap, we develop a\nsynthetic data generation algorithm called LintSeq. This algorithm refactors\nexisting code into a sequence of code edits by using a linter to procedurally\nsample across the error-free insertions that can be used to sequentially write\nprograms. It outputs edit sequences as text strings consisting of consecutive\nprogram diffs. To test LintSeq, we use it to refactor a dataset of instruction\n+ program pairs into instruction + program-diff-sequence tuples. Then, we\ninstruction finetune a series of smaller LLMs ranging from 2.6B to 14B\nparameters on both the re-factored and original versions of this dataset,\ncomparing zero-shot performance on code synthesis benchmarks. We show that\nduring repeated sampling, edit sequence finetuned models produce more diverse\nprograms than baselines. This results in better inference-time scaling for\nbenchmark coverage as a function of samples, i.e. the fraction of problems\n\"pass@k\" solved by any attempt given \"k\" tries. For example, on HumanEval\npass@50, small LLMs finetuned on synthetic edit sequences are competitive with\nGPT-4 and outperform models finetuned on the baseline dataset by +20% (+/-3%)\nin absolute score. Finally, we also pretrain our own tiny LMs for code\nunderstanding. We show that finetuning tiny models on synthetic code edits\nresults in state-of-the-art code synthesis for the on-device model class. Our\n150M parameter edit sequence LM matches or outperforms code models with twice\nas many parameters, both with and without repeated sampling, including Codex\nand AlphaCode.",
            "upvotes": 8,
            "discussionId": "66ff7b4043510cc1bd34789d"
        },
        "publishedAt": "2024-10-04T04:10:28.863Z",
        "title": "Training Language Models on Synthetic Edit Sequences Improves Code Synthesis",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02749.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669dbd709a4bf63e08f1ddc2/aV10ZJPPzH5LbnHFZNqc7.png",
            "fullname": "Yi Cui",
            "name": "onekq",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.02367",
            "authors": [
                {
                    "_id": "66ff54bfda291ca087699440",
                    "name": "Jintao Zhang",
                    "hidden": false
                },
                {
                    "_id": "66ff54bfda291ca087699441",
                    "name": "Jia wei",
                    "hidden": false
                },
                {
                    "_id": "66ff54bfda291ca087699442",
                    "name": "Pengle Zhang",
                    "hidden": false
                },
                {
                    "_id": "66ff54bfda291ca087699443",
                    "name": "Jun Zhu",
                    "hidden": false
                },
                {
                    "_id": "66ff54bfda291ca087699444",
                    "user": {
                        "_id": "65fcad0ba0d7adc40b54fac2",
                        "avatarUrl": "/avatars/7564b5642378fddb46ec3b5ae57c0402.svg",
                        "isPro": false,
                        "fullname": "Jianfei Chen",
                        "user": "surfingtomchen",
                        "type": "user"
                    },
                    "name": "Jianfei Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-04T09:08:18.301Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-03T10:25:23.000Z",
            "title": "SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference\n  Acceleration",
            "summary": "The transformer architecture predominates across various models. As the heart\nof the transformer, attention has a computational complexity of O(N^2),\ncompared to O(N) for linear transformations. When handling large sequence\nlengths, attention becomes the primary time-consuming component. Although\nquantization has proven to be an effective method for accelerating model\ninference, existing quantization methods primarily focus on optimizing the\nlinear layer. In response, we first analyze the feasibility of quantization in\nattention detailedly. Following that, we propose SageAttention, a highly\nefficient and accurate quantization method for attention. The OPS (operations\nper second) of our approach outperforms FlashAttention2 and xformers by about\n2.1 times and 2.7 times, respectively. SageAttention also achieves superior\naccuracy performance over FlashAttention3. Comprehensive experiments confirm\nthat our approach incurs almost no end-to-end metrics loss across diverse\nmodels, including those for large language processing, image generation, and\nvideo generation.",
            "upvotes": 7,
            "discussionId": "66ff54c1da291ca087699551"
        },
        "publishedAt": "2024-10-04T01:08:56.028Z",
        "title": "SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02367.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.02103",
            "authors": [
                {
                    "_id": "66ff51b1804d2dcece783a74",
                    "user": {
                        "_id": "64ad086c5d48838462e2eee1",
                        "avatarUrl": "/avatars/735b29d01bae05599e7d5ce61b223153.svg",
                        "isPro": false,
                        "fullname": "Du",
                        "user": "xiaobiaodu",
                        "type": "user"
                    },
                    "name": "Xiaobiao Du",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-04T07:31:06.175Z",
                    "hidden": false
                },
                {
                    "_id": "66ff51b1804d2dcece783a75",
                    "name": "Yida Wang",
                    "hidden": false
                },
                {
                    "_id": "66ff51b1804d2dcece783a76",
                    "name": "Xin Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-02T23:48:31.000Z",
            "title": "MVGS: Multi-view-regulated Gaussian Splatting for Novel View Synthesis",
            "summary": "Recent works in volume rendering, e.g. NeRF and 3D Gaussian\nSplatting (3DGS), significantly advance the rendering quality and efficiency\nwith the help of the learned implicit neural radiance field or 3D Gaussians.\nRendering on top of an explicit representation, the vanilla 3DGS and its\nvariants deliver real-time efficiency by optimizing the parametric model with\nsingle-view supervision per iteration during training which is adopted from\nNeRF. Consequently, certain views are overfitted, leading to unsatisfying\nappearance in novel-view synthesis and imprecise 3D geometries. To solve\naforementioned problems, we propose a new 3DGS optimization method embodying\nfour key novel contributions: 1) We transform the conventional single-view\ntraining paradigm into a multi-view training strategy. With our proposed\nmulti-view regulation, 3D Gaussian attributes are further optimized without\noverfitting certain training views. As a general solution, we improve the\noverall accuracy in a variety of scenarios and different Gaussian variants. 2)\nInspired by the benefit introduced by additional views, we further propose a\ncross-intrinsic guidance scheme, leading to a coarse-to-fine training procedure\nconcerning different resolutions. 3) Built on top of our multi-view regulated\ntraining, we further propose a cross-ray densification strategy, densifying\nmore Gaussian kernels in the ray-intersect regions from a selection of views.\n4) By further investigating the densification strategy, we found that the\neffect of densification should be enhanced when certain views are distinct\ndramatically. As a solution, we propose a novel multi-view augmented\ndensification strategy, where 3D Gaussians are encouraged to get densified to a\nsufficient number accordingly, resulting in improved reconstruction accuracy.",
            "upvotes": 6,
            "discussionId": "66ff51b4804d2dcece783c39"
        },
        "publishedAt": "2024-10-04T01:02:07.008Z",
        "title": "MVGS: Multi-view-regulated Gaussian Splatting for Novel View Synthesis",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02103.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/735b29d01bae05599e7d5ce61b223153.svg",
            "fullname": "Du",
            "name": "xiaobiaodu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.02115",
            "authors": [
                {
                    "_id": "66ff774b031160f2965e06b1",
                    "user": {
                        "_id": "64096ef79e9f790c905b846d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64096ef79e9f790c905b846d/hVzw656lXdzbCxTtnheud.jpeg",
                        "isPro": false,
                        "fullname": "Zecheng Tang",
                        "user": "ZetangForward",
                        "type": "user"
                    },
                    "name": "Zecheng Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-04T09:02:57.554Z",
                    "hidden": false
                },
                {
                    "_id": "66ff774b031160f2965e06b2",
                    "name": "Keyan Zhou",
                    "hidden": false
                },
                {
                    "_id": "66ff774b031160f2965e06b3",
                    "name": "Juntao Li",
                    "hidden": false
                },
                {
                    "_id": "66ff774b031160f2965e06b4",
                    "name": "Baibei Ji",
                    "hidden": false
                },
                {
                    "_id": "66ff774b031160f2965e06b5",
                    "name": "Jianye Hou",
                    "hidden": false
                },
                {
                    "_id": "66ff774b031160f2965e06b6",
                    "name": "Min Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-03T00:38:12.000Z",
            "title": "L-CiteEval: Do Long-Context Models Truly Leverage Context for\n  Responding?",
            "summary": "Long-context models (LCMs) have made remarkable strides in recent years,\noffering users great convenience for handling tasks that involve long context,\nsuch as document summarization. As the community increasingly prioritizes the\nfaithfulness of generated results, merely ensuring the accuracy of LCM outputs\nis insufficient, as it is quite challenging for humans to verify the results\nfrom the extremely lengthy context. Yet, although some efforts have been made\nto assess whether LCMs respond truly based on the context, these works either\nare limited to specific tasks or heavily rely on external evaluation resources\nlike GPT-4.In this work, we introduce L-CiteEval, a comprehensive multi-task\nbenchmark for long-context understanding with citations, aiming to evaluate\nboth the understanding capability and faithfulness of LCMs. L-CiteEval covers\n11 tasks from diverse domains, spanning context lengths from 8K to 48K, and\nprovides a fully automated evaluation suite. Through testing with 11\ncutting-edge closed-source and open-source LCMs, we find that although these\nmodels show minor differences in their generated results, open-source models\nsubstantially trail behind their closed-source counterparts in terms of\ncitation accuracy and recall. This suggests that current open-source LCMs are\nprone to responding based on their inherent knowledge rather than the given\ncontext, posing a significant risk to the user experience in practical\napplications. We also evaluate the RAG approach and observe that RAG can\nsignificantly improve the faithfulness of LCMs, albeit with a slight decrease\nin the generation quality. Furthermore, we discover a correlation between the\nattention mechanisms of LCMs and the citation generation process.",
            "upvotes": 5,
            "discussionId": "66ff774f031160f2965e07e0"
        },
        "publishedAt": "2024-10-04T03:35:24.620Z",
        "title": "L-CiteEval: Do Long-Context Models Truly Leverage Context for Responding?",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02115.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64096ef79e9f790c905b846d/hVzw656lXdzbCxTtnheud.jpeg",
            "fullname": "Zecheng Tang",
            "name": "ZetangForward",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.02458",
            "authors": [
                {
                    "_id": "66ff6a133ed87ca81d83e86a",
                    "user": {
                        "_id": "6660801ecf3bb553226c946d",
                        "avatarUrl": "/avatars/7ed4feccc3077668333f454a83101312.svg",
                        "isPro": false,
                        "fullname": "Gurucharan Marthi Krishna Kumar",
                        "user": "gurucharan-marthi",
                        "type": "user"
                    },
                    "name": "Gurucharan Marthi Krishna Kumar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-04T09:08:47.298Z",
                    "hidden": false
                },
                {
                    "_id": "66ff6a133ed87ca81d83e86b",
                    "user": {
                        "_id": "63a4754927f1f64ed7238dac",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
                        "isPro": false,
                        "fullname": "Aman Chadha",
                        "user": "amanchadha",
                        "type": "user"
                    },
                    "name": "Aman Chadha",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-10-04T04:39:08.781Z",
                    "hidden": false
                },
                {
                    "_id": "66ff6a133ed87ca81d83e86c",
                    "name": "Janine Mendola",
                    "hidden": false
                },
                {
                    "_id": "66ff6a133ed87ca81d83e86d",
                    "name": "Amir Shmuel",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-03T14:50:33.000Z",
            "title": "MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to\n  Enhance Medical Image Segmentation",
            "summary": "Large Language Models (LLMs), known for their versatility in textual data,\nare increasingly being explored for their potential to enhance medical image\nsegmentation, a crucial task for accurate diagnostic imaging. This study\nexplores enhancing Vision Transformers (ViTs) for medical image segmentation by\nintegrating pre-trained LLM transformer blocks. Our approach, which\nincorporates a frozen LLM transformer block into the encoder of a ViT-based\nmodel, leads to substantial improvements in segmentation performance across\nvarious medical imaging modalities. We propose a Hybrid Attention Mechanism\nthat combines global and local feature learning with a Multi-Scale Fusion Block\nfor aggregating features across different scales. The enhanced model shows\nsignificant performance gains, including an average Dice score increase from\n0.74 to 0.79 and improvements in accuracy, precision, and the Jaccard Index.\nThese results demonstrate the effectiveness of LLM-based transformers in\nrefining medical image segmentation, highlighting their potential to\nsignificantly boost model accuracy and robustness. The source code and our\nimplementation are available at: https://bit.ly/3zf2CVs",
            "upvotes": 4,
            "discussionId": "66ff6a163ed87ca81d83ea0b"
        },
        "publishedAt": "2024-10-04T03:07:56.024Z",
        "title": "MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02458.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "fullname": "Aman Chadha",
            "name": "amanchadha",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.02763",
            "authors": [
                {
                    "_id": "66ff5d6c661a30ae72a1b83b",
                    "name": "Jianrui Zhang",
                    "hidden": false
                },
                {
                    "_id": "66ff5d6c661a30ae72a1b83c",
                    "user": {
                        "_id": "63b7b2c6bd2d153522821766",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b7b2c6bd2d153522821766/aHtga-_OUdOrg_TRrXO08.jpeg",
                        "isPro": false,
                        "fullname": "Mu Cai",
                        "user": "mucai",
                        "type": "user"
                    },
                    "name": "Mu Cai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-04T09:16:17.446Z",
                    "hidden": false
                },
                {
                    "_id": "66ff5d6c661a30ae72a1b83d",
                    "user": {
                        "_id": "649f41ee70a478f8b36b2984",
                        "avatarUrl": "/avatars/1c9a76717a450ac4aeb62a1e823d2e4a.svg",
                        "isPro": false,
                        "fullname": "Yong Jae Lee",
                        "user": "yjlee0222",
                        "type": "user"
                    },
                    "name": "Yong Jae Lee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-04T09:15:57.098Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-03T17:59:58.000Z",
            "title": "Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short\n  Videos",
            "summary": "There has been growing sentiment recently that modern large multimodal models\n(LMMs) have addressed most of the key challenges related to short video\ncomprehension. As a result, both academia and industry are gradually shifting\ntheir attention towards the more complex challenges posed by understanding\nlong-form videos. However, is this really the case? Our studies indicate that\nLMMs still lack many fundamental reasoning capabilities even when dealing with\nshort videos. We introduce Vinoground, a temporal counterfactual LMM evaluation\nbenchmark encompassing 1000 short and natural video-caption pairs. We\ndemonstrate that existing LMMs severely struggle to distinguish temporal\ndifferences between different actions and object transformations. For example,\nthe best model GPT-4o only obtains ~50% on our text and video scores, showing a\nlarge gap compared to the human baseline of ~90%. All open-source multimodal\nmodels and CLIP-based models perform much worse, producing mostly random chance\nperformance. Through this work, we shed light onto the fact that temporal\nreasoning in short videos is a problem yet to be fully solved. The dataset and\nevaluation code are available at https://vinoground.github.io.",
            "upvotes": 3,
            "discussionId": "66ff5d6e661a30ae72a1b8c4"
        },
        "publishedAt": "2024-10-04T06:25:59.514Z",
        "title": "Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short Videos",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02763.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b7b2c6bd2d153522821766/aHtga-_OUdOrg_TRrXO08.jpeg",
            "fullname": "Mu Cai",
            "name": "mucai",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.02536",
            "authors": [
                {
                    "_id": "67001603d78e24f7e25061a2",
                    "name": "Shiyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "67001603d78e24f7e25061a3",
                    "user": {
                        "_id": "63955025254ed3075fb26d53",
                        "avatarUrl": "/avatars/c84049bbfff80c82b95e4bc76512e2ce.svg",
                        "isPro": false,
                        "fullname": "Aakash Patel",
                        "user": "aakashdp",
                        "type": "user"
                    },
                    "name": "Aakash Patel",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-10-04T16:23:47.544Z",
                    "hidden": false
                },
                {
                    "_id": "67001603d78e24f7e25061a4",
                    "name": "Syed A Rizvi",
                    "hidden": false
                },
                {
                    "_id": "67001603d78e24f7e25061a5",
                    "name": "Nianchen Liu",
                    "hidden": false
                },
                {
                    "_id": "67001603d78e24f7e25061a6",
                    "name": "Sizhuang He",
                    "hidden": false
                },
                {
                    "_id": "67001603d78e24f7e25061a7",
                    "name": "Amin Karbasi",
                    "hidden": false
                },
                {
                    "_id": "67001603d78e24f7e25061a8",
                    "name": "Emanuele Zappala",
                    "hidden": false
                },
                {
                    "_id": "67001603d78e24f7e25061a9",
                    "user": {
                        "_id": "6514a30dfe79293513eeea09",
                        "avatarUrl": "/avatars/bdc0a6338872568274c759fd1813fd63.svg",
                        "isPro": false,
                        "fullname": "David van Dijk",
                        "user": "davidvandijk",
                        "type": "user"
                    },
                    "name": "David van Dijk",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-10-04T16:21:25.022Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-03T14:42:34.000Z",
            "title": "Intelligence at the Edge of Chaos",
            "summary": "We explore the emergence of intelligent behavior in artificial systems by\ninvestigating how the complexity of rule-based systems influences the\ncapabilities of models trained to predict these rules. Our study focuses on\nelementary cellular automata (ECA), simple yet powerful one-dimensional systems\nthat generate behaviors ranging from trivial to highly complex. By training\ndistinct Large Language Models (LLMs) on different ECAs, we evaluated the\nrelationship between the complexity of the rules' behavior and the intelligence\nexhibited by the LLMs, as reflected in their performance on downstream tasks.\nOur findings reveal that rules with higher complexity lead to models exhibiting\ngreater intelligence, as demonstrated by their performance on reasoning and\nchess move prediction tasks. Both uniform and periodic systems, and often also\nhighly chaotic systems, resulted in poorer downstream performance, highlighting\na sweet spot of complexity conducive to intelligence. We conjecture that\nintelligence arises from the ability to predict complexity and that creating\nintelligence may require only exposure to complexity.",
            "upvotes": 2,
            "discussionId": "67001605d78e24f7e25061fe"
        },
        "publishedAt": "2024-10-04T15:23:17.394Z",
        "title": "Intelligence at the Edge of Chaos",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02536.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d98ef7a4839890b25eb78b/215-CSVLl81z6CAq0ECWU.jpeg",
            "fullname": "Fangyuan Yu",
            "name": "Ksgk-fy",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.02426",
            "authors": [
                {
                    "_id": "66ff3f5c1e307d098692d137",
                    "user": {
                        "_id": "6331bab56eae0bb0a01968ea",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6331bab56eae0bb0a01968ea/DxFrb2wZKFp0mmmdTa4tg.png",
                        "isPro": false,
                        "fullname": "Ben Fauber",
                        "user": "BFauber",
                        "type": "user"
                    },
                    "name": "Ben Fauber",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-04T01:05:37.180Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-03T12:19:49.000Z",
            "title": "Learning the Latent Rules of a Game from Data: A Chess Story",
            "summary": "We demonstrate that small pretrained foundational generative language models\nwith millions of parameters can learn the latent rules of a process from data\nassociated with the process. Inspired by Stefan Zweig's novella\n\"Schachnovelle,\" also known as \"The Royal Game\" in English, we show that 28M\nand 125M parameter pretrained foundational small language models (SLMs) can be\ninstruction fine-tuned with 1,000-to-1,000,000 examples to learn the rules of\nchess, propose legal moves, and accurately solve chess problems. We also\nexplore the impact of successive language model fine-tuning epochs on improved\noutcomes and demonstrate reductions in model hallucinations by increasing the\nnumber of instruction fine-tuning examples.",
            "upvotes": 2,
            "discussionId": "66ff3f5d1e307d098692d162"
        },
        "publishedAt": "2024-10-04T11:11:31.974Z",
        "title": "Learning the Latent Rules of a Game from Data: A Chess Story",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02426.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6331bab56eae0bb0a01968ea/DxFrb2wZKFp0mmmdTa4tg.png",
            "fullname": "Ben Fauber",
            "name": "BFauber",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.02056",
            "authors": [
                {
                    "_id": "66ff54d92f0dc1b2315c7bb0",
                    "user": {
                        "_id": "627a354cc488a8ce15a2dec5",
                        "avatarUrl": "/avatars/0d99a2fea8b193993fe5b9b7e5b74f40.svg",
                        "isPro": false,
                        "fullname": "Sreyan Ghosh",
                        "user": "SreyanG-NVIDIA",
                        "type": "user"
                    },
                    "name": "Sreyan Ghosh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-04T09:33:46.374Z",
                    "hidden": false
                },
                {
                    "_id": "66ff54d92f0dc1b2315c7bb1",
                    "name": "Sonal Kumar",
                    "hidden": false
                },
                {
                    "_id": "66ff54d92f0dc1b2315c7bb2",
                    "user": {
                        "_id": "652a4dfc36f031c5e6f8b8a6",
                        "avatarUrl": "/avatars/9fb56b025dc25f91ca6c31136eaf74b2.svg",
                        "isPro": false,
                        "fullname": "Zhifeng Kong",
                        "user": "ZhifengKong",
                        "type": "user"
                    },
                    "name": "Zhifeng Kong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-04T09:34:03.072Z",
                    "hidden": false
                },
                {
                    "_id": "66ff54d92f0dc1b2315c7bb3",
                    "user": {
                        "_id": "6440ddd65d600fb09518daa8",
                        "avatarUrl": "/avatars/ac5898afd2082d230e2ebf6fb867ad4f.svg",
                        "isPro": false,
                        "fullname": "Rafael Valle",
                        "user": "rafaelvalle",
                        "type": "user"
                    },
                    "name": "Rafael Valle",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-04T09:34:19.799Z",
                    "hidden": false
                },
                {
                    "_id": "66ff54d92f0dc1b2315c7bb4",
                    "user": {
                        "_id": "6311021788942700629e6247",
                        "avatarUrl": "/avatars/e7adc1632b76e80e7e4a590033d1c20a.svg",
                        "isPro": false,
                        "fullname": "Bryan Catanzaro",
                        "user": "ctnzr",
                        "type": "user"
                    },
                    "name": "Bryan Catanzaro",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-04T09:34:25.451Z",
                    "hidden": false
                },
                {
                    "_id": "66ff54d92f0dc1b2315c7bb5",
                    "user": {
                        "_id": "6537a569568d8be8fa096b8c",
                        "avatarUrl": "/avatars/bfda5cb252d8b5bc3ad737d99c0d7f49.svg",
                        "isPro": false,
                        "fullname": "Dinesh Manocha",
                        "user": "manocha",
                        "type": "user"
                    },
                    "name": "Dinesh Manocha",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-10-04T09:34:32.757Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-10-02T22:05:36.000Z",
            "title": "Synthio: Augmenting Small-Scale Audio Classification Datasets with\n  Synthetic Data",
            "summary": "We present Synthio, a novel approach for augmenting small-scale audio\nclassification datasets with synthetic data. Our goal is to improve audio\nclassification accuracy with limited labeled data. Traditional data\naugmentation techniques, which apply artificial transformations (e.g., adding\nrandom noise or masking segments), struggle to create data that captures the\ntrue diversity present in real-world audios. To address this shortcoming, we\npropose to augment the dataset with synthetic audio generated from\ntext-to-audio (T2A) diffusion models. However, synthesizing effective\naugmentations is challenging because not only should the generated data be\nacoustically consistent with the underlying small-scale dataset, but they\nshould also have sufficient compositional diversity. To overcome the first\nchallenge, we align the generations of the T2A model with the small-scale\ndataset using preference optimization. This ensures that the acoustic\ncharacteristics of the generated data remain consistent with the small-scale\ndataset. To address the second challenge, we propose a novel caption generation\ntechnique that leverages the reasoning capabilities of Large Language Models to\n(1) generate diverse and meaningful audio captions and (2) iteratively refine\ntheir quality. The generated captions are then used to prompt the aligned T2A\nmodel. We extensively evaluate Synthio on ten datasets and four simulated\nlimited-data settings. Results indicate our method consistently outperforms all\nbaselines by 0.1%-39% using a T2A model trained only on weakly-captioned\nAudioSet.",
            "upvotes": 2,
            "discussionId": "66ff54db2f0dc1b2315c7cc8"
        },
        "publishedAt": "2024-10-04T01:08:20.101Z",
        "title": "Synthio: Augmenting Small-Scale Audio Classification Datasets with Synthetic Data",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.02056.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/ca23ecdec2d31c99ecce97d9b180ae0c.svg",
            "fullname": "Ghosh",
            "name": "Sreyan88",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2410.00255",
            "authors": [
                {
                    "_id": "66fee5e214b824c86560ce72",
                    "user": {
                        "_id": "662050a24360f44332caf535",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662050a24360f44332caf535/oVLyKvjCxgVV79taeRJPH.png",
                        "isPro": false,
                        "fullname": "Weitai Kang",
                        "user": "weitaikang",
                        "type": "user"
                    },
                    "name": "Weitai Kang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-10-04T07:31:27.875Z",
                    "hidden": false
                },
                {
                    "_id": "66fee5e214b824c86560ce73",
                    "name": "Haifeng Huang",
                    "hidden": false
                },
                {
                    "_id": "66fee5e214b824c86560ce74",
                    "name": "Yuzhang Shang",
                    "hidden": false
                },
                {
                    "_id": "66fee5e214b824c86560ce75",
                    "name": "Mubarak Shah",
                    "hidden": false
                },
                {
                    "_id": "66fee5e214b824c86560ce76",
                    "name": "Yan Yan",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-09-30T21:55:38.000Z",
            "title": "Robin3D: Improving 3D Large Language Model via Robust Instruction Tuning",
            "summary": "Recent advancements in 3D Large Language Models (3DLLMs) have highlighted\ntheir potential in building general-purpose agents in the 3D real world, yet\nchallenges remain due to the lack of high-quality robust instruction-following\ndata, leading to limited discriminative power and generalization of 3DLLMs. In\nthis paper, we introduce Robin3D, a powerful 3DLLM trained on large-scale\ninstruction-following data generated by our novel data engine, Robust\nInstruction Generation (RIG) engine. RIG generates two key instruction data: 1)\nthe Adversarial Instruction-following data, which features mixed negative and\npositive samples to enhance the model's discriminative understanding. 2) the\nDiverse Instruction-following data, which contains various instruction styles\nto enhance model's generalization. As a result, we construct 1 million\ninstruction-following data, consisting of 344K Adversarial samples, 508K\nDiverse samples, and 165K benchmark training set samples. To better handle\nthese complex instructions, Robin3D first incorporates Relation-Augmented\nProjector to enhance spatial understanding, and then strengthens the object\nreferring and grounding ability through ID-Feature Bonding. Robin3D\nconsistently outperforms previous methods across five widely-used 3D multimodal\nlearning benchmarks, without the need for task-specific fine-tuning. Notably,\nwe achieve a 7.8\\% improvement in the grounding task (Multi3DRefer) and a 6.9\\%\nimprovement in the captioning task (Scan2Cap).",
            "upvotes": 1,
            "discussionId": "66fee5e314b824c86560cf08"
        },
        "publishedAt": "2024-10-04T15:08:16.327Z",
        "title": "Robin3D: Improving 3D Large Language Model via Robust Instruction Tuning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2410.00255.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662050a24360f44332caf535/oVLyKvjCxgVV79taeRJPH.png",
            "fullname": "Weitai Kang",
            "name": "weitaikang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    }
]