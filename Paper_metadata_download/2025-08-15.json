[
  {
    "paper": {
      "id": "2508.10433",
      "authors": [
        {
          "_id": "689e8afda4caabb4320e5cca",
          "name": "Runqi Qiao",
          "hidden": false
        },
        {
          "_id": "689e8afda4caabb4320e5ccb",
          "name": "Qiuna Tan",
          "hidden": false
        },
        {
          "_id": "689e8afda4caabb4320e5ccc",
          "name": "Peiqing Yang",
          "hidden": false
        },
        {
          "_id": "689e8afda4caabb4320e5ccd",
          "name": "Yanzi Wang",
          "hidden": false
        },
        {
          "_id": "689e8afda4caabb4320e5cce",
          "name": "Xiaowan Wang",
          "hidden": false
        },
        {
          "_id": "689e8afda4caabb4320e5ccf",
          "name": "Enhui Wan",
          "hidden": false
        },
        {
          "_id": "689e8afda4caabb4320e5cd0",
          "name": "Sitong Zhou",
          "hidden": false
        },
        {
          "_id": "689e8afda4caabb4320e5cd1",
          "name": "Guanting Dong",
          "hidden": false
        },
        {
          "_id": "689e8afda4caabb4320e5cd2",
          "name": "Yuchen Zeng",
          "hidden": false
        },
        {
          "_id": "689e8afda4caabb4320e5cd3",
          "name": "Yida Xu",
          "hidden": false
        },
        {
          "_id": "689e8afda4caabb4320e5cd4",
          "name": "Jie Wang",
          "hidden": false
        },
        {
          "_id": "689e8afda4caabb4320e5cd5",
          "name": "Chong Sun",
          "hidden": false
        },
        {
          "_id": "689e8afda4caabb4320e5cd6",
          "name": "Chen Li",
          "hidden": false
        },
        {
          "_id": "689e8afda4caabb4320e5cd7",
          "name": "Honggang Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6683a05e74fb1736a4b7c934/ByDtHPTXv1Xt6pmngd3no.png",
        "https://cdn-uploads.huggingface.co/production/uploads/6683a05e74fb1736a4b7c934/dzhRhIcGJYT-gHIG0J9JU.png"
      ],
      "publishedAt": "2025-08-14T08:15:41.000Z",
      "submittedOnDailyAt": "2025-08-15T01:02:46.436Z",
      "title": "We-Math 2.0: A Versatile MathBook System for Incentivizing Visual\n  Mathematical Reasoning",
      "submittedOnDailyBy": {
        "_id": "6683a05e74fb1736a4b7c934",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6683a05e74fb1736a4b7c934/eiz6qlqIUjAWGy5zfg8Cs.jpeg",
        "isPro": false,
        "fullname": "QRQ",
        "user": "RichardQRQ",
        "type": "user"
      },
      "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities across various tasks, but still struggle with complex mathematical\nreasoning. Existing research primarily focuses on dataset construction and\nmethod optimization, often overlooking two critical aspects: comprehensive\nknowledge-driven design and model-centric data space modeling. In this paper,\nwe introduce We-Math 2.0, a unified system that integrates a structured\nmathematical knowledge system, model-centric data space modeling, and a\nreinforcement learning (RL)-based training paradigm to comprehensively enhance\nthe mathematical reasoning abilities of MLLMs. The key contributions of We-Math\n2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level\nhierarchical system encompassing 491 knowledge points and 1,819 fundamental\nprinciples. (2) MathBook-Standard & Pro: We develop MathBook-Standard, a\ndataset that ensures broad conceptual coverage and flexibility through dual\nexpansion. Additionally, we define a three-dimensional difficulty space and\ngenerate 7 progressive variants per problem to build MathBook-Pro, a\nchallenging dataset for robust training. (3) MathBook-RL: We propose a\ntwo-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the\nmodel with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive\nAlignment RL, leveraging average-reward learning and dynamic data scheduling to\nachieve progressive alignment across difficulty levels. (4) MathBookEval: We\nintroduce a comprehensive benchmark covering all 491 knowledge points with\ndiverse reasoning step distributions. Experimental results show that\nMathBook-RL performs competitively with existing baselines on four widely-used\nbenchmarks and achieves strong results on MathBookEval, suggesting promising\ngeneralization in mathematical reasoning.",
      "upvotes": 92,
      "discussionId": "689e8afea4caabb4320e5cd8",
      "projectPage": "https://we-math2.github.io/",
      "githubRepo": "https://github.com/We-Math/We-Math2.0",
      "ai_summary": "We-Math 2.0 enhances MLLMs' mathematical reasoning through a structured knowledge system, model-centric data space modeling, and reinforcement learning, demonstrating competitive performance on benchmarks.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "MLLMs",
        "mathematical reasoning",
        "MathBook Knowledge System",
        "MathBook-Standard",
        "MathBook-Pro",
        "MathBook-RL",
        "Cold-Start Fine-tuning",
        "Progressive Alignment RL",
        "MathBookEval",
        "chain-of-thought reasoning",
        "average-reward learning",
        "dynamic data scheduling"
      ],
      "githubStars": 79
    },
    "publishedAt": "2025-08-14T04:15:41.000Z",
    "title": "We-Math 2.0: A Versatile MathBook System for Incentivizing Visual\n  Mathematical Reasoning",
    "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities across various tasks, but still struggle with complex mathematical\nreasoning. Existing research primarily focuses on dataset construction and\nmethod optimization, often overlooking two critical aspects: comprehensive\nknowledge-driven design and model-centric data space modeling. In this paper,\nwe introduce We-Math 2.0, a unified system that integrates a structured\nmathematical knowledge system, model-centric data space modeling, and a\nreinforcement learning (RL)-based training paradigm to comprehensively enhance\nthe mathematical reasoning abilities of MLLMs. The key contributions of We-Math\n2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level\nhierarchical system encompassing 491 knowledge points and 1,819 fundamental\nprinciples. (2) MathBook-Standard & Pro: We develop MathBook-Standard, a\ndataset that ensures broad conceptual coverage and flexibility through dual\nexpansion. Additionally, we define a three-dimensional difficulty space and\ngenerate 7 progressive variants per problem to build MathBook-Pro, a\nchallenging dataset for robust training. (3) MathBook-RL: We propose a\ntwo-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the\nmodel with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive\nAlignment RL, leveraging average-reward learning and dynamic data scheduling to\nachieve progressive alignment across difficulty levels. (4) MathBookEval: We\nintroduce a comprehensive benchmark covering all 491 knowledge points with\ndiverse reasoning step distributions. Experimental results show that\nMathBook-RL performs competitively with existing baselines on four widely-used\nbenchmarks and achieves strong results on MathBookEval, suggesting promising\ngeneralization in mathematical reasoning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6683a05e74fb1736a4b7c934/ByDtHPTXv1Xt6pmngd3no.png",
      "https://cdn-uploads.huggingface.co/production/uploads/6683a05e74fb1736a4b7c934/dzhRhIcGJYT-gHIG0J9JU.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.10433.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "6683a05e74fb1736a4b7c934",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6683a05e74fb1736a4b7c934/eiz6qlqIUjAWGy5zfg8Cs.jpeg",
      "fullname": "QRQ",
      "name": "RichardQRQ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.10711",
      "authors": [
        {
          "_id": "689ea023a4caabb4320e5d43",
          "name": "NextStep Team",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d44",
          "name": "Chunrui Han",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d45",
          "name": "Guopeng Li",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d46",
          "name": "Jingwei Wu",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d47",
          "name": "Quan Sun",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d48",
          "name": "Yan Cai",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d49",
          "name": "Yuang Peng",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d4a",
          "name": "Zheng Ge",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d4b",
          "name": "Deyu Zhou",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d4c",
          "name": "Haomiao Tang",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d4d",
          "name": "Hongyu Zhou",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d4e",
          "name": "Kenkun Liu",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d4f",
          "name": "Ailin Huang",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d50",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d51",
          "name": "Changxin Miao",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d52",
          "name": "Deshan Sun",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d53",
          "name": "En Yu",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d54",
          "name": "Fukun Yin",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d55",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d56",
          "name": "Hao Nie",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d57",
          "name": "Haoran Lv",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d58",
          "name": "Hanpeng Hu",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d59",
          "name": "Jia Wang",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d5a",
          "name": "Jian Zhou",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d5b",
          "name": "Jianjian Sun",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d5c",
          "name": "Kaijun Tan",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d5d",
          "name": "Kang An",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d5e",
          "name": "Kangheng Lin",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d5f",
          "name": "Liang Zhao",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d60",
          "name": "Mei Chen",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d61",
          "name": "Peng Xing",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d62",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d63",
          "name": "Shiyu Liu",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d64",
          "name": "Shutao Xia",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d65",
          "name": "Tianhao You",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d66",
          "name": "Wei Ji",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d67",
          "name": "Xianfang Zeng",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d68",
          "name": "Xin Han",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d69",
          "name": "Xuelin Zhang",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d6a",
          "name": "Yana Wei",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d6b",
          "name": "Yanming Xu",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d6c",
          "name": "Yimin Jiang",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d6d",
          "name": "Yingming Wang",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d6e",
          "name": "Yu Zhou",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d6f",
          "name": "Yucheng Han",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d70",
          "name": "Ziyang Meng",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d71",
          "name": "Binxing Jiao",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d72",
          "name": "Daxin Jiang",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d73",
          "name": "Xiangyu Zhang",
          "hidden": false
        },
        {
          "_id": "689ea023a4caabb4320e5d74",
          "name": "Yibo Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-14T14:54:22.000Z",
      "submittedOnDailyAt": "2025-08-15T01:41:51.831Z",
      "title": "NextStep-1: Toward Autoregressive Image Generation with Continuous\n  Tokens at Scale",
      "submittedOnDailyBy": {
        "_id": "631ee086c1a8269da39265c6",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631ee086c1a8269da39265c6/wUa1epGtTGcUv2mvrLUcD.png",
        "isPro": false,
        "fullname": "Yuang Peng",
        "user": "yuangpeng",
        "type": "user"
      },
      "summary": "Prevailing autoregressive (AR) models for text-to-image generation either\nrely on heavy, computationally-intensive diffusion models to process continuous\nimage tokens, or employ vector quantization (VQ) to obtain discrete tokens with\nquantization loss. In this paper, we push the autoregressive paradigm forward\nwith NextStep-1, a 14B autoregressive model paired with a 157M flow matching\nhead, training on discrete text tokens and continuous image tokens with\nnext-token prediction objectives. NextStep-1 achieves state-of-the-art\nperformance for autoregressive models in text-to-image generation tasks,\nexhibiting strong capabilities in high-fidelity image synthesis. Furthermore,\nour method shows strong performance in image editing, highlighting the power\nand versatility of our unified approach. To facilitate open research, we will\nrelease our code and models to the community.",
      "upvotes": 64,
      "discussionId": "689ea024a4caabb4320e5d75",
      "projectPage": "https://stepfun.ai/research/en/nextstep1",
      "githubRepo": "https://github.com/stepfun-ai/NextStep-1",
      "ai_summary": "NextStep-1, a 14B autoregressive model with a 157M flow matching head, achieves state-of-the-art performance in text-to-image generation and image editing by processing discrete text tokens and continuous image tokens.",
      "ai_keywords": [
        "autoregressive models",
        "diffusion models",
        "vector quantization",
        "flow matching",
        "next-token prediction",
        "high-fidelity image synthesis",
        "image editing"
      ],
      "githubStars": 159
    },
    "publishedAt": "2025-08-14T10:54:22.000Z",
    "title": "NextStep-1: Toward Autoregressive Image Generation with Continuous\n  Tokens at Scale",
    "summary": "Prevailing autoregressive (AR) models for text-to-image generation either\nrely on heavy, computationally-intensive diffusion models to process continuous\nimage tokens, or employ vector quantization (VQ) to obtain discrete tokens with\nquantization loss. In this paper, we push the autoregressive paradigm forward\nwith NextStep-1, a 14B autoregressive model paired with a 157M flow matching\nhead, training on discrete text tokens and continuous image tokens with\nnext-token prediction objectives. NextStep-1 achieves state-of-the-art\nperformance for autoregressive models in text-to-image generation tasks,\nexhibiting strong capabilities in high-fidelity image synthesis. Furthermore,\nour method shows strong performance in image editing, highlighting the power\nand versatility of our unified approach. To facilitate open research, we will\nrelease our code and models to the community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.10711.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631ee086c1a8269da39265c6",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631ee086c1a8269da39265c6/wUa1epGtTGcUv2mvrLUcD.png",
      "fullname": "Yuang Peng",
      "name": "yuangpeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.10833",
      "authors": [
        {
          "_id": "689e97cda4caabb4320e5ce7",
          "name": "Zhangxuan Gu",
          "hidden": false
        },
        {
          "_id": "689e97cda4caabb4320e5ce8",
          "name": "Zhengwen Zeng",
          "hidden": false
        },
        {
          "_id": "689e97cda4caabb4320e5ce9",
          "name": "Zhenyu Xu",
          "hidden": false
        },
        {
          "_id": "689e97cda4caabb4320e5cea",
          "name": "Xingran Zhou",
          "hidden": false
        },
        {
          "_id": "689e97cda4caabb4320e5ceb",
          "name": "Shuheng Shen",
          "hidden": false
        },
        {
          "_id": "689e97cda4caabb4320e5cec",
          "name": "Yunfei Liu",
          "hidden": false
        },
        {
          "_id": "689e97cda4caabb4320e5ced",
          "name": "Beitong Zhou",
          "hidden": false
        },
        {
          "_id": "689e97cda4caabb4320e5cee",
          "name": "Changhua Meng",
          "hidden": false
        },
        {
          "_id": "689e97cda4caabb4320e5cef",
          "name": "Tianyu Xia",
          "hidden": false
        },
        {
          "_id": "689e97cda4caabb4320e5cf0",
          "name": "Weizhi Chen",
          "hidden": false
        },
        {
          "_id": "689e97cda4caabb4320e5cf1",
          "name": "Yue Wen",
          "hidden": false
        },
        {
          "_id": "689e97cda4caabb4320e5cf2",
          "name": "Jingya Dou",
          "hidden": false
        },
        {
          "_id": "689e97cda4caabb4320e5cf3",
          "name": "Fei Tang",
          "hidden": false
        },
        {
          "_id": "689e97cda4caabb4320e5cf4",
          "name": "Jinzhen Lin",
          "hidden": false
        },
        {
          "_id": "689e97cda4caabb4320e5cf5",
          "name": "Yulin Liu",
          "hidden": false
        },
        {
          "_id": "689e97cda4caabb4320e5cf6",
          "name": "Zhenlin Guo",
          "hidden": false
        },
        {
          "_id": "689e97cda4caabb4320e5cf7",
          "name": "Yichen Gong",
          "hidden": false
        },
        {
          "_id": "689e97cda4caabb4320e5cf8",
          "name": "Heng Jia",
          "hidden": false
        },
        {
          "_id": "689e97cda4caabb4320e5cf9",
          "name": "Changlong Gao",
          "hidden": false
        },
        {
          "_id": "689e97cda4caabb4320e5cfa",
          "name": "Yuan Guo",
          "hidden": false
        },
        {
          "_id": "689e97cda4caabb4320e5cfb",
          "name": "Yong Deng",
          "hidden": false
        },
        {
          "_id": "689e97cda4caabb4320e5cfc",
          "name": "Zhenyu Guo",
          "hidden": false
        },
        {
          "_id": "689e97cda4caabb4320e5cfd",
          "name": "Liang Chen",
          "hidden": false
        },
        {
          "_id": "689e97cda4caabb4320e5cfe",
          "name": "Weiqiang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-14T16:58:07.000Z",
      "submittedOnDailyAt": "2025-08-15T01:21:22.553Z",
      "title": "UI-Venus Technical Report: Building High-performance UI Agents with RFT",
      "submittedOnDailyBy": {
        "_id": "60d2a2984956988b63753371",
        "avatarUrl": "/avatars/412879e82a5bfc88431d8aa561acf26a.svg",
        "isPro": false,
        "fullname": "zhangxgu",
        "user": "zhangxgu",
        "type": "user"
      },
      "summary": "We present UI-Venus, a native UI agent that takes only screenshots as input\nbased on a multimodal large language model. UI-Venus achieves SOTA performance\non both UI grounding and navigation tasks using only several hundred thousand\nhigh-quality training samples through reinforcement finetune (RFT) based on\nQwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% /\n50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e.,\nScreenspot-V2 / Pro, surpassing the previous SOTA baselines including\nopen-source GTA1 and closed-source UI-TARS-1.5.To show UI-Venus's summary and\nplaning ability, we also evaluate it on the AndroidWorld, an online UI\nnavigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9%\nsuccess rate, also beating existing models.To achieve this, we introduce\ncarefully designed reward functions for both UI grounding and navigation tasks\nand corresponding efficient data cleaning strategies.To further boost\nnavigation performance, we propose Self-Evolving Trajectory History Alignment\n\\& Sparse Action Enhancement that refine historical reasoning traces and\nbalances the distribution of sparse but critical actions, leading to more\ncoherent planning and better generalization in complex UI tasks. Our\ncontributions include the publish of SOTA open-source UI agents, comprehensive\ndata cleaning protocols and a novel self-evolving framework for improving\nnavigation performance, which encourage further research and development in the\ncommunity. Code is available at https://github.com/antgroup/UI-Venus.",
      "upvotes": 15,
      "discussionId": "689e97cda4caabb4320e5cff",
      "ai_summary": "UI-Venus, a multimodal large language model-based UI agent, achieves state-of-the-art performance in UI grounding and navigation tasks using reinforcement fine-tuning and novel self-evolving frameworks.",
      "ai_keywords": [
        "multimodal large language model",
        "reinforcement finetune",
        "Qwen2.5-VL",
        "UI grounding",
        "navigation tasks",
        "Screenspot-V2",
        "Pro",
        "AndroidWorld",
        "reward functions",
        "data cleaning strategies",
        "Self-Evolving Trajectory History Alignment",
        "Sparse Action Enhancement"
      ]
    },
    "publishedAt": "2025-08-14T12:58:07.000Z",
    "title": "UI-Venus Technical Report: Building High-performance UI Agents with RFT",
    "summary": "We present UI-Venus, a native UI agent that takes only screenshots as input\nbased on a multimodal large language model. UI-Venus achieves SOTA performance\non both UI grounding and navigation tasks using only several hundred thousand\nhigh-quality training samples through reinforcement finetune (RFT) based on\nQwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% /\n50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e.,\nScreenspot-V2 / Pro, surpassing the previous SOTA baselines including\nopen-source GTA1 and closed-source UI-TARS-1.5.To show UI-Venus's summary and\nplaning ability, we also evaluate it on the AndroidWorld, an online UI\nnavigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9%\nsuccess rate, also beating existing models.To achieve this, we introduce\ncarefully designed reward functions for both UI grounding and navigation tasks\nand corresponding efficient data cleaning strategies.To further boost\nnavigation performance, we propose Self-Evolving Trajectory History Alignment\n\\& Sparse Action Enhancement that refine historical reasoning traces and\nbalances the distribution of sparse but critical actions, leading to more\ncoherent planning and better generalization in complex UI tasks. Our\ncontributions include the publish of SOTA open-source UI agents, comprehensive\ndata cleaning protocols and a novel self-evolving framework for improving\nnavigation performance, which encourage further research and development in the\ncommunity. Code is available at https://github.com/antgroup/UI-Venus.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.10833.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60d2a2984956988b63753371",
      "avatarUrl": "/avatars/412879e82a5bfc88431d8aa561acf26a.svg",
      "fullname": "zhangxgu",
      "name": "zhangxgu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.09848",
      "authors": [
        {
          "_id": "689dc868b083e610d741eb30",
          "name": "Mo Yu",
          "hidden": false
        },
        {
          "_id": "689dc868b083e610d741eb31",
          "name": "Tsz Ting Chung",
          "hidden": false
        },
        {
          "_id": "689dc868b083e610d741eb32",
          "name": "Chulun Zhou",
          "hidden": false
        },
        {
          "_id": "689dc868b083e610d741eb33",
          "name": "Tong Li",
          "hidden": false
        },
        {
          "_id": "689dc868b083e610d741eb34",
          "name": "Rui Lu",
          "hidden": false
        },
        {
          "_id": "689dc868b083e610d741eb35",
          "name": "Jiangnan Li",
          "hidden": false
        },
        {
          "_id": "689dc868b083e610d741eb36",
          "name": "Liyan Xu",
          "hidden": false
        },
        {
          "_id": "689dc868b083e610d741eb37",
          "name": "Haoshu Lu",
          "hidden": false
        },
        {
          "_id": "689dc868b083e610d741eb38",
          "name": "Ning Zhang",
          "hidden": false
        },
        {
          "_id": "689dc868b083e610d741eb39",
          "name": "Jing Li",
          "hidden": false
        },
        {
          "_id": "689dc868b083e610d741eb3a",
          "name": "Jie Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-13T14:28:25.000Z",
      "submittedOnDailyAt": "2025-08-15T01:12:28.551Z",
      "title": "PRELUDE: A Benchmark Designed to Require Global Comprehension and\n  Reasoning over Long Contexts",
      "submittedOnDailyBy": {
        "_id": "60ab6b2ee3de7c7440abb845",
        "avatarUrl": "/avatars/22916bece3b5b951c016bf2ddd8dda1c.svg",
        "isPro": false,
        "fullname": "Cindy",
        "user": "ttchungc",
        "type": "user"
      },
      "summary": "We introduce PRELUDE, a benchmark for evaluating long-context understanding\nthrough the task of determining whether a character's prequel story is\nconsistent with the canonical narrative of the original book. Our task poses a\nstronger demand for global comprehension and deep reasoning than existing\nbenchmarks -- as the prequels are not part of the original story, assessing\ntheir plausibility typically requires searching and integrating information\nthat is only indirectly related. Empirically, 88% of instances require evidence\nfrom multiple parts of the narrative. Experimental results highlight the\nchallenge of our task: in-context learning, RAG and in-domain training with\nstate-of-the-art LLMs, and commercial DeepResearch services, lag behind humans\nby >15%. A further human study reveals that models often produce correct\nanswers with flawed reasoning, leading to an over 30% gap in reasoning accuracy\ncompared to humans. These findings underscore the substantial room for\nimprovement in long-context understanding and reasoning.",
      "upvotes": 13,
      "discussionId": "689dc869b083e610d741eb3b",
      "projectPage": "https://gorov.github.io/prelude",
      "ai_summary": "A benchmark called PRELUDE evaluates long-context understanding by assessing the consistency of prequel stories with original books, revealing significant challenges for models compared to humans.",
      "ai_keywords": [
        "PRELUDE",
        "long-context understanding",
        "in-context learning",
        "RAG",
        "state-of-the-art LLMs",
        "DeepResearch",
        "reasoning accuracy"
      ]
    },
    "publishedAt": "2025-08-13T10:28:25.000Z",
    "title": "PRELUDE: A Benchmark Designed to Require Global Comprehension and\n  Reasoning over Long Contexts",
    "summary": "We introduce PRELUDE, a benchmark for evaluating long-context understanding\nthrough the task of determining whether a character's prequel story is\nconsistent with the canonical narrative of the original book. Our task poses a\nstronger demand for global comprehension and deep reasoning than existing\nbenchmarks -- as the prequels are not part of the original story, assessing\ntheir plausibility typically requires searching and integrating information\nthat is only indirectly related. Empirically, 88% of instances require evidence\nfrom multiple parts of the narrative. Experimental results highlight the\nchallenge of our task: in-context learning, RAG and in-domain training with\nstate-of-the-art LLMs, and commercial DeepResearch services, lag behind humans\nby >15%. A further human study reveals that models often produce correct\nanswers with flawed reasoning, leading to an over 30% gap in reasoning accuracy\ncompared to humans. These findings underscore the substantial room for\nimprovement in long-context understanding and reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09848.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60ab6b2ee3de7c7440abb845",
      "avatarUrl": "/avatars/22916bece3b5b951c016bf2ddd8dda1c.svg",
      "fullname": "Cindy",
      "name": "ttchungc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.10881",
      "authors": [
        {
          "_id": "689ede65a4caabb4320e5e36",
          "name": "Lingen Li",
          "hidden": false
        },
        {
          "_id": "689ede65a4caabb4320e5e37",
          "name": "Guangzhi Wang",
          "hidden": false
        },
        {
          "_id": "689ede65a4caabb4320e5e38",
          "name": "Zhaoyang Zhang",
          "hidden": false
        },
        {
          "_id": "689ede65a4caabb4320e5e39",
          "name": "Yaowei Li",
          "hidden": false
        },
        {
          "_id": "689ede65a4caabb4320e5e3a",
          "name": "Xiaoyu Li",
          "hidden": false
        },
        {
          "_id": "689ede65a4caabb4320e5e3b",
          "name": "Qi Dou",
          "hidden": false
        },
        {
          "_id": "689ede65a4caabb4320e5e3c",
          "name": "Jinwei Gu",
          "hidden": false
        },
        {
          "_id": "689ede65a4caabb4320e5e3d",
          "name": "Tianfan Xue",
          "hidden": false
        },
        {
          "_id": "689ede65a4caabb4320e5e3e",
          "name": "Ying Shan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-14T17:50:11.000Z",
      "submittedOnDailyAt": "2025-08-15T06:05:45.449Z",
      "title": "ToonComposer: Streamlining Cartoon Production with Generative\n  Post-Keyframing",
      "submittedOnDailyBy": {
        "_id": "66837d3c48edefb453b0640a",
        "avatarUrl": "/avatars/b16385eaa612578728e2c6460a76b38f.svg",
        "isPro": false,
        "fullname": "Lingen Li",
        "user": "l-li",
        "type": "user"
      },
      "summary": "Traditional cartoon and anime production involves keyframing, inbetweening,\nand colorization stages, which require intensive manual effort. Despite recent\nadvances in AI, existing methods often handle these stages separately, leading\nto error accumulation and artifacts. For instance, inbetweening approaches\nstruggle with large motions, while colorization methods require dense per-frame\nsketches. To address this, we introduce ToonComposer, a generative model that\nunifies inbetweening and colorization into a single post-keyframing stage.\nToonComposer employs a sparse sketch injection mechanism to provide precise\ncontrol using keyframe sketches. Additionally, it uses a cartoon adaptation\nmethod with the spatial low-rank adapter to tailor a modern video foundation\nmodel to the cartoon domain while keeping its temporal prior intact. Requiring\nas few as a single sketch and a colored reference frame, ToonComposer excels\nwith sparse inputs, while also supporting multiple sketches at any temporal\nlocation for more precise motion control. This dual capability reduces manual\nworkload and improves flexibility, empowering artists in real-world scenarios.\nTo evaluate our model, we further created PKBench, a benchmark featuring\nhuman-drawn sketches that simulate real-world use cases. Our evaluation\ndemonstrates that ToonComposer outperforms existing methods in visual quality,\nmotion consistency, and production efficiency, offering a superior and more\nflexible solution for AI-assisted cartoon production.",
      "upvotes": 7,
      "discussionId": "689ede65a4caabb4320e5e3f",
      "projectPage": "https://lg-li.github.io/project/tooncomposer",
      "githubRepo": "https://github.com/TencentARC/ToonComposer",
      "ai_summary": "ToonComposer is a generative model that unifies inbetweening and colorization in cartoon production, using sparse sketches and a cartoon adaptation method to improve visual quality and efficiency.",
      "ai_keywords": [
        "sparse sketch injection",
        "cartoon adaptation",
        "spatial low-rank adapter",
        "video foundation model",
        "PKBench",
        "visual quality",
        "motion consistency",
        "production efficiency"
      ]
    },
    "publishedAt": "2025-08-14T13:50:11.000Z",
    "title": "ToonComposer: Streamlining Cartoon Production with Generative\n  Post-Keyframing",
    "summary": "Traditional cartoon and anime production involves keyframing, inbetweening,\nand colorization stages, which require intensive manual effort. Despite recent\nadvances in AI, existing methods often handle these stages separately, leading\nto error accumulation and artifacts. For instance, inbetweening approaches\nstruggle with large motions, while colorization methods require dense per-frame\nsketches. To address this, we introduce ToonComposer, a generative model that\nunifies inbetweening and colorization into a single post-keyframing stage.\nToonComposer employs a sparse sketch injection mechanism to provide precise\ncontrol using keyframe sketches. Additionally, it uses a cartoon adaptation\nmethod with the spatial low-rank adapter to tailor a modern video foundation\nmodel to the cartoon domain while keeping its temporal prior intact. Requiring\nas few as a single sketch and a colored reference frame, ToonComposer excels\nwith sparse inputs, while also supporting multiple sketches at any temporal\nlocation for more precise motion control. This dual capability reduces manual\nworkload and improves flexibility, empowering artists in real-world scenarios.\nTo evaluate our model, we further created PKBench, a benchmark featuring\nhuman-drawn sketches that simulate real-world use cases. Our evaluation\ndemonstrates that ToonComposer outperforms existing methods in visual quality,\nmotion consistency, and production efficiency, offering a superior and more\nflexible solution for AI-assisted cartoon production.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.10881.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66837d3c48edefb453b0640a",
      "avatarUrl": "/avatars/b16385eaa612578728e2c6460a76b38f.svg",
      "fullname": "Lingen Li",
      "name": "l-li",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.10576",
      "authors": [
        {
          "_id": "689e9eb4a4caabb4320e5d3a",
          "name": "Zheng Qin",
          "hidden": false
        },
        {
          "_id": "689e9eb4a4caabb4320e5d3b",
          "name": "Ruobing Zheng",
          "hidden": false
        },
        {
          "_id": "689e9eb4a4caabb4320e5d3c",
          "name": "Yabing Wang",
          "hidden": false
        },
        {
          "_id": "689e9eb4a4caabb4320e5d3d",
          "name": "Tianqi Li",
          "hidden": false
        },
        {
          "_id": "689e9eb4a4caabb4320e5d3e",
          "name": "Yi Yuan",
          "hidden": false
        },
        {
          "_id": "689e9eb4a4caabb4320e5d3f",
          "name": "Jingdong Chen",
          "hidden": false
        },
        {
          "_id": "689e9eb4a4caabb4320e5d40",
          "name": "Le Wang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/676a52b5c32a51c8a39d5307/LIaSWGn6wk1CRhZBtXa02.png",
        "https://cdn-uploads.huggingface.co/production/uploads/676a52b5c32a51c8a39d5307/nq95U2jcbJuumsLZUC39u.png",
        "https://cdn-uploads.huggingface.co/production/uploads/676a52b5c32a51c8a39d5307/zGxKiqoSYP3KXy5F8O7SU.png"
      ],
      "publishedAt": "2025-08-14T12:14:15.000Z",
      "submittedOnDailyAt": "2025-08-15T01:16:03.262Z",
      "title": "HumanSense: From Multimodal Perception to Empathetic Context-Aware\n  Responses through Reasoning MLLMs",
      "submittedOnDailyBy": {
        "_id": "676a52b5c32a51c8a39d5307",
        "avatarUrl": "/avatars/b7493ed8e8cc3aab8cc0ca89a326f3c4.svg",
        "isPro": false,
        "fullname": "ruobing zheng",
        "user": "RobinRoaR",
        "type": "user"
      },
      "summary": "While Multimodal Large Language Models (MLLMs) show immense promise for\nachieving truly human-like interactions, progress is hindered by the lack of\nfine-grained evaluation frameworks for human-centered scenarios, encompassing\nboth the understanding of complex human intentions and the provision of\nempathetic, context-aware responses. Here we introduce HumanSense, a\ncomprehensive benchmark designed to evaluate the human-centered perception and\ninteraction capabilities of MLLMs, with a particular focus on deep\nunderstanding of extended multimodal contexts and the formulation of rational\nfeedback. Our evaluation reveals that leading MLLMs still have considerable\nroom for improvement, particularly for advanced interaction-oriented tasks.\nSupplementing visual input with audio and text information yields substantial\nimprovements, and Omni-modal models show advantages on these tasks.\nFurthermore, we argue that appropriate feedback stems from a contextual\nanalysis of the interlocutor's needs and emotions, with reasoning ability\nserving as the key to unlocking it. Accordingly, we employ a multi-stage,\nmodality-progressive reinforcement learning to enhance the reasoning abilities\nof an Omni model, achieving substantial gains on evaluation results.\nAdditionally, we observe that successful reasoning processes exhibit highly\nconsistent thought patterns. By designing corresponding prompts, we also\nenhance the performance of non-reasoning models in a training-free manner.\nProject page:\nbrightpinkhttps://digital-avatar.github.io/ai/HumanSense/",
      "upvotes": 5,
      "discussionId": "689e9eb4a4caabb4320e5d41",
      "projectPage": "https://digital-avatar.github.io/ai/HumanSense/",
      "ai_summary": "HumanSense is a benchmark for evaluating human-centered perception and interaction in Multimodal Large Language Models, focusing on multimodal context understanding and rational feedback through reinforcement learning.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "HumanSense",
        "benchmark",
        "human-centered perception",
        "interaction capabilities",
        "extended multimodal contexts",
        "rational feedback",
        "reinforcement learning",
        "Omni-modal models",
        "contextual analysis",
        "reasoning abilities",
        "thought patterns",
        "prompts"
      ]
    },
    "publishedAt": "2025-08-14T08:14:15.000Z",
    "title": "HumanSense: From Multimodal Perception to Empathetic Context-Aware\n  Responses through Reasoning MLLMs",
    "summary": "While Multimodal Large Language Models (MLLMs) show immense promise for\nachieving truly human-like interactions, progress is hindered by the lack of\nfine-grained evaluation frameworks for human-centered scenarios, encompassing\nboth the understanding of complex human intentions and the provision of\nempathetic, context-aware responses. Here we introduce HumanSense, a\ncomprehensive benchmark designed to evaluate the human-centered perception and\ninteraction capabilities of MLLMs, with a particular focus on deep\nunderstanding of extended multimodal contexts and the formulation of rational\nfeedback. Our evaluation reveals that leading MLLMs still have considerable\nroom for improvement, particularly for advanced interaction-oriented tasks.\nSupplementing visual input with audio and text information yields substantial\nimprovements, and Omni-modal models show advantages on these tasks.\nFurthermore, we argue that appropriate feedback stems from a contextual\nanalysis of the interlocutor's needs and emotions, with reasoning ability\nserving as the key to unlocking it. Accordingly, we employ a multi-stage,\nmodality-progressive reinforcement learning to enhance the reasoning abilities\nof an Omni model, achieving substantial gains on evaluation results.\nAdditionally, we observe that successful reasoning processes exhibit highly\nconsistent thought patterns. By designing corresponding prompts, we also\nenhance the performance of non-reasoning models in a training-free manner.\nProject page:\nbrightpinkhttps://digital-avatar.github.io/ai/HumanSense/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/676a52b5c32a51c8a39d5307/LIaSWGn6wk1CRhZBtXa02.png",
      "https://cdn-uploads.huggingface.co/production/uploads/676a52b5c32a51c8a39d5307/nq95U2jcbJuumsLZUC39u.png",
      "https://cdn-uploads.huggingface.co/production/uploads/676a52b5c32a51c8a39d5307/zGxKiqoSYP3KXy5F8O7SU.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.10576.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "676a52b5c32a51c8a39d5307",
      "avatarUrl": "/avatars/b7493ed8e8cc3aab8cc0ca89a326f3c4.svg",
      "fullname": "ruobing zheng",
      "name": "RobinRoaR",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.10860",
      "authors": [
        {
          "_id": "689e9a49a4caabb4320e5d12",
          "name": "Zhaokun Jiang",
          "hidden": false
        },
        {
          "_id": "689e9a49a4caabb4320e5d13",
          "name": "Ziyin Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-14T17:31:18.000Z",
      "submittedOnDailyAt": "2025-08-15T00:56:04.904Z",
      "title": "From Black Box to Transparency: Enhancing Automated Interpreting\n  Assessment with Explainable AI in College Classrooms",
      "submittedOnDailyBy": {
        "_id": "6430bdd8cd31d174a9f900fb",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Y9SPnRfpKSbYc7MhNdP-H.jpeg",
        "isPro": false,
        "fullname": "Ziyin Zhang",
        "user": "Geralt-Targaryen",
        "type": "user"
      },
      "summary": "Recent advancements in machine learning have spurred growing interests in\nautomated interpreting quality assessment. Nevertheless, existing research\nsuffers from insufficient examination of language use quality, unsatisfactory\nmodeling effectiveness due to data scarcity and imbalance, and a lack of\nefforts to explain model predictions. To address these gaps, we propose a\nmulti-dimensional modeling framework that integrates feature engineering, data\naugmentation, and explainable machine learning. This approach prioritizes\nexplainability over ``black box'' predictions by utilizing only\nconstruct-relevant, transparent features and conducting Shapley Value (SHAP)\nanalysis. Our results demonstrate strong predictive performance on a novel\nEnglish-Chinese consecutive interpreting dataset, identifying BLEURT and\nCometKiwi scores to be the strongest predictive features for fidelity,\npause-related features for fluency, and Chinese-specific phraseological\ndiversity metrics for language use. Overall, by placing particular emphasis on\nexplainability, we present a scalable, reliable, and transparent alternative to\ntraditional human evaluation, facilitating the provision of detailed diagnostic\nfeedback for learners and supporting self-regulated learning advantages not\nafforded by automated scores in isolation.",
      "upvotes": 2,
      "discussionId": "689e9a4aa4caabb4320e5d14",
      "ai_summary": "A multi-dimensional modeling framework enhances automated interpreting quality assessment by integrating feature engineering, data augmentation, and explainable machine learning, focusing on transparency and detailed diagnostic feedback.",
      "ai_keywords": [
        "feature engineering",
        "data augmentation",
        "explainable machine learning",
        "Shapley Value (SHAP)",
        "BLEURT",
        "CometKiwi",
        "language use quality",
        "model predictions",
        "English-Chinese consecutive interpreting",
        "fidelity",
        "fluency",
        "Chinese-specific phraseological diversity metrics",
        "self-regulated learning"
      ]
    },
    "publishedAt": "2025-08-14T13:31:18.000Z",
    "title": "From Black Box to Transparency: Enhancing Automated Interpreting\n  Assessment with Explainable AI in College Classrooms",
    "summary": "Recent advancements in machine learning have spurred growing interests in\nautomated interpreting quality assessment. Nevertheless, existing research\nsuffers from insufficient examination of language use quality, unsatisfactory\nmodeling effectiveness due to data scarcity and imbalance, and a lack of\nefforts to explain model predictions. To address these gaps, we propose a\nmulti-dimensional modeling framework that integrates feature engineering, data\naugmentation, and explainable machine learning. This approach prioritizes\nexplainability over ``black box'' predictions by utilizing only\nconstruct-relevant, transparent features and conducting Shapley Value (SHAP)\nanalysis. Our results demonstrate strong predictive performance on a novel\nEnglish-Chinese consecutive interpreting dataset, identifying BLEURT and\nCometKiwi scores to be the strongest predictive features for fidelity,\npause-related features for fluency, and Chinese-specific phraseological\ndiversity metrics for language use. Overall, by placing particular emphasis on\nexplainability, we present a scalable, reliable, and transparent alternative to\ntraditional human evaluation, facilitating the provision of detailed diagnostic\nfeedback for learners and supporting self-regulated learning advantages not\nafforded by automated scores in isolation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.10860.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6430bdd8cd31d174a9f900fb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Y9SPnRfpKSbYc7MhNdP-H.jpeg",
      "fullname": "Ziyin Zhang",
      "name": "Geralt-Targaryen",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.10751",
      "authors": [
        {
          "_id": "689ec27fa4caabb4320e5df4",
          "name": "Zhipeng Chen",
          "hidden": false
        },
        {
          "_id": "689ec27fa4caabb4320e5df5",
          "name": "Xiaobo Qin",
          "hidden": false
        },
        {
          "_id": "689ec27fa4caabb4320e5df6",
          "name": "Youbin Wu",
          "hidden": false
        },
        {
          "_id": "689ec27fa4caabb4320e5df7",
          "name": "Yue Ling",
          "hidden": false
        },
        {
          "_id": "689ec27fa4caabb4320e5df8",
          "name": "Qinghao Ye",
          "hidden": false
        },
        {
          "_id": "689ec27fa4caabb4320e5df9",
          "name": "Wayne Xin Zhao",
          "hidden": false
        },
        {
          "_id": "689ec27fa4caabb4320e5dfa",
          "name": "Guang Shi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-14T15:34:47.000Z",
      "submittedOnDailyAt": "2025-08-15T03:47:57.550Z",
      "title": "Pass@k Training for Adaptively Balancing Exploration and Exploitation of\n  Large Reasoning Models",
      "submittedOnDailyBy": {
        "_id": "629b765ce1af194c641fcbc6",
        "avatarUrl": "/avatars/7c53a4c2a1e528c19641a2b601731754.svg",
        "isPro": false,
        "fullname": "Zhipeng Chen",
        "user": "TimothyCzp",
        "type": "user"
      },
      "summary": "Reinforcement learning with verifiable rewards (RLVR), which typically adopts\nPass@1 as the reward, has faced the issues in balancing exploration and\nexploitation, causing policies to prefer conservative actions, converging to a\nlocal optimum. Identifying an appropriate reward metric is therefore crucial.\nRegarding the prior work, although Pass@k has been used in evaluation, its\nconnection to LLM exploration ability in RLVR remains largely overlooked. To\ninvestigate this, we first use Pass@k as the reward to train the policy model\n(i.e., Pass@k Training), and observe the improvement on its\nexploration ability. Next, we derive an analytical solution for the advantage\nof Pass@k Training, leading to an efficient and effective process. Building on\nthis, our analysis reveals that exploration and exploitation are not inherently\nconflicting objectives, while they can mutually enhance each other. Moreover,\nPass@k Training with analytical derivation essentially involves directly\ndesigning the advantage function. Inspired by this, we preliminarily explore\nthe advantage design for RLVR, showing promising results and highlighting a\npotential future direction.",
      "upvotes": 1,
      "discussionId": "689ec280a4caabb4320e5dfb",
      "projectPage": "https://github.com/RUCAIBox/Passk_Training",
      "githubRepo": "https://github.com/RUCAIBox/Passk_Training",
      "ai_summary": "Using Pass@k as a reward in reinforcement learning with verifiable rewards improves exploration and reveals that exploration and exploitation can mutually enhance each other.",
      "ai_keywords": [
        "reinforcement learning",
        "verifiable rewards",
        "RLVR",
        "Pass@1",
        "Pass@k",
        "policy model",
        "exploration",
        "exploitation",
        "local optimum",
        "advantage function"
      ],
      "githubStars": 3
    },
    "publishedAt": "2025-08-14T11:34:47.000Z",
    "title": "Pass@k Training for Adaptively Balancing Exploration and Exploitation of\n  Large Reasoning Models",
    "summary": "Reinforcement learning with verifiable rewards (RLVR), which typically adopts\nPass@1 as the reward, has faced the issues in balancing exploration and\nexploitation, causing policies to prefer conservative actions, converging to a\nlocal optimum. Identifying an appropriate reward metric is therefore crucial.\nRegarding the prior work, although Pass@k has been used in evaluation, its\nconnection to LLM exploration ability in RLVR remains largely overlooked. To\ninvestigate this, we first use Pass@k as the reward to train the policy model\n(i.e., Pass@k Training), and observe the improvement on its\nexploration ability. Next, we derive an analytical solution for the advantage\nof Pass@k Training, leading to an efficient and effective process. Building on\nthis, our analysis reveals that exploration and exploitation are not inherently\nconflicting objectives, while they can mutually enhance each other. Moreover,\nPass@k Training with analytical derivation essentially involves directly\ndesigning the advantage function. Inspired by this, we preliminarily explore\nthe advantage design for RLVR, showing promising results and highlighting a\npotential future direction.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.10751.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "629b765ce1af194c641fcbc6",
      "avatarUrl": "/avatars/7c53a4c2a1e528c19641a2b601731754.svg",
      "fullname": "Zhipeng Chen",
      "name": "TimothyCzp",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  }
]