[
  {
    "paper": {
      "id": "2602.07026",
      "authors": [
        {
          "_id": "698a98541b2dc6b37d61af09",
          "name": "Xiaomin Yu",
          "hidden": false
        },
        {
          "_id": "698a98541b2dc6b37d61af0a",
          "name": "Yi Xin",
          "hidden": false
        },
        {
          "_id": "698a98541b2dc6b37d61af0b",
          "name": "Wenjie Zhang",
          "hidden": false
        },
        {
          "_id": "698a98541b2dc6b37d61af0c",
          "name": "Chonghan Liu",
          "hidden": false
        },
        {
          "_id": "698a98541b2dc6b37d61af0d",
          "name": "Hanzhen Zhao",
          "hidden": false
        },
        {
          "_id": "698a98541b2dc6b37d61af0e",
          "name": "Xiaoxing Hu",
          "hidden": false
        },
        {
          "_id": "698a98541b2dc6b37d61af0f",
          "name": "Xinlei Yu",
          "hidden": false
        },
        {
          "_id": "698a98541b2dc6b37d61af10",
          "name": "Ziyue Qiao",
          "hidden": false
        },
        {
          "_id": "698a98541b2dc6b37d61af11",
          "name": "Hao Tang",
          "hidden": false
        },
        {
          "_id": "698a98541b2dc6b37d61af12",
          "name": "Xue Yang",
          "hidden": false
        },
        {
          "_id": "698a98541b2dc6b37d61af13",
          "name": "Xiaobin Hu",
          "hidden": false
        },
        {
          "_id": "698a98541b2dc6b37d61af14",
          "name": "Chengwei Qin",
          "hidden": false
        },
        {
          "_id": "698a98541b2dc6b37d61af15",
          "name": "Hui Xiong",
          "hidden": false
        },
        {
          "_id": "698a98541b2dc6b37d61af16",
          "name": "Yu Qiao",
          "hidden": false
        },
        {
          "_id": "698a98541b2dc6b37d61af17",
          "name": "Shuicheng Yan",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-02T13:59:39.000Z",
      "submittedOnDailyAt": "2026-02-10T00:01:56.908Z",
      "title": "Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models",
      "submittedOnDailyBy": {
        "_id": "64084fa192033c150738e4f2",
        "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg",
        "isPro": false,
        "fullname": "Yu_xm",
        "user": "Yu2020",
        "type": "user"
      },
      "summary": "Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs.",
      "upvotes": 78,
      "discussionId": "698a98541b2dc6b37d61af18",
      "githubRepo": "https://github.com/Yu-xm/ReVision.git",
      "githubRepoAddedBy": "user",
      "ai_summary": "Researchers address the modality gap in multimodal learning by proposing a fixed-frame theory and a training-free alignment method that enables efficient scaling of multimodal models using unpaired data.",
      "ai_keywords": [
        "multimodal contrastive learning",
        "modality gap",
        "geometric anomaly",
        "isotropic assumptions",
        "Fixed-frame Modality Gap Theory",
        "ReAlign",
        "Anchor Alignment",
        "Trace Alignment",
        "Centroid Alignment",
        "ReVision",
        "Multimodal Large Language Models",
        "unpaired data",
        "visual representation distribution",
        "pretraining stage",
        "visual instruction tuning"
      ],
      "githubStars": 26
    },
    "publishedAt": "2026-02-02T08:59:39.000Z",
    "title": "Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models",
    "summary": "Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07026.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64084fa192033c150738e4f2",
      "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg",
      "fullname": "Yu_xm",
      "name": "Yu2020",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.08794",
      "authors": [
        {
          "_id": "698ac65d1b2dc6b37d61b1c2",
          "name": "SII-OpenMOSS Team",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1c4",
          "name": "Donghua Yu",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1c5",
          "name": "Mingshu Chen",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1c6",
          "name": "Qi Chen",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1c7",
          "name": "Qi Luo",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1c8",
          "name": "Qianyi Wu",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1c9",
          "name": "Qinyuan Cheng",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1ca",
          "name": "Ruixiao Li",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1cb",
          "name": "Tianyi Liang",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1cc",
          "name": "Wenbo Zhang",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1cd",
          "name": "Wenming Tu",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1ce",
          "name": "Xiangyu Peng",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1cf",
          "name": "Yang Gao",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1d0",
          "name": "Yanru Huo",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1d1",
          "name": "Ying Zhu",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1d2",
          "name": "Yinze Luo",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1d3",
          "name": "Yiyang Zhang",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1d4",
          "name": "Yuerong Song",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1d5",
          "name": "Zhe Xu",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1d6",
          "name": "Zhiyu Zhang",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1d7",
          "name": "Chenchen Yang",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1d8",
          "name": "Cheng Chang",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1d9",
          "name": "Chushu Zhou",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1da",
          "name": "Hanfu Chen",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1db",
          "name": "Hongnan Ma",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1dc",
          "name": "Jiaxi Li",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1dd",
          "name": "Jingqi Tong",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1de",
          "name": "Junxi Liu",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1df",
          "name": "Ke Chen",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1e0",
          "name": "Shimin Li",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1e1",
          "name": "Songlin Wang",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1e2",
          "name": "Wei Jiang",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1e3",
          "name": "Zhaoye Fei",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1e4",
          "name": "Zhiyuan Ning",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1e5",
          "name": "Chunguo Li",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1e6",
          "name": "Chenhui Li",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1e7",
          "name": "Ziwei He",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1e8",
          "name": "Zengfeng Huang",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1e9",
          "name": "Xie Chen",
          "hidden": false
        },
        {
          "_id": "698ac65d1b2dc6b37d61b1ea",
          "name": "Xipeng Qiu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-09T15:31:54.000Z",
      "submittedOnDailyAt": "2026-02-10T03:18:59.260Z",
      "title": "MOVA: Towards Scalable and Synchronized Video-Audio Generation",
      "submittedOnDailyBy": {
        "_id": "62c14609ac1b639c2d87192c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656833489364-noauth.png",
        "isPro": false,
        "fullname": "SII-liangtianyi",
        "user": "tianyilt",
        "type": "user"
      },
      "summary": "Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.",
      "upvotes": 69,
      "discussionId": "698ac65e1b2dc6b37d61b1eb",
      "projectPage": "https://mosi.cn/models/mova",
      "githubRepo": "https://github.com/OpenMOSS/MOVA",
      "githubRepoAddedBy": "user",
      "ai_summary": "MOVA is an open-source model that generates synchronized audio-visual content using a Mixture-of-Experts architecture with 32 billion parameters, supporting image-text to video-audio generation tasks.",
      "ai_keywords": [
        "Mixture-of-Experts",
        "MoE",
        "audio-visual content",
        "lip-synced speech",
        "sound effects",
        "content-aligned music",
        "IT2VA",
        "efficient inference",
        "LoRA fine-tuning",
        "prompt enhancement"
      ],
      "githubStars": 521,
      "organization": {
        "_id": "613b0dee83ec35d460684607",
        "name": "OpenMOSS-Team",
        "fullname": "OpenMOSS",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"
      }
    },
    "publishedAt": "2026-02-09T10:31:54.000Z",
    "title": "MOVA: Towards Scalable and Synchronized Video-Audio Generation",
    "summary": "Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08794.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62c14609ac1b639c2d87192c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656833489364-noauth.png",
      "fullname": "SII-liangtianyi",
      "name": "tianyilt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "613b0dee83ec35d460684607",
      "name": "OpenMOSS-Team",
      "fullname": "OpenMOSS",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.07085",
      "authors": [
        {
          "_id": "698ab6f91b2dc6b37d61b031",
          "name": "Jun Han",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b032",
          "name": "Shuo Zhang",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b033",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b034",
          "name": "Zhi Yang",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b035",
          "name": "Yifan Dong",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b036",
          "name": "Tu Hu",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b037",
          "name": "Jialuo Yuan",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b038",
          "name": "Xiaomin Yu",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b039",
          "name": "Yumo Zhu",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b03a",
          "name": "Fangqi Lou",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b03b",
          "name": "Xin Guo",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b03c",
          "name": "Zhaowei Liu",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b03d",
          "name": "Tianyi Jiang",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b03e",
          "name": "Ruichuan An",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b03f",
          "name": "Jingping Liu",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b040",
          "name": "Biao Wu",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b041",
          "name": "Rongze Chen",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b042",
          "name": "Kunyi Wang",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b043",
          "name": "Yifan Wang",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b044",
          "name": "Sen Hu",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b045",
          "name": "Xinbing Kong",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b046",
          "name": "Liwen Zhang",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b047",
          "name": "Ronghao Chen",
          "hidden": false
        },
        {
          "_id": "698ab6f91b2dc6b37d61b048",
          "name": "Huacan Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-06T08:08:04.000Z",
      "submittedOnDailyAt": "2026-02-10T02:19:22.216Z",
      "title": "QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining",
      "submittedOnDailyBy": {
        "_id": "64aa645404e7b379feccc490",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png",
        "isPro": false,
        "fullname": "Zhi Yang",
        "user": "yangzhi1",
        "type": "user"
      },
      "summary": "Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated experience. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that treats each end-to-end mining run as a trajectory and improves factors through trajectory-level mutation and crossover operations. QuantaAlpha localizes suboptimal steps in each trajectory for targeted revision and recombines complementary high-reward segments to reuse effective patterns, enabling structured exploration and refinement across mining iterations. During factor generation, QuantaAlpha enforces semantic consistency across the hypothesis, factor expression, and executable code, while constraining the complexity and redundancy of the generated factor to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains over strong baseline models and prior agentic systems. When utilizing GPT-5.2, QuantaAlpha achieves an Information Coefficient (IC) of 0.1501, with an Annualized Rate of Return (ARR) of 27.75% and a Maximum Drawdown (MDD) of 7.98%. Moreover, factors mined on CSI 300 transfer effectively to the China Securities Index 500 (CSI 500) and the Standard & Poor's 500 Index (S&P 500), delivering 160% and 137% cumulative excess return over four years, respectively, which indicates strong robustness of QuantaAlpha under market distribution shifts.",
      "upvotes": 42,
      "discussionId": "698ab6fa1b2dc6b37d61b049",
      "githubRepo": "https://github.com/QuantaAlpha/QuantaAlpha",
      "githubRepoAddedBy": "user",
      "githubStars": 11,
      "organization": {
        "_id": "68b33ab6a9ed99140481cf44",
        "name": "QuantaAlpha",
        "fullname": "QuantaAlpha",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"
      }
    },
    "publishedAt": "2026-02-06T03:08:04.000Z",
    "title": "QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining",
    "summary": "Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated experience. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that treats each end-to-end mining run as a trajectory and improves factors through trajectory-level mutation and crossover operations. QuantaAlpha localizes suboptimal steps in each trajectory for targeted revision and recombines complementary high-reward segments to reuse effective patterns, enabling structured exploration and refinement across mining iterations. During factor generation, QuantaAlpha enforces semantic consistency across the hypothesis, factor expression, and executable code, while constraining the complexity and redundancy of the generated factor to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains over strong baseline models and prior agentic systems. When utilizing GPT-5.2, QuantaAlpha achieves an Information Coefficient (IC) of 0.1501, with an Annualized Rate of Return (ARR) of 27.75% and a Maximum Drawdown (MDD) of 7.98%. Moreover, factors mined on CSI 300 transfer effectively to the China Securities Index 500 (CSI 500) and the Standard & Poor's 500 Index (S&P 500), delivering 160% and 137% cumulative excess return over four years, respectively, which indicates strong robustness of QuantaAlpha under market distribution shifts.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07085.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64aa645404e7b379feccc490",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png",
      "fullname": "Zhi Yang",
      "name": "yangzhi1",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "68b33ab6a9ed99140481cf44",
      "name": "QuantaAlpha",
      "fullname": "QuantaAlpha",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.06422",
      "authors": [
        {
          "_id": "698a9c501b2dc6b37d61af2f",
          "name": "Yunze Tong",
          "hidden": false
        },
        {
          "_id": "698a9c501b2dc6b37d61af30",
          "name": "Mushui Liu",
          "hidden": false
        },
        {
          "_id": "698a9c501b2dc6b37d61af31",
          "name": "Canyu Zhao",
          "hidden": false
        },
        {
          "_id": "698a9c501b2dc6b37d61af32",
          "name": "Wanggui He",
          "hidden": false
        },
        {
          "_id": "698a9c501b2dc6b37d61af33",
          "name": "Shiyi Zhang",
          "hidden": false
        },
        {
          "_id": "698a9c501b2dc6b37d61af34",
          "name": "Hongwei Zhang",
          "hidden": false
        },
        {
          "_id": "698a9c501b2dc6b37d61af35",
          "name": "Peng Zhang",
          "hidden": false
        },
        {
          "_id": "698a9c501b2dc6b37d61af36",
          "name": "Jinlong Liu",
          "hidden": false
        },
        {
          "_id": "698a9c501b2dc6b37d61af37",
          "name": "Ju Huang",
          "hidden": false
        },
        {
          "_id": "698a9c501b2dc6b37d61af38",
          "name": "Jiamang Wang",
          "hidden": false
        },
        {
          "_id": "698a9c501b2dc6b37d61af39",
          "name": "Hao Jiang",
          "hidden": false
        },
        {
          "_id": "698a9c501b2dc6b37d61af3a",
          "name": "Pipei Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-06T06:37:10.000Z",
      "submittedOnDailyAt": "2026-02-10T00:19:39.681Z",
      "title": "Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO",
      "submittedOnDailyBy": {
        "_id": "646efd223dd912a539e0bd46",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/EOFAv5xvOgJOzuDgh4nSb.png",
        "isPro": false,
        "fullname": "Canyu Zhao",
        "user": "Canyu",
        "type": "user"
      },
      "summary": "Deploying GRPO on Flow Matching models has proven effective for text-to-image generation. However, existing paradigms typically propagate an outcome-based reward to all preceding denoising steps without distinguishing the local effect of each step. Moreover, current group-wise ranking mainly compares trajectories at matched timesteps and ignores within-trajectory dependencies, where certain early denoising actions can affect later states via delayed, implicit interactions. We propose TurningPoint-GRPO (TP-GRPO), a GRPO framework that alleviates step-wise reward sparsity and explicitly models long-term effects within the denoising trajectory. TP-GRPO makes two key innovations: (i) it replaces outcome-based rewards with step-level incremental rewards, providing a dense, step-aware learning signal that better isolates each denoising action's \"pure\" effect, and (ii) it identifies turning points-steps that flip the local reward trend and make subsequent reward evolution consistent with the overall trajectory trend-and assigns these actions an aggregated long-term reward to capture their delayed impact. Turning points are detected solely via sign changes in incremental rewards, making TP-GRPO efficient and hyperparameter-free. Extensive experiments also demonstrate that TP-GRPO exploits reward signals more effectively and consistently improves generation. Demo code is available at https://github.com/YunzeTong/TurningPoint-GRPO.",
      "upvotes": 32,
      "discussionId": "698a9c501b2dc6b37d61af3b",
      "githubRepo": "https://github.com/YunzeTong/TurningPoint-GRPO",
      "githubRepoAddedBy": "user",
      "ai_summary": "TP-GRPO addresses reward sparsity in flow matching models by introducing step-level incremental rewards and identifying turning points to capture long-term effects in denoising trajectories.",
      "ai_keywords": [
        "GRPO",
        "flow matching models",
        "text-to-image generation",
        "denoising steps",
        "reward sparsity",
        "incremental rewards",
        "turning points",
        "denoising trajectory",
        "delayed impact",
        "reward evolution"
      ],
      "githubStars": 2,
      "organization": {
        "_id": "61bac2af530e5c78d7b99667",
        "name": "zju",
        "fullname": "Zhejiang University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
      }
    },
    "publishedAt": "2026-02-06T01:37:10.000Z",
    "title": "Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO",
    "summary": "Deploying GRPO on Flow Matching models has proven effective for text-to-image generation. However, existing paradigms typically propagate an outcome-based reward to all preceding denoising steps without distinguishing the local effect of each step. Moreover, current group-wise ranking mainly compares trajectories at matched timesteps and ignores within-trajectory dependencies, where certain early denoising actions can affect later states via delayed, implicit interactions. We propose TurningPoint-GRPO (TP-GRPO), a GRPO framework that alleviates step-wise reward sparsity and explicitly models long-term effects within the denoising trajectory. TP-GRPO makes two key innovations: (i) it replaces outcome-based rewards with step-level incremental rewards, providing a dense, step-aware learning signal that better isolates each denoising action's \"pure\" effect, and (ii) it identifies turning points-steps that flip the local reward trend and make subsequent reward evolution consistent with the overall trajectory trend-and assigns these actions an aggregated long-term reward to capture their delayed impact. Turning points are detected solely via sign changes in incremental rewards, making TP-GRPO efficient and hyperparameter-free. Extensive experiments also demonstrate that TP-GRPO exploits reward signals more effectively and consistently improves generation. Demo code is available at https://github.com/YunzeTong/TurningPoint-GRPO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06422.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646efd223dd912a539e0bd46",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/EOFAv5xvOgJOzuDgh4nSb.png",
      "fullname": "Canyu Zhao",
      "name": "Canyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 16,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "61bac2af530e5c78d7b99667",
      "name": "zju",
      "fullname": "Zhejiang University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.07845",
      "authors": [
        {
          "_id": "698ab2ef1b2dc6b37d61af7b",
          "name": "Yalcin Tur",
          "hidden": false
        },
        {
          "_id": "698ab2ef1b2dc6b37d61af7c",
          "name": "Jalal Naghiyev",
          "hidden": false
        },
        {
          "_id": "698ab2ef1b2dc6b37d61af7d",
          "name": "Haoquan Fang",
          "hidden": false
        },
        {
          "_id": "698ab2ef1b2dc6b37d61af7e",
          "name": "Wei-Chuan Tsai",
          "hidden": false
        },
        {
          "_id": "698ab2ef1b2dc6b37d61af7f",
          "name": "Jiafei Duan",
          "hidden": false
        },
        {
          "_id": "698ab2ef1b2dc6b37d61af80",
          "name": "Dieter Fox",
          "hidden": false
        },
        {
          "_id": "698ab2ef1b2dc6b37d61af81",
          "name": "Ranjay Krishna",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/632b42626110e37dba3d5bcb/QxggswtZU5KkZQawFarz8.mp4"
      ],
      "publishedAt": "2026-02-08T07:21:01.000Z",
      "submittedOnDailyAt": "2026-02-10T02:01:42.213Z",
      "title": "Recurrent-Depth VLA: Implicit Test-Time Compute Scaling of Vision-Language-Action Models via Latent Iterative Reasoning",
      "submittedOnDailyBy": {
        "_id": "632b42626110e37dba3d5bcb",
        "avatarUrl": "/avatars/ca70a15def71ee84f4f149db5e954843.svg",
        "isPro": false,
        "fullname": "Duan",
        "user": "Jiafei1224",
        "type": "user"
      },
      "summary": "Current Vision-Language-Action (VLA) models rely on fixed computational depth, expending the same amount of compute on simple adjustments and complex multi-step manipulation. While Chain-of-Thought (CoT) prompting enables variable computation, it scales memory linearly and is ill-suited for continuous action spaces. We introduce Recurrent-Depth VLA (RD-VLA), an architecture that achieves computational adaptivity via latent iterative refinement rather than explicit token generation. RD-VLA employs a recurrent, weight-tied action head that supports arbitrary inference depth with a constant memory footprint. The model is trained using truncated backpropagation through time (TBPTT) to efficiently supervise the refinement process. At inference, RD-VLA dynamically allocates compute using an adaptive stopping criterion based on latent convergence. Experiments on challenging manipulation tasks show that recurrent depth is critical: tasks that fail entirely (0 percent success) with single-iteration inference exceed 90 percent success with four iterations, while simpler tasks saturate rapidly. RD-VLA provides a scalable path to test-time compute in robotics, replacing token-based reasoning with latent reasoning to achieve constant memory usage and up to 80x inference speedup over prior reasoning-based VLA models. Project page: https://rd-vla.github.io/",
      "upvotes": 27,
      "discussionId": "698ab2ef1b2dc6b37d61af82",
      "projectPage": "https://rd-vla.github.io/",
      "githubRepo": "https://github.com/rd-vla/rd-vla",
      "githubRepoAddedBy": "user",
      "ai_summary": "RD-VLA introduces a recurrent architecture for vision-language-action models that adapts computational depth through latent iterative refinement, achieving constant memory usage and improved task success rates.",
      "ai_keywords": [
        "Vision-Language-Action models",
        "Chain-of-Thought prompting",
        "recurrent architecture",
        "weight-tied action head",
        "truncated backpropagation through time",
        "latent iterative refinement",
        "adaptive stopping criterion",
        "latent convergence",
        "computational adaptivity"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "5e70f3648ce3c604d78fe132",
        "name": "allenai",
        "fullname": "Ai2",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652db071b62cf1f8463221e2/CxxwFiaomTa1MCX_B7-pT.png"
      }
    },
    "publishedAt": "2026-02-08T02:21:01.000Z",
    "title": "Recurrent-Depth VLA: Implicit Test-Time Compute Scaling of Vision-Language-Action Models via Latent Iterative Reasoning",
    "summary": "Current Vision-Language-Action (VLA) models rely on fixed computational depth, expending the same amount of compute on simple adjustments and complex multi-step manipulation. While Chain-of-Thought (CoT) prompting enables variable computation, it scales memory linearly and is ill-suited for continuous action spaces. We introduce Recurrent-Depth VLA (RD-VLA), an architecture that achieves computational adaptivity via latent iterative refinement rather than explicit token generation. RD-VLA employs a recurrent, weight-tied action head that supports arbitrary inference depth with a constant memory footprint. The model is trained using truncated backpropagation through time (TBPTT) to efficiently supervise the refinement process. At inference, RD-VLA dynamically allocates compute using an adaptive stopping criterion based on latent convergence. Experiments on challenging manipulation tasks show that recurrent depth is critical: tasks that fail entirely (0 percent success) with single-iteration inference exceed 90 percent success with four iterations, while simpler tasks saturate rapidly. RD-VLA provides a scalable path to test-time compute in robotics, replacing token-based reasoning with latent reasoning to achieve constant memory usage and up to 80x inference speedup over prior reasoning-based VLA models. Project page: https://rd-vla.github.io/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/632b42626110e37dba3d5bcb/QxggswtZU5KkZQawFarz8.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07845.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "632b42626110e37dba3d5bcb",
      "avatarUrl": "/avatars/ca70a15def71ee84f4f149db5e954843.svg",
      "fullname": "Duan",
      "name": "Jiafei1224",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "5e70f3648ce3c604d78fe132",
      "name": "allenai",
      "fullname": "Ai2",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652db071b62cf1f8463221e2/CxxwFiaomTa1MCX_B7-pT.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.06025",
      "authors": [
        {
          "_id": "698608f09c78be977c104b8c",
          "name": "Haozhen Zhang",
          "hidden": false
        },
        {
          "_id": "698608f09c78be977c104b8d",
          "name": "Haodong Yue",
          "hidden": false
        },
        {
          "_id": "698608f09c78be977c104b8e",
          "name": "Tao Feng",
          "hidden": false
        },
        {
          "_id": "698608f09c78be977c104b8f",
          "name": "Quanyu Long",
          "hidden": false
        },
        {
          "_id": "698608f09c78be977c104b90",
          "name": "Jianzhu Bao",
          "hidden": false
        },
        {
          "_id": "698608f09c78be977c104b91",
          "name": "Bowen Jin",
          "hidden": false
        },
        {
          "_id": "698608f09c78be977c104b92",
          "name": "Weizhi Zhang",
          "hidden": false
        },
        {
          "_id": "698608f09c78be977c104b93",
          "name": "Xiao Li",
          "hidden": false
        },
        {
          "_id": "698608f09c78be977c104b94",
          "name": "Jiaxuan You",
          "hidden": false
        },
        {
          "_id": "698608f09c78be977c104b95",
          "name": "Chengwei Qin",
          "hidden": false
        },
        {
          "_id": "698608f09c78be977c104b96",
          "name": "Wenya Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-05T18:57:09.000Z",
      "submittedOnDailyAt": "2026-02-10T05:16:01.749Z",
      "title": "Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory",
      "submittedOnDailyBy": {
        "_id": "64f58f3468047192d6c7f335",
        "avatarUrl": "/avatars/88be16ee80da7d2eaa0feae878375001.svg",
        "isPro": false,
        "fullname": "XaiverZ",
        "user": "XaiverZ",
        "type": "user"
      },
      "summary": "Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present BudgetMem, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., Low/Mid/High). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.",
      "upvotes": 25,
      "discussionId": "698608f09c78be977c104b97",
      "projectPage": "https://viktoraxelsen.github.io/BudgetMem/",
      "githubRepo": "https://github.com/ViktorAxelsen/BudgetMem",
      "githubRepoAddedBy": "user",
      "ai_summary": "BudgetMem is a runtime memory framework for LLM agents that uses modular components with three budget tiers and a neural policy router to optimize performance-cost trade-offs in memory usage.",
      "ai_keywords": [
        "runtime memory utilization",
        "query-aware performance-cost control",
        "memory modules",
        "budget tiers",
        "lightweight router",
        "neural policy",
        "reinforcement learning",
        "LoCoMo",
        "LongMemEval",
        "HotpotQA"
      ],
      "githubStars": 6,
      "organization": {
        "_id": "6508b28cf36bb51c50faad98",
        "name": "NanyangTechnologicalUniversity",
        "fullname": "Nanyang Technological University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
      }
    },
    "publishedAt": "2026-02-05T13:57:09.000Z",
    "title": "Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory",
    "summary": "Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present BudgetMem, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., Low/Mid/High). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06025.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f58f3468047192d6c7f335",
      "avatarUrl": "/avatars/88be16ee80da7d2eaa0feae878375001.svg",
      "fullname": "XaiverZ",
      "name": "XaiverZ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6508b28cf36bb51c50faad98",
      "name": "NanyangTechnologicalUniversity",
      "fullname": "Nanyang Technological University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.08676",
      "authors": [
        {
          "_id": "698ab6fd1b2dc6b37d61b04b",
          "name": "Tiwei Bie",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b04c",
          "name": "Maosong Cao",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b04d",
          "name": "Xiang Cao",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b04e",
          "name": "Bingsen Chen",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b04f",
          "name": "Fuyuan Chen",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b050",
          "name": "Kun Chen",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b051",
          "name": "Lun Du",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b052",
          "name": "Daozhuo Feng",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b053",
          "name": "Haibo Feng",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b054",
          "name": "Mingliang Gong",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b055",
          "name": "Zhuocheng Gong",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b056",
          "name": "Yanmei Gu",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b057",
          "name": "Jian Guan",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b058",
          "name": "Kaiyuan Guan",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b059",
          "name": "Hongliang He",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b05a",
          "name": "Zenan Huang",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b05b",
          "name": "Juyong Jiang",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b05c",
          "name": "Zhonghui Jiang",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b05d",
          "name": "Zhenzhong Lan",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b05e",
          "name": "Chengxi Li",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b05f",
          "name": "Jianguo Li",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b060",
          "name": "Zehuan Li",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b061",
          "name": "Huabin Liu",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b062",
          "name": "Lin Liu",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b063",
          "name": "Guoshan Lu",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b064",
          "name": "Yuan Lu",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b065",
          "name": "Yuxin Ma",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b066",
          "name": "Xingyu Mou",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b067",
          "name": "Zhenxuan Pan",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b068",
          "name": "Kaida Qiu",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b069",
          "name": "Yuji Ren",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b06a",
          "name": "Jianfeng Tan",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b06b",
          "name": "Yiding Tian",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b06c",
          "name": "Zian Wang",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b06d",
          "name": "Lanning Wei",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b06e",
          "name": "Tao Wu",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b06f",
          "name": "Yipeng Xing",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b070",
          "name": "Wentao Ye",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b071",
          "name": "Liangyu Zha",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b072",
          "name": "Tianze Zhang",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b073",
          "name": "Xiaolu Zhang",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b074",
          "name": "Junbo Zhao",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b075",
          "name": "Da Zheng",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b076",
          "name": "Hao Zhong",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b077",
          "name": "Wanli Zhong",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b078",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b079",
          "name": "Junlin Zhou",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b07a",
          "name": "Liwang Zhu",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b07b",
          "name": "Muzhi Zhu",
          "hidden": false
        },
        {
          "_id": "698ab6fd1b2dc6b37d61b07c",
          "name": "Yihong Zhuang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-09T14:00:07.000Z",
      "submittedOnDailyAt": "2026-02-10T02:14:10.029Z",
      "title": "LLaDA2.1: Speeding Up Text Diffusion via Token Editing",
      "submittedOnDailyBy": {
        "_id": "673b5f24e863f1d28b402efc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/19gUgtPEY3-FtY0sNlI_-.png",
        "isPro": false,
        "fullname": "yihongzhuang",
        "user": "utdawn",
        "type": "user"
      },
      "summary": "While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, a paradigm shift designed to transcend this trade-off. By seamlessly weaving Token-to-Token (T2T) editing into the conventional Mask-to-Token (M2T) scheme, we introduce a joint, configurable threshold-decoding scheme. This structural innovation gives rise to two distinct personas: the Speedy Mode (S Mode), which audaciously lowers the M2T threshold to bypass traditional constraints while relying on T2T to refine the output; and the Quality Mode (Q Mode), which leans into conservative thresholds to secure superior benchmark performances with manageable efficiency degrade. Furthering this evolution, underpinned by an expansive context window, we implement the first large-scale Reinforcement Learning (RL) framework specifically tailored for dLLMs, anchored by specialized techniques for stable gradient estimation. This alignment not only sharpens reasoning precision but also elevates instruction-following fidelity, bridging the chasm between diffusion dynamics and complex human intent. We culminate this work by releasing LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Across 33 rigorous benchmarks, LLaDA2.1 delivers strong task performance and lightning-fast decoding speed. Despite its 100B volume, on coding tasks it attains an astounding 892 TPS on HumanEval+, 801 TPS on BigCodeBench, and 663 TPS on LiveCodeBench.",
      "upvotes": 24,
      "discussionId": "698ab6fd1b2dc6b37d61b07d",
      "githubRepo": "https://github.com/inclusionAI/LLaDA2.X",
      "githubRepoAddedBy": "user",
      "ai_summary": "LLaDA2.1 introduces a novel token-to-token editing approach with speed and quality modes, enhanced through reinforcement learning for improved reasoning and instruction following in large language diffusion models.",
      "ai_keywords": [
        "block-diffusion models",
        "decoding speed",
        "generation quality",
        "Token-to-Token editing",
        "Mask-to-Token scheme",
        "threshold-decoding scheme",
        "Speedy Mode",
        "Quality Mode",
        "Reinforcement Learning",
        "gradient estimation",
        "reasoning precision",
        "instruction-following",
        "large language diffusion models",
        "HumanEval+",
        "BigCodeBench",
        "LiveCodeBench"
      ],
      "githubStars": 242,
      "organization": {
        "_id": "67aea5c8f086ab0f70ed97c9",
        "name": "inclusionAI",
        "fullname": "inclusionAI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"
      }
    },
    "publishedAt": "2026-02-09T09:00:07.000Z",
    "title": "LLaDA2.1: Speeding Up Text Diffusion via Token Editing",
    "summary": "While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, a paradigm shift designed to transcend this trade-off. By seamlessly weaving Token-to-Token (T2T) editing into the conventional Mask-to-Token (M2T) scheme, we introduce a joint, configurable threshold-decoding scheme. This structural innovation gives rise to two distinct personas: the Speedy Mode (S Mode), which audaciously lowers the M2T threshold to bypass traditional constraints while relying on T2T to refine the output; and the Quality Mode (Q Mode), which leans into conservative thresholds to secure superior benchmark performances with manageable efficiency degrade. Furthering this evolution, underpinned by an expansive context window, we implement the first large-scale Reinforcement Learning (RL) framework specifically tailored for dLLMs, anchored by specialized techniques for stable gradient estimation. This alignment not only sharpens reasoning precision but also elevates instruction-following fidelity, bridging the chasm between diffusion dynamics and complex human intent. We culminate this work by releasing LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Across 33 rigorous benchmarks, LLaDA2.1 delivers strong task performance and lightning-fast decoding speed. Despite its 100B volume, on coding tasks it attains an astounding 892 TPS on HumanEval+, 801 TPS on BigCodeBench, and 663 TPS on LiveCodeBench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08676.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "673b5f24e863f1d28b402efc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/19gUgtPEY3-FtY0sNlI_-.png",
      "fullname": "yihongzhuang",
      "name": "utdawn",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "67aea5c8f086ab0f70ed97c9",
      "name": "inclusionAI",
      "fullname": "inclusionAI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.08439",
      "authors": [
        {
          "_id": "698ab3e51b2dc6b37d61afaa",
          "name": "Yuhao Dong",
          "hidden": false
        },
        {
          "_id": "698ab3e51b2dc6b37d61afab",
          "name": "Shulin Tian",
          "hidden": false
        },
        {
          "_id": "698ab3e51b2dc6b37d61afac",
          "name": "Shuai Liu",
          "hidden": false
        },
        {
          "_id": "698ab3e51b2dc6b37d61afad",
          "name": "Shuangrui Ding",
          "hidden": false
        },
        {
          "_id": "698ab3e51b2dc6b37d61afae",
          "name": "Yuhang Zang",
          "hidden": false
        },
        {
          "_id": "698ab3e51b2dc6b37d61afaf",
          "name": "Xiaoyi Dong",
          "hidden": false
        },
        {
          "_id": "698ab3e51b2dc6b37d61afb0",
          "name": "Yuhang Cao",
          "hidden": false
        },
        {
          "_id": "698ab3e51b2dc6b37d61afb1",
          "name": "Jiaqi Wang",
          "hidden": false
        },
        {
          "_id": "698ab3e51b2dc6b37d61afb2",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-09T09:51:29.000Z",
      "submittedOnDailyAt": "2026-02-10T02:03:30.996Z",
      "title": "Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition",
      "submittedOnDailyBy": {
        "_id": "652965773a416e1f2173443b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg",
        "isPro": true,
        "fullname": "Yuhao Dong",
        "user": "THUdyh",
        "type": "user"
      },
      "summary": "Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.",
      "upvotes": 22,
      "discussionId": "698ab3e51b2dc6b37d61afb3",
      "githubRepo": "https://github.com/dongyh20/Demo-ICL",
      "githubRepoAddedBy": "user",
      "ai_summary": "Researchers introduce a new video understanding task and benchmark that evaluates models' ability to learn from few-shot demonstrations, along with a specialized MLLM architecture trained using a two-stage approach combining video supervision and preference optimization.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "video understanding",
        "in-context learning",
        "video benchmarks",
        "Demo-ICL-Bench",
        "video-supervised fine-tuning",
        "direct preference optimization"
      ],
      "githubStars": 8,
      "organization": {
        "_id": "62d55f243bf5e059f7ca25ba",
        "name": "mmlab-ntu",
        "fullname": "MMLab@NTU",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1658151991971-62b5777f593a2c49da69dc02.png"
      }
    },
    "publishedAt": "2026-02-09T04:51:29.000Z",
    "title": "Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition",
    "summary": "Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08439.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "652965773a416e1f2173443b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg",
      "fullname": "Yuhao Dong",
      "name": "THUdyh",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 50,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "62d55f243bf5e059f7ca25ba",
      "name": "mmlab-ntu",
      "fullname": "MMLab@NTU",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1658151991971-62b5777f593a2c49da69dc02.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.08222",
      "authors": [
        {
          "_id": "698ad20e1b2dc6b37d61b227",
          "name": "Zehao Chen",
          "hidden": false
        },
        {
          "_id": "698ad20e1b2dc6b37d61b228",
          "name": "Gongxun Li",
          "hidden": false
        },
        {
          "_id": "698ad20e1b2dc6b37d61b229",
          "name": "Tianxiang Ai",
          "hidden": false
        },
        {
          "_id": "698ad20e1b2dc6b37d61b22a",
          "name": "Yifei Li",
          "hidden": false
        },
        {
          "_id": "698ad20e1b2dc6b37d61b22b",
          "name": "Zixuan Huang",
          "hidden": false
        },
        {
          "_id": "698ad20e1b2dc6b37d61b22c",
          "name": "Wang Zhou",
          "hidden": false
        },
        {
          "_id": "698ad20e1b2dc6b37d61b22d",
          "name": "Fuzhen Zhuang",
          "hidden": false
        },
        {
          "_id": "698ad20e1b2dc6b37d61b22e",
          "name": "Xianglong Liu",
          "hidden": false
        },
        {
          "_id": "698ad20e1b2dc6b37d61b22f",
          "name": "Jianxin Li",
          "hidden": false
        },
        {
          "_id": "698ad20e1b2dc6b37d61b230",
          "name": "Deqing Wang",
          "hidden": false
        },
        {
          "_id": "698ad20e1b2dc6b37d61b231",
          "name": "Yikun Ban",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-09T02:50:40.000Z",
      "submittedOnDailyAt": "2026-02-10T04:36:28.975Z",
      "title": "Weak-Driven Learning: How Weak Agents make Strong Agents Stronger",
      "submittedOnDailyBy": {
        "_id": "68345345f4bbf856e2d708e2",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg",
        "isPro": false,
        "fullname": "Yikun Ban",
        "user": "Yikunb",
        "type": "user"
      },
      "summary": "As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.",
      "upvotes": 22,
      "discussionId": "698ad20e1b2dc6b37d61b232",
      "githubRepo": "https://github.com/chenzehao82/Weak-Driven-Learning",
      "githubRepoAddedBy": "user",
      "ai_summary": "WMSS is a post-training paradigm that uses weak model checkpoints to identify and fill learning gaps, enabling continued improvement beyond conventional saturation points in large language models.",
      "ai_keywords": [
        "post-training optimization",
        "large language models",
        "saturation bottleneck",
        "weak checkpoints",
        "entropy dynamics",
        "compensatory learning",
        "learning gaps"
      ],
      "githubStars": 9,
      "organization": {
        "_id": "63ba7720fc454697637969f1",
        "name": "Beihang",
        "fullname": "Beihang University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"
      }
    },
    "publishedAt": "2026-02-08T21:50:40.000Z",
    "title": "Weak-Driven Learning: How Weak Agents make Strong Agents Stronger",
    "summary": "As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08222.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "68345345f4bbf856e2d708e2",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg",
      "fullname": "Yikun Ban",
      "name": "Yikunb",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "63ba7720fc454697637969f1",
      "name": "Beihang",
      "fullname": "Beihang University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.07962",
      "authors": [
        {
          "_id": "698a9f631b2dc6b37d61af47",
          "name": "Weihao Zeng",
          "hidden": false
        },
        {
          "_id": "698a9f631b2dc6b37d61af48",
          "name": "Yuzhen Huang",
          "hidden": false
        },
        {
          "_id": "698a9f631b2dc6b37d61af49",
          "name": "Junxian He",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-08T13:20:39.000Z",
      "submittedOnDailyAt": "2026-02-10T00:32:12.677Z",
      "title": "LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth",
      "submittedOnDailyBy": {
        "_id": "62751082b43ccfeef483424f",
        "avatarUrl": "/avatars/fec83e4478e7d1731ba6033328131852.svg",
        "isPro": false,
        "fullname": "WeihaoZeng",
        "user": "AndrewZeng",
        "type": "user"
      },
      "summary": "Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as the amount of context grows, their reliability often deteriorates, a phenomenon known as \"context rot\". Existing long-context benchmarks primarily focus on single-step settings that evaluate a model's ability to retrieve information from a long snippet. In realistic scenarios, however, LLMs often need to act as agents that explore environments, follow instructions and plans, extract useful information, and predict correct actions under a dynamically growing context. To assess language agents in such settings, we introduce LOCA-bench (a benchmark for LOng-Context Agents). Given a task prompt, LOCA-bench leverages automated and scalable control of environment states to regulate the agent's context length. This design enables LOCA-bench to extend the context length potentially to infinity in a controlled way while keeping the underlying task semantics fixed. LOCA-bench evaluates language agents as a combination of models and scaffolds, including various context management strategies. While agent performance generally degrades as the environment states grow more complex, advanced context management techniques can substantially improve the overall success rate. We open-source LOCA-bench to provide a platform for evaluating models and scaffolds in long-context, agentic scenarios: https://github.com/hkust-nlp/LOCA-bench",
      "upvotes": 21,
      "discussionId": "698a9f631b2dc6b37d61af4a",
      "githubRepo": "https://github.com/hkust-nlp/LOCA-bench",
      "githubRepoAddedBy": "user",
      "ai_summary": "LOCA-bench is introduced as a benchmark for evaluating language agents in long-context, agentic scenarios with controlled environment state management.",
      "ai_keywords": [
        "large language models",
        "context rot",
        "long-context benchmarks",
        "language agents",
        "LOCA-bench",
        "environment states",
        "context management strategies",
        "agent performance"
      ],
      "githubStars": 5,
      "organization": {
        "_id": "647693e442e5f529745b9ba6",
        "name": "hkust-nlp",
        "fullname": "HKUST NLP Group"
      }
    },
    "publishedAt": "2026-02-08T08:20:39.000Z",
    "title": "LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth",
    "summary": "Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as the amount of context grows, their reliability often deteriorates, a phenomenon known as \"context rot\". Existing long-context benchmarks primarily focus on single-step settings that evaluate a model's ability to retrieve information from a long snippet. In realistic scenarios, however, LLMs often need to act as agents that explore environments, follow instructions and plans, extract useful information, and predict correct actions under a dynamically growing context. To assess language agents in such settings, we introduce LOCA-bench (a benchmark for LOng-Context Agents). Given a task prompt, LOCA-bench leverages automated and scalable control of environment states to regulate the agent's context length. This design enables LOCA-bench to extend the context length potentially to infinity in a controlled way while keeping the underlying task semantics fixed. LOCA-bench evaluates language agents as a combination of models and scaffolds, including various context management strategies. While agent performance generally degrades as the environment states grow more complex, advanced context management techniques can substantially improve the overall success rate. We open-source LOCA-bench to provide a platform for evaluating models and scaffolds in long-context, agentic scenarios: https://github.com/hkust-nlp/LOCA-bench",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07962.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62751082b43ccfeef483424f",
      "avatarUrl": "/avatars/fec83e4478e7d1731ba6033328131852.svg",
      "fullname": "WeihaoZeng",
      "name": "AndrewZeng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "647693e442e5f529745b9ba6",
      "name": "hkust-nlp",
      "fullname": "HKUST NLP Group"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.09007",
      "authors": [
        {
          "_id": "698ad8ac1b2dc6b37d61b275",
          "name": "Haodong Li",
          "hidden": false
        },
        {
          "_id": "698ad8ac1b2dc6b37d61b276",
          "name": "Jingwei Wu",
          "hidden": false
        },
        {
          "_id": "698ad8ac1b2dc6b37d61b277",
          "name": "Quan Sun",
          "hidden": false
        },
        {
          "_id": "698ad8ac1b2dc6b37d61b278",
          "name": "Guopeng Li",
          "hidden": false
        },
        {
          "_id": "698ad8ac1b2dc6b37d61b279",
          "name": "Juanxi Tian",
          "hidden": false
        },
        {
          "_id": "698ad8ac1b2dc6b37d61b27a",
          "name": "Huanyu Zhang",
          "hidden": false
        },
        {
          "_id": "698ad8ac1b2dc6b37d61b27b",
          "name": "Yanlin Lai",
          "hidden": false
        },
        {
          "_id": "698ad8ac1b2dc6b37d61b27c",
          "name": "Ruichuan An",
          "hidden": false
        },
        {
          "_id": "698ad8ac1b2dc6b37d61b27d",
          "name": "Hongbo Peng",
          "hidden": false
        },
        {
          "_id": "698ad8ac1b2dc6b37d61b27e",
          "name": "Yuhong Dai",
          "hidden": false
        },
        {
          "_id": "698ad8ac1b2dc6b37d61b27f",
          "name": "Chenxi Li",
          "hidden": false
        },
        {
          "_id": "698ad8ac1b2dc6b37d61b280",
          "name": "Chunmei Qing",
          "hidden": false
        },
        {
          "_id": "698ad8ac1b2dc6b37d61b281",
          "name": "Jia Wang",
          "hidden": false
        },
        {
          "_id": "698ad8ac1b2dc6b37d61b282",
          "name": "Ziyang Meng",
          "hidden": false
        },
        {
          "_id": "698ad8ac1b2dc6b37d61b283",
          "name": "Zheng Ge",
          "hidden": false
        },
        {
          "_id": "698ad8ac1b2dc6b37d61b284",
          "name": "Xiangyu Zhang",
          "hidden": false
        },
        {
          "_id": "698ad8ac1b2dc6b37d61b285",
          "name": "Daxin Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-09T18:52:02.000Z",
      "submittedOnDailyAt": "2026-02-10T04:37:26.012Z",
      "title": "GEBench: Benchmarking Image Generation Models as GUI Environments",
      "submittedOnDailyBy": {
        "_id": "65ddea8b2d26e59a5a33330f",
        "avatarUrl": "/avatars/3104ddafd6dda3c05ea9a771dbf2deeb.svg",
        "isPro": false,
        "fullname": "li haodong",
        "user": "mickyhimself",
        "type": "user"
      },
      "summary": "Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.",
      "upvotes": 16,
      "discussionId": "698ad8ad1b2dc6b37d61b286",
      "ai_summary": "A new benchmark and evaluation metric are introduced for assessing temporal coherence and dynamic interaction in GUI generation models, revealing significant challenges in maintaining consistency over extended interaction sequences.",
      "ai_keywords": [
        "GUI generation",
        "temporal coherence",
        "dynamic interaction",
        "visual fidelity",
        "GUI-specific contexts",
        "GEBench",
        "GE-Score",
        "goal achievement",
        "interaction logic",
        "content consistency",
        "UI plausibility",
        "visual quality"
      ],
      "organization": {
        "_id": "66e43eae9d477f566f937935",
        "name": "stepfun-ai",
        "fullname": "StepFun",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"
      }
    },
    "publishedAt": "2026-02-09T13:52:02.000Z",
    "title": "GEBench: Benchmarking Image Generation Models as GUI Environments",
    "summary": "Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09007.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65ddea8b2d26e59a5a33330f",
      "avatarUrl": "/avatars/3104ddafd6dda3c05ea9a771dbf2deeb.svg",
      "fullname": "li haodong",
      "name": "mickyhimself",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "66e43eae9d477f566f937935",
      "name": "stepfun-ai",
      "fullname": "StepFun",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.07075",
      "authors": [
        {
          "_id": "698a9b8d1b2dc6b37d61af1a",
          "name": "Xinwu Ye",
          "hidden": false
        },
        {
          "_id": "698a9b8d1b2dc6b37d61af1b",
          "name": "Yicheng Mao",
          "hidden": false
        },
        {
          "_id": "698a9b8d1b2dc6b37d61af1c",
          "name": "Jia Zhang",
          "hidden": false
        },
        {
          "_id": "698a9b8d1b2dc6b37d61af1d",
          "name": "Yimeng Liu",
          "hidden": false
        },
        {
          "_id": "698a9b8d1b2dc6b37d61af1e",
          "name": "Li Hao",
          "hidden": false
        },
        {
          "_id": "698a9b8d1b2dc6b37d61af1f",
          "name": "Fang Wu",
          "hidden": false
        },
        {
          "_id": "698a9b8d1b2dc6b37d61af20",
          "name": "Zhiwei Li",
          "hidden": false
        },
        {
          "_id": "698a9b8d1b2dc6b37d61af21",
          "name": "Yuxuan Liao",
          "hidden": false
        },
        {
          "_id": "698a9b8d1b2dc6b37d61af22",
          "name": "Zehong Wang",
          "hidden": false
        },
        {
          "_id": "698a9b8d1b2dc6b37d61af23",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "698a9b8d1b2dc6b37d61af24",
          "name": "Zhenfei Yin",
          "hidden": false
        },
        {
          "_id": "698a9b8d1b2dc6b37d61af25",
          "name": "Li Yuan",
          "hidden": false
        },
        {
          "_id": "698a9b8d1b2dc6b37d61af26",
          "name": "Philip Torr",
          "hidden": false
        },
        {
          "_id": "698a9b8d1b2dc6b37d61af27",
          "name": "Huan Sun",
          "hidden": false
        },
        {
          "_id": "698a9b8d1b2dc6b37d61af28",
          "name": "Xiangxiang Zeng",
          "hidden": false
        },
        {
          "_id": "698a9b8d1b2dc6b37d61af29",
          "name": "Mengdi Wang",
          "hidden": false
        },
        {
          "_id": "698a9b8d1b2dc6b37d61af2a",
          "name": "Le Cong",
          "hidden": false
        },
        {
          "_id": "698a9b8d1b2dc6b37d61af2b",
          "name": "Shenghua Gao",
          "hidden": false
        },
        {
          "_id": "698a9b8d1b2dc6b37d61af2c",
          "name": "Xiangru Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-06T01:28:27.000Z",
      "submittedOnDailyAt": "2026-02-10T01:22:01.665Z",
      "title": "LatentChem: From Textual CoT to Latent Thinking in Chemical Reasoning",
      "submittedOnDailyBy": {
        "_id": "63357c608adfa81faf2ac180",
        "avatarUrl": "/avatars/ae0314c644f882251baf59b9134fd36f.svg",
        "isPro": false,
        "fullname": "Xiangru Tang",
        "user": "RTT1",
        "type": "user"
      },
      "summary": "Chemical large language models (LLMs) predominantly rely on explicit Chain-of-Thought (CoT) in natural language to perform complex reasoning. However, chemical reasoning is inherently continuous and structural, and forcing it into discrete linguistic tokens introduces a fundamental representation mismatch that constrains both efficiency and performance. We introduce LatentChem, a latent reasoning interface that decouples chemical computation from textual generation, enabling models to perform multi-step reasoning directly in continuous latent space while emitting language only for final outputs. Remarkably, we observe a consistent emergent behavior: when optimized solely for task success, models spontaneously internalize reasoning, progressively abandoning verbose textual derivations in favor of implicit latent computation. This shift is not merely stylistic but computationally advantageous. Across diverse chemical reasoning benchmarks, LatentChem achieves a 59.88\\% non-tie win rate over strong CoT-based baselines on ChemCoTBench, while delivering a 10.84times average inference speedup. Our results provide empirical evidence that chemical reasoning is more naturally and effectively realized as continuous latent dynamics rather than discretized linguistic trajectories.",
      "upvotes": 14,
      "discussionId": "698a9b8d1b2dc6b37d61af2d",
      "ai_summary": "LatentChem enables chemical reasoning through continuous latent space computations instead of discrete textual tokens, achieving superior performance and efficiency compared to traditional chain-of-thought approaches.",
      "ai_keywords": [
        "chemical large language models",
        "Chain-of-Thought",
        "latent reasoning",
        "continuous latent space",
        "textual generation",
        "multi-step reasoning",
        "ChemCoTBench",
        "inference speedup"
      ]
    },
    "publishedAt": "2026-02-05T20:28:27.000Z",
    "title": "LatentChem: From Textual CoT to Latent Thinking in Chemical Reasoning",
    "summary": "Chemical large language models (LLMs) predominantly rely on explicit Chain-of-Thought (CoT) in natural language to perform complex reasoning. However, chemical reasoning is inherently continuous and structural, and forcing it into discrete linguistic tokens introduces a fundamental representation mismatch that constrains both efficiency and performance. We introduce LatentChem, a latent reasoning interface that decouples chemical computation from textual generation, enabling models to perform multi-step reasoning directly in continuous latent space while emitting language only for final outputs. Remarkably, we observe a consistent emergent behavior: when optimized solely for task success, models spontaneously internalize reasoning, progressively abandoning verbose textual derivations in favor of implicit latent computation. This shift is not merely stylistic but computationally advantageous. Across diverse chemical reasoning benchmarks, LatentChem achieves a 59.88\\% non-tie win rate over strong CoT-based baselines on ChemCoTBench, while delivering a 10.84times average inference speedup. Our results provide empirical evidence that chemical reasoning is more naturally and effectively realized as continuous latent dynamics rather than discretized linguistic trajectories.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07075.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63357c608adfa81faf2ac180",
      "avatarUrl": "/avatars/ae0314c644f882251baf59b9134fd36f.svg",
      "fullname": "Xiangru Tang",
      "name": "RTT1",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.09022",
      "authors": [
        {
          "_id": "698ab6ec1b2dc6b37d61b023",
          "name": "Zehan Wang",
          "hidden": false
        },
        {
          "_id": "698ab6ec1b2dc6b37d61b024",
          "name": "Tengfei Wang",
          "hidden": false
        },
        {
          "_id": "698ab6ec1b2dc6b37d61b025",
          "name": "Haiyu Zhang",
          "hidden": false
        },
        {
          "_id": "698ab6ec1b2dc6b37d61b026",
          "name": "Xuhui Zuo",
          "hidden": false
        },
        {
          "_id": "698ab6ec1b2dc6b37d61b027",
          "name": "Junta Wu",
          "hidden": false
        },
        {
          "_id": "698ab6ec1b2dc6b37d61b028",
          "name": "Haoyuan Wang",
          "hidden": false
        },
        {
          "_id": "698ab6ec1b2dc6b37d61b029",
          "name": "Wenqiang Sun",
          "hidden": false
        },
        {
          "_id": "698ab6ec1b2dc6b37d61b02a",
          "name": "Zhenwei Wang",
          "hidden": false
        },
        {
          "_id": "698ab6ec1b2dc6b37d61b02b",
          "name": "Chenjie Cao",
          "hidden": false
        },
        {
          "_id": "698ab6ec1b2dc6b37d61b02c",
          "name": "Hengshuang Zhao",
          "hidden": false
        },
        {
          "_id": "698ab6ec1b2dc6b37d61b02d",
          "name": "Chunchao Guo",
          "hidden": false
        },
        {
          "_id": "698ab6ec1b2dc6b37d61b02e",
          "name": "Zhou Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-09T18:59:47.000Z",
      "submittedOnDailyAt": "2026-02-10T02:11:53.182Z",
      "title": "WorldCompass: Reinforcement Learning for Long-Horizon World Models",
      "submittedOnDailyBy": {
        "_id": "6425761a175bd295228311a0",
        "avatarUrl": "/avatars/dcd0d267445563d0616d5a31b5d754b7.svg",
        "isPro": false,
        "fullname": "zehan wang",
        "user": "sleetwang6",
        "type": "user"
      },
      "summary": "This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training framework for the long-horizon, interactive video-based world models, enabling them to explore the world more accurately and consistently based on interaction signals. To effectively \"steer\" the world model's exploration, we introduce three core innovations tailored to the autoregressive video generation paradigm: 1) Clip-level rollout Strategy: We generate and evaluate multiple samples at a single target clip, which significantly boosts rollout efficiency and provides fine-grained reward signals. 2) Complementary Reward Functions: We design reward functions for both interaction-following accuracy and visual quality, which provide direct supervision and effectively suppress reward-hacking behaviors. 3) Efficient RL Algorithm: We employ the negative-aware fine-tuning strategy coupled with various efficiency optimizations to efficiently and effectively enhance model capacity. Evaluations on the SoTA open-source world model, WorldPlay, demonstrate that WorldCompass significantly improves interaction accuracy and visual fidelity across various scenarios.",
      "upvotes": 13,
      "discussionId": "698ab6ec1b2dc6b37d61b02f",
      "projectPage": "https://3d-models.hunyuan.tencent.com/world/",
      "ai_summary": "WorldCompass enhances long-horizon video-based world models through reinforcement learning post-training with clip-level rollouts, complementary rewards, and efficient RL algorithms.",
      "ai_keywords": [
        "Reinforcement Learning",
        "world models",
        "video generation",
        "rollout strategy",
        "reward functions",
        "reward-hacking",
        "negative-aware fine-tuning",
        "efficiency optimizations"
      ],
      "organization": {
        "_id": "6645f953c39288df638dbdd5",
        "name": "Tencent-Hunyuan",
        "fullname": "Tencent Hunyuan",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"
      }
    },
    "publishedAt": "2026-02-09T13:59:47.000Z",
    "title": "WorldCompass: Reinforcement Learning for Long-Horizon World Models",
    "summary": "This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training framework for the long-horizon, interactive video-based world models, enabling them to explore the world more accurately and consistently based on interaction signals. To effectively \"steer\" the world model's exploration, we introduce three core innovations tailored to the autoregressive video generation paradigm: 1) Clip-level rollout Strategy: We generate and evaluate multiple samples at a single target clip, which significantly boosts rollout efficiency and provides fine-grained reward signals. 2) Complementary Reward Functions: We design reward functions for both interaction-following accuracy and visual quality, which provide direct supervision and effectively suppress reward-hacking behaviors. 3) Efficient RL Algorithm: We employ the negative-aware fine-tuning strategy coupled with various efficiency optimizations to efficiently and effectively enhance model capacity. Evaluations on the SoTA open-source world model, WorldPlay, demonstrate that WorldCompass significantly improves interaction accuracy and visual fidelity across various scenarios.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09022.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6425761a175bd295228311a0",
      "avatarUrl": "/avatars/dcd0d267445563d0616d5a31b5d754b7.svg",
      "fullname": "zehan wang",
      "name": "sleetwang6",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "6645f953c39288df638dbdd5",
      "name": "Tencent-Hunyuan",
      "fullname": "Tencent Hunyuan",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.07055",
      "authors": [
        {
          "_id": "698a94fc1b2dc6b37d61aecd",
          "name": "Pingyue Zhang",
          "hidden": false
        },
        {
          "_id": "698a94fc1b2dc6b37d61aece",
          "name": "Zihan Huang",
          "hidden": false
        },
        {
          "_id": "698a94fc1b2dc6b37d61aecf",
          "name": "Yue Wang",
          "hidden": false
        },
        {
          "_id": "698a94fc1b2dc6b37d61aed0",
          "name": "Jieyu Zhang",
          "hidden": false
        },
        {
          "_id": "698a94fc1b2dc6b37d61aed1",
          "name": "Letian Xue",
          "hidden": false
        },
        {
          "_id": "698a94fc1b2dc6b37d61aed2",
          "name": "Zihan Wang",
          "hidden": false
        },
        {
          "_id": "698a94fc1b2dc6b37d61aed3",
          "name": "Qineng Wang",
          "hidden": false
        },
        {
          "_id": "698a94fc1b2dc6b37d61aed4",
          "name": "Keshigeyan Chandrasegaran",
          "hidden": false
        },
        {
          "_id": "698a94fc1b2dc6b37d61aed5",
          "name": "Ruohan Zhang",
          "hidden": false
        },
        {
          "_id": "698a94fc1b2dc6b37d61aed6",
          "name": "Yejin Choi",
          "hidden": false
        },
        {
          "_id": "698a94fc1b2dc6b37d61aed7",
          "name": "Ranjay Krishna",
          "hidden": false
        },
        {
          "_id": "698a94fc1b2dc6b37d61aed8",
          "name": "Jiajun Wu",
          "hidden": false
        },
        {
          "_id": "698a94fc1b2dc6b37d61aed9",
          "name": "Li Fei-Fei",
          "hidden": false
        },
        {
          "_id": "698a94fc1b2dc6b37d61aeda",
          "name": "Manling Li",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-04T19:06:40.000Z",
      "submittedOnDailyAt": "2026-02-10T03:40:15.544Z",
      "title": "Theory of Space: Can Foundation Models Construct Spatial Beliefs through Active Exploration?",
      "submittedOnDailyBy": {
        "_id": "640131b08ba76abe4b71b5d0",
        "avatarUrl": "/avatars/2288b96a9a0ae8f584768f54e098def1.svg",
        "isPro": false,
        "fullname": "Jieyu Zhang",
        "user": "jieyuz2",
        "type": "user"
      },
      "summary": "Spatial embodied intelligence requires agents to act to acquire information under partial observability. While multimodal foundation models excel at passive perception, their capacity for active, self-directed exploration remains understudied. We propose Theory of Space, defined as an agent's ability to actively acquire information through self-directed, active exploration and to construct, revise, and exploit a spatial belief from sequential, partial observations. We evaluate this through a benchmark where the goal is curiosity-driven exploration to build an accurate cognitive map. A key innovation is spatial belief probing, which prompts models to reveal their internal spatial representations at each step. Our evaluation of state-of-the-art models reveals several critical bottlenecks. First, we identify an Active-Passive Gap, where performance drops significantly when agents must autonomously gather information. Second, we find high inefficiency, as models explore unsystematically compared to program-based proxies. Through belief probing, we diagnose that while perception is an initial bottleneck, global beliefs suffer from instability that causes spatial knowledge to degrade over time. Finally, using a false belief paradigm, we uncover Belief Inertia, where agents fail to update obsolete priors with new evidence. This issue is present in text-based agents but is particularly severe in vision-based models. Our findings suggest that current foundation models struggle to maintain coherent, revisable spatial beliefs during active exploration.",
      "upvotes": 13,
      "discussionId": "698a94fc1b2dc6b37d61aedb",
      "projectPage": "https://theory-of-space.github.io/",
      "githubRepo": "https://github.com/mll-lab-nu/Theory-of-Space",
      "githubRepoAddedBy": "user",
      "ai_summary": "Current multimodal foundation models show limitations in maintaining coherent spatial beliefs during active exploration, exhibiting gaps between active and passive performance, inefficient exploration strategies, and difficulties in updating outdated spatial knowledge.",
      "ai_keywords": [
        "spatial embodied intelligence",
        "multimodal foundation models",
        "active exploration",
        "spatial belief",
        "cognitive mapping",
        "spatial belief probing",
        "Active-Passive Gap",
        "belief inertia",
        "vision-based models",
        "text-based agents"
      ],
      "githubStars": 2
    },
    "publishedAt": "2026-02-04T14:06:40.000Z",
    "title": "Theory of Space: Can Foundation Models Construct Spatial Beliefs through Active Exploration?",
    "summary": "Spatial embodied intelligence requires agents to act to acquire information under partial observability. While multimodal foundation models excel at passive perception, their capacity for active, self-directed exploration remains understudied. We propose Theory of Space, defined as an agent's ability to actively acquire information through self-directed, active exploration and to construct, revise, and exploit a spatial belief from sequential, partial observations. We evaluate this through a benchmark where the goal is curiosity-driven exploration to build an accurate cognitive map. A key innovation is spatial belief probing, which prompts models to reveal their internal spatial representations at each step. Our evaluation of state-of-the-art models reveals several critical bottlenecks. First, we identify an Active-Passive Gap, where performance drops significantly when agents must autonomously gather information. Second, we find high inefficiency, as models explore unsystematically compared to program-based proxies. Through belief probing, we diagnose that while perception is an initial bottleneck, global beliefs suffer from instability that causes spatial knowledge to degrade over time. Finally, using a false belief paradigm, we uncover Belief Inertia, where agents fail to update obsolete priors with new evidence. This issue is present in text-based agents but is particularly severe in vision-based models. Our findings suggest that current foundation models struggle to maintain coherent, revisable spatial beliefs during active exploration.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07055.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "640131b08ba76abe4b71b5d0",
      "avatarUrl": "/avatars/2288b96a9a0ae8f584768f54e098def1.svg",
      "fullname": "Jieyu Zhang",
      "name": "jieyuz2",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.08543",
      "authors": [
        {
          "_id": "698abf921b2dc6b37d61b16a",
          "name": "Yutao Zhu",
          "hidden": false
        },
        {
          "_id": "698abf921b2dc6b37d61b16b",
          "name": "Xingshuo Zhang",
          "hidden": false
        },
        {
          "_id": "698abf921b2dc6b37d61b16c",
          "name": "Maosen Zhang",
          "hidden": false
        },
        {
          "_id": "698abf921b2dc6b37d61b16d",
          "name": "Jiajie Jin",
          "hidden": false
        },
        {
          "_id": "698abf921b2dc6b37d61b16e",
          "name": "Liancheng Zhang",
          "hidden": false
        },
        {
          "_id": "698abf921b2dc6b37d61b16f",
          "name": "Xiaoshuai Song",
          "hidden": false
        },
        {
          "_id": "698abf921b2dc6b37d61b170",
          "name": "Kangzhi Zhao",
          "hidden": false
        },
        {
          "_id": "698abf921b2dc6b37d61b171",
          "name": "Wencong Zeng",
          "hidden": false
        },
        {
          "_id": "698abf921b2dc6b37d61b172",
          "name": "Ruiming Tang",
          "hidden": false
        },
        {
          "_id": "698abf921b2dc6b37d61b173",
          "name": "Han Li",
          "hidden": false
        },
        {
          "_id": "698abf921b2dc6b37d61b174",
          "name": "Ji-Rong Wen",
          "hidden": false
        },
        {
          "_id": "698abf921b2dc6b37d61b175",
          "name": "Zhicheng Dou",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-09T11:44:15.000Z",
      "submittedOnDailyAt": "2026-02-10T02:50:16.823Z",
      "title": "GISA: A Benchmark for General Information-Seeking Assistant",
      "submittedOnDailyBy": {
        "_id": "625e62452a7279d3c77b5c38",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625e62452a7279d3c77b5c38/zJINew6U4_Gup4WTobb-0.jpeg",
        "isPro": false,
        "fullname": "Yutao Zhu",
        "user": "yutaozhu94",
        "type": "user"
      },
      "summary": "The advancement of large language models (LLMs) has significantly accelerated the development of search agents capable of autonomously gathering information through multi-turn web interactions. Various benchmarks have been proposed to evaluate such agents. However, existing benchmarks often construct queries backward from answers, producing unnatural tasks misaligned with real-world needs. Moreover, these benchmarks tend to focus on either locating specific information or aggregating information from multiple sources, while relying on static answer sets prone to data contamination. To bridge these gaps, we introduce GISA, a benchmark for General Information-Seeking Assistants comprising 373 human-crafted queries that reflect authentic information-seeking scenarios. GISA features four structured answer formats (item, set, list, and table), enabling deterministic evaluation. It integrates both deep reasoning and broad information aggregation within unified tasks, and includes a live subset with periodically updated answers to resist memorization. Notably, GISA provides complete human search trajectories for every query, offering gold-standard references for process-level supervision and imitation learning. Experiments on mainstream LLMs and commercial search products reveal that even the best-performing model achieves only 19.30\\% exact match score, with performance notably degrading on tasks requiring complex planning and comprehensive information gathering. These findings highlight substantial room for future improvement.",
      "upvotes": 12,
      "discussionId": "698abf921b2dc6b37d61b176",
      "ai_summary": "A new benchmark called GISA is introduced for evaluating information-seeking assistants, featuring human-crafted queries with structured answer formats and live updates to prevent memorization.",
      "ai_keywords": [
        "large language models",
        "search agents",
        "information-seeking assistants",
        "benchmarks",
        "deep reasoning",
        "information aggregation",
        "exact match score",
        "complex planning"
      ],
      "organization": {
        "_id": "622177ac43826d6f261f8208",
        "name": "RUC",
        "fullname": "Renmin University of China",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"
      }
    },
    "publishedAt": "2026-02-09T06:44:15.000Z",
    "title": "GISA: A Benchmark for General Information-Seeking Assistant",
    "summary": "The advancement of large language models (LLMs) has significantly accelerated the development of search agents capable of autonomously gathering information through multi-turn web interactions. Various benchmarks have been proposed to evaluate such agents. However, existing benchmarks often construct queries backward from answers, producing unnatural tasks misaligned with real-world needs. Moreover, these benchmarks tend to focus on either locating specific information or aggregating information from multiple sources, while relying on static answer sets prone to data contamination. To bridge these gaps, we introduce GISA, a benchmark for General Information-Seeking Assistants comprising 373 human-crafted queries that reflect authentic information-seeking scenarios. GISA features four structured answer formats (item, set, list, and table), enabling deterministic evaluation. It integrates both deep reasoning and broad information aggregation within unified tasks, and includes a live subset with periodically updated answers to resist memorization. Notably, GISA provides complete human search trajectories for every query, offering gold-standard references for process-level supervision and imitation learning. Experiments on mainstream LLMs and commercial search products reveal that even the best-performing model achieves only 19.30\\% exact match score, with performance notably degrading on tasks requiring complex planning and comprehensive information gathering. These findings highlight substantial room for future improvement.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08543.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "625e62452a7279d3c77b5c38",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625e62452a7279d3c77b5c38/zJINew6U4_Gup4WTobb-0.jpeg",
      "fullname": "Yutao Zhu",
      "name": "yutaozhu94",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "622177ac43826d6f261f8208",
      "name": "RUC",
      "fullname": "Renmin University of China",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.08990",
      "authors": [
        {
          "_id": "698abe481b2dc6b37d61b12c",
          "name": "Shiyang Feng",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b12d",
          "name": "Runmin Ma",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b12e",
          "name": "Xiangchao Yan",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b12f",
          "name": "Yue Fan",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b130",
          "name": "Yusong Hu",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b131",
          "name": "Songtao Huang",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b132",
          "name": "Shuaiyu Zhang",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b133",
          "name": "Zongsheng Cao",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b134",
          "name": "Tianshuo Peng",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b135",
          "name": "Jiakang Yuan",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b136",
          "name": "Zijie Guo",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b137",
          "name": "Zhijie Zhong",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b138",
          "name": "Shangheng Du",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b139",
          "name": "Weida Wang",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b13a",
          "name": "Jinxin Shi",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b13b",
          "name": "Yuhao Zhou",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b13c",
          "name": "Xiaohan He",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b13d",
          "name": "Zhiyin Yu",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b13e",
          "name": "Fangchen Yu",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b13f",
          "name": "Qihao Zheng",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b140",
          "name": "Jiamin Wu",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b141",
          "name": "Mianxin Liu",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b142",
          "name": "Chi Zhang",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b143",
          "name": "Shaowei Hou",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b144",
          "name": "Shuya Li",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b145",
          "name": "Yankai Jiang",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b146",
          "name": "Wenjie Lou",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b147",
          "name": "Lilong Wang",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b148",
          "name": "Zifu Wang",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b149",
          "name": "Jiong Wang",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b14a",
          "name": "Wanghan Xu",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b14b",
          "name": "Yue Deng",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b14c",
          "name": "Dongrui Liu",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b14d",
          "name": "Yiheng Wang",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b14e",
          "name": "Wenlong Zhang",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b14f",
          "name": "Fenghua Ling",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b150",
          "name": "Shufei Zhang",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b151",
          "name": "Xiaosong Wang",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b152",
          "name": "Shuangjia Zheng",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b153",
          "name": "Xun Huang",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b154",
          "name": "Siqi Sun",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b155",
          "name": "Shuyue Hu",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b156",
          "name": "Peng Ye",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b157",
          "name": "Chunfeng Song",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b158",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b159",
          "name": "Conghui He",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b15a",
          "name": "Yihao Liu",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b15b",
          "name": "Xin Li",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b15c",
          "name": "Qibin Hou",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b15d",
          "name": "Tao Chen",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b15e",
          "name": "Xiangyu Yue",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b15f",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b160",
          "name": "Liang He",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b161",
          "name": "Dahua Lin",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b162",
          "name": "Bowen Zhou",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b163",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "698abe481b2dc6b37d61b164",
          "name": "Lei Bai",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-09T18:36:06.000Z",
      "submittedOnDailyAt": "2026-02-10T02:55:11.896Z",
      "title": "InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.",
      "upvotes": 10,
      "discussionId": "698abe481b2dc6b37d61b165",
      "ai_summary": "InternAgent-1.5 is a unified system for autonomous scientific discovery that integrates computational modeling and experimental research through coordinated subsystems for generation, verification, and evolution.",
      "ai_keywords": [
        "scientific discovery",
        "computational modeling",
        "laboratory experimentation",
        "unified system",
        "deep research",
        "solution optimization",
        "long horizon memory",
        "scientific reasoning benchmarks",
        "algorithm discovery",
        "empirical discovery"
      ]
    },
    "publishedAt": "2026-02-09T13:36:06.000Z",
    "title": "InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery",
    "summary": "We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08990.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 230,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.06454",
      "authors": [
        {
          "_id": "698a9d531b2dc6b37d61af3d",
          "name": "Jiwon Song",
          "hidden": false
        },
        {
          "_id": "698a9d531b2dc6b37d61af3e",
          "name": "Yoongon Kim",
          "hidden": false
        },
        {
          "_id": "698a9d531b2dc6b37d61af3f",
          "name": "Jae-Joon Kim",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-06T07:35:01.000Z",
      "submittedOnDailyAt": "2026-02-10T00:28:46.008Z",
      "title": "RelayGen: Intra-Generation Model Switching for Efficient Reasoning",
      "submittedOnDailyBy": {
        "_id": "662672eaebdfec5cfdf1d034",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662672eaebdfec5cfdf1d034/RhsKly3KvbtPkDuVnEdWb.jpeg",
        "isPro": false,
        "fullname": "Jiwon Song",
        "user": "jiwonsong",
        "type": "user"
      },
      "summary": "Large reasoning models (LRMs) achieve strong performance on complex reasoning tasks by generating long, multi-step reasoning trajectories, but inference-time scaling incurs substantial deployment cost. A key challenge is that generation difficulty varies within a single output, whereas existing efficiency-oriented approaches either ignore this intra-generation variation or rely on supervised token-level routing with high system complexity. We present RelayGen, a training-free, segment-level runtime model switching framework that exploits difficulty variation in long-form reasoning. Through offline analysis of generation uncertainty using token probability margins, we show that coarse-grained segment-level control is sufficient to capture difficulty transitions within a reasoning trajectory. RelayGen identifies model-specific switch cues that signal transitions to lower-difficulty segments and dynamically delegates their continuation to a smaller model, while preserving high-difficulty reasoning on the large model. Across multiple reasoning benchmarks, RelayGen substantially reduces inference latency while preserving most of the accuracy of large models. When combined with speculative decoding, RelayGen achieves up to 2.2times end-to-end speedup with less than 2\\% accuracy degradation, without requiring additional training or learned routing components.",
      "upvotes": 10,
      "discussionId": "698a9d531b2dc6b37d61af40",
      "githubRepo": "https://github.com/jiwonsong-dev/RelayGen",
      "githubRepoAddedBy": "user",
      "ai_summary": "RelayGen is a training-free framework that dynamically switches between large and small models during reasoning by identifying difficulty transitions at the segment level, achieving faster inference with minimal accuracy loss.",
      "ai_keywords": [
        "large reasoning models",
        "multi-step reasoning trajectories",
        "inference-time scaling",
        "token probability margins",
        "segment-level control",
        "model switching",
        "speculative decoding",
        "end-to-end speedup"
      ],
      "githubStars": 1,
      "organization": {
        "_id": "698422913a080cd2873577a4",
        "name": "SNU-VLSI",
        "fullname": "Seoul National University VLSI Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662672eaebdfec5cfdf1d034/7kyeWE2-6lCuFC2PG3xhz.png"
      }
    },
    "publishedAt": "2026-02-06T02:35:01.000Z",
    "title": "RelayGen: Intra-Generation Model Switching for Efficient Reasoning",
    "summary": "Large reasoning models (LRMs) achieve strong performance on complex reasoning tasks by generating long, multi-step reasoning trajectories, but inference-time scaling incurs substantial deployment cost. A key challenge is that generation difficulty varies within a single output, whereas existing efficiency-oriented approaches either ignore this intra-generation variation or rely on supervised token-level routing with high system complexity. We present RelayGen, a training-free, segment-level runtime model switching framework that exploits difficulty variation in long-form reasoning. Through offline analysis of generation uncertainty using token probability margins, we show that coarse-grained segment-level control is sufficient to capture difficulty transitions within a reasoning trajectory. RelayGen identifies model-specific switch cues that signal transitions to lower-difficulty segments and dynamically delegates their continuation to a smaller model, while preserving high-difficulty reasoning on the large model. Across multiple reasoning benchmarks, RelayGen substantially reduces inference latency while preserving most of the accuracy of large models. When combined with speculative decoding, RelayGen achieves up to 2.2times end-to-end speedup with less than 2\\% accuracy degradation, without requiring additional training or learned routing components.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06454.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "662672eaebdfec5cfdf1d034",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662672eaebdfec5cfdf1d034/RhsKly3KvbtPkDuVnEdWb.jpeg",
      "fullname": "Jiwon Song",
      "name": "jiwonsong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "698422913a080cd2873577a4",
      "name": "SNU-VLSI",
      "fullname": "Seoul National University VLSI Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662672eaebdfec5cfdf1d034/7kyeWE2-6lCuFC2PG3xhz.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.06540",
      "authors": [
        {
          "_id": "6989512bbeecc443208d2656",
          "name": "Yishan Li",
          "hidden": false
        },
        {
          "_id": "6989512bbeecc443208d2657",
          "user": {
            "_id": "64f5abc2e8f27f20a067a596",
            "avatarUrl": "/avatars/d0eac39488fac0c9c08d76109cabaa9f.svg",
            "isPro": false,
            "fullname": "cwt",
            "user": "yiye2023",
            "type": "user"
          },
          "name": "Wentong Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T08:29:47.521Z",
          "hidden": false
        },
        {
          "_id": "6989512bbeecc443208d2658",
          "name": "Yukun Yan",
          "hidden": false
        },
        {
          "_id": "6989512bbeecc443208d2659",
          "name": "Mingwei Li",
          "hidden": false
        },
        {
          "_id": "6989512bbeecc443208d265a",
          "name": "Sen Mei",
          "hidden": false
        },
        {
          "_id": "6989512bbeecc443208d265b",
          "name": "Xiaorong Wang",
          "hidden": false
        },
        {
          "_id": "6989512bbeecc443208d265c",
          "name": "Kunpeng Liu",
          "hidden": false
        },
        {
          "_id": "6989512bbeecc443208d265d",
          "name": "Xin Cong",
          "hidden": false
        },
        {
          "_id": "6989512bbeecc443208d265e",
          "name": "Shuo Wang",
          "hidden": false
        },
        {
          "_id": "6989512bbeecc443208d265f",
          "name": "Zhong Zhang",
          "hidden": false
        },
        {
          "_id": "6989512bbeecc443208d2660",
          "name": "Yaxi Lu",
          "hidden": false
        },
        {
          "_id": "6989512bbeecc443208d2661",
          "name": "Zhenghao Liu",
          "hidden": false
        },
        {
          "_id": "6989512bbeecc443208d2662",
          "name": "Yankai Lin",
          "hidden": false
        },
        {
          "_id": "6989512bbeecc443208d2663",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "6989512bbeecc443208d2664",
          "name": "Maosong Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-06T09:45:04.000Z",
      "submittedOnDailyAt": "2026-02-10T00:57:57.523Z",
      "title": "AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research",
      "submittedOnDailyBy": {
        "_id": "64f5abc2e8f27f20a067a596",
        "avatarUrl": "/avatars/d0eac39488fac0c9c08d76109cabaa9f.svg",
        "isPro": false,
        "fullname": "cwt",
        "user": "yiye2023",
        "type": "user"
      },
      "summary": "Generating deep research reports requires large-scale information acquisition and the synthesis of insight-driven analysis, posing a significant challenge for current language models. Most existing approaches follow a plan-then-write paradigm, whose performance heavily depends on the quality of the initial outline. However, constructing a comprehensive outline itself demands strong reasoning ability, causing current deep research systems to rely almost exclusively on closed-source or online large models. This reliance raises practical barriers to deployment and introduces safety and privacy concerns for user-authored data. In this work, we present AgentCPM-Report, a lightweight yet high-performing local solution composed of a framework that mirrors the human writing process and an 8B-parameter deep research agent. Our framework uses a Writing As Reasoning Policy (WARP), which enables models to dynamically revise outlines during report generation. Under this policy, the agent alternates between Evidence-Based Drafting and Reasoning-Driven Deepening, jointly supporting information acquisition, knowledge refinement, and iterative outline evolution. To effectively equip small models with this capability, we introduce a Multi-Stage Agentic Training strategy, consisting of cold-start, atomic skill RL, and holistic pipeline RL. Experiments on DeepResearch Bench, DeepConsult, and DeepResearch Gym demonstrate that AgentCPM-Report outperforms leading closed-source systems, with substantial gains in Insight.",
      "upvotes": 9,
      "discussionId": "6989512bbeecc443208d2665",
      "githubRepo": "https://github.com/OpenBMB/AgentCPM/tree/main/AgentCPM-Report",
      "githubRepoAddedBy": "user",
      "ai_summary": "AgentCPM-Report presents a lightweight local solution for deep research report generation using a Writing As Reasoning Policy framework and multi-stage agentic training to enhance small models' reasoning and outline evolution capabilities.",
      "ai_keywords": [
        "Writing As Reasoning Policy",
        "WARP",
        "Evidence-Based Drafting",
        "Reasoning-Driven Deepening",
        "Multi-Stage Agentic Training",
        "cold-start",
        "atomic skill RL",
        "holistic pipeline RL",
        "deep research agent",
        "insight-driven analysis",
        "plan-then-write paradigm"
      ],
      "githubStars": 726,
      "organization": {
        "_id": "633fe81429b5a95f6e16e34a",
        "name": "openbmb",
        "fullname": "OpenBMB",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670387859384-633fe7784b362488336bbfad.png"
      }
    },
    "publishedAt": "2026-02-06T04:45:04.000Z",
    "title": "AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research",
    "summary": "Generating deep research reports requires large-scale information acquisition and the synthesis of insight-driven analysis, posing a significant challenge for current language models. Most existing approaches follow a plan-then-write paradigm, whose performance heavily depends on the quality of the initial outline. However, constructing a comprehensive outline itself demands strong reasoning ability, causing current deep research systems to rely almost exclusively on closed-source or online large models. This reliance raises practical barriers to deployment and introduces safety and privacy concerns for user-authored data. In this work, we present AgentCPM-Report, a lightweight yet high-performing local solution composed of a framework that mirrors the human writing process and an 8B-parameter deep research agent. Our framework uses a Writing As Reasoning Policy (WARP), which enables models to dynamically revise outlines during report generation. Under this policy, the agent alternates between Evidence-Based Drafting and Reasoning-Driven Deepening, jointly supporting information acquisition, knowledge refinement, and iterative outline evolution. To effectively equip small models with this capability, we introduce a Multi-Stage Agentic Training strategy, consisting of cold-start, atomic skill RL, and holistic pipeline RL. Experiments on DeepResearch Bench, DeepConsult, and DeepResearch Gym demonstrate that AgentCPM-Report outperforms leading closed-source systems, with substantial gains in Insight.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06540.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64f5abc2e8f27f20a067a596",
      "avatarUrl": "/avatars/d0eac39488fac0c9c08d76109cabaa9f.svg",
      "fullname": "cwt",
      "name": "yiye2023",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "633fe81429b5a95f6e16e34a",
      "name": "openbmb",
      "fullname": "OpenBMB",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670387859384-633fe7784b362488336bbfad.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2602.06694",
      "authors": [
        {
          "_id": "69894d2abeecc443208d2610",
          "user": {
            "_id": "6670d2ec92412fd464eac919",
            "avatarUrl": "/avatars/f76013e72d19b12feddd80f3a4b5d71f.svg",
            "isPro": false,
            "fullname": "Hyochan Chong",
            "user": "d7chong",
            "type": "user"
          },
          "name": "Hyochan Chong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T08:29:50.005Z",
          "hidden": false
        },
        {
          "_id": "69894d2abeecc443208d2611",
          "name": "Dongkyu Kim",
          "hidden": false
        },
        {
          "_id": "69894d2abeecc443208d2612",
          "name": "Changdong Kim",
          "hidden": false
        },
        {
          "_id": "69894d2abeecc443208d2613",
          "name": "Minseop Choi",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-06T13:26:44.000Z",
      "submittedOnDailyAt": "2026-02-10T00:03:08.555Z",
      "title": "NanoQuant: Efficient Sub-1-Bit Quantization of Large Language Models",
      "submittedOnDailyBy": {
        "_id": "6670d2ec92412fd464eac919",
        "avatarUrl": "/avatars/f76013e72d19b12feddd80f3a4b5d71f.svg",
        "isPro": false,
        "fullname": "Hyochan Chong",
        "user": "d7chong",
        "type": "user"
      },
      "summary": "Weight-only quantization has become a standard approach for efficiently serving large language models (LLMs). However, existing methods fail to efficiently compress models to binary (1-bit) levels, as they either require large amounts of data and compute or incur additional storage. In this work, we propose NanoQuant, the first post-training quantization (PTQ) method to compress LLMs to both binary and sub-1-bit levels. NanoQuant formulates quantization as a low-rank binary factorization problem, and compresses full-precision weights to low-rank binary matrices and scales. Specifically, it utilizes an efficient alternating direction method of multipliers (ADMM) method to precisely initialize latent binary matrices and scales, and then tune the initialized parameters through a block and model reconstruction process. Consequently, NanoQuant establishes a new Pareto frontier in low-memory post-training quantization, achieving state-of-the-art accuracy even at sub-1-bit compression rates. NanoQuant makes large-scale deployment feasible on consumer hardware. For example, it compresses Llama2-70B by 25.8times in just 13 hours on a single H100, enabling a 70B model to operate on a consumer 8 GB GPU.",
      "upvotes": 5,
      "discussionId": "69894d2bbeecc443208d2614",
      "ai_summary": "NanoQuant enables efficient post-training quantization of large language models to binary and sub-1-bit levels using low-rank binary factorization and ADMM optimization, achieving state-of-the-art accuracy while reducing memory requirements for consumer hardware deployment.",
      "ai_keywords": [
        "post-training quantization",
        "low-rank binary factorization",
        "alternating direction method of multipliers",
        "binary quantization",
        "sub-1-bit compression",
        "latent binary matrices",
        "block reconstruction",
        "model reconstruction"
      ],
      "organization": {
        "_id": "686df54910a52f2c2cf03c06",
        "name": "SamsungResearch",
        "fullname": "Samsung Research",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60ffc3e62403168abcae811d/lBrkzrpjrJ8k-3CGLKRLr.jpeg"
      }
    },
    "publishedAt": "2026-02-06T08:26:44.000Z",
    "title": "NanoQuant: Efficient Sub-1-Bit Quantization of Large Language Models",
    "summary": "Weight-only quantization has become a standard approach for efficiently serving large language models (LLMs). However, existing methods fail to efficiently compress models to binary (1-bit) levels, as they either require large amounts of data and compute or incur additional storage. In this work, we propose NanoQuant, the first post-training quantization (PTQ) method to compress LLMs to both binary and sub-1-bit levels. NanoQuant formulates quantization as a low-rank binary factorization problem, and compresses full-precision weights to low-rank binary matrices and scales. Specifically, it utilizes an efficient alternating direction method of multipliers (ADMM) method to precisely initialize latent binary matrices and scales, and then tune the initialized parameters through a block and model reconstruction process. Consequently, NanoQuant establishes a new Pareto frontier in low-memory post-training quantization, achieving state-of-the-art accuracy even at sub-1-bit compression rates. NanoQuant makes large-scale deployment feasible on consumer hardware. For example, it compresses Llama2-70B by 25.8times in just 13 hours on a single H100, enabling a 70B model to operate on a consumer 8 GB GPU.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06694.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6670d2ec92412fd464eac919",
      "avatarUrl": "/avatars/f76013e72d19b12feddd80f3a4b5d71f.svg",
      "fullname": "Hyochan Chong",
      "name": "d7chong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "686df54910a52f2c2cf03c06",
      "name": "SamsungResearch",
      "fullname": "Samsung Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60ffc3e62403168abcae811d/lBrkzrpjrJ8k-3CGLKRLr.jpeg"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2602.08236",
      "authors": [
        {
          "_id": "698aa1cf1b2dc6b37d61af4c",
          "name": "Shoubin Yu",
          "hidden": false
        },
        {
          "_id": "698aa1cf1b2dc6b37d61af4d",
          "name": "Yue Zhang",
          "hidden": false
        },
        {
          "_id": "698aa1cf1b2dc6b37d61af4e",
          "name": "Zun Wang",
          "hidden": false
        },
        {
          "_id": "698aa1cf1b2dc6b37d61af4f",
          "name": "Jaehong Yoon",
          "hidden": false
        },
        {
          "_id": "698aa1cf1b2dc6b37d61af50",
          "name": "Huaxiu Yao",
          "hidden": false
        },
        {
          "_id": "698aa1cf1b2dc6b37d61af51",
          "name": "Mingyu Ding",
          "hidden": false
        },
        {
          "_id": "698aa1cf1b2dc6b37d61af52",
          "name": "Mohit Bansal",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-09T03:21:48.000Z",
      "submittedOnDailyAt": "2026-02-10T00:42:02.821Z",
      "title": "When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning",
      "submittedOnDailyBy": {
        "_id": "63ee8fb05f1300034de097fd",
        "avatarUrl": "/avatars/bceedb88927d8633948266503c2dd0b1.svg",
        "isPro": true,
        "fullname": "Yu",
        "user": "Shoubin",
        "type": "user"
      },
      "summary": "Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.",
      "upvotes": 4,
      "discussionId": "698aa1cf1b2dc6b37d61af53",
      "projectPage": "https://adaptive-visual-tts.github.io/",
      "githubRepo": "https://github.com/Yui010206/Adaptive-Visual-Imagination-Control/",
      "githubRepoAddedBy": "user",
      "ai_summary": "Adaptive test-time framework with world models enables selective visual imagination for spatial reasoning, improving efficiency and reliability by determining when imagination is necessary.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "visual spatial reasoning",
        "world models",
        "visual imagination",
        "test-time adaptation",
        "embodied navigation",
        "SAT",
        "MMSI",
        "R2R"
      ],
      "githubStars": 1,
      "organization": {
        "_id": "669f9d1fec8789263c0e355a",
        "name": "UNC-ChapelHill",
        "fullname": "University of North Carolina at Chapel Hill",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/669f9c85bd649dba3b88e581/H5uB8_MCewnMtxEUnAvTL.png"
      }
    },
    "publishedAt": "2026-02-08T22:21:48.000Z",
    "title": "When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning",
    "summary": "Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08236.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63ee8fb05f1300034de097fd",
      "avatarUrl": "/avatars/bceedb88927d8633948266503c2dd0b1.svg",
      "fullname": "Yu",
      "name": "Shoubin",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "669f9d1fec8789263c0e355a",
      "name": "UNC-ChapelHill",
      "fullname": "University of North Carolina at Chapel Hill",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/669f9c85bd649dba3b88e581/H5uB8_MCewnMtxEUnAvTL.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.08145",
      "authors": [
        {
          "_id": "698ab9ea1b2dc6b37d61b0a9",
          "name": "Xinyu Yang",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0aa",
          "name": "Junlin Han",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0ab",
          "name": "Rishi Bommasani",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0ac",
          "name": "Jinqi Luo",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0ad",
          "name": "Wenjie Qu",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0ae",
          "name": "Wangchunshu Zhou",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0af",
          "name": "Adel Bibi",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0b0",
          "name": "Xiyao Wang",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0b1",
          "name": "Jaehong Yoon",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0b2",
          "name": "Elias Stengel-Eskin",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0b3",
          "name": "Shengbang Tong",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0b4",
          "name": "Lingfeng Shen",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0b5",
          "name": "Rafael Rafailov",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0b6",
          "name": "Runjia Li",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0b7",
          "name": "Zhaoyang Wang",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0b8",
          "name": "Yiyang Zhou",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0b9",
          "name": "Chenhang Cui",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0ba",
          "name": "Yu Wang",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0bb",
          "name": "Wenhao Zheng",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0bc",
          "name": "Huichi Zhou",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0bd",
          "name": "Jindong Gu",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0be",
          "name": "Zhaorun Chen",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0bf",
          "name": "Peng Xia",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0c0",
          "name": "Tony Lee",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0c1",
          "name": "Thomas Zollo",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0c2",
          "name": "Vikash Sehwag",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0c3",
          "name": "Jixuan Leng",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0c4",
          "name": "Jiuhai Chen",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0c5",
          "name": "Yuxin Wen",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0c6",
          "name": "Huan Zhang",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0c7",
          "name": "Zhun Deng",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0c8",
          "name": "Linjun Zhang",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0c9",
          "name": "Pavel Izmailov",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0ca",
          "name": "Pang Wei Koh",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0cb",
          "name": "Yulia Tsvetkov",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0cc",
          "name": "Andrew Wilson",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0cd",
          "name": "Jiaheng Zhang",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0ce",
          "name": "James Zou",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0cf",
          "name": "Cihang Xie",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0d0",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0d1",
          "name": "Philip Torr",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0d2",
          "name": "Julian McAuley",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0d3",
          "name": "David Alvarez-Melis",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0d4",
          "name": "Florian Tramr",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0d5",
          "name": "Kaidi Xu",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0d6",
          "name": "Suman Jana",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0d7",
          "name": "Chris Callison-Burch",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0d8",
          "name": "Rene Vidal",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0d9",
          "name": "Filippos Kokkinos",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0da",
          "name": "Mohit Bansal",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0db",
          "name": "Beidi Chen",
          "hidden": false
        },
        {
          "_id": "698ab9ea1b2dc6b37d61b0dc",
          "name": "Huaxiu Yao",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-04T17:25:03.000Z",
      "submittedOnDailyAt": "2026-02-10T02:26:53.485Z",
      "title": "Reliable and Responsible Foundation Models: A Comprehensive Survey",
      "submittedOnDailyBy": {
        "_id": "6279a4f6812ee439d9c72d3f",
        "avatarUrl": "/avatars/a35a5674d1168d345d9fc5018485283e.svg",
        "isPro": false,
        "fullname": "Jinqi Luo",
        "user": "peterljq",
        "type": "user"
      },
      "summary": "Foundation models, including Large Language Models (LLMs), Multimodal Large Language Models (MLLMs), Image Generative Models (i.e, Text-to-Image Models and Image-Editing Models), and Video Generative Models, have become essential tools with broad applications across various domains such as law, medicine, education, finance, science, and beyond. As these models see increasing real-world deployment, ensuring their reliability and responsibility has become critical for academia, industry, and government. This survey addresses the reliable and responsible development of foundation models. We explore critical issues, including bias and fairness, security and privacy, uncertainty, explainability, and distribution shift. Our research also covers model limitations, such as hallucinations, as well as methods like alignment and Artificial Intelligence-Generated Content (AIGC) detection. For each area, we review the current state of the field and outline concrete future research directions. Additionally, we discuss the intersections between these areas, highlighting their connections and shared challenges. We hope our survey fosters the development of foundation models that are not only powerful but also ethical, trustworthy, reliable, and socially responsible.",
      "upvotes": 3,
      "discussionId": "698ab9ea1b2dc6b37d61b0dd",
      "ai_summary": "Foundation models including LLMs, MLLMs, and generative models require reliable and responsible development addressing bias, security, explainability, and other critical issues for trustworthy deployment across multiple domains.",
      "ai_keywords": [
        "Large Language Models",
        "Multimodal Large Language Models",
        "Text-to-Image Models",
        "Image-Editing Models",
        "Video Generative Models",
        "Artificial Intelligence-Generated Content",
        "hallucinations",
        "alignment",
        "AIGC detection",
        "bias",
        "fairness",
        "security",
        "privacy",
        "uncertainty",
        "explainability",
        "distribution shift",
        "model limitations"
      ],
      "organization": {
        "_id": "691d9a1012cc4d473e1c862f",
        "name": "CarnegieMellonU",
        "fullname": "Carnegie Mellon University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/6I146aJvxxlRCEbYFFAeQ.png"
      }
    },
    "publishedAt": "2026-02-04T12:25:03.000Z",
    "title": "Reliable and Responsible Foundation Models: A Comprehensive Survey",
    "summary": "Foundation models, including Large Language Models (LLMs), Multimodal Large Language Models (MLLMs), Image Generative Models (i.e, Text-to-Image Models and Image-Editing Models), and Video Generative Models, have become essential tools with broad applications across various domains such as law, medicine, education, finance, science, and beyond. As these models see increasing real-world deployment, ensuring their reliability and responsibility has become critical for academia, industry, and government. This survey addresses the reliable and responsible development of foundation models. We explore critical issues, including bias and fairness, security and privacy, uncertainty, explainability, and distribution shift. Our research also covers model limitations, such as hallucinations, as well as methods like alignment and Artificial Intelligence-Generated Content (AIGC) detection. For each area, we review the current state of the field and outline concrete future research directions. Additionally, we discuss the intersections between these areas, highlighting their connections and shared challenges. We hope our survey fosters the development of foundation models that are not only powerful but also ethical, trustworthy, reliable, and socially responsible.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08145.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6279a4f6812ee439d9c72d3f",
      "avatarUrl": "/avatars/a35a5674d1168d345d9fc5018485283e.svg",
      "fullname": "Jinqi Luo",
      "name": "peterljq",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "691d9a1012cc4d473e1c862f",
      "name": "CarnegieMellonU",
      "fullname": "Carnegie Mellon University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/6I146aJvxxlRCEbYFFAeQ.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2601.21363",
      "authors": [
        {
          "_id": "69899c2abeecc443208d2798",
          "user": {
            "_id": "65c0ae04b7db0ab09541beed",
            "avatarUrl": "/avatars/d98cbcc3ad1669221c4a50eccfabc9d0.svg",
            "isPro": false,
            "fullname": "Weidong Huang",
            "user": "Weidong-Huang",
            "type": "user"
          },
          "name": "Weidong Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T14:31:38.475Z",
          "hidden": false
        },
        {
          "_id": "69899c2abeecc443208d2799",
          "name": "Zhehan Li",
          "hidden": false
        },
        {
          "_id": "69899c2abeecc443208d279a",
          "name": "Hangxin Liu",
          "hidden": false
        },
        {
          "_id": "69899c2abeecc443208d279b",
          "name": "Biao Hou",
          "hidden": false
        },
        {
          "_id": "69899c2abeecc443208d279c",
          "name": "Yao Su",
          "hidden": false
        },
        {
          "_id": "69899c2abeecc443208d279d",
          "name": "Jingwen Zhang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65c0ae04b7db0ab09541beed/yo-qXj6GMi-WuCBDvTBbK.mp4"
      ],
      "publishedAt": "2026-01-29T07:43:24.000Z",
      "submittedOnDailyAt": "2026-02-10T00:26:35.151Z",
      "title": "Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control",
      "submittedOnDailyBy": {
        "_id": "65c0ae04b7db0ab09541beed",
        "avatarUrl": "/avatars/d98cbcc3ad1669221c4a50eccfabc9d0.svg",
        "isPro": false,
        "fullname": "Weidong Huang",
        "user": "Weidong-Huang",
        "type": "user"
      },
      "summary": "Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning.",
      "upvotes": 3,
      "discussionId": "69899c2bbeecc443208d279e",
      "projectPage": "https://lift-humanoid.github.io/",
      "githubRepo": "https://github.com/bigai-ai/LIFT-humanoid",
      "githubRepoAddedBy": "user",
      "ai_summary": "Off-policy Soft Actor-Critic with large-batch updates enables efficient humanoid locomotion policy pretraining, while model-based methods facilitate safe adaptation through deterministic data collection and stochastic exploration within physics-informed world models.",
      "ai_keywords": [
        "Proximal Policy Optimization",
        "Soft Actor-Critic",
        "on-policy methods",
        "off-policy RL",
        "model-based RL",
        "large-scale parallel simulation",
        "zero-shot deployment",
        "sample efficiency",
        "large-batch update",
        "Update-To-Data ratio",
        "deterministic policy",
        "stochastic exploration",
        "physics-informed world model"
      ],
      "githubStars": 52
    },
    "publishedAt": "2026-01-29T02:43:24.000Z",
    "title": "Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control",
    "summary": "Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65c0ae04b7db0ab09541beed/yo-qXj6GMi-WuCBDvTBbK.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21363.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "65c0ae04b7db0ab09541beed",
      "avatarUrl": "/avatars/d98cbcc3ad1669221c4a50eccfabc9d0.svg",
      "fullname": "Weidong Huang",
      "name": "Weidong-Huang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2602.08961",
      "authors": [
        {
          "_id": "698ab60e1b2dc6b37d61aff8",
          "name": "Ruijie Zhu",
          "hidden": false
        },
        {
          "_id": "698ab60e1b2dc6b37d61aff9",
          "name": "Jiahao Lu",
          "hidden": false
        },
        {
          "_id": "698ab60e1b2dc6b37d61affa",
          "name": "Wenbo Hu",
          "hidden": false
        },
        {
          "_id": "698ab60e1b2dc6b37d61affb",
          "name": "Xiaoguang Han",
          "hidden": false
        },
        {
          "_id": "698ab60e1b2dc6b37d61affc",
          "name": "Jianfei Cai",
          "hidden": false
        },
        {
          "_id": "698ab60e1b2dc6b37d61affd",
          "name": "Ying Shan",
          "hidden": false
        },
        {
          "_id": "698ab60e1b2dc6b37d61affe",
          "name": "Chuanxia Zheng",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6697ac8427e4e21a3a92da27/rpQaSmVHqXA-1zkmqY0dK.qt"
      ],
      "publishedAt": "2026-02-09T17:58:12.000Z",
      "submittedOnDailyAt": "2026-02-10T03:56:29.177Z",
      "title": "MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE",
      "submittedOnDailyBy": {
        "_id": "6697ac8427e4e21a3a92da27",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6697ac8427e4e21a3a92da27/9vn07-1_BBDk9zfDtDpcG.png",
        "isPro": false,
        "fullname": "Ruijie Zhu",
        "user": "RuijieZhu",
        "type": "user"
      },
      "summary": "We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometry and estimates dense motion from a monocular video. The core of our method is a novel joint representation of dense 3D point maps and 3D scene flows in a shared coordinate system, and a novel 4D VAE to effectively learn this representation. Unlike prior work that forces the 3D value and latents to align strictly with RGB VAE latents-despite their fundamentally different distributions-we show that such alignment is unnecessary and leads to suboptimal performance. Instead, we introduce a new data normalization and VAE training strategy that better transfers diffusion priors and greatly improves reconstruction quality. Extensive experiments across multiple datasets demonstrate that MotionCrafter achieves state-of-the-art performance in both geometry reconstruction and dense scene flow estimation, delivering 38.64% and 25.0% improvements in geometry and motion reconstruction, respectively, all without any post-optimization. Project page: https://ruijiezhu94.github.io/MotionCrafter_Page",
      "upvotes": 2,
      "discussionId": "698ab60e1b2dc6b37d61afff",
      "projectPage": "https://ruijiezhu94.github.io/MotionCrafter_Page",
      "githubRepo": "https://github.com/TencentARC/MotionCrafter",
      "githubRepoAddedBy": "user",
      "ai_summary": "MotionCrafter is a video diffusion framework that jointly reconstructs 4D geometry and estimates dense motion using a novel joint representation and 4D VAE architecture.",
      "ai_keywords": [
        "video diffusion",
        "4D geometry",
        "dense motion estimation",
        "3D point maps",
        "3D scene flows",
        "shared coordinate system",
        "4D VAE",
        "RGB VAE latents",
        "data normalization",
        "VAE training strategy",
        "diffusion priors"
      ],
      "githubStars": 12,
      "organization": {
        "_id": "60e3f7f641ca131919975fe5",
        "name": "TencentARC",
        "fullname": "ARC Lab, Tencent PCG",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1625552871844-60e272ca6c78a8c122b12127.png"
      }
    },
    "publishedAt": "2026-02-09T12:58:12.000Z",
    "title": "MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE",
    "summary": "We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometry and estimates dense motion from a monocular video. The core of our method is a novel joint representation of dense 3D point maps and 3D scene flows in a shared coordinate system, and a novel 4D VAE to effectively learn this representation. Unlike prior work that forces the 3D value and latents to align strictly with RGB VAE latents-despite their fundamentally different distributions-we show that such alignment is unnecessary and leads to suboptimal performance. Instead, we introduce a new data normalization and VAE training strategy that better transfers diffusion priors and greatly improves reconstruction quality. Extensive experiments across multiple datasets demonstrate that MotionCrafter achieves state-of-the-art performance in both geometry reconstruction and dense scene flow estimation, delivering 38.64% and 25.0% improvements in geometry and motion reconstruction, respectively, all without any post-optimization. Project page: https://ruijiezhu94.github.io/MotionCrafter_Page",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6697ac8427e4e21a3a92da27/rpQaSmVHqXA-1zkmqY0dK.qt"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08961.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6697ac8427e4e21a3a92da27",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6697ac8427e4e21a3a92da27/9vn07-1_BBDk9zfDtDpcG.png",
      "fullname": "Ruijie Zhu",
      "name": "RuijieZhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "60e3f7f641ca131919975fe5",
      "name": "TencentARC",
      "fullname": "ARC Lab, Tencent PCG",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1625552871844-60e272ca6c78a8c122b12127.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.08808",
      "authors": [
        {
          "_id": "698ac3981b2dc6b37d61b1a9",
          "name": "Yapei Chang",
          "hidden": false
        },
        {
          "_id": "698ac3981b2dc6b37d61b1aa",
          "name": "Kyle Lo",
          "hidden": false
        },
        {
          "_id": "698ac3981b2dc6b37d61b1ab",
          "name": "Mohit Iyyer",
          "hidden": false
        },
        {
          "_id": "698ac3981b2dc6b37d61b1ac",
          "name": "Luca Soldaini",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-09T15:47:14.000Z",
      "submittedOnDailyAt": "2026-02-10T03:05:33.584Z",
      "title": "How2Everything: Mining the Web for How-To Procedures to Evaluate and Improve LLMs",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Generating step-by-step \"how-to\" procedures is a key LLM capability: how-to advice is commonly requested in chatbots, and step-by-step planning is critical for reasoning over complex tasks. Yet, measuring and improving procedural validity at scale on real-world tasks remains challenging and understudied. To address this, we introduce How2Everything, a scalable framework to evaluate and improve goal-conditioned procedure generation. Our framework includes How2Mine, which mines 351K procedures from 980K web pages across 14 topics and readily scales to larger corpora. From this pool we build How2Bench, a 7K-example evaluation set balanced across topics. To reliably score model outputs, we develop How2Score, an evaluation protocol that uses an LLM judge to detect whether a generation contains any critical failure that would prevent achieving the goal. For low-cost, reproducible evaluation, we distill a frontier model into an open 8B model, achieving 80.5% agreement with human annotators. How2Bench reveals clear scaling trends across model sizes and training stages, providing signal early in pretraining. Finally, RL using How2Score as a reward improves performance on How2Bench by >10 points across three models without systematic regressions on standard benchmarks, with gains robust to superficial source-document memorization or format compliance. Taken together, How2Everything shows how pretraining web data can support a closed loop of capability evaluation and improvement at scale.",
      "upvotes": 2,
      "discussionId": "698ac3981b2dc6b37d61b1ad",
      "githubRepo": "https://github.com/lilakk/how2everything",
      "githubRepoAddedBy": "user",
      "ai_summary": "A scalable framework for evaluating and improving goal-conditioned procedure generation using large-scale web mining, automated scoring, and reinforcement learning to enhance step-by-step instruction quality.",
      "ai_keywords": [
        "goal-conditioned procedure generation",
        "How2Mine",
        "How2Bench",
        "How2Score",
        "LLM judge",
        "distillation",
        "reinforcement learning",
        "pretraining",
        "closed loop evaluation"
      ],
      "githubStars": 1,
      "organization": {
        "_id": "5e70f3648ce3c604d78fe132",
        "name": "allenai",
        "fullname": "Ai2",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652db071b62cf1f8463221e2/CxxwFiaomTa1MCX_B7-pT.png"
      }
    },
    "publishedAt": "2026-02-09T10:47:14.000Z",
    "title": "How2Everything: Mining the Web for How-To Procedures to Evaluate and Improve LLMs",
    "summary": "Generating step-by-step \"how-to\" procedures is a key LLM capability: how-to advice is commonly requested in chatbots, and step-by-step planning is critical for reasoning over complex tasks. Yet, measuring and improving procedural validity at scale on real-world tasks remains challenging and understudied. To address this, we introduce How2Everything, a scalable framework to evaluate and improve goal-conditioned procedure generation. Our framework includes How2Mine, which mines 351K procedures from 980K web pages across 14 topics and readily scales to larger corpora. From this pool we build How2Bench, a 7K-example evaluation set balanced across topics. To reliably score model outputs, we develop How2Score, an evaluation protocol that uses an LLM judge to detect whether a generation contains any critical failure that would prevent achieving the goal. For low-cost, reproducible evaluation, we distill a frontier model into an open 8B model, achieving 80.5% agreement with human annotators. How2Bench reveals clear scaling trends across model sizes and training stages, providing signal early in pretraining. Finally, RL using How2Score as a reward improves performance on How2Bench by >10 points across three models without systematic regressions on standard benchmarks, with gains robust to superficial source-document memorization or format compliance. Taken together, How2Everything shows how pretraining web data can support a closed loop of capability evaluation and improvement at scale.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08808.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 230,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "5e70f3648ce3c604d78fe132",
      "name": "allenai",
      "fullname": "Ai2",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652db071b62cf1f8463221e2/CxxwFiaomTa1MCX_B7-pT.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.08829",
      "authors": [
        {
          "_id": "698ab50a1b2dc6b37d61afdb",
          "name": "Hao Peng",
          "hidden": false
        },
        {
          "_id": "698ab50a1b2dc6b37d61afdc",
          "name": "Yunjia Qi",
          "hidden": false
        },
        {
          "_id": "698ab50a1b2dc6b37d61afdd",
          "name": "Xiaozhi Wang",
          "hidden": false
        },
        {
          "_id": "698ab50a1b2dc6b37d61afde",
          "name": "Zijun Yao",
          "hidden": false
        },
        {
          "_id": "698ab50a1b2dc6b37d61afdf",
          "name": "Lei Hou",
          "hidden": false
        },
        {
          "_id": "698ab50a1b2dc6b37d61afe0",
          "name": "Juanzi Li",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-09T16:00:30.000Z",
      "submittedOnDailyAt": "2026-02-10T02:25:20.733Z",
      "title": "WildReward: Learning Reward Models from In-the-Wild Human Interactions",
      "submittedOnDailyBy": {
        "_id": "625a5446f1063e7085d5178a",
        "avatarUrl": "/avatars/5e78186f13f74b14e01583e06ff6c4dc.svg",
        "isPro": false,
        "fullname": "Hao Peng",
        "user": "Wesleythu",
        "type": "user"
      },
      "summary": "Reward models (RMs) are crucial for the training of large language models (LLMs), yet they typically rely on large-scale human-annotated preference pairs. With the widespread deployment of LLMs, in-the-wild interactions have emerged as a rich source of implicit reward signals. This raises the question: Can we develop reward models directly from in-the-wild interactions? In this work, we explore this possibility by adopting WildChat as an interaction source and proposing a pipeline to extract reliable human feedback, yielding 186k high-quality instances for training WildReward via ordinal regression directly on user feedback without preference pairs. Extensive experiments demonstrate that WildReward achieves comparable or even superior performance compared to conventional reward models, with improved calibration and cross-sample consistency. We also observe that WildReward benefits directly from user diversity, where more users yield stronger reward models. Finally, we apply WildReward to online DPO training and observe significant improvements across various tasks. Code and data are released at https://github.com/THU-KEG/WildReward.",
      "upvotes": 1,
      "discussionId": "698ab50a1b2dc6b37d61afe1",
      "ai_summary": "WildReward demonstrates that reward models can be effectively trained from in-the-wild user interactions using ordinal regression, achieving performance comparable to traditional methods while benefiting from user diversity.",
      "ai_keywords": [
        "reward models",
        "large language models",
        "ordinal regression",
        "in-the-wild interactions",
        "user feedback",
        "online DPO training"
      ],
      "organization": {
        "_id": "64db4fc57266618e854318f4",
        "name": "THU-KEG",
        "fullname": "Knowledge Engineer Group @ Tsinghua University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/648c4b46e549be47af1aafcd/5atqdE9AUWvYAHm9FNkG_.png"
      }
    },
    "publishedAt": "2026-02-09T11:00:30.000Z",
    "title": "WildReward: Learning Reward Models from In-the-Wild Human Interactions",
    "summary": "Reward models (RMs) are crucial for the training of large language models (LLMs), yet they typically rely on large-scale human-annotated preference pairs. With the widespread deployment of LLMs, in-the-wild interactions have emerged as a rich source of implicit reward signals. This raises the question: Can we develop reward models directly from in-the-wild interactions? In this work, we explore this possibility by adopting WildChat as an interaction source and proposing a pipeline to extract reliable human feedback, yielding 186k high-quality instances for training WildReward via ordinal regression directly on user feedback without preference pairs. Extensive experiments demonstrate that WildReward achieves comparable or even superior performance compared to conventional reward models, with improved calibration and cross-sample consistency. We also observe that WildReward benefits directly from user diversity, where more users yield stronger reward models. Finally, we apply WildReward to online DPO training and observe significant improvements across various tasks. Code and data are released at https://github.com/THU-KEG/WildReward.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08829.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "625a5446f1063e7085d5178a",
      "avatarUrl": "/avatars/5e78186f13f74b14e01583e06ff6c4dc.svg",
      "fullname": "Hao Peng",
      "name": "Wesleythu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "64db4fc57266618e854318f4",
      "name": "THU-KEG",
      "fullname": "Knowledge Engineer Group @ Tsinghua University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/648c4b46e549be47af1aafcd/5atqdE9AUWvYAHm9FNkG_.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.07970",
      "authors": [
        {
          "_id": "698ad28e1b2dc6b37d61b234",
          "name": "Zheyuan Hu",
          "hidden": false
        },
        {
          "_id": "698ad28e1b2dc6b37d61b235",
          "name": "Weitao Chen",
          "hidden": false
        },
        {
          "_id": "698ad28e1b2dc6b37d61b236",
          "name": "Cengiz ztireli",
          "hidden": false
        },
        {
          "_id": "698ad28e1b2dc6b37d61b237",
          "name": "Chenliang Zhou",
          "hidden": false
        },
        {
          "_id": "698ad28e1b2dc6b37d61b238",
          "name": "Fangcheng Zhong",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-08T13:44:36.000Z",
      "submittedOnDailyAt": "2026-02-10T04:10:11.879Z",
      "title": "Learning-guided Kansa collocation for forward and inverse PDEs beyond linearity",
      "submittedOnDailyBy": {
        "_id": "653053c6657ae56cdb5c490b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653053c6657ae56cdb5c490b/Zl0bZZ7-tD9AR6muE8esU.png",
        "isPro": false,
        "fullname": "Peter Hu",
        "user": "Peter2023HuggingFace",
        "type": "user"
      },
      "summary": "Partial Differential Equations are precise in modelling the physical, biological and graphical phenomena. However, the numerical methods suffer from the curse of dimensionality, high computation costs and domain-specific discretization. We aim to explore pros and cons of different PDE solvers, and apply them to specific scientific simulation problems, including forwarding solution, inverse problems and equations discovery. In particular, we extend the recent CNF (NeurIPS 2023) framework solver to multi-dependent-variable and non-linear settings, together with down-stream applications. The outcomes include implementation of selected methods, self-tuning techniques, evaluation on benchmark problems and a comprehensive survey of neural PDE solvers and scientific simulation applications.",
      "upvotes": 1,
      "discussionId": "698ad28e1b2dc6b37d61b239",
      "ai_summary": "Research explores PDE solvers including neural frameworks for scientific simulations, examining forward solutions, inverse problems, and equation discovery across multi-variable and non-linear systems.",
      "ai_keywords": [
        "PDE solvers",
        "neural PDE solvers",
        "scientific simulation",
        "forward solution",
        "inverse problems",
        "equation discovery",
        "CNF",
        "multi-dependent-variable",
        "non-linear settings"
      ]
    },
    "publishedAt": "2026-02-08T08:44:36.000Z",
    "title": "Learning-guided Kansa collocation for forward and inverse PDEs beyond linearity",
    "summary": "Partial Differential Equations are precise in modelling the physical, biological and graphical phenomena. However, the numerical methods suffer from the curse of dimensionality, high computation costs and domain-specific discretization. We aim to explore pros and cons of different PDE solvers, and apply them to specific scientific simulation problems, including forwarding solution, inverse problems and equations discovery. In particular, we extend the recent CNF (NeurIPS 2023) framework solver to multi-dependent-variable and non-linear settings, together with down-stream applications. The outcomes include implementation of selected methods, self-tuning techniques, evaluation on benchmark problems and a comprehensive survey of neural PDE solvers and scientific simulation applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07970.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653053c6657ae56cdb5c490b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653053c6657ae56cdb5c490b/Zl0bZZ7-tD9AR6muE8esU.png",
      "fullname": "Peter Hu",
      "name": "Peter2023HuggingFace",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.07803",
      "authors": [
        {
          "_id": "698acb2d1b2dc6b37d61b1f1",
          "name": "Jiale Qian",
          "hidden": false
        },
        {
          "_id": "698acb2d1b2dc6b37d61b1f2",
          "name": "Hao Meng",
          "hidden": false
        },
        {
          "_id": "698acb2d1b2dc6b37d61b1f3",
          "name": "Tian Zheng",
          "hidden": false
        },
        {
          "_id": "698acb2d1b2dc6b37d61b1f4",
          "name": "Pengcheng Zhu",
          "hidden": false
        },
        {
          "_id": "698acb2d1b2dc6b37d61b1f5",
          "name": "Haopeng Lin",
          "hidden": false
        },
        {
          "_id": "698acb2d1b2dc6b37d61b1f6",
          "name": "Yuhang Dai",
          "hidden": false
        },
        {
          "_id": "698acb2d1b2dc6b37d61b1f7",
          "name": "Hanke Xie",
          "hidden": false
        },
        {
          "_id": "698acb2d1b2dc6b37d61b1f8",
          "name": "Wenxiao Cao",
          "hidden": false
        },
        {
          "_id": "698acb2d1b2dc6b37d61b1f9",
          "name": "Ruixuan Shang",
          "hidden": false
        },
        {
          "_id": "698acb2d1b2dc6b37d61b1fa",
          "name": "Jun Wu",
          "hidden": false
        },
        {
          "_id": "698acb2d1b2dc6b37d61b1fb",
          "name": "Hongmei Liu",
          "hidden": false
        },
        {
          "_id": "698acb2d1b2dc6b37d61b1fc",
          "name": "Hanlin Wen",
          "hidden": false
        },
        {
          "_id": "698acb2d1b2dc6b37d61b1fd",
          "name": "Jian Zhao",
          "hidden": false
        },
        {
          "_id": "698acb2d1b2dc6b37d61b1fe",
          "name": "Zhonglin Jiang",
          "hidden": false
        },
        {
          "_id": "698acb2d1b2dc6b37d61b1ff",
          "name": "Yong Chen",
          "hidden": false
        },
        {
          "_id": "698acb2d1b2dc6b37d61b200",
          "name": "Shunshun Yin",
          "hidden": false
        },
        {
          "_id": "698acb2d1b2dc6b37d61b201",
          "name": "Ming Tao",
          "hidden": false
        },
        {
          "_id": "698acb2d1b2dc6b37d61b202",
          "name": "Jianguo Wei",
          "hidden": false
        },
        {
          "_id": "698acb2d1b2dc6b37d61b203",
          "name": "Lei Xie",
          "hidden": false
        },
        {
          "_id": "698acb2d1b2dc6b37d61b204",
          "name": "Xinsheng Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-08T03:51:23.000Z",
      "submittedOnDailyAt": "2026-02-10T03:38:46.400Z",
      "title": "SoulX-Singer: Towards High-Quality Zero-Shot Singing Voice Synthesis",
      "submittedOnDailyBy": {
        "_id": "6564364649f816b798ac3b4e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6564364649f816b798ac3b4e/4cOQ-YtP4UQF94r9nel3B.jpeg",
        "isPro": false,
        "fullname": "Xinsheng Wang",
        "user": "Xinsheng-Wang",
        "type": "user"
      },
      "summary": "While recent years have witnessed rapid progress in speech synthesis, open-source singing voice synthesis (SVS) systems still face significant barriers to industrial deployment, particularly in terms of robustness and zero-shot generalization. In this report, we introduce SoulX-Singer, a high-quality open-source SVS system designed with practical deployment considerations in mind. SoulX-Singer supports controllable singing generation conditioned on either symbolic musical scores (MIDI) or melodic representations, enabling flexible and expressive control in real-world production workflows. Trained on more than 42,000 hours of vocal data, the system supports Mandarin Chinese, English, and Cantonese and consistently achieves state-of-the-art synthesis quality across languages under diverse musical conditions. Furthermore, to enable reliable evaluation of zero-shot SVS performance in practical scenarios, we construct SoulX-Singer-Eval, a dedicated benchmark with strict training-test disentanglement, facilitating systematic assessment in zero-shot settings.",
      "upvotes": 1,
      "discussionId": "698acb2e1b2dc6b37d61b205",
      "ai_summary": "A high-quality open-source singing voice synthesis system is presented with support for multiple languages and controllable generation, along with a dedicated benchmark for evaluating zero-shot performance.",
      "ai_keywords": [
        "singing voice synthesis",
        "symbolic musical scores",
        "melodic representations",
        "zero-shot generalization",
        "speech synthesis",
        "Mandarin Chinese",
        "English",
        "Cantonese"
      ],
      "organization": {
        "_id": "68c932c483517d3b908445b1",
        "name": "Soul-AILab",
        "fullname": "Soul-AILab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6564364649f816b798ac3b4e/hdbm42BRO080btTwwQTD1.jpeg"
      }
    },
    "publishedAt": "2026-02-07T22:51:23.000Z",
    "title": "SoulX-Singer: Towards High-Quality Zero-Shot Singing Voice Synthesis",
    "summary": "While recent years have witnessed rapid progress in speech synthesis, open-source singing voice synthesis (SVS) systems still face significant barriers to industrial deployment, particularly in terms of robustness and zero-shot generalization. In this report, we introduce SoulX-Singer, a high-quality open-source SVS system designed with practical deployment considerations in mind. SoulX-Singer supports controllable singing generation conditioned on either symbolic musical scores (MIDI) or melodic representations, enabling flexible and expressive control in real-world production workflows. Trained on more than 42,000 hours of vocal data, the system supports Mandarin Chinese, English, and Cantonese and consistently achieves state-of-the-art synthesis quality across languages under diverse musical conditions. Furthermore, to enable reliable evaluation of zero-shot SVS performance in practical scenarios, we construct SoulX-Singer-Eval, a dedicated benchmark with strict training-test disentanglement, facilitating systematic assessment in zero-shot settings.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07803.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6564364649f816b798ac3b4e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6564364649f816b798ac3b4e/4cOQ-YtP4UQF94r9nel3B.jpeg",
      "fullname": "Xinsheng Wang",
      "name": "Xinsheng-Wang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "68c932c483517d3b908445b1",
      "name": "Soul-AILab",
      "fullname": "Soul-AILab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6564364649f816b798ac3b4e/hdbm42BRO080btTwwQTD1.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.07796",
      "authors": [
        {
          "_id": "698ab61c1b2dc6b37d61b001",
          "name": "Jiatong Li",
          "hidden": false
        },
        {
          "_id": "698ab61c1b2dc6b37d61b002",
          "name": "Changdae Oh",
          "hidden": false
        },
        {
          "_id": "698ab61c1b2dc6b37d61b003",
          "name": "Hyeong Kyu Choi",
          "hidden": false
        },
        {
          "_id": "698ab61c1b2dc6b37d61b004",
          "name": "Jindong Wang",
          "hidden": false
        },
        {
          "_id": "698ab61c1b2dc6b37d61b005",
          "name": "Sharon Li",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-08T03:23:22.000Z",
      "submittedOnDailyAt": "2026-02-10T04:51:37.816Z",
      "title": "Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents",
      "submittedOnDailyBy": {
        "_id": "672fc8ede7c89e44c9757259",
        "avatarUrl": "/avatars/caa0d0de519ea96992c81328c89b3843.svg",
        "isPro": false,
        "fullname": "Changdae Oh",
        "user": "changdae",
        "type": "user"
      },
      "summary": "Eliciting reasoning has emerged as a powerful technique for improving the performance of large language models (LLMs) on complex tasks by inducing thinking. However, their effectiveness in realistic user-engaged agent scenarios remains unclear. In this paper, we conduct a comprehensive study on the effect of explicit thinking in user-engaged LLM agents. Our experiments span across seven models, three benchmarks, and two thinking instantiations, and we evaluate them through both a quantitative response taxonomy analysis and qualitative failure propagation case studies. Contrary to expectations, we find that mandatory thinking often backfires on agents in user-engaged settings, causing anomalous performance degradation across various LLMs. Our key finding reveals that thinking makes agents more ``introverted'' by shortening responses and reducing information disclosure to users, which weakens agent-user information exchange and leads to downstream task failures. Furthermore, we demonstrate that explicitly prompting for information disclosure reliably improves performance across diverse model families, suggesting that proactive transparency is a vital lever for agent optimization. Overall, our study suggests that information transparency awareness is a crucial yet underexplored perspective for the future design of reasoning agents in real-world scenarios. Our code is available at https://github.com/deeplearning-wisc/Thinking-Agent.",
      "upvotes": 1,
      "discussionId": "698ab61c1b2dc6b37d61b006",
      "ai_summary": "Explicit reasoning in LLM agents can degrade performance in user-engaged scenarios by reducing information disclosure and weakening agent-user communication, with transparency-aware prompting showing better results.",
      "ai_keywords": [
        "large language models",
        "reasoning",
        "agent scenarios",
        "information disclosure",
        "performance degradation",
        "transparent prompting"
      ],
      "organization": {
        "_id": "61d090ec03bc10eb8e1c2970",
        "name": "uw-madison",
        "fullname": "University of Wisconsin - Madison",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/IYmUaLUc_rDVNC6F7-k8M.png"
      }
    },
    "publishedAt": "2026-02-07T22:23:22.000Z",
    "title": "Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents",
    "summary": "Eliciting reasoning has emerged as a powerful technique for improving the performance of large language models (LLMs) on complex tasks by inducing thinking. However, their effectiveness in realistic user-engaged agent scenarios remains unclear. In this paper, we conduct a comprehensive study on the effect of explicit thinking in user-engaged LLM agents. Our experiments span across seven models, three benchmarks, and two thinking instantiations, and we evaluate them through both a quantitative response taxonomy analysis and qualitative failure propagation case studies. Contrary to expectations, we find that mandatory thinking often backfires on agents in user-engaged settings, causing anomalous performance degradation across various LLMs. Our key finding reveals that thinking makes agents more ``introverted'' by shortening responses and reducing information disclosure to users, which weakens agent-user information exchange and leads to downstream task failures. Furthermore, we demonstrate that explicitly prompting for information disclosure reliably improves performance across diverse model families, suggesting that proactive transparency is a vital lever for agent optimization. Overall, our study suggests that information transparency awareness is a crucial yet underexplored perspective for the future design of reasoning agents in real-world scenarios. Our code is available at https://github.com/deeplearning-wisc/Thinking-Agent.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07796.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "672fc8ede7c89e44c9757259",
      "avatarUrl": "/avatars/caa0d0de519ea96992c81328c89b3843.svg",
      "fullname": "Changdae Oh",
      "name": "changdae",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "61d090ec03bc10eb8e1c2970",
      "name": "uw-madison",
      "fullname": "University of Wisconsin - Madison",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/IYmUaLUc_rDVNC6F7-k8M.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.07090",
      "authors": [
        {
          "_id": "698ac44a1b2dc6b37d61b1af",
          "name": "Yu-Che Tsai",
          "hidden": false
        },
        {
          "_id": "698ac44a1b2dc6b37d61b1b0",
          "name": "Hsiang Hsiao",
          "hidden": false
        },
        {
          "_id": "698ac44a1b2dc6b37d61b1b1",
          "name": "Kuan-Yu Chen",
          "hidden": false
        },
        {
          "_id": "698ac44a1b2dc6b37d61b1b2",
          "name": "Shou-De Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-06T09:28:55.000Z",
      "submittedOnDailyAt": "2026-02-10T03:11:37.804Z",
      "title": "Concept-Aware Privacy Mechanisms for Defending Embedding Inversion Attacks",
      "submittedOnDailyBy": {
        "_id": "64970396bf36c2cc700ed3cb",
        "avatarUrl": "/avatars/dbbf82c204c34afe62f8f1e73e9d1818.svg",
        "isPro": false,
        "fullname": "Yu-Che Tsai",
        "user": "Roytsai27",
        "type": "user"
      },
      "summary": "Text embeddings enable numerous NLP applications but face severe privacy risks from embedding inversion attacks, which can expose sensitive attributes or reconstruct raw text. Existing differential privacy defenses assume uniform sensitivity across embedding dimensions, leading to excessive noise and degraded utility. We propose SPARSE, a user-centric framework for concept-specific privacy protection in text embeddings. SPARSE combines (1) differentiable mask learning to identify privacy-sensitive dimensions for user-defined concepts, and (2) the Mahalanobis mechanism that applies elliptical noise calibrated by dimension sensitivity. Unlike traditional spherical noise injection, SPARSE selectively perturbs privacy-sensitive dimensions while preserving non-sensitive semantics. Evaluated across six datasets with three embedding models and attack scenarios, SPARSE consistently reduces privacy leakage while achieving superior downstream performance compared to state-of-the-art DP methods.",
      "upvotes": 1,
      "discussionId": "698ac44a1b2dc6b37d61b1b3",
      "ai_summary": "SPARSE is a user-centric framework that protects text embeddings from privacy leaks by selectively perturbing sensitive dimensions using differentiable masking and Mahalanobis noise calibration.",
      "ai_keywords": [
        "text embeddings",
        "differential privacy",
        "embedding inversion attacks",
        "concept-specific privacy protection",
        "differentiable mask learning",
        "Mahalanobis mechanism",
        "elliptical noise",
        "privacy-sensitive dimensions",
        "downstream performance"
      ]
    },
    "publishedAt": "2026-02-06T04:28:55.000Z",
    "title": "Concept-Aware Privacy Mechanisms for Defending Embedding Inversion Attacks",
    "summary": "Text embeddings enable numerous NLP applications but face severe privacy risks from embedding inversion attacks, which can expose sensitive attributes or reconstruct raw text. Existing differential privacy defenses assume uniform sensitivity across embedding dimensions, leading to excessive noise and degraded utility. We propose SPARSE, a user-centric framework for concept-specific privacy protection in text embeddings. SPARSE combines (1) differentiable mask learning to identify privacy-sensitive dimensions for user-defined concepts, and (2) the Mahalanobis mechanism that applies elliptical noise calibrated by dimension sensitivity. Unlike traditional spherical noise injection, SPARSE selectively perturbs privacy-sensitive dimensions while preserving non-sensitive semantics. Evaluated across six datasets with three embedding models and attack scenarios, SPARSE consistently reduces privacy leakage while achieving superior downstream performance compared to state-of-the-art DP methods.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07090.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64970396bf36c2cc700ed3cb",
      "avatarUrl": "/avatars/dbbf82c204c34afe62f8f1e73e9d1818.svg",
      "fullname": "Yu-Che Tsai",
      "name": "Roytsai27",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.06445",
      "authors": [
        {
          "_id": "6989adb6beecc443208d27de",
          "user": {
            "_id": "65c0ae04b7db0ab09541beed",
            "avatarUrl": "/avatars/d98cbcc3ad1669221c4a50eccfabc9d0.svg",
            "isPro": false,
            "fullname": "Weidong Huang",
            "user": "Weidong-Huang",
            "type": "user"
          },
          "name": "Weidong Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2026-02-09T14:31:28.409Z",
          "hidden": false
        },
        {
          "_id": "6989adb6beecc443208d27df",
          "name": "Jingwen Zhang",
          "hidden": false
        },
        {
          "_id": "6989adb6beecc443208d27e0",
          "name": "Jiongye Li",
          "hidden": false
        },
        {
          "_id": "6989adb6beecc443208d27e1",
          "name": "Shibowen Zhang",
          "hidden": false
        },
        {
          "_id": "6989adb6beecc443208d27e2",
          "name": "Jiayang Wu",
          "hidden": false
        },
        {
          "_id": "6989adb6beecc443208d27e3",
          "name": "Jiayi Wang",
          "hidden": false
        },
        {
          "_id": "6989adb6beecc443208d27e4",
          "name": "Hangxin Liu",
          "hidden": false
        },
        {
          "_id": "6989adb6beecc443208d27e5",
          "name": "Yaodong Yang",
          "hidden": false
        },
        {
          "_id": "6989adb6beecc443208d27e6",
          "name": "Yao Su",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-06T07:14:43.000Z",
      "submittedOnDailyAt": "2026-02-10T00:27:21.524Z",
      "title": "ECO: Energy-Constrained Optimization with Reinforcement Learning for Humanoid Walking",
      "submittedOnDailyBy": {
        "_id": "65c0ae04b7db0ab09541beed",
        "avatarUrl": "/avatars/d98cbcc3ad1669221c4a50eccfabc9d0.svg",
        "isPro": false,
        "fullname": "Weidong Huang",
        "user": "Weidong-Huang",
        "type": "user"
      },
      "summary": "Achieving stable and energy-efficient locomotion is essential for humanoid robots to operate continuously in real-world applications. Existing MPC and RL approaches often rely on energy-related metrics embedded within a multi-objective optimization framework, which require extensive hyperparameter tuning and often result in suboptimal policies. To address these challenges, we propose ECO (Energy-Constrained Optimization), a constrained RL framework that separates energy-related metrics from rewards, reformulating them as explicit inequality constraints. This method provides a clear and interpretable physical representation of energy costs, enabling more efficient and intuitive hyperparameter tuning for improved energy efficiency. ECO introduces dedicated constraints for energy consumption and reference motion, enforced by the Lagrangian method, to achieve stable, symmetric, and energy-efficient walking for humanoid robots. We evaluated ECO against MPC, standard RL with reward shaping, and four state-of-the-art constrained RL methods. Experiments, including sim-to-sim and sim-to-real transfers on the kid-sized humanoid robot BRUCE, demonstrate that ECO significantly reduces energy consumption compared to baselines while maintaining robust walking performance. These results highlight a substantial advancement in energy-efficient humanoid locomotion. All experimental demonstrations can be found on the project website: https://sites.google.com/view/eco-humanoid.",
      "upvotes": 1,
      "discussionId": "6989adb6beecc443208d27e7",
      "projectPage": "https://sites.google.com/view/eco-humanoid",
      "githubRepo": "https://github.com/bigai-ai/ECO-humanoid",
      "githubRepoAddedBy": "user",
      "ai_summary": "Energy-constrained optimization framework separates energy metrics from rewards using Lagrangian method to achieve stable, energy-efficient humanoid robot locomotion with reduced hyperparameter tuning.",
      "ai_keywords": [
        "model predictive control",
        "reinforcement learning",
        "constrained optimization",
        "Lagrangian method",
        "energy-constrained optimization",
        "humanoid robotics",
        "sim-to-sim transfer",
        "sim-to-real transfer"
      ],
      "githubStars": 0
    },
    "publishedAt": "2026-02-06T02:14:43.000Z",
    "title": "ECO: Energy-Constrained Optimization with Reinforcement Learning for Humanoid Walking",
    "summary": "Achieving stable and energy-efficient locomotion is essential for humanoid robots to operate continuously in real-world applications. Existing MPC and RL approaches often rely on energy-related metrics embedded within a multi-objective optimization framework, which require extensive hyperparameter tuning and often result in suboptimal policies. To address these challenges, we propose ECO (Energy-Constrained Optimization), a constrained RL framework that separates energy-related metrics from rewards, reformulating them as explicit inequality constraints. This method provides a clear and interpretable physical representation of energy costs, enabling more efficient and intuitive hyperparameter tuning for improved energy efficiency. ECO introduces dedicated constraints for energy consumption and reference motion, enforced by the Lagrangian method, to achieve stable, symmetric, and energy-efficient walking for humanoid robots. We evaluated ECO against MPC, standard RL with reward shaping, and four state-of-the-art constrained RL methods. Experiments, including sim-to-sim and sim-to-real transfers on the kid-sized humanoid robot BRUCE, demonstrate that ECO significantly reduces energy consumption compared to baselines while maintaining robust walking performance. These results highlight a substantial advancement in energy-efficient humanoid locomotion. All experimental demonstrations can be found on the project website: https://sites.google.com/view/eco-humanoid.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06445.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65c0ae04b7db0ab09541beed",
      "avatarUrl": "/avatars/d98cbcc3ad1669221c4a50eccfabc9d0.svg",
      "fullname": "Weidong Huang",
      "name": "Weidong-Huang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "isUserFollowing": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2602.07054",
      "authors": [
        {
          "_id": "698ad51b1b2dc6b37d61b251",
          "name": "Ashutosh Chaubey",
          "hidden": false
        },
        {
          "_id": "698ad51b1b2dc6b37d61b252",
          "name": "Jiacheng Pang",
          "hidden": false
        },
        {
          "_id": "698ad51b1b2dc6b37d61b253",
          "name": "Maksim Siniukov",
          "hidden": false
        },
        {
          "_id": "698ad51b1b2dc6b37d61b254",
          "name": "Mohammad Soleymani",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6541185dbd60d2bd193f7999/t1LVgPhBIAsDHQQOOF3_H.png"
      ],
      "publishedAt": "2026-02-04T18:24:25.000Z",
      "submittedOnDailyAt": "2026-02-10T04:34:30.828Z",
      "title": "AVERE: Improving Audiovisual Emotion Reasoning with Preference Optimization",
      "submittedOnDailyBy": {
        "_id": "6541185dbd60d2bd193f7999",
        "avatarUrl": "/avatars/4ce4a0feff9bfbb87e9f40431718ba00.svg",
        "isPro": false,
        "fullname": "Ashutosh Chaubey",
        "user": "chaubeyG",
        "type": "user"
      },
      "summary": "Emotion understanding is essential for building socially intelligent agents. Although recent multimodal large language models have shown strong performance on this task, two key challenges remain - spurious associations between emotions and irrelevant audiovisual cues, and hallucinations of audiovisual cues driven by text priors in the language model backbone. To quantify and understand these issues, we introduce EmoReAlM, a benchmark designed to evaluate MLLMs for cue-emotion associations, hallucinations and modality agreement. We then propose AVEm-DPO, a preference optimization technique that aligns model responses with both audiovisual inputs and emotion-centric queries. Specifically, we construct preferences over responses exhibiting spurious associations or hallucinations, and audiovisual input pairs guided by textual prompts. We also include a regularization term that penalizes reliance on text priors, thereby mitigating modality-specific cue hallucinations. Experimental results on DFEW, RAVDESS and EMER demonstrate that our method significantly improves the performance of the reference baseline models with 6-19% of relative performance gains in zero-shot settings. By providing both a rigorous benchmark and a robust optimization framework, this work enables principled evaluation and improvement of MLLMs for emotion understanding and social AI. Code, models and benchmark will be released at https://avere-iclr.github.io.",
      "upvotes": 1,
      "discussionId": "698ad51b1b2dc6b37d61b255",
      "projectPage": "https://avere-iclr.github.io/",
      "ai_summary": "A benchmark and optimization technique are presented to improve multimodal large language models' emotion understanding by addressing spurious associations and hallucinations in audiovisual cues.",
      "ai_keywords": [
        "multimodal large language models",
        "preference optimization",
        "cue-emotion associations",
        "hallucinations",
        "modality agreement",
        "AVEm-DPO",
        "spurious associations",
        "text priors",
        "audiovisual inputs",
        "emotion-centric queries"
      ],
      "organization": {
        "_id": "66a403d0dcb5bbc6e98bb7d0",
        "name": "UniversityofSouthernCalifornia",
        "fullname": "University of Southern California",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a403728069e3c30e0d8524/tkYCfeIJfF1FxtYiRZ8bf.png"
      }
    },
    "publishedAt": "2026-02-04T13:24:25.000Z",
    "title": "AVERE: Improving Audiovisual Emotion Reasoning with Preference Optimization",
    "summary": "Emotion understanding is essential for building socially intelligent agents. Although recent multimodal large language models have shown strong performance on this task, two key challenges remain - spurious associations between emotions and irrelevant audiovisual cues, and hallucinations of audiovisual cues driven by text priors in the language model backbone. To quantify and understand these issues, we introduce EmoReAlM, a benchmark designed to evaluate MLLMs for cue-emotion associations, hallucinations and modality agreement. We then propose AVEm-DPO, a preference optimization technique that aligns model responses with both audiovisual inputs and emotion-centric queries. Specifically, we construct preferences over responses exhibiting spurious associations or hallucinations, and audiovisual input pairs guided by textual prompts. We also include a regularization term that penalizes reliance on text priors, thereby mitigating modality-specific cue hallucinations. Experimental results on DFEW, RAVDESS and EMER demonstrate that our method significantly improves the performance of the reference baseline models with 6-19% of relative performance gains in zero-shot settings. By providing both a rigorous benchmark and a robust optimization framework, this work enables principled evaluation and improvement of MLLMs for emotion understanding and social AI. Code, models and benchmark will be released at https://avere-iclr.github.io.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6541185dbd60d2bd193f7999/t1LVgPhBIAsDHQQOOF3_H.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07054.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6541185dbd60d2bd193f7999",
      "avatarUrl": "/avatars/4ce4a0feff9bfbb87e9f40431718ba00.svg",
      "fullname": "Ashutosh Chaubey",
      "name": "chaubeyG",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "66a403d0dcb5bbc6e98bb7d0",
      "name": "UniversityofSouthernCalifornia",
      "fullname": "University of Southern California",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a403728069e3c30e0d8524/tkYCfeIJfF1FxtYiRZ8bf.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.07775",
      "authors": [
        {
          "_id": "698ac1fc1b2dc6b37d61b183",
          "name": "Haodong Li",
          "hidden": false
        },
        {
          "_id": "698ac1fc1b2dc6b37d61b184",
          "name": "Shaoteng Liu",
          "hidden": false
        },
        {
          "_id": "698ac1fc1b2dc6b37d61b185",
          "name": "Zhe Lin",
          "hidden": false
        },
        {
          "_id": "698ac1fc1b2dc6b37d61b186",
          "name": "Manmohan Chandraker",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/5LzL-wYavOE2BRuVcFIqL.mp4"
      ],
      "publishedAt": "2026-02-08T02:16:02.000Z",
      "submittedOnDailyAt": "2026-02-10T02:58:29.492Z",
      "title": "Rolling Sink: Bridging Limited-Horizon Training and Open-Ended Testing in Autoregressive Video Diffusion",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recently, autoregressive (AR) video diffusion models has achieved remarkable performance. However, due to their limited training durations, a train-test gap emerges when testing at longer horizons, leading to rapid visual degradations. Following Self Forcing, which studies the train-test gap within the training duration, this work studies the train-test gap beyond the training duration, i.e., the gap between the limited horizons during training and open-ended horizons during testing. Since open-ended testing can extend beyond any finite training window, and long-video training is computationally expensive, we pursue a training-free solution to bridge this gap. To explore a training-free solution, we conduct a systematic analysis of AR cache maintenance. These insights lead to Rolling Sink. Built on Self Forcing (trained on only 5s clips), Rolling Sink effectively scales the AR video synthesis to ultra-long durations (e.g., 5-30 minutes at 16 FPS) at test time, with consistent subjects, stable colors, coherent structures, and smooth motions. As demonstrated by extensive experiments, Rolling Sink achieves superior long-horizon visual fidelity and temporal consistency compared to SOTA baselines. Project page: https://rolling-sink.github.io/",
      "upvotes": 0,
      "discussionId": "698ac1fd1b2dc6b37d61b187",
      "projectPage": "https://rolling-sink.github.io/",
      "ai_summary": "Autoregressive video diffusion models suffer from train-test gaps when generating long videos, but a training-free approach called Rolling Sink addresses this by maintaining AR cache and enabling ultra-long video synthesis.",
      "ai_keywords": [
        "autoregressive video diffusion models",
        "train-test gap",
        "self forcing",
        "rolling sink",
        "AR cache maintenance",
        "long-horizon video synthesis",
        "temporal consistency",
        "visual fidelity"
      ],
      "organization": {
        "_id": "637b318856db0404b7c5a0c2",
        "name": "adobe-research",
        "fullname": "Adobe Research",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1669033410364-624bebf604abc7ebb01789af.png"
      }
    },
    "publishedAt": "2026-02-07T21:16:02.000Z",
    "title": "Rolling Sink: Bridging Limited-Horizon Training and Open-Ended Testing in Autoregressive Video Diffusion",
    "summary": "Recently, autoregressive (AR) video diffusion models has achieved remarkable performance. However, due to their limited training durations, a train-test gap emerges when testing at longer horizons, leading to rapid visual degradations. Following Self Forcing, which studies the train-test gap within the training duration, this work studies the train-test gap beyond the training duration, i.e., the gap between the limited horizons during training and open-ended horizons during testing. Since open-ended testing can extend beyond any finite training window, and long-video training is computationally expensive, we pursue a training-free solution to bridge this gap. To explore a training-free solution, we conduct a systematic analysis of AR cache maintenance. These insights lead to Rolling Sink. Built on Self Forcing (trained on only 5s clips), Rolling Sink effectively scales the AR video synthesis to ultra-long durations (e.g., 5-30 minutes at 16 FPS) at test time, with consistent subjects, stable colors, coherent structures, and smooth motions. As demonstrated by extensive experiments, Rolling Sink achieves superior long-horizon visual fidelity and temporal consistency compared to SOTA baselines. Project page: https://rolling-sink.github.io/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/5LzL-wYavOE2BRuVcFIqL.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07775.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 230,
      "isUserFollowing": false
    },
    "organization": {
      "_id": "637b318856db0404b7c5a0c2",
      "name": "adobe-research",
      "fullname": "Adobe Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1669033410364-624bebf604abc7ebb01789af.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2602.07040",
      "authors": [
        {
          "_id": "698abed21b2dc6b37d61b167",
          "name": "Emmett Bicker",
          "hidden": false
        }
      ],
      "publishedAt": "2026-02-03T19:01:23.000Z",
      "submittedOnDailyAt": "2026-02-10T02:44:58.980Z",
      "title": "Aster: Autonomous Scientific Discovery over 20x Faster Than Existing Methods",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce Aster, an AI agent for autonomous scientific discovery capable of operating over 20 times faster than existing frameworks. Given a task, an initial program, and a script to evaluate the performance of the program, Aster iteratively improves the program, often leading to new state-of-the-art performances. Aster's significant reduction in the number of iterations required for novel discovery expands the domain of tractable problems to include tasks with long evaluation durations, such as multi-hour machine learning training runs.\n  We applied Aster to problems in mathematics, GPU kernel engineering, biology, neuroscience, and language model training. More specifically: the Erdos minimum overlap problem, optimizing the TriMul kernel, a single-cell analysis denoising problem, training a neural activity prediction model to perform well on ZAPBench, and the NanoGPT Speedrun Competition. Aster attains SOTA results in every task, except for ZAPBench, where it matches the performance of the best human solution with less than 1/190th of the compute.\n  Aster is accessible via a web interface and API at asterlab.ai.",
      "upvotes": 0,
      "discussionId": "698abed21b2dc6b37d61b168",
      "projectPage": "https://www.asterlab.ai/",
      "ai_summary": "Aster is an AI agent that accelerates scientific discovery by iteratively improving programs, achieving state-of-the-art results across multiple domains including mathematics, biology, and machine learning with significantly reduced computational requirements.",
      "ai_keywords": [
        "AI agent",
        "autonomous scientific discovery",
        "iterative program improvement",
        "state-of-the-art performance",
        "computational efficiency",
        "web interface",
        "API"
      ]
    },
    "publishedAt": "2026-02-03T14:01:23.000Z",
    "title": "Aster: Autonomous Scientific Discovery over 20x Faster Than Existing Methods",
    "summary": "We introduce Aster, an AI agent for autonomous scientific discovery capable of operating over 20 times faster than existing frameworks. Given a task, an initial program, and a script to evaluate the performance of the program, Aster iteratively improves the program, often leading to new state-of-the-art performances. Aster's significant reduction in the number of iterations required for novel discovery expands the domain of tractable problems to include tasks with long evaluation durations, such as multi-hour machine learning training runs.\n  We applied Aster to problems in mathematics, GPU kernel engineering, biology, neuroscience, and language model training. More specifically: the Erdos minimum overlap problem, optimizing the TriMul kernel, a single-cell analysis denoising problem, training a neural activity prediction model to perform well on ZAPBench, and the NanoGPT Speedrun Competition. Aster attains SOTA results in every task, except for ZAPBench, where it matches the performance of the best human solution with less than 1/190th of the compute.\n  Aster is accessible via a web interface and API at asterlab.ai.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07040.png",
    "numComments": 0,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 230,
      "isUserFollowing": false
    },
    "isAuthorParticipating": false
  }
]