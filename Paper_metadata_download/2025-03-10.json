[
  {
    "paper": {
      "id": "2503.05236",
      "authors": [
        {
          "_id": "67ce37239f9aaaae837f3894",
          "name": "Yibin Wang",
          "hidden": false
        },
        {
          "_id": "67ce37239f9aaaae837f3895",
          "name": "Yuhang Zang",
          "hidden": false
        },
        {
          "_id": "67ce37239f9aaaae837f3896",
          "name": "Hao Li",
          "hidden": false
        },
        {
          "_id": "67ce37239f9aaaae837f3897",
          "name": "Cheng Jin",
          "hidden": false
        },
        {
          "_id": "67ce37239f9aaaae837f3898",
          "name": "Jiaqi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T08:36:05.000Z",
      "title": "Unified Reward Model for Multimodal Understanding and Generation",
      "summary": "Recent advances in human preference alignment have significantly enhanced\nmultimodal generation and understanding. A key approach is training reward\nmodels to guide preference optimization. However, existing models are often\ntask-specific, limiting their adaptability across diverse visual applications.\nWe also argue that jointly learning to assess multiple tasks may foster a\nsynergistic effect, where improved image understanding enhances image\ngeneration assessment, and refined image evaluation benefits video assessment\nthrough better frame analysis. To this end, this paper proposes UnifiedReward,\nthe first unified reward model for multimodal understanding and generation\nassessment, enabling both pairwise ranking and pointwise scoring, which can be\nemployed for vision model preference alignment. Specifically, (1) we first\ndevelop UnifiedReward on our constructed large-scale human preference dataset,\nincluding both image and video generation/understanding tasks. (2) Then, it is\nutilized to automatically construct high-quality preference pair data based on\nthe vision models, fine-gradually filtering their outputs through pair ranking\nand point sifting. (3) Finally, these data are used for their preference\nalignment through Direct Preference Optimization (DPO). Experimental results\ndemonstrate that joint learning to assess diverse visual tasks can lead to\nsubstantial mutual benefits and we apply our pipeline to both image and video\nunderstanding/generation tasks, significantly improving the performance in each\ndomain.",
      "upvotes": 65,
      "discussionId": "67ce37259f9aaaae837f3948",
      "projectPage": "https://codegoat24.github.io/UnifiedReward/",
      "githubRepo": "https://github.com/CodeGoat24/UnifiedReward"
    },
    "publishedAt": "2025-03-09T22:20:09.137Z",
    "title": "Unified Reward Model for Multimodal Understanding and Generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05236.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "654c6845bac6e6e49895a5b5",
      "avatarUrl": "/avatars/ed1f140abcd4d76669e2e48db1d1193f.svg",
      "fullname": "Yibin Wang",
      "name": "CodeGoat24",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.05179",
      "authors": [
        {
          "_id": "67ce4bff5847e4787a7ebedd",
          "name": "Simon A. Aytes",
          "hidden": false
        },
        {
          "_id": "67ce4bff5847e4787a7ebede",
          "name": "Jinheon Baek",
          "hidden": false
        },
        {
          "_id": "67ce4bff5847e4787a7ebedf",
          "name": "Sung Ju Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T06:57:17.000Z",
      "title": "Sketch-of-Thought: Efficient LLM Reasoning with Adaptive\n  Cognitive-Inspired Sketching",
      "summary": "Recent advances in large language models have demonstrated remarkable\nreasoning capabilities through Chain of Thought (CoT) prompting, but often at\nthe cost of excessive verbosity in their intermediate outputs, which increases\ncomputational overhead. We introduce Sketch-of-Thought (SoT), a novel prompting\nframework that combines cognitive-inspired reasoning paradigms with linguistic\nconstraints to minimize token usage while preserving reasoning accuracy. SoT is\ndesigned as a flexible framework that can incorporate any custom reasoning\nparadigms based on cognitive science, and we instantiate it with three such\nparadigms - Conceptual Chaining, Chunked Symbolism, and Expert Lexicons - each\ntailored to different reasoning tasks and selected dynamically via a\nlightweight routing model. Through comprehensive evaluation across 15 reasoning\ndatasets with multiple languages and multimodal scenarios, we demonstrate that\nSoT achieves token reductions of 76% with negligible accuracy impact. In\ncertain domains like mathematical and multi-hop reasoning, it even improves\naccuracy while using significantly fewer tokens. Our code is publicly\navailable: https://www.github.com/SimonAytes/SoT.",
      "upvotes": 24,
      "discussionId": "67ce4c035847e4787a7ebf4c",
      "projectPage": "https://huggingface.co/saytes/SoT_DistilBERT",
      "githubRepo": "https://github.com/SimonAytes/SoT"
    },
    "publishedAt": "2025-03-09T22:25:52.244Z",
    "title": "Sketch-of-Thought: Efficient LLM Reasoning with Adaptive Cognitive-Inspired Sketching",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05179.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63036b6c5c70c21d0ea79d48",
      "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
      "fullname": "Jinheon Baek",
      "name": "jinheon",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.02130",
      "authors": [
        {
          "_id": "67cc697fa029f09af72cca01",
          "user": {
            "_id": "6694cc1009326cb83f2d11bb",
            "avatarUrl": "/avatars/1ddaaed70a16ac475a9404848aef5d48.svg",
            "isPro": false,
            "fullname": "Zhixuan Lin",
            "user": "zhixuan-lin",
            "type": "user"
          },
          "name": "Zhixuan Lin",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-08T16:00:21.933Z",
          "hidden": false
        },
        {
          "_id": "67cc697fa029f09af72cca02",
          "name": "Evgenii Nikishin",
          "hidden": false
        },
        {
          "_id": "67cc697fa029f09af72cca03",
          "user": {
            "_id": "66906c4e37eadb9c577984d3",
            "avatarUrl": "/avatars/b81765472942fdf94c0ee885ca62df2d.svg",
            "isPro": false,
            "fullname": "Owen He",
            "user": "littleowen",
            "type": "user"
          },
          "name": "Xu Owen He",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-08T16:00:01.392Z",
          "hidden": false
        },
        {
          "_id": "67cc697fa029f09af72cca04",
          "name": "Aaron Courville",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T23:35:23.000Z",
      "title": "Forgetting Transformer: Softmax Attention with a Forget Gate",
      "summary": "An essential component of modern recurrent sequence models is the forget\ngate. While Transformers do not have an explicit recurrent form, we show that a\nforget gate can be naturally incorporated into Transformers by down-weighting\nthe unnormalized attention scores in a data-dependent way. We name this\nattention mechanism the Forgetting Attention and the resulting model the\nForgetting Transformer (FoX). We show that FoX outperforms the Transformer on\nlong-context language modeling, length extrapolation, and short-context\ndownstream tasks, while performing on par with the Transformer on long-context\ndownstream tasks. Moreover, it is compatible with the FlashAttention algorithm\nand does not require any positional embeddings. Several analyses, including the\nneedle-in-the-haystack test, show that FoX also retains the Transformer's\nsuperior long-context capabilities over recurrent sequence models such as\nMamba-2, HGRN2, and DeltaNet. We also introduce a \"Pro\" block design that\nincorporates some common architectural components in recurrent sequence models\nand find it significantly improves the performance of both FoX and the\nTransformer. Our code is available at\nhttps://github.com/zhixuan-lin/forgetting-transformer.",
      "upvotes": 9,
      "discussionId": "67cc6981a029f09af72ccac1",
      "githubRepo": "https://github.com/zhixuan-lin/forgetting-transformer"
    },
    "publishedAt": "2025-03-09T22:02:39.842Z",
    "title": "Forgetting Transformer: Softmax Attention with a Forget Gate",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.02130.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6694cc1009326cb83f2d11bb",
      "avatarUrl": "/avatars/1ddaaed70a16ac475a9404848aef5d48.svg",
      "fullname": "Zhixuan Lin",
      "name": "zhixuan-lin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.05592",
      "authors": [
        {
          "_id": "67ce5fd2e5cdfda52b9123a4",
          "name": "Huatong Song",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123a5",
          "name": "Jinhao Jiang",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123a6",
          "name": "Yingqian Min",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123a7",
          "name": "Jie Chen",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123a8",
          "name": "Zhipeng Chen",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123a9",
          "name": "Wayne Xin Zhao",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123aa",
          "name": "Lei Fang",
          "hidden": false
        },
        {
          "_id": "67ce5fd2e5cdfda52b9123ab",
          "name": "Ji-Rong Wen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T17:14:44.000Z",
      "title": "R1-Searcher: Incentivizing the Search Capability in LLMs via\n  Reinforcement Learning",
      "summary": "Existing Large Reasoning Models (LRMs) have shown the potential of\nreinforcement learning (RL) to enhance the complex reasoning capabilities of\nLarge Language Models~(LLMs). While they achieve remarkable performance on\nchallenging tasks such as mathematics and coding, they often rely on their\ninternal knowledge to solve problems, which can be inadequate for\ntime-sensitive or knowledge-intensive questions, leading to inaccuracies and\nhallucinations. To address this, we propose R1-Searcher, a novel\ntwo-stage outcome-based RL approach designed to enhance the search capabilities\nof LLMs. This method allows LLMs to autonomously invoke external search systems\nto access additional knowledge during the reasoning process. Our framework\nrelies exclusively on RL, without requiring process rewards or distillation for\na cold start. % effectively generalizing to out-of-domain datasets and\nsupporting both Base and Instruct models. Our experiments demonstrate that our\nmethod significantly outperforms previous strong RAG methods, even when\ncompared to the closed-source GPT-4o-mini.",
      "upvotes": 8,
      "discussionId": "67ce5fd3e5cdfda52b912436"
    },
    "publishedAt": "2025-03-09T23:43:27.151Z",
    "title": "R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05592.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6316
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.05652",
      "authors": [
        {
          "_id": "67ce524ee969bc5fd69c9388",
          "name": "Yunfan Jiang",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c9389",
          "name": "Ruohan Zhang",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c938a",
          "name": "Josiah Wong",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c938b",
          "name": "Chen Wang",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c938c",
          "name": "Yanjie Ze",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c938d",
          "name": "Hang Yin",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c938e",
          "name": "Cem Gokmen",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c938f",
          "name": "Shuran Song",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c9390",
          "name": "Jiajun Wu",
          "hidden": false
        },
        {
          "_id": "67ce524ee969bc5fd69c9391",
          "name": "Li Fei-Fei",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T18:15:21.000Z",
      "title": "BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation\n  for Everyday Household Activities",
      "summary": "Real-world household tasks present significant challenges for mobile\nmanipulation robots. An analysis of existing robotics benchmarks reveals that\nsuccessful task performance hinges on three key whole-body control\ncapabilities: bimanual coordination, stable and precise navigation, and\nextensive end-effector reachability. Achieving these capabilities requires\ncareful hardware design, but the resulting system complexity further\ncomplicates visuomotor policy learning. To address these challenges, we\nintroduce the BEHAVIOR Robot Suite (BRS), a comprehensive framework for\nwhole-body manipulation in diverse household tasks. Built on a bimanual,\nwheeled robot with a 4-DoF torso, BRS integrates a cost-effective whole-body\nteleoperation interface for data collection and a novel algorithm for learning\nwhole-body visuomotor policies. We evaluate BRS on five challenging household\ntasks that not only emphasize the three core capabilities but also introduce\nadditional complexities, such as long-range navigation, interaction with\narticulated and deformable objects, and manipulation in confined spaces. We\nbelieve that BRS's integrated robotic embodiment, data collection interface,\nand learning framework mark a significant step toward enabling real-world\nwhole-body manipulation for everyday household tasks. BRS is open-sourced at\nhttps://behavior-robot-suite.github.io/",
      "upvotes": 6,
      "discussionId": "67ce5294e969bc5fd69c9a2c",
      "projectPage": "https://behavior-robot-suite.github.io/",
      "githubRepo": "https://github.com/behavior-robot-suite/brs-algo"
    },
    "publishedAt": "2025-03-09T22:51:04.616Z",
    "title": "BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/61e9f5398c237a147a3f4ab5/WD3cV-QLiHRgh4IUOrikN.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05652.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "61e9f5398c237a147a3f4ab5",
      "avatarUrl": "/avatars/afd4ec17cb132b5ab56e50a678c4786d.svg",
      "fullname": "Yunfan Jiang",
      "name": "yunfanj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.05638",
      "authors": [
        {
          "_id": "67ce8388764226f050ad18b3",
          "name": "Mark YU",
          "hidden": false
        },
        {
          "_id": "67ce8388764226f050ad18b4",
          "name": "Wenbo Hu",
          "hidden": false
        },
        {
          "_id": "67ce8388764226f050ad18b5",
          "name": "Jinbo Xing",
          "hidden": false
        },
        {
          "_id": "67ce8388764226f050ad18b6",
          "name": "Ying Shan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T17:57:53.000Z",
      "title": "TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos\n  via Diffusion Models",
      "summary": "We present TrajectoryCrafter, a novel approach to redirect camera\ntrajectories for monocular videos. By disentangling deterministic view\ntransformations from stochastic content generation, our method achieves precise\ncontrol over user-specified camera trajectories. We propose a novel dual-stream\nconditional video diffusion model that concurrently integrates point cloud\nrenders and source videos as conditions, ensuring accurate view transformations\nand coherent 4D content generation. Instead of leveraging scarce multi-view\nvideos, we curate a hybrid training dataset combining web-scale monocular\nvideos with static multi-view datasets, by our innovative double-reprojection\nstrategy, significantly fostering robust generalization across diverse scenes.\nExtensive evaluations on multi-view and large-scale monocular videos\ndemonstrate the superior performance of our method.",
      "upvotes": 4,
      "discussionId": "67ce838a764226f050ad1952",
      "projectPage": "https://trajectorycrafter.github.io/",
      "githubRepo": "https://github.com/TrajectoryCrafter/TrajectoryCrafter"
    },
    "publishedAt": "2025-03-10T02:24:39.763Z",
    "title": "TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos via Diffusion Models",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/657a7458afbb0117ba15c59f/lpXbCmGz-upwRVSEUzBjV.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05638.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "657a7458afbb0117ba15c59f",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657a7458afbb0117ba15c59f/8_iwTS1UG_mKnfylFbLsY.jpeg",
      "fullname": "Wenbo Hu",
      "name": "wbhu-tc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.05379",
      "authors": [
        {
          "_id": "67ce5f2389663abdbc364495",
          "name": "Jiaxing Zhao",
          "hidden": false
        },
        {
          "_id": "67ce5f2389663abdbc364496",
          "name": "Xihan Wei",
          "hidden": false
        },
        {
          "_id": "67ce5f2389663abdbc364497",
          "name": "Liefeng Bo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T12:46:42.000Z",
      "title": "R1-Omni: Explainable Omni-Multimodal Emotion Recognition with\n  Reinforcing Learning",
      "summary": "In this work, we present the first application of Reinforcement Learning with\nVerifiable Reward (RLVR) to an Omni-multimodal large language model in the\ncontext of emotion recognition, a task where both visual and audio modalities\nplay crucial roles. We leverage RLVR to optimize the Omni model, significantly\nenhancing its performance in three key aspects: reasoning capability, emotion\nrecognition accuracy, and generalization ability. The introduction of RLVR not\nonly improves the model's overall performance on in-distribution data but also\ndemonstrates superior robustness when evaluated on out-of-distribution\ndatasets. More importantly, the improved reasoning capability enables clear\nanalysis of the contributions of different modalities, particularly visual and\naudio information, in the emotion recognition process. This provides valuable\ninsights into the optimization of multimodal large language models.",
      "upvotes": 4,
      "discussionId": "67ce5f2489663abdbc3644d0"
    },
    "publishedAt": "2025-03-09T23:40:46.906Z",
    "title": "R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcing Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05379.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6316
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.01713",
      "authors": [
        {
          "_id": "67c75e18cb29e2a4b0eb0293",
          "user": {
            "_id": "66c0a08bac74db25de8427ec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
            "isPro": false,
            "fullname": "Jintao Zhang",
            "user": "jt-zhang",
            "type": "user"
          },
          "name": "Jintao Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-07T09:26:50.367Z",
          "hidden": false
        },
        {
          "_id": "67c75e18cb29e2a4b0eb0294",
          "name": "Guoliang Li",
          "hidden": false
        },
        {
          "_id": "67c75e18cb29e2a4b0eb0295",
          "name": "Jinyang Su",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T16:25:58.000Z",
      "title": "SAGE: A Framework of Precise Retrieval for RAG",
      "summary": "Retrieval-augmented generation (RAG) has demonstrated significant proficiency\nin conducting question-answering (QA) tasks within a specified corpus.\nNonetheless, numerous failure instances of RAG in QA still exist. These\nfailures are not solely attributable to the limitations of Large Language\nModels (LLMs); instead, they predominantly arise from the retrieval of\ninaccurate information for LLMs due to two limitations: (1) Current RAG methods\nsegment the corpus without considering semantics, making it difficult to find\nrelevant context due to impaired correlation between questions and the\nsegments. (2) There is a trade-off between missing essential context with fewer\ncontext retrieved and getting irrelevant context with more context retrieved.\n  In this paper, we introduce a RAG framework (SAGE), to overcome these\nlimitations. First, to address the segmentation issue without considering\nsemantics, we propose to train a semantic segmentation model. This model is\ntrained to segment the corpus into semantically complete chunks. Second, to\nensure that only the most relevant chunks are retrieved while the irrelevant\nones are ignored, we design a chunk selection algorithm to dynamically select\nchunks based on the decreasing speed of the relevance score, leading to a more\nrelevant selection. Third, to further ensure the precision of the retrieved\nchunks, we propose letting LLMs assess whether retrieved chunks are excessive\nor lacking and then adjust the amount of context accordingly. Experiments show\nthat SAGE outperforms baselines by 61.25% in the quality of QA on average.\nMoreover, by avoiding retrieving noisy context, SAGE lowers the cost of the\ntokens consumed in LLM inference and achieves a 49.41% enhancement in cost\nefficiency on average. Additionally, our work offers valuable insights for\nboosting RAG.",
      "upvotes": 3,
      "discussionId": "67c75e1ccb29e2a4b0eb03a9"
    },
    "publishedAt": "2025-03-10T03:23:51.482Z",
    "title": "SAGE: A Framework of Precise Retrieval for RAG",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01713.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66c0a08bac74db25de8427ec",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
      "fullname": "Jintao Zhang",
      "name": "jt-zhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2503.04872",
      "authors": [
        {
          "_id": "67ce5deedb623d45a95deb72",
          "name": "Lin Sun",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb73",
          "name": "Guangxiang Zhao",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb74",
          "name": "Xiaoqi Jian",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb75",
          "name": "Yuhan Wu",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb76",
          "name": "Weihong Lin",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb77",
          "name": "Yongfu Zhu",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb78",
          "name": "Change Jia",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb79",
          "name": "Linglin Zhang",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb7a",
          "name": "Jinzhu Wu",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb7b",
          "name": "Junfeng Ran",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb7c",
          "name": "Sai-er Hu",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb7d",
          "name": "Zihan Jiang",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb7e",
          "name": "Junting Zhou",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb7f",
          "name": "Wenrui Liu",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb80",
          "name": "Bin Cui",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb81",
          "name": "Tong Yang",
          "hidden": false
        },
        {
          "_id": "67ce5deedb623d45a95deb82",
          "name": "Xiangzheng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-06T16:25:53.000Z",
      "title": "TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation",
      "summary": "The challenge of reducing the size of Large Language Models (LLMs) while\nmaintaining their performance has gained significant attention. However,\nexisting methods, such as model distillation and transfer learning, often fail\nto achieve high accuracy. To address this limitation, we introduce the\nBranch-Merge distillation approach, which enhances model compression through\ntwo phases: (1) the Branch Phase, where knowledge from a large teacher model is\nselectively distilled into specialized student models via\ndomain-specific supervised fine-tuning (SFT); And (2) the Merge Phase, where\nthese student models are merged to enable cross-domain knowledge transfer and\nimprove generalization. We validate our distillation approach using DeepSeek-R1\nas the teacher and DeepSeek-R1-Distill-Qwen-32B as the student. The resulting\nmerged model, TinyR1-32B-Preview, outperforms its counterpart\nDeepSeek-R1-Distill-Qwen-32B across multiple benchmarks, including Mathematics\n(+5.5 points), Coding (+4.4 points) and Science (+2.9 points), while achieving\nnear-equal performance to DeepSeek-R1 on AIME 2024. The Branch-Merge\ndistillation approach provides a scalable solution for creating smaller,\nhigh-performing LLMs with reduced computational cost and time.",
      "upvotes": 3,
      "discussionId": "67ce5df0db623d45a95dec1f"
    },
    "publishedAt": "2025-03-09T23:35:58.424Z",
    "title": "TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04872.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6316
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.05500",
      "authors": [
        {
          "_id": "67ce9626e5cdfda52b9e8839",
          "name": "Nicolas Boizard",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e883a",
          "name": "Hippolyte Gisserot-Boukhlef",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e883b",
          "name": "Duarte M. Alves",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e883c",
          "name": "André Martins",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e883d",
          "name": "Ayoub Hammal",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e883e",
          "name": "Caio Corro",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e883f",
          "name": "Céline Hudelot",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8840",
          "name": "Emmanuel Malherbe",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8841",
          "name": "Etienne Malaboeuf",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8842",
          "name": "Fanny Jourdan",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8843",
          "name": "Gabriel Hautreux",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8844",
          "name": "João Alves",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8845",
          "name": "Kevin El-Haddad",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8846",
          "name": "Manuel Faysse",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8847",
          "name": "Maxime Peyrard",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8848",
          "name": "Nuno M. Guerreiro",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e8849",
          "name": "Patrick Fernandes",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e884a",
          "name": "Ricardo Rei",
          "hidden": false
        },
        {
          "_id": "67ce9626e5cdfda52b9e884b",
          "name": "Pierre Colombo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T15:13:58.000Z",
      "title": "EuroBERT: Scaling Multilingual Encoders for European Languages",
      "summary": "General-purpose multilingual vector representations, used in retrieval,\nregression and classification, are traditionally obtained from bidirectional\nencoder models. Despite their wide applicability, encoders have been recently\novershadowed by advances in generative decoder-only models. However, many\ninnovations driving this progress are not inherently tied to decoders. In this\npaper, we revisit the development of multilingual encoders through the lens of\nthese advances, and introduce EuroBERT, a family of multilingual encoders\ncovering European and widely spoken global languages. Our models outperform\nexisting alternatives across a diverse range of tasks, spanning multilingual\ncapabilities, mathematics, and coding, and natively supporting sequences of up\nto 8,192 tokens. We also examine the design decisions behind EuroBERT, offering\ninsights into our dataset composition and training pipeline. We publicly\nrelease the EuroBERT models, including intermediate training checkpoints,\ntogether with our training framework.",
      "upvotes": 1,
      "discussionId": "67ce9627e5cdfda52b9e88a4"
    },
    "publishedAt": "2025-03-10T03:42:45.848Z",
    "title": "EuroBERT: Scaling Multilingual Encoders for European Languages",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/62be186a5f59ff2320e6e32b/NxwS9WJrRc9D3q9awbn_X.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05500.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62be186a5f59ff2320e6e32b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62be186a5f59ff2320e6e32b/W_emoC2uItM-MJZyCfIKI.png",
      "fullname": "Nicolas-BZRD",
      "name": "Nicolas-BZRD",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 13
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.05315",
      "authors": [
        {
          "_id": "67ce6db07110b8bedb3344a7",
          "name": "Saumya Chaturvedi",
          "hidden": false
        },
        {
          "_id": "67ce6db07110b8bedb3344a8",
          "name": "Aman Chadha",
          "hidden": false
        },
        {
          "_id": "67ce6db07110b8bedb3344a9",
          "name": "Laurent Bindschaedler",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T10:50:45.000Z",
      "title": "LoRACode: LoRA Adapters for Code Embeddings",
      "summary": "Code embeddings are essential for semantic code search; however, current\napproaches often struggle to capture the precise syntactic and contextual\nnuances inherent in code. Open-source models such as CodeBERT and UniXcoder\nexhibit limitations in scalability and efficiency, while high-performing\nproprietary systems impose substantial computational costs. We introduce a\nparameter-efficient fine-tuning method based on Low-Rank Adaptation (LoRA) to\nconstruct task-specific adapters for code retrieval. Our approach reduces the\nnumber of trainable parameters to less than two percent of the base model,\nenabling rapid fine-tuning on extensive code corpora (2 million samples in 25\nminutes on two H100 GPUs). Experiments demonstrate an increase of up to 9.1% in\nMean Reciprocal Rank (MRR) for Code2Code search, and up to 86.69% for Text2Code\nsearch tasks across multiple programming languages. Distinction in task-wise\nand language-wise adaptation helps explore the sensitivity of code retrieval\nfor syntactical and linguistic variations.",
      "upvotes": 1,
      "discussionId": "67ce6db17110b8bedb3344c5"
    },
    "publishedAt": "2025-03-10T00:51:02.203Z",
    "title": "LoRACode: LoRA Adapters for Code Embeddings",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05315.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4754927f1f64ed7238dac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
      "fullname": "Aman Chadha",
      "name": "amanchadha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.05132",
      "authors": [
        {
          "_id": "67ce5ec17c6e6ea1cc5649c2",
          "name": "Hengguang Zhou",
          "hidden": false
        },
        {
          "_id": "67ce5ec17c6e6ea1cc5649c3",
          "name": "Xirui Li",
          "hidden": false
        },
        {
          "_id": "67ce5ec17c6e6ea1cc5649c4",
          "name": "Ruochen Wang",
          "hidden": false
        },
        {
          "_id": "67ce5ec17c6e6ea1cc5649c5",
          "name": "Minhao Cheng",
          "hidden": false
        },
        {
          "_id": "67ce5ec17c6e6ea1cc5649c6",
          "name": "Tianyi Zhou",
          "hidden": false
        },
        {
          "_id": "67ce5ec17c6e6ea1cc5649c7",
          "name": "Cho-Jui Hsieh",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T04:21:47.000Z",
      "title": "R1-Zero's \"Aha Moment\" in Visual Reasoning on a 2B Non-SFT Model",
      "summary": "Recently DeepSeek R1 demonstrated how reinforcement learning with simple\nrule-based incentives can enable autonomous development of complex reasoning in\nlarge language models, characterized by the \"aha moment\", in which the model\nmanifest self-reflection and increased response length during training.\nHowever, attempts to extend this success to multimodal reasoning often failed\nto reproduce these key characteristics. In this report, we present the first\nsuccessful replication of these emergent characteristics for multimodal\nreasoning on only a non-SFT 2B model. Starting with Qwen2-VL-2B and applying\nreinforcement learning directly on the SAT dataset, our model achieves 59.47%\naccuracy on CVBench, outperforming the base model by approximately ~30% and\nexceeding both SFT setting by ~2%. In addition, we share our failed attempts\nand insights in attempting to achieve R1-like reasoning using RL with instruct\nmodels. aiming to shed light on the challenges involved. Our key observations\ninclude: (1) applying RL on instruct model often results in trivial reasoning\ntrajectories, and (2) naive length reward are ineffective in eliciting\nreasoning capabilities. The project code is available at\nhttps://github.com/turningpoint-ai/VisualThinker-R1-Zero",
      "upvotes": 1,
      "discussionId": "67ce5ec27c6e6ea1cc564a01"
    },
    "publishedAt": "2025-03-09T23:39:12.374Z",
    "title": "R1-Zero's \"Aha Moment\" in Visual Reasoning on a 2B Non-SFT Model",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05132.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6316
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.04808",
      "authors": [
        {
          "_id": "67ce5c7065b141ae6b0d3957",
          "name": "Stephen Chung",
          "hidden": false
        },
        {
          "_id": "67ce5c7065b141ae6b0d3958",
          "name": "Wenyu Du",
          "hidden": false
        },
        {
          "_id": "67ce5c7065b141ae6b0d3959",
          "name": "Jie Fu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-04T02:53:39.000Z",
      "title": "Learning from Failures in Multi-Attempt Reinforcement Learning",
      "summary": "Recent advancements in reinforcement learning (RL) for large language models\n(LLMs), exemplified by DeepSeek R1, have shown that even a simple\nquestion-answering task can substantially improve an LLM's reasoning\ncapabilities. In this work, we extend this approach by modifying the task into\na multi-attempt setting. Instead of generating a single response per question,\nthe model is given multiple attempts, with feedback provided after incorrect\nresponses. The multi-attempt task encourages the model to refine its previous\nattempts and improve search efficiency. Experimental results show that even a\nsmall LLM trained on a multi-attempt task achieves significantly higher\naccuracy when evaluated with more attempts, improving from 45.6% with 1 attempt\nto 52.5% with 2 attempts on the math benchmark. In contrast, the same LLM\ntrained on a standard single-turn task exhibits only a marginal improvement,\nincreasing from 42.3% to 43.2% when given more attempts during evaluation. The\nresults indicate that, compared to the standard single-turn task, an LLM\ntrained on a multi-attempt task achieves slightly better performance on math\nbenchmarks while also learning to refine its responses more effectively based\non user feedback. Full code is available at\nhttps://github.com/DualityRL/multi-attempt",
      "upvotes": 1,
      "discussionId": "67ce5c7165b141ae6b0d39c6"
    },
    "publishedAt": "2025-03-09T23:29:35.505Z",
    "title": "Learning from Failures in Multi-Attempt Reinforcement Learning",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.04808.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "60f1abe7544c2adfd699860c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
      "fullname": "AK",
      "name": "akhaliq",
      "type": "user",
      "isPro": false,
      "isHf": true,
      "isMod": false,
      "followerCount": 6316
    },
    "isAuthorParticipating": false
  }
]