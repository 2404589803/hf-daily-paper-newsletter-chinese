[
  {
    "paper": {
      "id": "2503.08638",
      "authors": [
        {
          "_id": "67d1027435066eade61549ae",
          "name": "Ruibin Yuan",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549af",
          "name": "Hanfeng Lin",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b0",
          "name": "Shuyue Guo",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b1",
          "user": {
            "_id": "638efcf4c67af472d316d424",
            "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
            "isPro": false,
            "fullname": "Ge Zhang",
            "user": "zhangysk",
            "type": "user"
          },
          "name": "Ge Zhang",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-03-12T06:24:13.961Z",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b2",
          "name": "Jiahao Pan",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b3",
          "name": "Yongyi Zang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b4",
          "name": "Haohe Liu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b5",
          "name": "Yiming Liang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b6",
          "name": "Wenye Ma",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b7",
          "name": "Xingjian Du",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b8",
          "name": "Xinrun Du",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549b9",
          "name": "Zhen Ye",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549ba",
          "name": "Tianyu Zheng",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549bb",
          "name": "Yinghao Ma",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549bc",
          "name": "Minghao Liu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549bd",
          "name": "Zeyue Tian",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549be",
          "name": "Ziya Zhou",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549bf",
          "name": "Liumeng Xue",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c0",
          "user": {
            "_id": "6628adb14277eae0da5eee28",
            "avatarUrl": "/avatars/6cb41b80cc5e014e455dfc2a22682e64.svg",
            "isPro": true,
            "fullname": "HKUST Audio",
            "user": "HKUST-Audio",
            "type": "user"
          },
          "name": "Xingwei Qu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-03-12T03:41:43.139Z",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c1",
          "name": "Yizhi Li",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c2",
          "name": "Shangda Wu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c3",
          "name": "Tianhao Shen",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c4",
          "name": "Ziyang Ma",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c5",
          "name": "Jun Zhan",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c6",
          "name": "Chunhui Wang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c7",
          "name": "Yatian Wang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c8",
          "name": "Xiaowei Chi",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549c9",
          "name": "Xinyue Zhang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549ca",
          "name": "Zhenzhu Yang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549cb",
          "name": "Xiangzhou Wang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549cc",
          "name": "Shansong Liu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549cd",
          "name": "Lingrui Mei",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549ce",
          "name": "Peng Li",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549cf",
          "name": "Junjie Wang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d0",
          "name": "Jianwei Yu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d1",
          "name": "Guojian Pang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d2",
          "name": "Xu Li",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d3",
          "name": "Zihao Wang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d4",
          "name": "Xiaohuan Zhou",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d5",
          "name": "Lijun Yu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d6",
          "name": "Emmanouil Benetos",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d7",
          "name": "Yong Chen",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d8",
          "name": "Chenghua Lin",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549d9",
          "name": "Xie Chen",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549da",
          "name": "Gus Xia",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549db",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549dc",
          "name": "Chao Zhang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549dd",
          "name": "Wenhu Chen",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549de",
          "name": "Xinyu Zhou",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549df",
          "name": "Xipeng Qiu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e0",
          "name": "Roger Dannenberg",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e1",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e2",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e3",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e4",
          "name": "Wei Xue",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e5",
          "name": "Xu Tan",
          "hidden": false
        },
        {
          "_id": "67d1027435066eade61549e6",
          "name": "Yike Guo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T17:26:50.000Z",
      "title": "YuE: Scaling Open Foundation Models for Long-Form Music Generation",
      "summary": "We tackle the task of long-form music generation--particularly the\nchallenging lyrics-to-song problem--by introducing YuE, a family of\nopen foundation models based on the LLaMA2 architecture. Specifically, YuE\nscales to trillions of tokens and generates up to five minutes of music while\nmaintaining lyrical alignment, coherent musical structure, and engaging vocal\nmelodies with appropriate accompaniment. It achieves this through (1)\ntrack-decoupled next-token prediction to overcome dense mixture signals, (2)\nstructural progressive conditioning for long-context lyrical alignment, and (3)\na multitask, multiphase pre-training recipe to converge and generalize. In\naddition, we redesign the in-context learning technique for music generation,\nenabling versatile style transfer (e.g., converting Japanese city pop into an\nEnglish rap while preserving the original accompaniment) and bidirectional\ngeneration. Through extensive evaluation, we demonstrate that YuE matches or\neven surpasses some of the proprietary systems in musicality and vocal agility.\nIn addition, fine-tuning YuE enables additional controls and enhanced support\nfor tail languages. Furthermore, beyond generation, we show that YuE's learned\nrepresentations can perform well on music understanding tasks, where the\nresults of YuE match or exceed state-of-the-art methods on the MARBLE\nbenchmark. Keywords: lyrics2song, song generation, long-form, foundation model,\nmusic generation",
      "upvotes": 34,
      "discussionId": "67d1027735066eade6154a7e",
      "ai_keywords": [
        "track-decoupled next-token prediction",
        "dense mixture signals",
        "structural progressive conditioning",
        "long-context lyrical alignment",
        "multitask, multiphase pre-training",
        "in-context learning",
        "versatile style transfer",
        "bidirectional generation",
        "musicality",
        "vocal agility",
        "tail languages",
        "music understanding tasks",
        "MARBLE benchmark"
      ]
    },
    "publishedAt": "2025-03-11T13:26:50.000Z",
    "title": "YuE: Scaling Open Foundation Models for Long-Form Music Generation",
    "summary": "We tackle the task of long-form music generation--particularly the\nchallenging lyrics-to-song problem--by introducing YuE, a family of\nopen foundation models based on the LLaMA2 architecture. Specifically, YuE\nscales to trillions of tokens and generates up to five minutes of music while\nmaintaining lyrical alignment, coherent musical structure, and engaging vocal\nmelodies with appropriate accompaniment. It achieves this through (1)\ntrack-decoupled next-token prediction to overcome dense mixture signals, (2)\nstructural progressive conditioning for long-context lyrical alignment, and (3)\na multitask, multiphase pre-training recipe to converge and generalize. In\naddition, we redesign the in-context learning technique for music generation,\nenabling versatile style transfer (e.g., converting Japanese city pop into an\nEnglish rap while preserving the original accompaniment) and bidirectional\ngeneration. Through extensive evaluation, we demonstrate that YuE matches or\neven surpasses some of the proprietary systems in musicality and vocal agility.\nIn addition, fine-tuning YuE enables additional controls and enhanced support\nfor tail languages. Furthermore, beyond generation, we show that YuE's learned\nrepresentations can perform well on music understanding tasks, where the\nresults of YuE match or exceed state-of-the-art methods on the MARBLE\nbenchmark. Keywords: lyrics2song, song generation, long-form, foundation model,\nmusic generation",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08638.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07920",
      "authors": [
        {
          "_id": "67d0f9c95f0fcc0c38902b8e",
          "name": "Samuel Cahyawijaya",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b8f",
          "name": "Holy Lovenia",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b90",
          "name": "Joel Ruben Antony Moniz",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b91",
          "name": "Tack Hwa Wong",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b92",
          "name": "Mohammad Rifqi Farhansyah",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b93",
          "name": "Thant Thiri Maung",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b94",
          "name": "Frederikus Hudi",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b95",
          "name": "David Anugraha",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b96",
          "name": "Muhammad Ravi Shulthan Habibi",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b97",
          "name": "Muhammad Reza Qorib",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b98",
          "name": "Amit Agarwal",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b99",
          "name": "Joseph Marvin Imperial",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b9a",
          "name": "Hitesh Laxmichand Patel",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b9b",
          "name": "Vicky Feliren",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b9c",
          "name": "Bahrul Ilmi Nasution",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b9d",
          "name": "Manuel Antonio Rufino",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b9e",
          "name": "Genta Indra Winata",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902b9f",
          "name": "Rian Adam Rajagede",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba0",
          "name": "Carlos Rafael Catalan",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba1",
          "name": "Mohamed Fazli Imam",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba2",
          "name": "Priyaranjan Pattnayak",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba3",
          "name": "Salsabila Zahirah Pranida",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba4",
          "name": "Kevin Pratama",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba5",
          "name": "Yeshil Bangera",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba6",
          "name": "Adisai Na-Thalang",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba7",
          "name": "Patricia Nicole Monderin",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba8",
          "name": "Yueqi Song",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902ba9",
          "name": "Christian Simon",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902baa",
          "name": "Lynnette Hui Xian Ng",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bab",
          "name": "Richardy Lobo' Sapan",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bac",
          "name": "Taki Hasan Rafi",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bad",
          "name": "Bin Wang",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bae",
          "name": "Supryadi",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902baf",
          "name": "Kanyakorn Veerakanjana",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb0",
          "name": "Piyalitt Ittichaiwong",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb1",
          "name": "Matthew Theodore Roque",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb2",
          "name": "Karissa Vincentio",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb3",
          "name": "Takdanai Kreangphet",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb4",
          "name": "Phakphum Artkaew",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb5",
          "name": "Kadek Hendrawan Palgunadi",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb6",
          "name": "Yanzhi Yu",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb7",
          "name": "Rochana Prih Hastuti",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb8",
          "name": "William Nixon",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bb9",
          "name": "Mithil Bangera",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bba",
          "name": "Adrian Xuan Wei Lim",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bbb",
          "name": "Aye Hninn Khine",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bbc",
          "name": "Hanif Muhammad Zhafran",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bbd",
          "name": "Teddy Ferdinan",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bbe",
          "name": "Audra Aurora Izzani",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bbf",
          "name": "Ayushman Singh",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc0",
          "name": "Evan",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc1",
          "name": "Jauza Akbar Krito",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc2",
          "name": "Michael Anugraha",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc3",
          "name": "Fenal Ashokbhai Ilasariya",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc4",
          "name": "Haochen Li",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc5",
          "name": "John Amadeo Daniswara",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc6",
          "name": "Filbert Aurelian Tjiaranata",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc7",
          "name": "Eryawan Presma Yulianrifat",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc8",
          "name": "Can Udomcharoenchaikit",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bc9",
          "name": "Fadil Risdian Ansori",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bca",
          "name": "Mahardika Krisna Ihsani",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bcb",
          "name": "Giang Nguyen",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bcc",
          "name": "Anab Maulana Barik",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bcd",
          "name": "Dan John Velasco",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bce",
          "name": "Rifo Ahmad Genadi",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bcf",
          "name": "Saptarshi Saha",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd0",
          "name": "Chengwei Wei",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd1",
          "name": "Isaiah Flores",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd2",
          "name": "Kenneth Ko Han Chen",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd3",
          "name": "Anjela Gail Santos",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd4",
          "name": "Wan Shen Lim",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd5",
          "name": "Kaung Si Phyo",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd6",
          "name": "Tim Santos",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd7",
          "name": "Meisyarah Dwiastuti",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd8",
          "name": "Jiayun Luo",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bd9",
          "name": "Jan Christian Blaise Cruz",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bda",
          "name": "Ming Shan Hee",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bdb",
          "name": "Ikhlasul Akmal Hanif",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bdc",
          "name": "M. Alif Al Hakim",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bdd",
          "name": "Muhammad Rizky Sya'ban",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bde",
          "name": "Kun Kerdthaisong",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902bdf",
          "name": "Lester James V. Miranda",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be0",
          "name": "Fajri Koto",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be1",
          "name": "Tirana Noor Fatyanosa",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be2",
          "name": "Alham Fikri Aji",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be3",
          "name": "Jostin Jerico Rosal",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be4",
          "name": "Jun Kevin",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be5",
          "name": "Robert Wijaya",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be6",
          "name": "Onno P. Kampman",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be7",
          "name": "Ruochen Zhang",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be8",
          "name": "BÃ¶rje F. Karlsson",
          "hidden": false
        },
        {
          "_id": "67d0f9c95f0fcc0c38902be9",
          "name": "Peerat Limkonchotiwat",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T23:54:52.000Z",
      "title": "Crowdsource, Crawl, or Generate? Creating SEA-VL, a Multicultural\n  Vision-Language Dataset for Southeast Asia",
      "summary": "Southeast Asia (SEA) is a region of extraordinary linguistic and cultural\ndiversity, yet it remains significantly underrepresented in vision-language\n(VL) research. This often results in artificial intelligence (AI) models that\nfail to capture SEA cultural nuances. To fill this gap, we present SEA-VL, an\nopen-source initiative dedicated to developing high-quality, culturally\nrelevant data for SEA languages. By involving contributors from SEA countries,\nSEA-VL aims to ensure better cultural relevance and diversity, fostering\ngreater inclusivity of underrepresented languages in VL research. Beyond\ncrowdsourcing, our initiative goes one step further in the exploration of the\nautomatic collection of culturally relevant images through crawling and image\ngeneration. First, we find that image crawling achieves approximately ~85%\ncultural relevance while being more cost- and time-efficient than\ncrowdsourcing. Second, despite the substantial progress in generative vision\nmodels, synthetic images remain unreliable in accurately reflecting SEA\ncultures. The generated images often fail to reflect the nuanced traditions and\ncultural contexts of the region. Collectively, we gather 1.28M SEA\nculturally-relevant images, more than 50 times larger than other existing\ndatasets. Through SEA-VL, we aim to bridge the representation gap in SEA,\nfostering the development of more inclusive AI systems that authentically\nrepresent diverse cultures across SEA.",
      "upvotes": 29,
      "discussionId": "67d0f9cd5f0fcc0c38902cdf",
      "ai_keywords": [
        "vision-language (VL) research",
        "cultural relevance",
        "crowdsourcing",
        "image crawling",
        "image generation",
        "generative vision models",
        "synthesized images",
        "datasets"
      ]
    },
    "publishedAt": "2025-03-10T19:54:52.000Z",
    "title": "Crowdsource, Crawl, or Generate? Creating SEA-VL, a Multicultural\n  Vision-Language Dataset for Southeast Asia",
    "summary": "Southeast Asia (SEA) is a region of extraordinary linguistic and cultural\ndiversity, yet it remains significantly underrepresented in vision-language\n(VL) research. This often results in artificial intelligence (AI) models that\nfail to capture SEA cultural nuances. To fill this gap, we present SEA-VL, an\nopen-source initiative dedicated to developing high-quality, culturally\nrelevant data for SEA languages. By involving contributors from SEA countries,\nSEA-VL aims to ensure better cultural relevance and diversity, fostering\ngreater inclusivity of underrepresented languages in VL research. Beyond\ncrowdsourcing, our initiative goes one step further in the exploration of the\nautomatic collection of culturally relevant images through crawling and image\ngeneration. First, we find that image crawling achieves approximately ~85%\ncultural relevance while being more cost- and time-efficient than\ncrowdsourcing. Second, despite the substantial progress in generative vision\nmodels, synthetic images remain unreliable in accurately reflecting SEA\ncultures. The generated images often fail to reflect the nuanced traditions and\ncultural contexts of the region. Collectively, we gather 1.28M SEA\nculturally-relevant images, more than 50 times larger than other existing\ndatasets. Through SEA-VL, we aim to bridge the representation gap in SEA,\nfostering the development of more inclusive AI systems that authentically\nrepresent diverse cultures across SEA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07920.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08120",
      "authors": [
        {
          "_id": "67d0f5ace3c8042929eea946",
          "name": "Junzhe Li",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea947",
          "name": "Xuerui Qiu",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea948",
          "name": "Linrui Xu",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea949",
          "name": "Liya Guo",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea94a",
          "name": "Delin Qu",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea94b",
          "name": "Tingting Long",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea94c",
          "name": "Chun Fan",
          "hidden": false
        },
        {
          "_id": "67d0f5ace3c8042929eea94d",
          "name": "Ming Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T07:34:59.000Z",
      "title": "UniF^2ace: Fine-grained Face Understanding and Generation\n  with Unified Multimodal Models",
      "summary": "Unified multimodal models (UMMs) have emerged as a powerful paradigm in\nfoundational computer vision research, demonstrating significant potential in\nboth image understanding and generation. However, existing research in the face\ndomain primarily focuses on coarse facial attribute understanding,\nwith limited capacity to handle fine-grained facial attributes and\nwithout addressing generation capabilities. To overcome these limitations, we\npropose UniF^2ace, the first UMM tailored specifically for\nfine-grained face understanding and generation. In general, we train\nUniF^2ace on a self-constructed, specialized dataset utilizing two\nmutually beneficial diffusion techniques and a two-level mixture-of-experts\narchitecture. Specifically, we first build a large-scale facial dataset,\nUniF^2ace-130K, which contains 130K image-text pairs with one\nmillion question-answering pairs that span a wide range of facial attributes.\nSecond, we establish a theoretical connection between discrete diffusion score\nmatching and masked generative models, optimizing both evidence lower bounds\nsimultaneously, which significantly improves the model's ability to synthesize\nfacial details. Finally, we introduce both token-level and sequence-level\nmixture-of-experts, enabling efficient fine-grained representation learning for\nboth understanding and generation tasks. Extensive experiments on\nUniF^2ace-130K demonstrate that UniF^2ace outperforms\nexisting UMMs and generative models, achieving superior performance across both\nunderstanding and generation tasks.",
      "upvotes": 22,
      "discussionId": "67d0f5b4e3c8042929eeab49",
      "ai_keywords": [
        "diffusion score matching",
        "masked generative models",
        "evidence lower bounds",
        "mixture-of-experts",
        "token-level",
        "sequence-level"
      ]
    },
    "publishedAt": "2025-03-11T03:34:59.000Z",
    "title": "UniF^2ace: Fine-grained Face Understanding and Generation\n  with Unified Multimodal Models",
    "summary": "Unified multimodal models (UMMs) have emerged as a powerful paradigm in\nfoundational computer vision research, demonstrating significant potential in\nboth image understanding and generation. However, existing research in the face\ndomain primarily focuses on coarse facial attribute understanding,\nwith limited capacity to handle fine-grained facial attributes and\nwithout addressing generation capabilities. To overcome these limitations, we\npropose UniF^2ace, the first UMM tailored specifically for\nfine-grained face understanding and generation. In general, we train\nUniF^2ace on a self-constructed, specialized dataset utilizing two\nmutually beneficial diffusion techniques and a two-level mixture-of-experts\narchitecture. Specifically, we first build a large-scale facial dataset,\nUniF^2ace-130K, which contains 130K image-text pairs with one\nmillion question-answering pairs that span a wide range of facial attributes.\nSecond, we establish a theoretical connection between discrete diffusion score\nmatching and masked generative models, optimizing both evidence lower bounds\nsimultaneously, which significantly improves the model's ability to synthesize\nfacial details. Finally, we introduce both token-level and sequence-level\nmixture-of-experts, enabling efficient fine-grained representation learning for\nboth understanding and generation tasks. Extensive experiments on\nUniF^2ace-130K demonstrate that UniF^2ace outperforms\nexisting UMMs and generative models, achieving superior performance across both\nunderstanding and generation tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08120.png",
    "numComments": 2,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07703",
      "authors": [
        {
          "_id": "67d0f422a3158b8e55d3562f",
          "name": "Lixue Gong",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35630",
          "name": "Xiaoxia Hou",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35631",
          "name": "Fanshi Li",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35632",
          "name": "Liang Li",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35633",
          "name": "Xiaochen Lian",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35634",
          "name": "Fei Liu",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35635",
          "name": "Liyang Liu",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35636",
          "name": "Wei Liu",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35637",
          "name": "Wei Lu",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35638",
          "name": "Yichun Shi",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35639",
          "name": "Shiqi Sun",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3563a",
          "name": "Yu Tian",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3563b",
          "name": "Zhi Tian",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3563c",
          "name": "Peng Wang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3563d",
          "name": "Xun Wang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3563e",
          "name": "Ye Wang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3563f",
          "name": "Guofeng Wu",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35640",
          "name": "Jie Wu",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35641",
          "name": "Xin Xia",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35642",
          "name": "Xuefeng Xiao",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35643",
          "name": "Linjie Yang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35644",
          "name": "Zhonghua Zhai",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35645",
          "name": "Xinyu Zhang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35646",
          "name": "Qi Zhang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35647",
          "name": "Yuwei Zhang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35648",
          "name": "Shijia Zhao",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d35649",
          "name": "Jianchao Yang",
          "hidden": false
        },
        {
          "_id": "67d0f422a3158b8e55d3564a",
          "name": "Weilin Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:58:33.000Z",
      "title": "Seedream 2.0: A Native Chinese-English Bilingual Image Generation\n  Foundation Model",
      "summary": "Rapid advancement of diffusion models has catalyzed remarkable progress in\nthe field of image generation. However, prevalent models such as Flux, SD3.5\nand Midjourney, still grapple with issues like model bias, limited text\nrendering capabilities, and insufficient understanding of Chinese cultural\nnuances. To address these limitations, we present Seedream 2.0, a native\nChinese-English bilingual image generation foundation model that excels across\ndiverse dimensions, which adeptly manages text prompt in both Chinese and\nEnglish, supporting bilingual image generation and text rendering. We develop a\npowerful data system that facilitates knowledge integration, and a caption\nsystem that balances the accuracy and richness for image description.\nParticularly, Seedream is integrated with a self-developed bilingual large\nlanguage model as a text encoder, allowing it to learn native knowledge\ndirectly from massive data. This enable it to generate high-fidelity images\nwith accurate cultural nuances and aesthetic expressions described in either\nChinese or English. Beside, Glyph-Aligned ByT5 is applied for flexible\ncharacter-level text rendering, while a Scaled ROPE generalizes well to\nuntrained resolutions. Multi-phase post-training optimizations, including SFT\nand RLHF iterations, further improve the overall capability. Through extensive\nexperimentation, we demonstrate that Seedream 2.0 achieves state-of-the-art\nperformance across multiple aspects, including prompt-following, aesthetics,\ntext rendering, and structural correctness. Furthermore, Seedream 2.0 has been\noptimized through multiple RLHF iterations to closely align its output with\nhuman preferences, as revealed by its outstanding ELO score. In addition, it\ncan be readily adapted to an instruction-based image editing model, such as\nSeedEdit, with strong editing capability that balances instruction-following\nand image consistency.",
      "upvotes": 20,
      "discussionId": "67d0f42fa3158b8e55d358ea",
      "projectPage": "https://team.doubao.com/zh/tech/seedream",
      "ai_keywords": [
        "diffusion models",
        "Flux",
        "SD3.5",
        "Midjourney",
        "model bias",
        "Seedream 2.0",
        "bilingual image generation",
        "text prompt",
        "data system",
        "caption system",
        "bilingual large language model",
        "high-fidelity images",
        "cultural nuances",
        "aesthetic expressions",
        "Glyph-Aligned ByT5",
        "character-level text rendering",
        "Scaled ROPE",
        "multi-phase post-training optimizations",
        "SFT",
        "RLHF",
        "prompt-following",
        "structural correctness",
        "ELO score",
        "instruction-based image editing model",
        "SeedEdit"
      ]
    },
    "publishedAt": "2025-03-10T13:58:33.000Z",
    "title": "Seedream 2.0: A Native Chinese-English Bilingual Image Generation\n  Foundation Model",
    "summary": "Rapid advancement of diffusion models has catalyzed remarkable progress in\nthe field of image generation. However, prevalent models such as Flux, SD3.5\nand Midjourney, still grapple with issues like model bias, limited text\nrendering capabilities, and insufficient understanding of Chinese cultural\nnuances. To address these limitations, we present Seedream 2.0, a native\nChinese-English bilingual image generation foundation model that excels across\ndiverse dimensions, which adeptly manages text prompt in both Chinese and\nEnglish, supporting bilingual image generation and text rendering. We develop a\npowerful data system that facilitates knowledge integration, and a caption\nsystem that balances the accuracy and richness for image description.\nParticularly, Seedream is integrated with a self-developed bilingual large\nlanguage model as a text encoder, allowing it to learn native knowledge\ndirectly from massive data. This enable it to generate high-fidelity images\nwith accurate cultural nuances and aesthetic expressions described in either\nChinese or English. Beside, Glyph-Aligned ByT5 is applied for flexible\ncharacter-level text rendering, while a Scaled ROPE generalizes well to\nuntrained resolutions. Multi-phase post-training optimizations, including SFT\nand RLHF iterations, further improve the overall capability. Through extensive\nexperimentation, we demonstrate that Seedream 2.0 achieves state-of-the-art\nperformance across multiple aspects, including prompt-following, aesthetics,\ntext rendering, and structural correctness. Furthermore, Seedream 2.0 has been\noptimized through multiple RLHF iterations to closely align its output with\nhuman preferences, as revealed by its outstanding ELO score. In addition, it\ncan be readily adapted to an instruction-based image editing model, such as\nSeedEdit, with strong editing capability that balances instruction-following\nand image consistency.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07703.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07536",
      "authors": [
        {
          "_id": "67d04f248f79213c2fc0ba04",
          "name": "Yingzhe Peng",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba05",
          "name": "Gongrui Zhang",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba06",
          "name": "Miaosen Zhang",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba07",
          "name": "Zhiyuan You",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba08",
          "name": "Jie Liu",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba09",
          "name": "Qipeng Zhu",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba0a",
          "name": "Kai Yang",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba0b",
          "name": "Xingzhong Xu",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba0c",
          "name": "Xin Geng",
          "hidden": false
        },
        {
          "_id": "67d04f248f79213c2fc0ba0d",
          "name": "Xu Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:04:14.000Z",
      "title": "LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through\n  Two-Stage Rule-Based RL",
      "summary": "Enhancing reasoning in Large Multimodal Models (LMMs) faces unique challenges\nfrom the complex interplay between visual perception and logical reasoning,\nparticularly in compact 3B-parameter architectures where architectural\nconstraints limit reasoning capacity and modality alignment.\n  While rule-based reinforcement learning (RL) excels in text-only domains, its\nmultimodal extension confronts two critical barriers: (1) data limitations due\nto ambiguous answers and scarce complex reasoning examples, and (2) degraded\nfoundational reasoning induced by multimodal pretraining.\n  To address these challenges, we propose \\method, a two-stage\nframework adapting rule-based RL for multimodal reasoning through\nFoundational Reasoning Enhancement (FRE) followed by\nMultimodal Generalization Training (MGT). The FRE stage first\nstrengthens reasoning abilities using text-only data with rule-based RL, then\nthe MGT stage generalizes these reasoning capabilities to multimodal domains.\n  Experiments on Qwen2.5-VL-Instruct-3B demonstrate that \\method achieves\n4.83\\% and 4.5\\% average improvements over baselines in multimodal and\ntext-only benchmarks, respectively, with a 3.63\\% gain in complex Football Game\ntasks. These results validate that text-based reasoning enhancement enables\neffective multimodal generalization, offering a data-efficient paradigm that\nbypasses costly high-quality multimodal training data.",
      "upvotes": 20,
      "discussionId": "67d04f268f79213c2fc0ba8b",
      "projectPage": "https://forjadeforest.github.io/LMM-R1-ProjectPage",
      "githubRepo": "https://github.com/TideDra/lmm-r1",
      "ai_keywords": [
        "Large Multimodal Models (LMMs)",
        "visual perception",
        "logical reasoning",
        "3B-parameter architectures",
        "rule-based reinforcement learning (RL)",
        "multimodal extension",
        "ambiguous answers",
        "complex reasoning examples",
        "degraded foundational reasoning",
        "multimodal pretraining",
        "Foundational Reasoning Enhancement (FRE)",
        "Multimodal Generalization Training (MGT)",
        "Qwen2.5-VL-Instruct-3B",
        "multimodal benchmarks",
        "text-only benchmarks",
        "complex Football Game tasks",
        "text-based reasoning enhancement",
        "data-efficient paradigm"
      ]
    },
    "publishedAt": "2025-03-10T13:04:14.000Z",
    "title": "LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through\n  Two-Stage Rule-Based RL",
    "summary": "Enhancing reasoning in Large Multimodal Models (LMMs) faces unique challenges\nfrom the complex interplay between visual perception and logical reasoning,\nparticularly in compact 3B-parameter architectures where architectural\nconstraints limit reasoning capacity and modality alignment.\n  While rule-based reinforcement learning (RL) excels in text-only domains, its\nmultimodal extension confronts two critical barriers: (1) data limitations due\nto ambiguous answers and scarce complex reasoning examples, and (2) degraded\nfoundational reasoning induced by multimodal pretraining.\n  To address these challenges, we propose \\method, a two-stage\nframework adapting rule-based RL for multimodal reasoning through\nFoundational Reasoning Enhancement (FRE) followed by\nMultimodal Generalization Training (MGT). The FRE stage first\nstrengthens reasoning abilities using text-only data with rule-based RL, then\nthe MGT stage generalizes these reasoning capabilities to multimodal domains.\n  Experiments on Qwen2.5-VL-Instruct-3B demonstrate that \\method achieves\n4.83\\% and 4.5\\% average improvements over baselines in multimodal and\ntext-only benchmarks, respectively, with a 3.63\\% gain in complex Football Game\ntasks. These results validate that text-based reasoning enhancement enables\neffective multimodal generalization, offering a data-efficient paradigm that\nbypasses costly high-quality multimodal training data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07536.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08625",
      "authors": [
        {
          "_id": "67d0fd74f8595b656f921a48",
          "name": "Muzhi Zhu",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a49",
          "name": "Yuzhuo Tian",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a4a",
          "name": "Hao Chen",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a4b",
          "name": "Chunluan Zhou",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a4c",
          "name": "Qingpei Guo",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a4d",
          "name": "Yang Liu",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a4e",
          "name": "Ming Yang",
          "hidden": false
        },
        {
          "_id": "67d0fd74f8595b656f921a4f",
          "name": "Chunhua Shen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T17:08:54.000Z",
      "title": "SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by\n  Imitating Human Annotator Trajectories",
      "summary": "While MLLMs have demonstrated adequate image understanding capabilities, they\nstill struggle with pixel-level comprehension, limiting their practical\napplications. Current evaluation tasks like VQA and visual grounding remain too\ncoarse to assess fine-grained pixel comprehension accurately. Though\nsegmentation is foundational for pixel-level understanding, existing methods\noften require MLLMs to generate implicit tokens, decoded through external pixel\ndecoders. This approach disrupts the MLLM's text output space, potentially\ncompromising language capabilities and reducing flexibility and extensibility,\nwhile failing to reflect the model's intrinsic pixel-level understanding.\n  Thus, we introduce the Human-Like Mask Annotation Task (HLMAT), a new\nparadigm where MLLMs mimic human annotators using interactive segmentation\ntools. Modeling segmentation as a multi-step Markov Decision Process, HLMAT\nenables MLLMs to iteratively generate text-based click points, achieving\nhigh-quality masks without architectural changes or implicit tokens. Through\nthis setup, we develop SegAgent, a model fine-tuned on human-like annotation\ntrajectories, which achieves performance comparable to state-of-the-art (SOTA)\nmethods and supports additional tasks like mask refinement and annotation\nfiltering.\n  HLMAT provides a protocol for assessing fine-grained pixel understanding in\nMLLMs and introduces a vision-centric, multi-step decision-making task that\nfacilitates exploration of MLLMs' visual reasoning abilities. Our adaptations\nof policy improvement method StaR and PRM-guided tree search further enhance\nmodel robustness in complex segmentation tasks, laying a foundation for future\nadvancements in fine-grained visual perception and multi-step decision-making\nfor MLLMs.",
      "upvotes": 18,
      "discussionId": "67d0fd76f8595b656f921ae8",
      "projectPage": "https://aim-uofa.github.io/SegAgent/",
      "githubRepo": "https://github.com/aim-uofa/SegAgent",
      "ai_keywords": [
        "Human-Like Mask Annotation Task (HLMAT)",
        "Markov Decision Process",
        "multi-step decision-making",
        "click points",
        "masks",
        "policy improvement method StaR",
        "PRM-guided tree search",
        "mask refinement",
        "annotation filtering",
        "fine-grained pixel understanding",
        "vision-centric task",
        "visual reasoning abilities"
      ]
    },
    "publishedAt": "2025-03-11T13:08:54.000Z",
    "title": "SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by\n  Imitating Human Annotator Trajectories",
    "summary": "While MLLMs have demonstrated adequate image understanding capabilities, they\nstill struggle with pixel-level comprehension, limiting their practical\napplications. Current evaluation tasks like VQA and visual grounding remain too\ncoarse to assess fine-grained pixel comprehension accurately. Though\nsegmentation is foundational for pixel-level understanding, existing methods\noften require MLLMs to generate implicit tokens, decoded through external pixel\ndecoders. This approach disrupts the MLLM's text output space, potentially\ncompromising language capabilities and reducing flexibility and extensibility,\nwhile failing to reflect the model's intrinsic pixel-level understanding.\n  Thus, we introduce the Human-Like Mask Annotation Task (HLMAT), a new\nparadigm where MLLMs mimic human annotators using interactive segmentation\ntools. Modeling segmentation as a multi-step Markov Decision Process, HLMAT\nenables MLLMs to iteratively generate text-based click points, achieving\nhigh-quality masks without architectural changes or implicit tokens. Through\nthis setup, we develop SegAgent, a model fine-tuned on human-like annotation\ntrajectories, which achieves performance comparable to state-of-the-art (SOTA)\nmethods and supports additional tasks like mask refinement and annotation\nfiltering.\n  HLMAT provides a protocol for assessing fine-grained pixel understanding in\nMLLMs and introduces a vision-centric, multi-step decision-making task that\nfacilitates exploration of MLLMs' visual reasoning abilities. Our adaptations\nof policy improvement method StaR and PRM-guided tree search further enhance\nmodel robustness in complex segmentation tasks, laying a foundation for future\nadvancements in fine-grained visual perception and multi-step decision-making\nfor MLLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08625.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.05978",
      "authors": [
        {
          "_id": "67d129d732b4bbfb938321a1",
          "name": "Hongwei Yi",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a2",
          "name": "Tian Ye",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a3",
          "name": "Shitong Shao",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a4",
          "name": "Xuancheng Yang",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a5",
          "name": "Jiantong Zhao",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a6",
          "name": "Hanzhong Guo",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a7",
          "name": "Terrance Wang",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a8",
          "name": "Qingyu Yin",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321a9",
          "name": "Zeke Xie",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321aa",
          "name": "Lei Zhu",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321ab",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321ac",
          "name": "Michael Lingelbach",
          "hidden": false
        },
        {
          "_id": "67d129d732b4bbfb938321ad",
          "name": "Daquan Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T23:21:11.000Z",
      "title": "MagicInfinite: Generating Infinite Talking Videos with Your Words and\n  Voice",
      "summary": "We present MagicInfinite, a novel diffusion Transformer (DiT) framework that\novercomes traditional portrait animation limitations, delivering high-fidelity\nresults across diverse character types-realistic humans, full-body figures, and\nstylized anime characters. It supports varied facial poses, including\nback-facing views, and animates single or multiple characters with input masks\nfor precise speaker designation in multi-character scenes. Our approach tackles\nkey challenges with three innovations: (1) 3D full-attention mechanisms with a\nsliding window denoising strategy, enabling infinite video generation with\ntemporal coherence and visual quality across diverse character styles; (2) a\ntwo-stage curriculum learning scheme, integrating audio for lip sync, text for\nexpressive dynamics, and reference images for identity preservation, enabling\nflexible multi-modal control over long sequences; and (3) region-specific masks\nwith adaptive loss functions to balance global textual control and local audio\nguidance, supporting speaker-specific animations. Efficiency is enhanced via\nour innovative unified step and cfg distillation techniques, achieving a 20x\ninference speed boost over the basemodel: generating a 10 second 540x540p video\nin 10 seconds or 720x720p in 30 seconds on 8 H100 GPUs, without quality loss.\nEvaluations on our new benchmark demonstrate MagicInfinite's superiority in\naudio-lip synchronization, identity preservation, and motion naturalness across\ndiverse scenarios. It is publicly available at https://www.hedra.com/, with\nexamples at https://magicinfinite.github.io/.",
      "upvotes": 16,
      "discussionId": "67d129e332b4bbfb938324a0",
      "projectPage": "https://magicinfinite.github.io/",
      "ai_keywords": [
        "diffusion Transformer (DiT)",
        "3D full-attention mechanisms",
        "sliding window denoising strategy",
        "infinite video generation",
        "temporal coherence",
        "two-stage curriculum learning scheme",
        "audio for lip sync",
        "text for expressive dynamics",
        "reference images for identity preservation",
        "region-specific masks",
        "adaptive loss functions",
        "unified step and cfg distillation techniques",
        "inference speed",
        "audio-lip synchronization",
        "identity preservation",
        "motion naturalness"
      ]
    },
    "publishedAt": "2025-03-07T18:21:11.000Z",
    "title": "MagicInfinite: Generating Infinite Talking Videos with Your Words and\n  Voice",
    "summary": "We present MagicInfinite, a novel diffusion Transformer (DiT) framework that\novercomes traditional portrait animation limitations, delivering high-fidelity\nresults across diverse character types-realistic humans, full-body figures, and\nstylized anime characters. It supports varied facial poses, including\nback-facing views, and animates single or multiple characters with input masks\nfor precise speaker designation in multi-character scenes. Our approach tackles\nkey challenges with three innovations: (1) 3D full-attention mechanisms with a\nsliding window denoising strategy, enabling infinite video generation with\ntemporal coherence and visual quality across diverse character styles; (2) a\ntwo-stage curriculum learning scheme, integrating audio for lip sync, text for\nexpressive dynamics, and reference images for identity preservation, enabling\nflexible multi-modal control over long sequences; and (3) region-specific masks\nwith adaptive loss functions to balance global textual control and local audio\nguidance, supporting speaker-specific animations. Efficiency is enhanced via\nour innovative unified step and cfg distillation techniques, achieving a 20x\ninference speed boost over the basemodel: generating a 10 second 540x540p video\nin 10 seconds or 720x720p in 30 seconds on 8 H100 GPUs, without quality loss.\nEvaluations on our new benchmark demonstrate MagicInfinite's superiority in\naudio-lip synchronization, identity preservation, and motion naturalness across\ndiverse scenarios. It is publicly available at https://www.hedra.com/, with\nexamples at https://magicinfinite.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05978.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07604",
      "authors": [
        {
          "_id": "67cfa4ecd8cb8688d7d6d8b5",
          "name": "Tianhe Lin",
          "hidden": false
        },
        {
          "_id": "67cfa4ecd8cb8688d7d6d8b6",
          "name": "Jian Xie",
          "hidden": false
        },
        {
          "_id": "67cfa4ecd8cb8688d7d6d8b7",
          "name": "Siyu Yuan",
          "hidden": false
        },
        {
          "_id": "67cfa4ecd8cb8688d7d6d8b8",
          "name": "Deqing Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:58:31.000Z",
      "title": "Implicit Reasoning in Transformers is Reasoning through Shortcuts",
      "summary": "Test-time compute is emerging as a new paradigm for enhancing language\nmodels' complex multi-step reasoning capabilities, as demonstrated by the\nsuccess of OpenAI's o1 and o3, as well as DeepSeek's R1. Compared to explicit\nreasoning in test-time compute, implicit reasoning is more inference-efficient,\nrequiring fewer generated tokens. However, why does the advanced reasoning\ncapability fail to emerge in the implicit reasoning style? In this work, we\ntrain GPT-2 from scratch on a curated multi-step mathematical reasoning dataset\nand conduct analytical experiments to investigate how language models perform\nimplicit reasoning in multi-step tasks. Our findings reveal: 1) Language models\ncan perform step-by-step reasoning and achieve high accuracy in both in-domain\nand out-of-domain tests via implicit reasoning. However, this capability only\nemerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning\nabilities emerging from training on unfixed-pattern data tend to overfit a\nspecific pattern and fail to generalize further. Notably, this limitation is\nalso observed in state-of-the-art large language models. These findings suggest\nthat language models acquire implicit reasoning through shortcut learning,\nenabling strong performance on tasks with similar patterns while lacking\ngeneralization.",
      "upvotes": 14,
      "discussionId": "67cfa4edd8cb8688d7d6d908",
      "githubRepo": "https://github.com/TianheL/LM-Implicit-Reasoning",
      "ai_keywords": [
        "test-time compute",
        "multi-step reasoning",
        "OpenAI's o1",
        "OpenAI's o3",
        "DeepSeek's R1",
        "implicit reasoning",
        "inference-efficient",
        "generated tokens",
        "explicit reasoning",
        "GPT-2",
        "multi-step mathematical reasoning dataset",
        "step-by-step reasoning",
        "in-domain tests",
        "out-of-domain tests",
        "fixed-pattern data",
        "unfixed-pattern data",
        "overfit",
        "generalization",
        "shortcut learning"
      ]
    },
    "publishedAt": "2025-03-10T13:58:31.000Z",
    "title": "Implicit Reasoning in Transformers is Reasoning through Shortcuts",
    "summary": "Test-time compute is emerging as a new paradigm for enhancing language\nmodels' complex multi-step reasoning capabilities, as demonstrated by the\nsuccess of OpenAI's o1 and o3, as well as DeepSeek's R1. Compared to explicit\nreasoning in test-time compute, implicit reasoning is more inference-efficient,\nrequiring fewer generated tokens. However, why does the advanced reasoning\ncapability fail to emerge in the implicit reasoning style? In this work, we\ntrain GPT-2 from scratch on a curated multi-step mathematical reasoning dataset\nand conduct analytical experiments to investigate how language models perform\nimplicit reasoning in multi-step tasks. Our findings reveal: 1) Language models\ncan perform step-by-step reasoning and achieve high accuracy in both in-domain\nand out-of-domain tests via implicit reasoning. However, this capability only\nemerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning\nabilities emerging from training on unfixed-pattern data tend to overfit a\nspecific pattern and fail to generalize further. Notably, this limitation is\nalso observed in state-of-the-art large language models. These findings suggest\nthat language models acquire implicit reasoning through shortcut learning,\nenabling strong performance on tasks with similar patterns while lacking\ngeneralization.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07604.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08605",
      "authors": [
        {
          "_id": "67d0ed0877b0c8ac3f304ef1",
          "name": "Subin Kim",
          "hidden": false
        },
        {
          "_id": "67d0ed0877b0c8ac3f304ef2",
          "name": "Seoung Wug Oh",
          "hidden": false
        },
        {
          "_id": "67d0ed0877b0c8ac3f304ef3",
          "name": "Jui-Hsien Wang",
          "hidden": false
        },
        {
          "_id": "67d0ed0877b0c8ac3f304ef4",
          "name": "Joon-Young Lee",
          "hidden": false
        },
        {
          "_id": "67d0ed0877b0c8ac3f304ef5",
          "name": "Jinwoo Shin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T16:43:45.000Z",
      "title": "Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled\n  Sampling",
      "summary": "While recent advancements in text-to-video diffusion models enable\nhigh-quality short video generation from a single prompt, generating real-world\nlong videos in a single pass remains challenging due to limited data and high\ncomputational costs. To address this, several works propose tuning-free\napproaches, i.e., extending existing models for long video generation,\nspecifically using multiple prompts to allow for dynamic and controlled content\nchanges. However, these methods primarily focus on ensuring smooth transitions\nbetween adjacent frames, often leading to content drift and a gradual loss of\nsemantic coherence over longer sequences. To tackle such an issue, we propose\nSynchronized Coupled Sampling (SynCoS), a novel inference framework that\nsynchronizes denoising paths across the entire video, ensuring long-range\nconsistency across both adjacent and distant frames. Our approach combines two\ncomplementary sampling strategies: reverse and optimization-based sampling,\nwhich ensure seamless local transitions and enforce global coherence,\nrespectively. However, directly alternating between these samplings misaligns\ndenoising trajectories, disrupting prompt guidance and introducing unintended\ncontent changes as they operate independently. To resolve this, SynCoS\nsynchronizes them through a grounded timestep and a fixed baseline noise,\nensuring fully coupled sampling with aligned denoising paths. Extensive\nexperiments show that SynCoS significantly improves multi-event long video\ngeneration, achieving smoother transitions and superior long-range coherence,\noutperforming previous approaches both quantitatively and qualitatively.",
      "upvotes": 13,
      "discussionId": "67d0ed0b77b0c8ac3f304f7c",
      "projectPage": "https://syncos2025.github.io/",
      "githubRepo": "https://github.com/subin-kim-cv/SynCoS",
      "ai_keywords": [
        "text-to-video diffusion models",
        "high-quality short video generation",
        "long video generation",
        "tuning-free approaches",
        "multiple prompts",
        "dynamic content changes",
        "smooth transitions",
        "content drift",
        "semantic coherence",
        "Synchronized Coupled Sampling (SynCoS)",
        "denoising paths",
        "reverse sampling",
        "optimization-based sampling",
        "seamless local transitions",
        "global coherence",
        "grounded timestep",
        "fixed baseline noise",
        "multi-event long video generation",
        "long-range consistency"
      ]
    },
    "publishedAt": "2025-03-11T12:43:45.000Z",
    "title": "Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled\n  Sampling",
    "summary": "While recent advancements in text-to-video diffusion models enable\nhigh-quality short video generation from a single prompt, generating real-world\nlong videos in a single pass remains challenging due to limited data and high\ncomputational costs. To address this, several works propose tuning-free\napproaches, i.e., extending existing models for long video generation,\nspecifically using multiple prompts to allow for dynamic and controlled content\nchanges. However, these methods primarily focus on ensuring smooth transitions\nbetween adjacent frames, often leading to content drift and a gradual loss of\nsemantic coherence over longer sequences. To tackle such an issue, we propose\nSynchronized Coupled Sampling (SynCoS), a novel inference framework that\nsynchronizes denoising paths across the entire video, ensuring long-range\nconsistency across both adjacent and distant frames. Our approach combines two\ncomplementary sampling strategies: reverse and optimization-based sampling,\nwhich ensure seamless local transitions and enforce global coherence,\nrespectively. However, directly alternating between these samplings misaligns\ndenoising trajectories, disrupting prompt guidance and introducing unintended\ncontent changes as they operate independently. To resolve this, SynCoS\nsynchronizes them through a grounded timestep and a fixed baseline noise,\nensuring fully coupled sampling with aligned denoising paths. Extensive\nexperiments show that SynCoS significantly improves multi-event long video\ngeneration, achieving smoother transitions and superior long-range coherence,\noutperforming previous approaches both quantitatively and qualitatively.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08605.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08619",
      "authors": [
        {
          "_id": "67d0eb9cec69694dca382208",
          "name": "Xianfeng Wu",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca382209",
          "name": "Yajing Bai",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca38220a",
          "name": "Haoze Zheng",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca38220b",
          "name": "Harold Haodong Chen",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca38220c",
          "name": "Yexin Liu",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca38220d",
          "name": "Zihao Wang",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca38220e",
          "name": "Xuran Ma",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca38220f",
          "name": "Wen-Jie Shu",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca382210",
          "name": "Xianzu Wu",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca382211",
          "name": "Harry Yang",
          "hidden": false
        },
        {
          "_id": "67d0eb9cec69694dca382212",
          "name": "Ser-Nam Lim",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T16:58:02.000Z",
      "title": "LightGen: Efficient Image Generation through Knowledge Distillation and\n  Direct Preference Optimization",
      "summary": "Recent advances in text-to-image generation have primarily relied on\nextensive datasets and parameter-heavy architectures. These requirements\nseverely limit accessibility for researchers and practitioners who lack\nsubstantial computational resources. In this paper, we introduce \\model, an\nefficient training paradigm for image generation models that uses knowledge\ndistillation (KD) and Direct Preference Optimization (DPO). Drawing inspiration\nfrom the success of data KD techniques widely adopted in Multi-Modal Large\nLanguage Models (MLLMs), LightGen distills knowledge from state-of-the-art\n(SOTA) text-to-image models into a compact Masked Autoregressive (MAR)\narchitecture with only 0.7B parameters. Using a compact synthetic dataset of\njust 2M high-quality images generated from varied captions, we demonstrate\nthat data diversity significantly outweighs data volume in determining model\nperformance. This strategy dramatically reduces computational demands and\nreduces pre-training time from potentially thousands of GPU-days to merely 88\nGPU-days. Furthermore, to address the inherent shortcomings of synthetic data,\nparticularly poor high-frequency details and spatial inaccuracies, we integrate\nthe DPO technique that refines image fidelity and positional accuracy.\nComprehensive experiments confirm that LightGen achieves image generation\nquality comparable to SOTA models while significantly reducing computational\nresources and expanding accessibility for resource-constrained environments.\nCode is available at https://github.com/XianfengWu01/LightGen",
      "upvotes": 9,
      "discussionId": "67d0eba3ec69694dca3823a0",
      "ai_keywords": [
        "knowledge distillation (KD)",
        "Direct Preference Optimization (DPO)",
        "Multi-Modal Large Language Models (MLLMs)",
        "Masked Autoregressive (MAR)",
        "synthetic dataset",
        "data diversity",
        "data volume",
        "model performance",
        "computational demands",
        "pre-training time",
        "synthetic data",
        "high-frequency details",
        "spatial inaccuracies",
        "image fidelity",
        "positional accuracy"
      ]
    },
    "publishedAt": "2025-03-11T12:58:02.000Z",
    "title": "LightGen: Efficient Image Generation through Knowledge Distillation and\n  Direct Preference Optimization",
    "summary": "Recent advances in text-to-image generation have primarily relied on\nextensive datasets and parameter-heavy architectures. These requirements\nseverely limit accessibility for researchers and practitioners who lack\nsubstantial computational resources. In this paper, we introduce \\model, an\nefficient training paradigm for image generation models that uses knowledge\ndistillation (KD) and Direct Preference Optimization (DPO). Drawing inspiration\nfrom the success of data KD techniques widely adopted in Multi-Modal Large\nLanguage Models (MLLMs), LightGen distills knowledge from state-of-the-art\n(SOTA) text-to-image models into a compact Masked Autoregressive (MAR)\narchitecture with only 0.7B parameters. Using a compact synthetic dataset of\njust 2M high-quality images generated from varied captions, we demonstrate\nthat data diversity significantly outweighs data volume in determining model\nperformance. This strategy dramatically reduces computational demands and\nreduces pre-training time from potentially thousands of GPU-days to merely 88\nGPU-days. Furthermore, to address the inherent shortcomings of synthetic data,\nparticularly poor high-frequency details and spatial inaccuracies, we integrate\nthe DPO technique that refines image fidelity and positional accuracy.\nComprehensive experiments confirm that LightGen achieves image generation\nquality comparable to SOTA models while significantly reducing computational\nresources and expanding accessibility for resource-constrained environments.\nCode is available at https://github.com/XianfengWu01/LightGen",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08619.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07860",
      "authors": [
        {
          "_id": "67d0e915d0038007e5a75178",
          "name": "James Burgess",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a75179",
          "name": "Xiaohan Wang",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a7517a",
          "name": "Yuhui Zhang",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a7517b",
          "name": "Anita Rau",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a7517c",
          "name": "Alejandro Lozano",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a7517d",
          "name": "Lisa Dunlap",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a7517e",
          "name": "Trevor Darrell",
          "hidden": false
        },
        {
          "_id": "67d0e915d0038007e5a7517f",
          "name": "Serena Yeung-Levy",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T21:18:32.000Z",
      "title": "Video Action Differencing",
      "summary": "How do two individuals differ when performing the same action? In this work,\nwe introduce Video Action Differencing (VidDiff), the novel task of identifying\nsubtle differences between videos of the same action, which has many\napplications, such as coaching and skill learning. To enable development on\nthis new task, we first create VidDiffBench, a benchmark dataset containing 549\nvideo pairs, with human annotations of 4,469 fine-grained action differences\nand 2,075 localization timestamps indicating where these differences occur. Our\nexperiments demonstrate that VidDiffBench poses a significant challenge for\nstate-of-the-art large multimodal models (LMMs), such as GPT-4o and Qwen2-VL.\nBy analyzing failure cases of LMMs on VidDiffBench, we highlight two key\nchallenges for this task: localizing relevant sub-actions over two videos and\nfine-grained frame comparison. To overcome these, we propose the VidDiff\nmethod, an agentic workflow that breaks the task into three stages: action\ndifference proposal, keyframe localization, and frame differencing, each stage\nutilizing specialized foundation models. To encourage future research in this\nnew task, we release the benchmark at\nhttps://huggingface.co/datasets/jmhb/VidDiffBench and code at\nhttp://jmhb0.github.io/viddiff.",
      "upvotes": 9,
      "discussionId": "67d0e917d0038007e5a751e9",
      "projectPage": "https://jmhb0.github.io/viddiff/",
      "githubRepo": "https://github.com/jmhb0/viddiff",
      "ai_keywords": [
        "Video Action Differencing (VidDiff)",
        "VidDiffBench",
        "multimodal models (LMMs)",
        "GPT-4o",
        "Qwen2-VL",
        "action difference proposal",
        "keyframe localization",
        "frame differencing",
        "agentic workflow",
        "fine-grained action differences",
        "localization timestamps"
      ]
    },
    "publishedAt": "2025-03-10T17:18:32.000Z",
    "title": "Video Action Differencing",
    "summary": "How do two individuals differ when performing the same action? In this work,\nwe introduce Video Action Differencing (VidDiff), the novel task of identifying\nsubtle differences between videos of the same action, which has many\napplications, such as coaching and skill learning. To enable development on\nthis new task, we first create VidDiffBench, a benchmark dataset containing 549\nvideo pairs, with human annotations of 4,469 fine-grained action differences\nand 2,075 localization timestamps indicating where these differences occur. Our\nexperiments demonstrate that VidDiffBench poses a significant challenge for\nstate-of-the-art large multimodal models (LMMs), such as GPT-4o and Qwen2-VL.\nBy analyzing failure cases of LMMs on VidDiffBench, we highlight two key\nchallenges for this task: localizing relevant sub-actions over two videos and\nfine-grained frame comparison. To overcome these, we propose the VidDiff\nmethod, an agentic workflow that breaks the task into three stages: action\ndifference proposal, keyframe localization, and frame differencing, each stage\nutilizing specialized foundation models. To encourage future research in this\nnew task, we release the benchmark at\nhttps://huggingface.co/datasets/jmhb/VidDiffBench and code at\nhttp://jmhb0.github.io/viddiff.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07860.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07891",
      "authors": [
        {
          "_id": "67d108c56bd6c57bab0b6f07",
          "name": "Jinhyuk Lee",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f08",
          "name": "Feiyang Chen",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f09",
          "name": "Sahil Dua",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f0a",
          "name": "Daniel Cer",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f0b",
          "name": "Madhuri Shanbhogue",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f0c",
          "name": "Iftekhar Naim",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f0d",
          "name": "Gustavo HernÃ¡ndez Ãbrego",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f0e",
          "name": "Zhe Li",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f0f",
          "name": "Kaifeng Chen",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f10",
          "name": "Henrique Schechter Vera",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f11",
          "name": "Xiaoqi Ren",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f12",
          "name": "Shanfeng Zhang",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f13",
          "name": "Daniel Salz",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f14",
          "name": "Michael Boratko",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f15",
          "name": "Jay Han",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f16",
          "name": "Blair Chen",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f17",
          "name": "Shuo Huang",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f18",
          "name": "Vikram Rao",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f19",
          "name": "Paul Suganthan",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f1a",
          "name": "Feng Han",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f1b",
          "name": "Andreas Doumanoglou",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f1c",
          "name": "Nithi Gupta",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f1d",
          "name": "Fedor Moiseev",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f1e",
          "name": "Cathy Yip",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f1f",
          "name": "Aashi Jain",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f20",
          "name": "Simon Baumgartner",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f21",
          "name": "Shahrokh Shahi",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f22",
          "name": "Frank Palma Gomez",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f23",
          "name": "Sandeep Mariserla",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f24",
          "name": "Min Choi",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f25",
          "name": "Parashar Shah",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f26",
          "name": "Sonam Goenka",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f27",
          "name": "Ke Chen",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f28",
          "name": "Ye Xia",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f29",
          "name": "Koert Chen",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f2a",
          "name": "Sai Meher Karthik Duddu",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f2b",
          "name": "Yichang Chen",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f2c",
          "name": "Trevor Walker",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f2d",
          "name": "Wenlei Zhou",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f2e",
          "name": "Rakesh Ghiya",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f2f",
          "name": "Zach Gleicher",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f30",
          "name": "Karan Gill",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f31",
          "name": "Zhe Dong",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f32",
          "name": "Mojtaba Seyedhosseini",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f33",
          "name": "Yunhsuan Sung",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f34",
          "name": "Raphael Hoffmann",
          "hidden": false
        },
        {
          "_id": "67d108c56bd6c57bab0b6f35",
          "name": "Tom Duerig",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T22:16:45.000Z",
      "title": "Gemini Embedding: Generalizable Embeddings from Gemini",
      "summary": "In this report, we introduce Gemini Embedding, a state-of-the-art embedding\nmodel leveraging the power of Gemini, Google's most capable large language\nmodel. Capitalizing on Gemini's inherent multilingual and code understanding\ncapabilities, Gemini Embedding produces highly generalizable embeddings for\ntext spanning numerous languages and textual modalities. The representations\ngenerated by Gemini Embedding can be precomputed and applied to a variety of\ndownstream tasks including classification, similarity, clustering, ranking, and\nretrieval. Evaluated on the Massive Multilingual Text Embedding Benchmark\n(MMTEB), which includes over one hundred tasks across 250+ languages, Gemini\nEmbedding substantially outperforms prior state-of-the-art models,\ndemonstrating considerable improvements in embedding quality. Achieving\nstate-of-the-art performance across MMTEB's multilingual, English, and code\nbenchmarks, our unified model demonstrates strong capabilities across a broad\nselection of tasks and surpasses specialized domain-specific models.",
      "upvotes": 8,
      "discussionId": "67d108c66bd6c57bab0b6f6e",
      "ai_keywords": [
        "Gemini Embedding",
        "large language model",
        "multilingual",
        "code understanding",
        "representations",
        "downstream tasks",
        "classification",
        "similarity",
        "clustering",
        "ranking",
        "retrieval",
        "Massive Multilingual Text Embedding Benchmark (MMTEB)",
        "embedding quality",
        "specialized domain-specific models"
      ]
    },
    "publishedAt": "2025-03-10T18:16:45.000Z",
    "title": "Gemini Embedding: Generalizable Embeddings from Gemini",
    "summary": "In this report, we introduce Gemini Embedding, a state-of-the-art embedding\nmodel leveraging the power of Gemini, Google's most capable large language\nmodel. Capitalizing on Gemini's inherent multilingual and code understanding\ncapabilities, Gemini Embedding produces highly generalizable embeddings for\ntext spanning numerous languages and textual modalities. The representations\ngenerated by Gemini Embedding can be precomputed and applied to a variety of\ndownstream tasks including classification, similarity, clustering, ranking, and\nretrieval. Evaluated on the Massive Multilingual Text Embedding Benchmark\n(MMTEB), which includes over one hundred tasks across 250+ languages, Gemini\nEmbedding substantially outperforms prior state-of-the-art models,\ndemonstrating considerable improvements in embedding quality. Achieving\nstate-of-the-art performance across MMTEB's multilingual, English, and code\nbenchmarks, our unified model demonstrates strong capabilities across a broad\nselection of tasks and surpasses specialized domain-specific models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07891.png",
    "numComments": 2,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07572",
      "authors": [
        {
          "_id": "67d0e38171b6b577dbb8c72c",
          "name": "Yuxiao Qu",
          "hidden": false
        },
        {
          "_id": "67d0e38171b6b577dbb8c72d",
          "name": "Matthew Y. R. Yang",
          "hidden": false
        },
        {
          "_id": "67d0e38171b6b577dbb8c72e",
          "name": "Amrith Setlur",
          "hidden": false
        },
        {
          "_id": "67d0e38171b6b577dbb8c72f",
          "name": "Lewis Tunstall",
          "hidden": false
        },
        {
          "_id": "67d0e38171b6b577dbb8c730",
          "name": "Edward Emanuel Beeching",
          "hidden": false
        },
        {
          "_id": "67d0e38171b6b577dbb8c731",
          "name": "Ruslan Salakhutdinov",
          "hidden": false
        },
        {
          "_id": "67d0e38171b6b577dbb8c732",
          "name": "Aviral Kumar",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:40:43.000Z",
      "title": "Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning",
      "summary": "Training models to effectively use test-time compute is crucial for improving\nthe reasoning performance of LLMs. Current methods mostly do so via fine-tuning\non search traces or running RL with 0/1 outcome reward, but do these approaches\nefficiently utilize test-time compute? Would these approaches continue to scale\nas the budget improves? In this paper, we try to answer these questions. We\nformalize the problem of optimizing test-time compute as a meta-reinforcement\nlearning (RL) problem, which provides a principled perspective on spending\ntest-time compute. This perspective enables us to view the long output stream\nfrom the LLM as consisting of several episodes run at test time and leads us to\nuse a notion of cumulative regret over output tokens as a way to measure the\nefficacy of test-time compute. Akin to how RL algorithms can best tradeoff\nexploration and exploitation over training, minimizing cumulative regret would\nalso provide the best balance between exploration and exploitation in the token\nstream. While we show that state-of-the-art models do not minimize regret, one\ncan do so by maximizing a dense reward bonus in conjunction with the outcome\n0/1 reward RL. This bonus is the ''progress'' made by each subsequent block in\nthe output stream, quantified by the change in the likelihood of eventual\nsuccess. Using these insights, we develop Meta Reinforcement Fine-Tuning, or\nMRT, a new class of fine-tuning methods for optimizing test-time compute. MRT\nleads to a 2-3x relative gain in performance and roughly a 1.5x gain in token\nefficiency for math reasoning compared to outcome-reward RL.",
      "upvotes": 7,
      "discussionId": "67d0e38271b6b577dbb8c7b7",
      "projectPage": "https://cohenqu.github.io/mrt.github.io/",
      "ai_keywords": [
        "meta-reinforcement learning (RL)",
        "cumulative regret",
        "token stream",
        "exploration and exploitation",
        "dense reward bonus",
        "likelihood of eventual success",
        "Meta Reinforcement Fine-Tuning (MRT)"
      ]
    },
    "publishedAt": "2025-03-10T13:40:43.000Z",
    "title": "Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning",
    "summary": "Training models to effectively use test-time compute is crucial for improving\nthe reasoning performance of LLMs. Current methods mostly do so via fine-tuning\non search traces or running RL with 0/1 outcome reward, but do these approaches\nefficiently utilize test-time compute? Would these approaches continue to scale\nas the budget improves? In this paper, we try to answer these questions. We\nformalize the problem of optimizing test-time compute as a meta-reinforcement\nlearning (RL) problem, which provides a principled perspective on spending\ntest-time compute. This perspective enables us to view the long output stream\nfrom the LLM as consisting of several episodes run at test time and leads us to\nuse a notion of cumulative regret over output tokens as a way to measure the\nefficacy of test-time compute. Akin to how RL algorithms can best tradeoff\nexploration and exploitation over training, minimizing cumulative regret would\nalso provide the best balance between exploration and exploitation in the token\nstream. While we show that state-of-the-art models do not minimize regret, one\ncan do so by maximizing a dense reward bonus in conjunction with the outcome\n0/1 reward RL. This bonus is the ''progress'' made by each subsequent block in\nthe output stream, quantified by the change in the likelihood of eventual\nsuccess. Using these insights, we develop Meta Reinforcement Fine-Tuning, or\nMRT, a new class of fine-tuning methods for optimizing test-time compute. MRT\nleads to a 2-3x relative gain in performance and roughly a 1.5x gain in token\nefficiency for math reasoning compared to outcome-reward RL.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07572.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08686",
      "authors": [
        {
          "_id": "67d0f892a189f3978638e154",
          "name": "Jialv Zou",
          "hidden": false
        },
        {
          "_id": "67d0f892a189f3978638e155",
          "name": "Bencheng Liao",
          "hidden": false
        },
        {
          "_id": "67d0f892a189f3978638e156",
          "name": "Qian Zhang",
          "hidden": false
        },
        {
          "_id": "67d0f892a189f3978638e157",
          "name": "Wenyu Liu",
          "hidden": false
        },
        {
          "_id": "67d0f892a189f3978638e158",
          "name": "Xinggang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T17:59:46.000Z",
      "title": "OmniMamba: Efficient and Unified Multimodal Understanding and Generation\n  via State Space Models",
      "summary": "Recent advancements in unified multimodal understanding and visual generation\n(or multimodal generation) models have been hindered by their quadratic\ncomputational complexity and dependence on large-scale training data. We\npresent OmniMamba, the first linear-architecture-based multimodal generation\nmodel that generates both text and images through a unified next-token\nprediction paradigm. The model fully leverages Mamba-2's high computational and\nmemory efficiency, extending its capabilities from text generation to\nmultimodal generation. To address the data inefficiency of existing unified\nmodels, we propose two key innovations: (1) decoupled vocabularies to guide\nmodality-specific generation, and (2) task-specific LoRA for\nparameter-efficient adaptation. Furthermore, we introduce a decoupled two-stage\ntraining strategy to mitigate data imbalance between two tasks. Equipped with\nthese techniques, OmniMamba achieves competitive performance with JanusFlow\nwhile surpassing Show-o across benchmarks, despite being trained on merely 2M\nimage-text pairs, which is 1,000 times fewer than Show-o. Notably, OmniMamba\nstands out with outstanding inference efficiency, achieving up to a 119.2 times\nspeedup and 63% GPU memory reduction for long-sequence generation compared to\nTransformer-based counterparts. Code and models are released at\nhttps://github.com/hustvl/OmniMamba",
      "upvotes": 5,
      "discussionId": "67d0f894a189f3978638e1b7",
      "ai_keywords": [
        "OmniMamba",
        "linear-architecture-based multimodal generation model",
        "unified next-token prediction paradigm",
        "Mamba-2",
        "computational efficiency",
        "memory efficiency",
        "decoupled vocabularies",
        "task-specific LoRA",
        "parameter-efficient adaptation",
        "decoupled two-stage training strategy",
        "data imbalance",
        "Show-o",
        "benchmark",
        "inference efficiency",
        "Transformer-based counterparts"
      ]
    },
    "publishedAt": "2025-03-11T13:59:46.000Z",
    "title": "OmniMamba: Efficient and Unified Multimodal Understanding and Generation\n  via State Space Models",
    "summary": "Recent advancements in unified multimodal understanding and visual generation\n(or multimodal generation) models have been hindered by their quadratic\ncomputational complexity and dependence on large-scale training data. We\npresent OmniMamba, the first linear-architecture-based multimodal generation\nmodel that generates both text and images through a unified next-token\nprediction paradigm. The model fully leverages Mamba-2's high computational and\nmemory efficiency, extending its capabilities from text generation to\nmultimodal generation. To address the data inefficiency of existing unified\nmodels, we propose two key innovations: (1) decoupled vocabularies to guide\nmodality-specific generation, and (2) task-specific LoRA for\nparameter-efficient adaptation. Furthermore, we introduce a decoupled two-stage\ntraining strategy to mitigate data imbalance between two tasks. Equipped with\nthese techniques, OmniMamba achieves competitive performance with JanusFlow\nwhile surpassing Show-o across benchmarks, despite being trained on merely 2M\nimage-text pairs, which is 1,000 times fewer than Show-o. Notably, OmniMamba\nstands out with outstanding inference efficiency, achieving up to a 119.2 times\nspeedup and 63% GPU memory reduction for long-sequence generation compared to\nTransformer-based counterparts. Code and models are released at\nhttps://github.com/hustvl/OmniMamba",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08686.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08588",
      "authors": [
        {
          "_id": "67d125362da8f91f8ef0412f",
          "name": "Xin Xu",
          "hidden": false
        },
        {
          "_id": "67d125362da8f91f8ef04130",
          "name": "Wei Xu",
          "hidden": false
        },
        {
          "_id": "67d125362da8f91f8ef04131",
          "name": "Ningyu Zhang",
          "hidden": false
        },
        {
          "_id": "67d125362da8f91f8ef04132",
          "name": "Julian McAuley",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T16:25:36.000Z",
      "title": "BiasEdit: Debiasing Stereotyped Language Models via Model Editing",
      "summary": "Previous studies have established that language models manifest stereotyped\nbiases. Existing debiasing strategies, such as retraining a model with\ncounterfactual data, representation projection, and prompting often fail to\nefficiently eliminate bias or directly alter the models' biased internal\nrepresentations. To address these issues, we propose BiasEdit, an efficient\nmodel editing method to remove stereotypical bias from language models through\nlightweight networks that act as editors to generate parameter updates.\nBiasEdit employs a debiasing loss guiding editor networks to conduct local\nedits on partial parameters of a language model for debiasing while preserving\nthe language modeling abilities during editing through a retention loss.\nExperiments on StereoSet and Crows-Pairs demonstrate the effectiveness,\nefficiency, and robustness of BiasEdit in eliminating bias compared to\ntangental debiasing baselines and little to no impact on the language models'\ngeneral capabilities. In addition, we conduct bias tracing to probe bias in\nvarious modules and explore bias editing impacts on different components of\nlanguage models.",
      "upvotes": 5,
      "discussionId": "67d125382da8f91f8ef041d0",
      "githubRepo": "https://github.com/zjunlp/BiasEdit",
      "ai_keywords": [
        "language models",
        "counterfactual data",
        "representation projection",
        "prompting",
        "BiasEdit",
        "parameter updates",
        "debiasing loss",
        "retention loss",
        "StereoSet",
        "Crows-Pairs",
        "language modeling abilities",
        "bias tracing"
      ]
    },
    "publishedAt": "2025-03-11T12:25:36.000Z",
    "title": "BiasEdit: Debiasing Stereotyped Language Models via Model Editing",
    "summary": "Previous studies have established that language models manifest stereotyped\nbiases. Existing debiasing strategies, such as retraining a model with\ncounterfactual data, representation projection, and prompting often fail to\nefficiently eliminate bias or directly alter the models' biased internal\nrepresentations. To address these issues, we propose BiasEdit, an efficient\nmodel editing method to remove stereotypical bias from language models through\nlightweight networks that act as editors to generate parameter updates.\nBiasEdit employs a debiasing loss guiding editor networks to conduct local\nedits on partial parameters of a language model for debiasing while preserving\nthe language modeling abilities during editing through a retention loss.\nExperiments on StereoSet and Crows-Pairs demonstrate the effectiveness,\nefficiency, and robustness of BiasEdit in eliminating bias compared to\ntangental debiasing baselines and little to no impact on the language models'\ngeneral capabilities. In addition, we conduct bias tracing to probe bias in\nvarious modules and explore bias editing impacts on different components of\nlanguage models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08588.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08689",
      "authors": [
        {
          "_id": "67d0f759cb5bf46c22ac8af1",
          "name": "Yongdong Luo",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af2",
          "name": "Wang Chen",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af3",
          "name": "Xiawu Zheng",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af4",
          "name": "Weizhong Huang",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af5",
          "name": "Shukang Yin",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af6",
          "name": "Haojia Lin",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af7",
          "name": "Chaoyou Fu",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af8",
          "name": "Jinfa Huang",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8af9",
          "name": "Jiayi Ji",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8afa",
          "name": "Jiebo Luo",
          "hidden": false
        },
        {
          "_id": "67d0f759cb5bf46c22ac8afb",
          "name": "Rongrong Ji",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T17:59:57.000Z",
      "title": "QuoTA: Query-oriented Token Assignment via CoT Query Decouple for Long\n  Video Comprehension",
      "summary": "Recent advances in long video understanding typically mitigate visual\nredundancy through visual token pruning based on attention distribution.\nHowever, while existing methods employ post-hoc low-response token pruning in\ndecoder layers, they overlook the input-level semantic correlation between\nvisual tokens and instructions (query). In this paper, we propose QuoTA, an\nante-hoc training-free modular that extends existing large video-language\nmodels (LVLMs) for visual token assignment based on query-oriented frame-level\nimportance assessment. The query-oriented token selection is crucial as it\naligns visual processing with task-specific requirements, optimizing token\nbudget utilization while preserving semantically relevant content.\nSpecifically, (i) QuoTA strategically allocates frame-level importance scores\nbased on query relevance, enabling one-time visual token assignment before\ncross-modal interactions in decoder layers, (ii) we decouple the query through\nChain-of-Thoughts reasoning to facilitate more precise LVLM-based frame\nimportance scoring, and (iii) QuoTA offers a plug-and-play functionality that\nextends to existing LVLMs. Extensive experimental results demonstrate that\nimplementing QuoTA with LLaVA-Video-7B yields an average performance\nimprovement of 3.2% across six benchmarks (including Video-MME and MLVU) while\noperating within an identical visual token budget as the baseline. Codes are\nopen-sourced at https://github.com/MAC-AutoML/QuoTA.",
      "upvotes": 4,
      "discussionId": "67d0f75bcb5bf46c22ac8b70",
      "githubRepo": "https://github.com/MAC-AutoML/QuoTA",
      "ai_keywords": [
        "QuoTA",
        "ante-hoc",
        "training-free",
        "modular",
        "long video understanding",
        "visual token pruning",
        "attention distribution",
        "decoder layers",
        "input-level semantic correlation",
        "visual tokens",
        "instructions",
        "query",
        "frame-level importance assessment",
        "task-specific requirements",
        "token budget utilization",
        "semantically relevant content",
        "Chain-of-Thoughts reasoning",
        "cross-modal interactions",
        "plug-and-play functionality",
        "LLaVA-Video-7B",
        "Video-MME",
        "MLVU"
      ]
    },
    "publishedAt": "2025-03-11T13:59:57.000Z",
    "title": "QuoTA: Query-oriented Token Assignment via CoT Query Decouple for Long\n  Video Comprehension",
    "summary": "Recent advances in long video understanding typically mitigate visual\nredundancy through visual token pruning based on attention distribution.\nHowever, while existing methods employ post-hoc low-response token pruning in\ndecoder layers, they overlook the input-level semantic correlation between\nvisual tokens and instructions (query). In this paper, we propose QuoTA, an\nante-hoc training-free modular that extends existing large video-language\nmodels (LVLMs) for visual token assignment based on query-oriented frame-level\nimportance assessment. The query-oriented token selection is crucial as it\naligns visual processing with task-specific requirements, optimizing token\nbudget utilization while preserving semantically relevant content.\nSpecifically, (i) QuoTA strategically allocates frame-level importance scores\nbased on query relevance, enabling one-time visual token assignment before\ncross-modal interactions in decoder layers, (ii) we decouple the query through\nChain-of-Thoughts reasoning to facilitate more precise LVLM-based frame\nimportance scoring, and (iii) QuoTA offers a plug-and-play functionality that\nextends to existing LVLMs. Extensive experimental results demonstrate that\nimplementing QuoTA with LLaVA-Video-7B yields an average performance\nimprovement of 3.2% across six benchmarks (including Video-MME and MLVU) while\noperating within an identical visual token budget as the baseline. Codes are\nopen-sourced at https://github.com/MAC-AutoML/QuoTA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08689.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08685",
      "authors": [
        {
          "_id": "67d0f7032eaba9be7bf76e0e",
          "name": "Xin Wen",
          "hidden": false
        },
        {
          "_id": "67d0f7032eaba9be7bf76e0f",
          "name": "Bingchen Zhao",
          "hidden": false
        },
        {
          "_id": "67d0f7032eaba9be7bf76e10",
          "name": "Ismail Elezi",
          "hidden": false
        },
        {
          "_id": "67d0f7032eaba9be7bf76e11",
          "name": "Jiankang Deng",
          "hidden": false
        },
        {
          "_id": "67d0f7032eaba9be7bf76e12",
          "name": "Xiaojuan Qi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T17:59:41.000Z",
      "title": "\"Principal Components\" Enable A New Language of Images",
      "summary": "We introduce a novel visual tokenization framework that embeds a provable\nPCA-like structure into the latent token space. While existing visual\ntokenizers primarily optimize for reconstruction fidelity, they often neglect\nthe structural properties of the latent space -- a critical factor for both\ninterpretability and downstream tasks. Our method generates a 1D causal token\nsequence for images, where each successive token contributes non-overlapping\ninformation with mathematically guaranteed decreasing explained variance,\nanalogous to principal component analysis. This structural constraint ensures\nthe tokenizer extracts the most salient visual features first, with each\nsubsequent token adding diminishing yet complementary information.\nAdditionally, we identified and resolved a semantic-spectrum coupling effect\nthat causes the unwanted entanglement of high-level semantic content and\nlow-level spectral details in the tokens by leveraging a diffusion decoder.\nExperiments demonstrate that our approach achieves state-of-the-art\nreconstruction performance and enables better interpretability to align with\nthe human vision system. Moreover, auto-regressive models trained on our token\nsequences achieve performance comparable to current state-of-the-art methods\nwhile requiring fewer tokens for training and inference.",
      "upvotes": 4,
      "discussionId": "67d0f7052eaba9be7bf76eac",
      "projectPage": "https://visual-gen.github.io/semanticist/",
      "githubRepo": "https://github.com/visual-gen/semanticist",
      "ai_keywords": [
        "visual tokenization framework",
        "PCA-like structure",
        "latent token space",
        "reconstruction fidelity",
        "structural properties",
        "interpretabiliy",
        "1D causal token sequence",
        "explained variance",
        "principal component analysis",
        "salient visual features",
        "semantic-spectrum coupling effect",
        "diffusion decoder",
        "reconstruction performance",
        "human vision system",
        "auto-regressive models",
        "state-of-the-art methods"
      ]
    },
    "publishedAt": "2025-03-11T13:59:41.000Z",
    "title": "\"Principal Components\" Enable A New Language of Images",
    "summary": "We introduce a novel visual tokenization framework that embeds a provable\nPCA-like structure into the latent token space. While existing visual\ntokenizers primarily optimize for reconstruction fidelity, they often neglect\nthe structural properties of the latent space -- a critical factor for both\ninterpretability and downstream tasks. Our method generates a 1D causal token\nsequence for images, where each successive token contributes non-overlapping\ninformation with mathematically guaranteed decreasing explained variance,\nanalogous to principal component analysis. This structural constraint ensures\nthe tokenizer extracts the most salient visual features first, with each\nsubsequent token adding diminishing yet complementary information.\nAdditionally, we identified and resolved a semantic-spectrum coupling effect\nthat causes the unwanted entanglement of high-level semantic content and\nlow-level spectral details in the tokens by leveraging a diffusion decoder.\nExperiments demonstrate that our approach achieves state-of-the-art\nreconstruction performance and enables better interpretability to align with\nthe human vision system. Moreover, auto-regressive models trained on our token\nsequences achieve performance comparable to current state-of-the-art methods\nwhile requiring fewer tokens for training and inference.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08685.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07699",
      "authors": [
        {
          "_id": "67d114912264403cbf39d0ba",
          "name": "Huiyang Shao",
          "hidden": false
        },
        {
          "_id": "67d114912264403cbf39d0bb",
          "name": "Xin Xia",
          "hidden": false
        },
        {
          "_id": "67d114912264403cbf39d0bc",
          "name": "Yuhong Yang",
          "hidden": false
        },
        {
          "_id": "67d114912264403cbf39d0bd",
          "name": "Yuxi Ren",
          "hidden": false
        },
        {
          "_id": "67d114912264403cbf39d0be",
          "name": "Xing Wang",
          "hidden": false
        },
        {
          "_id": "67d114912264403cbf39d0bf",
          "name": "Xuefeng Xiao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-10T17:20:52.000Z",
      "title": "RayFlow: Instance-Aware Diffusion Acceleration via Adaptive Flow\n  Trajectories",
      "summary": "Diffusion models have achieved remarkable success across various domains.\nHowever, their slow generation speed remains a critical challenge. Existing\nacceleration methods, while aiming to reduce steps, often compromise sample\nquality, controllability, or introduce training complexities. Therefore, we\npropose RayFlow, a novel diffusion framework that addresses these limitations.\nUnlike previous methods, RayFlow guides each sample along a unique path towards\nan instance-specific target distribution. This method minimizes sampling steps\nwhile preserving generation diversity and stability. Furthermore, we introduce\nTime Sampler, an importance sampling technique to enhance training efficiency\nby focusing on crucial timesteps. Extensive experiments demonstrate RayFlow's\nsuperiority in generating high-quality images with improved speed, control, and\ntraining efficiency compared to existing acceleration techniques.",
      "upvotes": 3,
      "discussionId": "67d114922264403cbf39d0f8",
      "ai_keywords": [
        "diffusion models",
        "generation speed",
        "RayFlow",
        "instance-specific target distribution",
        "sampling steps",
        "generation diversity",
        "stability",
        "Time Sampler",
        "importance sampling",
        "training efficiency",
        "high-quality images"
      ]
    },
    "publishedAt": "2025-03-10T13:20:52.000Z",
    "title": "RayFlow: Instance-Aware Diffusion Acceleration via Adaptive Flow\n  Trajectories",
    "summary": "Diffusion models have achieved remarkable success across various domains.\nHowever, their slow generation speed remains a critical challenge. Existing\nacceleration methods, while aiming to reduce steps, often compromise sample\nquality, controllability, or introduce training complexities. Therefore, we\npropose RayFlow, a novel diffusion framework that addresses these limitations.\nUnlike previous methods, RayFlow guides each sample along a unique path towards\nan instance-specific target distribution. This method minimizes sampling steps\nwhile preserving generation diversity and stability. Furthermore, we introduce\nTime Sampler, an importance sampling technique to enhance training efficiency\nby focusing on crucial timesteps. Extensive experiments demonstrate RayFlow's\nsuperiority in generating high-quality images with improved speed, control, and\ntraining efficiency compared to existing acceleration techniques.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07699.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.07639",
      "authors": [
        {
          "_id": "67d0e3ede3afecf451915d0a",
          "name": "Xingyi Yang",
          "hidden": false
        },
        {
          "_id": "67d0e3ede3afecf451915d0b",
          "name": "Constantin Venhoff",
          "hidden": false
        },
        {
          "_id": "67d0e3ede3afecf451915d0c",
          "name": "Ashkan Khakzar",
          "hidden": false
        },
        {
          "_id": "67d0e3ede3afecf451915d0d",
          "name": "Christian Schroeder de Witt",
          "hidden": false
        },
        {
          "_id": "67d0e3ede3afecf451915d0e",
          "name": "Puneet K. Dokania",
          "hidden": false
        },
        {
          "_id": "67d0e3ede3afecf451915d0f",
          "name": "Adel Bibi",
          "hidden": false
        },
        {
          "_id": "67d0e3ede3afecf451915d10",
          "name": "Philip Torr",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-05T17:40:54.000Z",
      "title": "Mixture of Experts Made Intrinsically Interpretable",
      "summary": "Neurons in large language models often exhibit polysemanticity,\nsimultaneously encoding multiple unrelated concepts and obscuring\ninterpretability. Instead of relying on post-hoc methods, we present\nMoE-X, a Mixture-of-Experts (MoE) language model designed to be\nintrinsically interpretable. Our approach is motivated by the\nobservation that, in language models, wider networks with sparse activations\nare more likely to capture interpretable factors. However, directly training\nsuch large sparse networks is computationally prohibitive. MoE architectures\noffer a scalable alternative by activating only a subset of experts for any\ngiven input, inherently aligning with interpretability objectives. In MoE-X, we\nestablish this connection by rewriting the MoE layer as an equivalent sparse,\nlarge MLP. This approach enables efficient scaling of the hidden size while\nmaintaining sparsity. To further enhance interpretability, we enforce sparse\nactivation within each expert and redesign the routing mechanism to prioritize\nexperts with the highest activation sparsity. These designs ensure that only\nthe most salient features are routed and processed by the experts. We evaluate\nMoE-X on chess and natural language tasks, showing that it achieves performance\ncomparable to dense models while significantly improving interpretability.\nMoE-X achieves a perplexity better than GPT-2, with interpretability surpassing\neven sparse autoencoder (SAE)-based approaches.",
      "upvotes": 2,
      "discussionId": "67d0e3f0e3afecf451915dfa",
      "ai_keywords": [
        "polysemanticity",
        "Mixture-of-Experts (MoE)",
        "interpretable",
        "sparse activations",
        "sparsity",
        "sparse networks",
        "hidden size",
        "sparse activation",
        "routing mechanism",
        "salient features",
        "perplexity",
        "GPT-2",
        "sparse autoencoder (SAE)"
      ]
    },
    "publishedAt": "2025-03-05T12:40:54.000Z",
    "title": "Mixture of Experts Made Intrinsically Interpretable",
    "summary": "Neurons in large language models often exhibit polysemanticity,\nsimultaneously encoding multiple unrelated concepts and obscuring\ninterpretability. Instead of relying on post-hoc methods, we present\nMoE-X, a Mixture-of-Experts (MoE) language model designed to be\nintrinsically interpretable. Our approach is motivated by the\nobservation that, in language models, wider networks with sparse activations\nare more likely to capture interpretable factors. However, directly training\nsuch large sparse networks is computationally prohibitive. MoE architectures\noffer a scalable alternative by activating only a subset of experts for any\ngiven input, inherently aligning with interpretability objectives. In MoE-X, we\nestablish this connection by rewriting the MoE layer as an equivalent sparse,\nlarge MLP. This approach enables efficient scaling of the hidden size while\nmaintaining sparsity. To further enhance interpretability, we enforce sparse\nactivation within each expert and redesign the routing mechanism to prioritize\nexperts with the highest activation sparsity. These designs ensure that only\nthe most salient features are routed and processed by the experts. We evaluate\nMoE-X on chess and natural language tasks, showing that it achieves performance\ncomparable to dense models while significantly improving interpretability.\nMoE-X achieves a perplexity better than GPT-2, with interpretability surpassing\neven sparse autoencoder (SAE)-based approaches.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.07639.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2502.18858",
      "authors": [
        {
          "_id": "67d1080b2264403cbf36b0ad",
          "name": "Jingtao Zhan",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0ae",
          "name": "Jiahao Zhao",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0af",
          "name": "Jiayu Li",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b0",
          "name": "Yiqun Liu",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b1",
          "name": "Bo Zhang",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b2",
          "name": "Qingyao Ai",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b3",
          "name": "Jiaxin Mao",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b4",
          "name": "Hongning Wang",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b5",
          "name": "Min Zhang",
          "hidden": false
        },
        {
          "_id": "67d1080b2264403cbf36b0b6",
          "name": "Shaoping Ma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-02-26T05:59:45.000Z",
      "title": "Evaluating Intelligence via Trial and Error",
      "summary": "Intelligence is a crucial trait for species to find solutions within a\nlimited number of trial-and-error attempts. Building on this idea, we introduce\nSurvival Game as a framework to evaluate intelligence based on the number of\nfailed attempts in a trial-and-error process. Fewer failures indicate higher\nintelligence. When the expectation and variance of failure counts are both\nfinite, it signals the ability to consistently find solutions to new\nchallenges, which we define as the Autonomous Level of intelligence. Using\nSurvival Game, we comprehensively evaluate existing AI systems. Our results\nshow that while AI systems achieve the Autonomous Level in simple tasks, they\nare still far from it in more complex tasks, such as vision, search,\nrecommendation, and language. While scaling current AI technologies might help,\nthis would come at an astronomical cost. Projections suggest that achieving the\nAutonomous Level for general tasks would require 10^{26} parameters. To put\nthis into perspective, loading such a massive model requires so many H100 GPUs\nthat their total value is 10^{7} times that of Apple Inc.'s market value.\nEven with Moore's Law, supporting such a parameter scale would take 70 years.\nThis staggering cost highlights the complexity of human tasks and the\ninadequacies of current AI technologies. To further investigate this\nphenomenon, we conduct a theoretical analysis of Survival Game and its\nexperimental results. Our findings suggest that human tasks possess a\ncriticality property. As a result, Autonomous Level requires a deep\nunderstanding of the task's underlying mechanisms. Current AI systems, however,\ndo not fully grasp these mechanisms and instead rely on superficial mimicry,\nmaking it difficult for them to reach an autonomous level. We believe Survival\nGame can not only guide the future development of AI but also offer profound\ninsights into human intelligence.",
      "upvotes": 2,
      "discussionId": "67d108112264403cbf36b1e9",
      "githubRepo": "https://github.com/jingtaozhan/IntelligenceTest"
    },
    "publishedAt": "2025-02-26T00:59:45.000Z",
    "title": "Evaluating Intelligence via Trial and Error",
    "summary": "Intelligence is a crucial trait for species to find solutions within a\nlimited number of trial-and-error attempts. Building on this idea, we introduce\nSurvival Game as a framework to evaluate intelligence based on the number of\nfailed attempts in a trial-and-error process. Fewer failures indicate higher\nintelligence. When the expectation and variance of failure counts are both\nfinite, it signals the ability to consistently find solutions to new\nchallenges, which we define as the Autonomous Level of intelligence. Using\nSurvival Game, we comprehensively evaluate existing AI systems. Our results\nshow that while AI systems achieve the Autonomous Level in simple tasks, they\nare still far from it in more complex tasks, such as vision, search,\nrecommendation, and language. While scaling current AI technologies might help,\nthis would come at an astronomical cost. Projections suggest that achieving the\nAutonomous Level for general tasks would require 10^{26} parameters. To put\nthis into perspective, loading such a massive model requires so many H100 GPUs\nthat their total value is 10^{7} times that of Apple Inc.'s market value.\nEven with Moore's Law, supporting such a parameter scale would take 70 years.\nThis staggering cost highlights the complexity of human tasks and the\ninadequacies of current AI technologies. To further investigate this\nphenomenon, we conduct a theoretical analysis of Survival Game and its\nexperimental results. Our findings suggest that human tasks possess a\ncriticality property. As a result, Autonomous Level requires a deep\nunderstanding of the task's underlying mechanisms. Current AI systems, however,\ndo not fully grasp these mechanisms and instead rely on superficial mimicry,\nmaking it difficult for them to reach an autonomous level. We believe Survival\nGame can not only guide the future development of AI but also offer profound\ninsights into human intelligence.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.18858.png",
    "numComments": 2,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08102",
      "authors": [
        {
          "_id": "67d12c32428a3d8d5281f310",
          "name": "Jiale Wei",
          "hidden": false
        },
        {
          "_id": "67d12c32428a3d8d5281f311",
          "name": "Xiang Ying",
          "hidden": false
        },
        {
          "_id": "67d12c32428a3d8d5281f312",
          "name": "Tao Gao",
          "hidden": false
        },
        {
          "_id": "67d12c32428a3d8d5281f313",
          "name": "Felix Tao",
          "hidden": false
        },
        {
          "_id": "67d12c32428a3d8d5281f314",
          "name": "Jingbo Shang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T07:05:52.000Z",
      "title": "AI-native Memory 2.0: Second Me",
      "summary": "Human interaction with the external world fundamentally involves the exchange\nof personal memory, whether with other individuals, websites, applications, or,\nin the future, AI agents. A significant portion of this interaction is\nredundant, requiring users to repeatedly provide the same information across\ndifferent contexts. Existing solutions, such as browser-stored credentials,\nautofill mechanisms, and unified authentication systems, have aimed to mitigate\nthis redundancy by serving as intermediaries that store and retrieve commonly\nused user data. The advent of large language models (LLMs) presents an\nopportunity to redefine memory management through an AI-native paradigm: SECOND\nME. SECOND ME acts as an intelligent, persistent memory offload system that\nretains, organizes, and dynamically utilizes user-specific knowledge. By\nserving as an intermediary in user interactions, it can autonomously generate\ncontext-aware responses, prefill required information, and facilitate seamless\ncommunication with external systems, significantly reducing cognitive load and\ninteraction friction. Unlike traditional memory storage solutions, SECOND ME\nextends beyond static data retention by leveraging LLM-based memory\nparameterization. This enables structured organization, contextual reasoning,\nand adaptive knowledge retrieval, facilitating a more systematic and\nintelligent approach to memory management. As AI-driven personal agents like\nSECOND ME become increasingly integrated into digital ecosystems, SECOND ME\nfurther represents a critical step toward augmenting human-world interaction\nwith persistent, contextually aware, and self-optimizing memory systems. We\nhave open-sourced the fully localizable deployment system at GitHub:\nhttps://github.com/Mindverse/Second-Me.",
      "upvotes": 1,
      "discussionId": "67d12c33428a3d8d5281f346",
      "ai_keywords": [
        "large language models (LLMs)",
        "intelligent, persistent memory offload system",
        "context-aware responses",
        "structured organization",
        "contextual reasoning",
        "adaptive knowledge retrieval",
        "self-optimizing memory systems"
      ]
    },
    "publishedAt": "2025-03-11T03:05:52.000Z",
    "title": "AI-native Memory 2.0: Second Me",
    "summary": "Human interaction with the external world fundamentally involves the exchange\nof personal memory, whether with other individuals, websites, applications, or,\nin the future, AI agents. A significant portion of this interaction is\nredundant, requiring users to repeatedly provide the same information across\ndifferent contexts. Existing solutions, such as browser-stored credentials,\nautofill mechanisms, and unified authentication systems, have aimed to mitigate\nthis redundancy by serving as intermediaries that store and retrieve commonly\nused user data. The advent of large language models (LLMs) presents an\nopportunity to redefine memory management through an AI-native paradigm: SECOND\nME. SECOND ME acts as an intelligent, persistent memory offload system that\nretains, organizes, and dynamically utilizes user-specific knowledge. By\nserving as an intermediary in user interactions, it can autonomously generate\ncontext-aware responses, prefill required information, and facilitate seamless\ncommunication with external systems, significantly reducing cognitive load and\ninteraction friction. Unlike traditional memory storage solutions, SECOND ME\nextends beyond static data retention by leveraging LLM-based memory\nparameterization. This enables structured organization, contextual reasoning,\nand adaptive knowledge retrieval, facilitating a more systematic and\nintelligent approach to memory management. As AI-driven personal agents like\nSECOND ME become increasingly integrated into digital ecosystems, SECOND ME\nfurther represents a critical step toward augmenting human-world interaction\nwith persistent, contextually aware, and self-optimizing memory systems. We\nhave open-sourced the fully localizable deployment system at GitHub:\nhttps://github.com/Mindverse/Second-Me.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08102.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.08478",
      "authors": [
        {
          "_id": "67d12d0b44be28339053b965",
          "name": "Han-Wei Kung",
          "hidden": false
        },
        {
          "_id": "67d12d0b44be28339053b966",
          "name": "Tuomas Varanka",
          "hidden": false
        },
        {
          "_id": "67d12d0b44be28339053b967",
          "name": "Terence Sim",
          "hidden": false
        },
        {
          "_id": "67d12d0b44be28339053b968",
          "name": "Nicu Sebe",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-11T14:29:37.000Z",
      "title": "NullFace: Training-Free Localized Face Anonymization",
      "summary": "Privacy concerns around ever increasing number of cameras are increasing in\ntoday's digital age. Although existing anonymization methods are able to\nobscure identity information, they often struggle to preserve the utility of\nthe images. In this work, we introduce a training-free method for face\nanonymization that preserves key non-identity-related attributes. Our approach\nutilizes a pre-trained text-to-image diffusion model without requiring\noptimization or training. It begins by inverting the input image to recover its\ninitial noise. The noise is then denoised through an identity-conditioned\ndiffusion process, where modified identity embeddings ensure the anonymized\nface is distinct from the original identity. Our approach also supports\nlocalized anonymization, giving users control over which facial regions are\nanonymized or kept intact. Comprehensive evaluations against state-of-the-art\nmethods show our approach excels in anonymization, attribute preservation, and\nimage quality. Its flexibility, robustness, and practicality make it\nwell-suited for real-world applications. Code and data can be found at\nhttps://github.com/hanweikung/nullface .",
      "upvotes": 0,
      "discussionId": "67d12d1044be28339053baab",
      "ai_keywords": [
        "text-to-image diffusion model",
        "identity-conditioned diffusion",
        "identity embeddings",
        "localized anonymization"
      ]
    },
    "publishedAt": "2025-03-11T10:29:37.000Z",
    "title": "NullFace: Training-Free Localized Face Anonymization",
    "summary": "Privacy concerns around ever increasing number of cameras are increasing in\ntoday's digital age. Although existing anonymization methods are able to\nobscure identity information, they often struggle to preserve the utility of\nthe images. In this work, we introduce a training-free method for face\nanonymization that preserves key non-identity-related attributes. Our approach\nutilizes a pre-trained text-to-image diffusion model without requiring\noptimization or training. It begins by inverting the input image to recover its\ninitial noise. The noise is then denoised through an identity-conditioned\ndiffusion process, where modified identity embeddings ensure the anonymized\nface is distinct from the original identity. Our approach also supports\nlocalized anonymization, giving users control over which facial regions are\nanonymized or kept intact. Comprehensive evaluations against state-of-the-art\nmethods show our approach excels in anonymization, attribute preservation, and\nimage quality. Its flexibility, robustness, and practicality make it\nwell-suited for real-world applications. Code and data can be found at\nhttps://github.com/hanweikung/nullface .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.08478.png",
    "numComments": 1,
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2503.06492",
      "authors": [
        {
          "_id": "67cfe557ad91643b5cb7d2c6",
          "user": {
            "_id": "67cd327432668b04f4555270",
            "avatarUrl": "/avatars/15e2cef976cbe05c4c5858c88dccf4af.svg",
            "isPro": false,
            "fullname": "Yanling Wang",
            "user": "WYLing",
            "type": "user"
          },
          "name": "Yanling Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T16:09:28.071Z",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2c7",
          "name": "Yihan Zhao",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2c8",
          "name": "Xiaodong Chen",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2c9",
          "name": "Shasha Guo",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2ca",
          "name": "Lixin Liu",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2cb",
          "name": "Haoyang Li",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2cc",
          "name": "Yong Xiao",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2cd",
          "name": "Jing Zhang",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2ce",
          "name": "Qi Li",
          "hidden": false
        },
        {
          "_id": "67cfe557ad91643b5cb7d2cf",
          "name": "Ke Xu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-09T07:25:32.000Z",
      "title": "VisualSimpleQA: A Benchmark for Decoupled Evaluation of Large\n  Vision-Language Models in Fact-Seeking Question Answering",
      "summary": "Large vision-language models (LVLMs) have demonstrated remarkable\nachievements, yet the generation of non-factual responses remains prevalent in\nfact-seeking question answering (QA). Current multimodal fact-seeking\nbenchmarks primarily focus on comparing model outputs to ground truth answers,\nproviding limited insights into the performance of modality-specific modules.\nTo bridge this gap, we introduce VisualSimpleQA, a multimodal fact-seeking\nbenchmark with two key features. First, it enables streamlined and decoupled\nevaluation of LVLMs in visual and linguistic modalities. Second, it\nincorporates well-defined difficulty criteria to guide human annotation and\nfacilitates the extraction of a challenging subset, VisualSimpleQA-hard.\nExperiments on 15 LVLMs show that even state-of-the-art models such as GPT-4o\nachieve merely 60%+ correctness in multimodal fact-seeking QA on VisualSimpleQA\nand 30%+ on VisualSimpleQA-hard. Furthermore, the decoupled evaluation across\nthese models highlights substantial opportunities for improvement in both\nvisual and linguistic modules. The dataset is available at\nhttps://huggingface.co/datasets/WYLing/VisualSimpleQA.",
      "upvotes": 0,
      "discussionId": "67cfe55bad91643b5cb7d3fb",
      "ai_keywords": [
        "Large vision-language models",
        "fact-seeking question answering",
        "multimodal benchmarks",
        "visual modality",
        "linguistic modality",
        "VisualSimpleQA",
        "VisualSimpleQA-hard",
        "GPT-4",
        "multimodal fact-seeking QA",
        "decoupled evaluation"
      ]
    },
    "publishedAt": "2025-03-09T03:25:32.000Z",
    "title": "VisualSimpleQA: A Benchmark for Decoupled Evaluation of Large\n  Vision-Language Models in Fact-Seeking Question Answering",
    "summary": "Large vision-language models (LVLMs) have demonstrated remarkable\nachievements, yet the generation of non-factual responses remains prevalent in\nfact-seeking question answering (QA). Current multimodal fact-seeking\nbenchmarks primarily focus on comparing model outputs to ground truth answers,\nproviding limited insights into the performance of modality-specific modules.\nTo bridge this gap, we introduce VisualSimpleQA, a multimodal fact-seeking\nbenchmark with two key features. First, it enables streamlined and decoupled\nevaluation of LVLMs in visual and linguistic modalities. Second, it\nincorporates well-defined difficulty criteria to guide human annotation and\nfacilitates the extraction of a challenging subset, VisualSimpleQA-hard.\nExperiments on 15 LVLMs show that even state-of-the-art models such as GPT-4o\nachieve merely 60%+ correctness in multimodal fact-seeking QA on VisualSimpleQA\nand 30%+ on VisualSimpleQA-hard. Furthermore, the decoupled evaluation across\nthese models highlights substantial opportunities for improvement in both\nvisual and linguistic modules. The dataset is available at\nhttps://huggingface.co/datasets/WYLing/VisualSimpleQA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.06492.png",
    "numComments": 1,
    "isAuthorParticipating": true
  }
]