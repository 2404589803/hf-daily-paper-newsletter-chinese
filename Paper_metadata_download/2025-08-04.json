[
  {
    "paper": {
      "id": "2508.00819",
      "authors": [
        {
          "_id": "689020e10a411b3b8d28d67f",
          "name": "Jinsong Li",
          "hidden": false
        },
        {
          "_id": "689020e10a411b3b8d28d680",
          "name": "Xiaoyi Dong",
          "hidden": false
        },
        {
          "_id": "689020e10a411b3b8d28d681",
          "name": "Yuhang Zang",
          "hidden": false
        },
        {
          "_id": "689020e10a411b3b8d28d682",
          "name": "Yuhang Cao",
          "hidden": false
        },
        {
          "_id": "689020e10a411b3b8d28d683",
          "name": "Jiaqi Wang",
          "hidden": false
        },
        {
          "_id": "689020e10a411b3b8d28d684",
          "name": "Dahua Lin",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-01T17:56:07.000Z",
      "submittedOnDailyAt": "2025-08-04T01:25:12.035Z",
      "title": "Beyond Fixed: Variable-Length Denoising for Diffusion Large Language\n  Models",
      "submittedOnDailyBy": {
        "_id": "64b4eec4faa3181a5eab9c46",
        "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
        "isPro": true,
        "fullname": "Jiaqi Wang",
        "user": "myownskyW7",
        "type": "user"
      },
      "summary": "Diffusion Large Language Models (DLLMs) are emerging as a powerful\nalternative to the dominant Autoregressive Large Language Models, offering\nefficient parallel generation and capable global context modeling. However, the\npractical application of DLLMs is hindered by a critical architectural\nconstraint: the need for a statically predefined generation length. This static\nlength allocation leads to a problematic trade-off: insufficient lengths\ncripple performance on complex tasks, while excessive lengths incur significant\ncomputational overhead and sometimes result in performance degradation. While\nthe inference framework is rigid, we observe that the model itself possesses\ninternal signals that correlate with the optimal response length for a given\ntask. To bridge this gap, we leverage these latent signals and introduce\nDAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive\nLength Expansion for Diffusion Large Language Models. DAEDAL operates in two\nphases: 1) Before the denoising process, DAEDAL starts from a short initial\nlength and iteratively expands it to a coarse task-appropriate length, guided\nby a sequence completion metric. 2) During the denoising process, DAEDAL\ndynamically intervenes by pinpointing and expanding insufficient generation\nregions through mask token insertion, ensuring the final output is fully\ndeveloped. Extensive experiments on DLLMs demonstrate that DAEDAL achieves\nperformance comparable, and in some cases superior, to meticulously tuned\nfixed-length baselines, while simultaneously enhancing computational efficiency\nby achieving a higher effective token ratio. By resolving the static length\nconstraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap\nwith their Autoregressive counterparts and paving the way for more efficient\nand capable generation.",
      "upvotes": 21,
      "discussionId": "689020e20a411b3b8d28d685",
      "ai_summary": "DAEDAL, a novel training-free denoising strategy, enables dynamic length adaptation in Diffusion Large Language Models, improving performance and computational efficiency.",
      "ai_keywords": [
        "Diffusion Large Language Models",
        "DLLMs",
        "Autoregressive Large Language Models",
        "denoising strategy",
        "Dynamic Adaptive Length Expansion",
        "sequence completion metric",
        "mask token insertion",
        "effective token ratio"
      ]
    },
    "publishedAt": "2025-08-01T13:56:07.000Z",
    "title": "Beyond Fixed: Variable-Length Denoising for Diffusion Large Language\n  Models",
    "summary": "Diffusion Large Language Models (DLLMs) are emerging as a powerful\nalternative to the dominant Autoregressive Large Language Models, offering\nefficient parallel generation and capable global context modeling. However, the\npractical application of DLLMs is hindered by a critical architectural\nconstraint: the need for a statically predefined generation length. This static\nlength allocation leads to a problematic trade-off: insufficient lengths\ncripple performance on complex tasks, while excessive lengths incur significant\ncomputational overhead and sometimes result in performance degradation. While\nthe inference framework is rigid, we observe that the model itself possesses\ninternal signals that correlate with the optimal response length for a given\ntask. To bridge this gap, we leverage these latent signals and introduce\nDAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive\nLength Expansion for Diffusion Large Language Models. DAEDAL operates in two\nphases: 1) Before the denoising process, DAEDAL starts from a short initial\nlength and iteratively expands it to a coarse task-appropriate length, guided\nby a sequence completion metric. 2) During the denoising process, DAEDAL\ndynamically intervenes by pinpointing and expanding insufficient generation\nregions through mask token insertion, ensuring the final output is fully\ndeveloped. Extensive experiments on DLLMs demonstrate that DAEDAL achieves\nperformance comparable, and in some cases superior, to meticulously tuned\nfixed-length baselines, while simultaneously enhancing computational efficiency\nby achieving a higher effective token ratio. By resolving the static length\nconstraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap\nwith their Autoregressive counterparts and paving the way for more efficient\nand capable generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.00819.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b4eec4faa3181a5eab9c46",
      "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
      "fullname": "Jiaqi Wang",
      "name": "myownskyW7",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.23268",
      "authors": [
        {
          "_id": "688c1ec68c434640078cc386",
          "name": "Shuai Wang",
          "hidden": false
        },
        {
          "_id": "688c1ec68c434640078cc387",
          "name": "Ziteng Gao",
          "hidden": false
        },
        {
          "_id": "688c1ec68c434640078cc388",
          "name": "Chenhui Zhu",
          "hidden": false
        },
        {
          "_id": "688c1ec68c434640078cc389",
          "name": "Weilin Huang",
          "hidden": false
        },
        {
          "_id": "688c1ec68c434640078cc38a",
          "name": "Limin Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-31T06:07:20.000Z",
      "submittedOnDailyAt": "2025-08-04T00:45:14.702Z",
      "title": "PixNerd: Pixel Neural Field Diffusion",
      "submittedOnDailyBy": {
        "_id": "66615c855fd9d736e670e0a9",
        "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
        "isPro": false,
        "fullname": "wangshuai",
        "user": "wangsssssss",
        "type": "user"
      },
      "summary": "The current success of diffusion transformers heavily depends on the\ncompressed latent space shaped by the pre-trained variational autoencoder(VAE).\nHowever, this two-stage training paradigm inevitably introduces accumulated\nerrors and decoding artifacts. To address the aforementioned problems,\nresearchers return to pixel space at the cost of complicated cascade pipelines\nand increased token complexity. In contrast to their efforts, we propose to\nmodel the patch-wise decoding with neural field and present a single-scale,\nsingle-stage, efficient, end-to-end solution, coined as pixel neural field\ndiffusion~(PixelNerd). Thanks to the efficient neural field representation in\nPixNerd, we directly achieved 2.15 FID on ImageNet 256times256 and 2.84 FID\non ImageNet 512times512 without any complex cascade pipeline or VAE. We also\nextend our PixNerd framework to text-to-image applications. Our PixNerd-XXL/16\nachieved a competitive 0.73 overall score on the GenEval benchmark and 80.9\noverall score on the DPG benchmark.",
      "upvotes": 14,
      "discussionId": "688c1ec68c434640078cc38b",
      "projectPage": "https://huggingface.co/spaces/MCG-NJU/PixNerd",
      "githubRepo": "https://github.com/MCG-NJU/PixNerd",
      "ai_summary": "Pixel Neural Field Diffusion (PixNerd) achieves high-quality image generation in a single-scale, single-stage process without VAEs or complex pipelines, and extends to text-to-image applications with competitive performance.",
      "ai_keywords": [
        "diffusion transformers",
        "compressed latent space",
        "pre-trained variational autoencoder",
        "pixel space",
        "patch-wise decoding",
        "neural field",
        "single-scale",
        "single-stage",
        "end-to-end solution",
        "pixel neural field diffusion",
        "PixNerd",
        "FID",
        "ImageNet",
        "text-to-image",
        "GenEval benchmark",
        "DPG benchmark"
      ],
      "githubStars": 13
    },
    "publishedAt": "2025-07-31T02:07:20.000Z",
    "title": "PixNerd: Pixel Neural Field Diffusion",
    "summary": "The current success of diffusion transformers heavily depends on the\ncompressed latent space shaped by the pre-trained variational autoencoder(VAE).\nHowever, this two-stage training paradigm inevitably introduces accumulated\nerrors and decoding artifacts. To address the aforementioned problems,\nresearchers return to pixel space at the cost of complicated cascade pipelines\nand increased token complexity. In contrast to their efforts, we propose to\nmodel the patch-wise decoding with neural field and present a single-scale,\nsingle-stage, efficient, end-to-end solution, coined as pixel neural field\ndiffusion~(PixelNerd). Thanks to the efficient neural field representation in\nPixNerd, we directly achieved 2.15 FID on ImageNet 256times256 and 2.84 FID\non ImageNet 512times512 without any complex cascade pipeline or VAE. We also\nextend our PixNerd framework to text-to-image applications. Our PixNerd-XXL/16\nachieved a competitive 0.73 overall score on the GenEval benchmark and 80.9\noverall score on the DPG benchmark.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.23268.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "66615c855fd9d736e670e0a9",
      "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
      "fullname": "wangshuai",
      "name": "wangsssssss",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.23361",
      "authors": [
        {
          "_id": "688c53f38c434640078cc47c",
          "name": "Silin Chen",
          "hidden": false
        },
        {
          "_id": "688c53f38c434640078cc47d",
          "name": "Shaoxin Lin",
          "hidden": false
        },
        {
          "_id": "688c53f38c434640078cc47e",
          "name": "Xiaodong Gu",
          "hidden": false
        },
        {
          "_id": "688c53f38c434640078cc47f",
          "name": "Yuling Shi",
          "hidden": false
        },
        {
          "_id": "688c53f38c434640078cc480",
          "name": "Heng Lian",
          "hidden": false
        },
        {
          "_id": "688c53f38c434640078cc481",
          "name": "Longfei Yun",
          "hidden": false
        },
        {
          "_id": "688c53f38c434640078cc482",
          "name": "Dong Chen",
          "hidden": false
        },
        {
          "_id": "688c53f38c434640078cc483",
          "name": "Weiguo Sun",
          "hidden": false
        },
        {
          "_id": "688c53f38c434640078cc484",
          "name": "Lin Cao",
          "hidden": false
        },
        {
          "_id": "688c53f38c434640078cc485",
          "name": "Qianxiang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-31T09:13:42.000Z",
      "submittedOnDailyAt": "2025-08-04T01:12:58.702Z",
      "title": "SWE-Exp: Experience-Driven Software Issue Resolution",
      "submittedOnDailyBy": {
        "_id": "645b0c3ec35da9c7afd95421",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
        "isPro": false,
        "fullname": "Yuling",
        "user": "YerbaPage",
        "type": "user"
      },
      "summary": "Recent advances in large language model (LLM) agents have shown remarkable\nprogress in software issue resolution, leveraging advanced techniques such as\nmulti-agent collaboration and Monte Carlo Tree Search (MCTS). However, current\nagents act as memoryless explorers - treating each problem separately without\nretaining or reusing knowledge from previous repair experiences. This leads to\nredundant exploration of failed trajectories and missed chances to adapt\nsuccessful issue resolution methods to similar problems. To address this\nproblem, we introduce SWE-Exp, an experience - enhanced approach that distills\nconcise and actionable experience from prior agent trajectories, enabling\ncontinuous learning across issues. Our method introduces a multi-faceted\nexperience bank that captures both successful and failed repair attempts.\nSpecifically, it extracts reusable issue resolution knowledge at different\nlevels - from high-level problem comprehension to specific code changes.\nExperiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6%\nPass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach\nestablishes a new paradigm in which automated software engineering agents\nsystematically accumulate and leverage repair expertise, fundamentally shifting\nfrom trial-and-error exploration to strategic, experience-driven issue\nresolution.",
      "upvotes": 4,
      "discussionId": "688c53f38c434640078cc486",
      "githubRepo": "https://github.com/YerbaPage/SWE-Exp",
      "ai_summary": "SWE-Exp enhances software issue resolution by systematically accumulating and leveraging repair expertise from past agent experiences, improving resolution rates.",
      "ai_keywords": [
        "large language model (LLM)",
        "multi-agent collaboration",
        "Monte Carlo Tree Search (MCTS)",
        "experience bank",
        "issue resolution knowledge",
        "SWE-bench-Verified",
        "open-source agent frameworks"
      ],
      "githubStars": 6
    },
    "publishedAt": "2025-07-31T05:13:42.000Z",
    "title": "SWE-Exp: Experience-Driven Software Issue Resolution",
    "summary": "Recent advances in large language model (LLM) agents have shown remarkable\nprogress in software issue resolution, leveraging advanced techniques such as\nmulti-agent collaboration and Monte Carlo Tree Search (MCTS). However, current\nagents act as memoryless explorers - treating each problem separately without\nretaining or reusing knowledge from previous repair experiences. This leads to\nredundant exploration of failed trajectories and missed chances to adapt\nsuccessful issue resolution methods to similar problems. To address this\nproblem, we introduce SWE-Exp, an experience - enhanced approach that distills\nconcise and actionable experience from prior agent trajectories, enabling\ncontinuous learning across issues. Our method introduces a multi-faceted\nexperience bank that captures both successful and failed repair attempts.\nSpecifically, it extracts reusable issue resolution knowledge at different\nlevels - from high-level problem comprehension to specific code changes.\nExperiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6%\nPass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach\nestablishes a new paradigm in which automated software engineering agents\nsystematically accumulate and leverage repair expertise, fundamentally shifting\nfrom trial-and-error exploration to strategic, experience-driven issue\nresolution.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.23361.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b0c3ec35da9c7afd95421",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
      "fullname": "Yuling",
      "name": "YerbaPage",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 274
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.23348",
      "authors": [
        {
          "_id": "688c53d88c434640078cc471",
          "name": "Han Li",
          "hidden": false
        },
        {
          "_id": "688c53d88c434640078cc472",
          "name": "Yuling Shi",
          "hidden": false
        },
        {
          "_id": "688c53d88c434640078cc473",
          "name": "Shaoxin Lin",
          "hidden": false
        },
        {
          "_id": "688c53d88c434640078cc474",
          "name": "Xiaodong Gu",
          "hidden": false
        },
        {
          "_id": "688c53d88c434640078cc475",
          "name": "Heng Lian",
          "hidden": false
        },
        {
          "_id": "688c53d88c434640078cc476",
          "name": "Xin Wang",
          "hidden": false
        },
        {
          "_id": "688c53d88c434640078cc477",
          "name": "Yantao Jia",
          "hidden": false
        },
        {
          "_id": "688c53d88c434640078cc478",
          "name": "Tao Huang",
          "hidden": false
        },
        {
          "_id": "688c53d88c434640078cc479",
          "name": "Qianxiang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-31T08:54:46.000Z",
      "submittedOnDailyAt": "2025-08-04T01:11:57.584Z",
      "title": "SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution",
      "submittedOnDailyBy": {
        "_id": "645b0c3ec35da9c7afd95421",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
        "isPro": false,
        "fullname": "Yuling",
        "user": "YerbaPage",
        "type": "user"
      },
      "summary": "Issue resolution has made remarkable progress thanks to the advanced\nreasoning capabilities of large language models (LLMs). Recently, agent-based\nframeworks such as SWE-agent have further advanced this progress by enabling\nautonomous, tool-using agents to tackle complex software engineering tasks.\nWhile existing agent-based issue resolution approaches are primarily based on\nagents' independent explorations, they often get stuck in local solutions and\nfail to identify issue patterns that span across different parts of the\ncodebase. To address this limitation, we propose SWE-Debate, a competitive\nmulti-agent debate framework that encourages diverse reasoning paths and\nachieves more consolidated issue localization. SWE-Debate first creates\nmultiple fault propagation traces as localization proposals by traversing a\ncode dependency graph. Then, it organizes a three-round debate among\nspecialized agents, each embodying distinct reasoning perspectives along the\nfault propagation trace. This structured competition enables agents to\ncollaboratively converge on a consolidated fix plan. Finally, this consolidated\nfix plan is integrated into an MCTS-based code modification agent for patch\ngeneration. Experiments on the SWE-bench benchmark show that SWE-Debate\nachieves new state-of-the-art results in open-source agent frameworks and\noutperforms baselines by a large margin.",
      "upvotes": 4,
      "discussionId": "688c53d98c434640078cc47a",
      "githubRepo": "https://github.com/YerbaPage/SWE-Debate",
      "ai_summary": "SWE-Debate, a competitive multi-agent framework, enhances issue resolution in software engineering by promoting diverse reasoning and achieving better issue localization and fix planning.",
      "ai_keywords": [
        "large language models",
        "agent-based frameworks",
        "SWE-agent",
        "autonomous agents",
        "tool-using agents",
        "software engineering tasks",
        "local solutions",
        "issue patterns",
        "codebase",
        "competitive multi-agent debate",
        "fault propagation traces",
        "code dependency graph",
        "specialized agents",
        "reasoning perspectives",
        "structured competition",
        "consolidated fix plan",
        "MCTS-based code modification agent",
        "SWE-bench benchmark"
      ],
      "githubStars": 4
    },
    "publishedAt": "2025-07-31T04:54:46.000Z",
    "title": "SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution",
    "summary": "Issue resolution has made remarkable progress thanks to the advanced\nreasoning capabilities of large language models (LLMs). Recently, agent-based\nframeworks such as SWE-agent have further advanced this progress by enabling\nautonomous, tool-using agents to tackle complex software engineering tasks.\nWhile existing agent-based issue resolution approaches are primarily based on\nagents' independent explorations, they often get stuck in local solutions and\nfail to identify issue patterns that span across different parts of the\ncodebase. To address this limitation, we propose SWE-Debate, a competitive\nmulti-agent debate framework that encourages diverse reasoning paths and\nachieves more consolidated issue localization. SWE-Debate first creates\nmultiple fault propagation traces as localization proposals by traversing a\ncode dependency graph. Then, it organizes a three-round debate among\nspecialized agents, each embodying distinct reasoning perspectives along the\nfault propagation trace. This structured competition enables agents to\ncollaboratively converge on a consolidated fix plan. Finally, this consolidated\nfix plan is integrated into an MCTS-based code modification agent for patch\ngeneration. Experiments on the SWE-bench benchmark show that SWE-Debate\nachieves new state-of-the-art results in open-source agent frameworks and\noutperforms baselines by a large margin.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.23348.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "645b0c3ec35da9c7afd95421",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
      "fullname": "Yuling",
      "name": "YerbaPage",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 274
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.00265",
      "authors": [
        {
          "_id": "6890159b0a411b3b8d28d650",
          "name": "Henghui Ding",
          "hidden": false
        },
        {
          "_id": "6890159b0a411b3b8d28d651",
          "name": "Song Tang",
          "hidden": false
        },
        {
          "_id": "6890159b0a411b3b8d28d652",
          "name": "Shuting He",
          "hidden": false
        },
        {
          "_id": "6890159b0a411b3b8d28d653",
          "name": "Chang Liu",
          "hidden": false
        },
        {
          "_id": "6890159b0a411b3b8d28d654",
          "name": "Zuxuan Wu",
          "hidden": false
        },
        {
          "_id": "6890159b0a411b3b8d28d655",
          "name": "Yu-Gang Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-01T02:14:00.000Z",
      "submittedOnDailyAt": "2025-08-04T00:37:52.118Z",
      "title": "Multimodal Referring Segmentation: A Survey",
      "submittedOnDailyBy": {
        "_id": "67ff29ecbf6889a333c69c7a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
        "isPro": false,
        "fullname": "Henghui Ding",
        "user": "HenghuiDing",
        "type": "user"
      },
      "summary": "Multimodal referring segmentation aims to segment target objects in visual\nscenes, such as images, videos, and 3D scenes, based on referring expressions\nin text or audio format. This task plays a crucial role in practical\napplications requiring accurate object perception based on user instructions.\nOver the past decade, it has gained significant attention in the multimodal\ncommunity, driven by advances in convolutional neural networks, transformers,\nand large language models, all of which have substantially improved multimodal\nperception capabilities. This paper provides a comprehensive survey of\nmultimodal referring segmentation. We begin by introducing this field's\nbackground, including problem definitions and commonly used datasets. Next, we\nsummarize a unified meta architecture for referring segmentation and review\nrepresentative methods across three primary visual scenes, including images,\nvideos, and 3D scenes. We further discuss Generalized Referring Expression\n(GREx) methods to address the challenges of real-world complexity, along with\nrelated tasks and practical applications. Extensive performance comparisons on\nstandard benchmarks are also provided. We continually track related works at\nhttps://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation.",
      "upvotes": 3,
      "discussionId": "6890159b0a411b3b8d28d656",
      "ai_summary": "A survey of multimodal referring segmentation techniques, covering advancements in convolutional neural networks, transformers, and large language models for segmenting objects in images, videos, and 3D scenes based on text or audio instructions.",
      "ai_keywords": [
        "convolutional neural networks",
        "transformers",
        "large language models",
        "multimodal referring segmentation",
        "Generalized Referring Expression (GREx)"
      ]
    },
    "publishedAt": "2025-07-31T22:14:00.000Z",
    "title": "Multimodal Referring Segmentation: A Survey",
    "summary": "Multimodal referring segmentation aims to segment target objects in visual\nscenes, such as images, videos, and 3D scenes, based on referring expressions\nin text or audio format. This task plays a crucial role in practical\napplications requiring accurate object perception based on user instructions.\nOver the past decade, it has gained significant attention in the multimodal\ncommunity, driven by advances in convolutional neural networks, transformers,\nand large language models, all of which have substantially improved multimodal\nperception capabilities. This paper provides a comprehensive survey of\nmultimodal referring segmentation. We begin by introducing this field's\nbackground, including problem definitions and commonly used datasets. Next, we\nsummarize a unified meta architecture for referring segmentation and review\nrepresentative methods across three primary visual scenes, including images,\nvideos, and 3D scenes. We further discuss Generalized Referring Expression\n(GREx) methods to address the challenges of real-world complexity, along with\nrelated tasks and practical applications. Extensive performance comparisons on\nstandard benchmarks are also provided. We continually track related works at\nhttps://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.00265.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67ff29ecbf6889a333c69c7a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
      "fullname": "Henghui Ding",
      "name": "HenghuiDing",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.23478",
      "authors": [
        {
          "_id": "6890356a0a411b3b8d28d6c3",
          "name": "Ting Huang",
          "hidden": false
        },
        {
          "_id": "6890356a0a411b3b8d28d6c4",
          "name": "Zeyu Zhang",
          "hidden": false
        },
        {
          "_id": "6890356a0a411b3b8d28d6c5",
          "name": "Hao Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-31T11:59:06.000Z",
      "submittedOnDailyAt": "2025-08-04T02:53:23.346Z",
      "title": "3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding",
      "submittedOnDailyBy": {
        "_id": "64ec877bb93654d4ca5c92e9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
        "isPro": false,
        "fullname": "Zeyu Zhang",
        "user": "SteveZeyuZhang",
        "type": "user"
      },
      "summary": "Large vision-language models (VLMs) have made significant strides in 2D\nvisual understanding tasks, sparking interest in extending these capabilities\nto 3D scene understanding. However, current 3D VLMs often struggle with robust\nreasoning and generalization due to limitations in high-quality spatial data\nand the static nature of viewpoint assumptions. To address these challenges, we\npropose 3D-R1, a foundation model that enhances the reasoning capabilities of\n3D VLMs. Specifically, we first construct a high-quality synthetic dataset with\nCoT, named Scene-30K, leveraging existing 3D-VL datasets and a data engine\nbased on Gemini 2.5 Pro. It serves as cold-start initialization data for 3D-R1.\nMoreover, we leverage RLHF policy such as GRPO in the reinforcement learning\ntraining process to enhance reasoning capabilities and introduce three reward\nfunctions: a perception reward, a semantic similarity reward and a format\nreward to maintain detection accuracy and answer semantic precision.\nFurthermore, we introduce a dynamic view selection strategy that adaptively\nchooses the most informative perspectives for 3D scene understanding. Extensive\nexperiments demonstrate that 3D-R1 delivers an average improvement of 10%\nacross various 3D scene benchmarks, highlighting its effectiveness in enhancing\nreasoning and generalization in 3D scene understanding. Code:\nhttps://github.com/AIGeeksGroup/3D-R1. Website:\nhttps://aigeeksgroup.github.io/3D-R1.",
      "upvotes": 3,
      "discussionId": "6890356b0a411b3b8d28d6c6",
      "projectPage": "https://aigeeksgroup.github.io/3D-R1",
      "githubRepo": "https://github.com/AIGeeksGroup/3D-R1",
      "ai_summary": "3D-R1 enhances 3D scene understanding through a high-quality synthetic dataset, reinforcement learning with GRPO, and dynamic view selection, achieving significant improvements in reasoning and generalization.",
      "ai_keywords": [
        "3D-R1",
        "VLMs",
        "3D scene understanding",
        "Scene-30K",
        "Gemini 2.5 Pro",
        "RLHF policy",
        "GRPO",
        "perception reward",
        "semantic similarity reward",
        "format reward",
        "dynamic view selection"
      ],
      "githubStars": 26
    },
    "publishedAt": "2025-07-31T07:59:06.000Z",
    "title": "3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding",
    "summary": "Large vision-language models (VLMs) have made significant strides in 2D\nvisual understanding tasks, sparking interest in extending these capabilities\nto 3D scene understanding. However, current 3D VLMs often struggle with robust\nreasoning and generalization due to limitations in high-quality spatial data\nand the static nature of viewpoint assumptions. To address these challenges, we\npropose 3D-R1, a foundation model that enhances the reasoning capabilities of\n3D VLMs. Specifically, we first construct a high-quality synthetic dataset with\nCoT, named Scene-30K, leveraging existing 3D-VL datasets and a data engine\nbased on Gemini 2.5 Pro. It serves as cold-start initialization data for 3D-R1.\nMoreover, we leverage RLHF policy such as GRPO in the reinforcement learning\ntraining process to enhance reasoning capabilities and introduce three reward\nfunctions: a perception reward, a semantic similarity reward and a format\nreward to maintain detection accuracy and answer semantic precision.\nFurthermore, we introduce a dynamic view selection strategy that adaptively\nchooses the most informative perspectives for 3D scene understanding. Extensive\nexperiments demonstrate that 3D-R1 delivers an average improvement of 10%\nacross various 3D scene benchmarks, highlighting its effectiveness in enhancing\nreasoning and generalization in 3D scene understanding. Code:\nhttps://github.com/AIGeeksGroup/3D-R1. Website:\nhttps://aigeeksgroup.github.io/3D-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.23478.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ec877bb93654d4ca5c92e9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
      "fullname": "Zeyu Zhang",
      "name": "SteveZeyuZhang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.00823",
      "authors": [
        {
          "_id": "68902cbc0a411b3b8d28d6ba",
          "name": "Wenxuan Guo",
          "hidden": false
        },
        {
          "_id": "68902cbc0a411b3b8d28d6bb",
          "name": "Xiuwei Xu",
          "hidden": false
        },
        {
          "_id": "68902cbc0a411b3b8d28d6bc",
          "name": "Hang Yin",
          "hidden": false
        },
        {
          "_id": "68902cbc0a411b3b8d28d6bd",
          "name": "Ziwei Wang",
          "hidden": false
        },
        {
          "_id": "68902cbc0a411b3b8d28d6be",
          "name": "Jianjiang Feng",
          "hidden": false
        },
        {
          "_id": "68902cbc0a411b3b8d28d6bf",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "68902cbc0a411b3b8d28d6c0",
          "name": "Jiwen Lu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-01T17:59:56.000Z",
      "submittedOnDailyAt": "2025-08-04T02:17:55.656Z",
      "title": "IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation",
      "submittedOnDailyBy": {
        "_id": "67b2cf648a276e7b4856e307",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lHepLDwyYeQC4IQhVKgz6.png",
        "isPro": false,
        "fullname": "Wenxuan Guo",
        "user": "gwx22",
        "type": "user"
      },
      "summary": "Visual navigation with an image as goal is a fundamental and challenging\nproblem. Conventional methods either rely on end-to-end RL learning or\nmodular-based policy with topological graph or BEV map as memory, which cannot\nfully model the geometric relationship between the explored 3D environment and\nthe goal image. In order to efficiently and accurately localize the goal image\nin 3D space, we build our navigation system upon the renderable 3D gaussian\n(3DGS) representation. However, due to the computational intensity of 3DGS\noptimization and the large search space of 6-DoF camera pose, directly\nleveraging 3DGS for image localization during agent exploration process is\nprohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D\nGaussian Localization framework for efficient and 3D-aware image-goal\nnavigation. Specifically, we incrementally update the scene representation as\nnew images arrive with feed-forward monocular prediction. Then we coarsely\nlocalize the goal by leveraging the geometric information for discrete space\nmatching, which can be equivalent to efficient 3D convolution. When the agent\nis close to the goal, we finally solve the fine target pose with optimization\nvia differentiable rendering. The proposed IGL-Nav outperforms existing\nstate-of-the-art methods by a large margin across diverse experimental\nconfigurations. It can also handle the more challenging free-view image-goal\nsetting and be deployed on real-world robotic platform using a cellphone to\ncapture goal image at arbitrary pose. Project page:\nhttps://gwxuan.github.io/IGL-Nav/.",
      "upvotes": 1,
      "discussionId": "68902cbc0a411b3b8d28d6c1",
      "projectPage": "https://gwxuan.github.io/IGL-Nav/",
      "ai_summary": "IGL-Nav uses an incremental 3D Gaussian representation for efficient and accurate image-goal navigation in 3D space, outperforming existing methods and applicable in real-world settings.",
      "ai_keywords": [
        "renderable 3D gaussian",
        "3DGS",
        "6-DoF camera pose",
        "feed-forward monocular prediction",
        "discrete space matching",
        "differentiable rendering",
        "IGL-Nav",
        "image-goal navigation",
        "free-view image-goal setting"
      ]
    },
    "publishedAt": "2025-08-01T13:59:56.000Z",
    "title": "IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation",
    "summary": "Visual navigation with an image as goal is a fundamental and challenging\nproblem. Conventional methods either rely on end-to-end RL learning or\nmodular-based policy with topological graph or BEV map as memory, which cannot\nfully model the geometric relationship between the explored 3D environment and\nthe goal image. In order to efficiently and accurately localize the goal image\nin 3D space, we build our navigation system upon the renderable 3D gaussian\n(3DGS) representation. However, due to the computational intensity of 3DGS\noptimization and the large search space of 6-DoF camera pose, directly\nleveraging 3DGS for image localization during agent exploration process is\nprohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D\nGaussian Localization framework for efficient and 3D-aware image-goal\nnavigation. Specifically, we incrementally update the scene representation as\nnew images arrive with feed-forward monocular prediction. Then we coarsely\nlocalize the goal by leveraging the geometric information for discrete space\nmatching, which can be equivalent to efficient 3D convolution. When the agent\nis close to the goal, we finally solve the fine target pose with optimization\nvia differentiable rendering. The proposed IGL-Nav outperforms existing\nstate-of-the-art methods by a large margin across diverse experimental\nconfigurations. It can also handle the more challenging free-view image-goal\nsetting and be deployed on real-world robotic platform using a cellphone to\ncapture goal image at arbitrary pose. Project page:\nhttps://gwxuan.github.io/IGL-Nav/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.00823.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67b2cf648a276e7b4856e307",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lHepLDwyYeQC4IQhVKgz6.png",
      "fullname": "Wenxuan Guo",
      "name": "gwx22",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.00454",
      "authors": [
        {
          "_id": "689022980a411b3b8d28d69e",
          "name": "Yuqi Tang",
          "hidden": false
        },
        {
          "_id": "689022980a411b3b8d28d69f",
          "name": "Kehua Feng",
          "hidden": false
        },
        {
          "_id": "689022980a411b3b8d28d6a0",
          "name": "Yunfeng Wang",
          "hidden": false
        },
        {
          "_id": "689022980a411b3b8d28d6a1",
          "name": "Zhiwen Chen",
          "hidden": false
        },
        {
          "_id": "689022980a411b3b8d28d6a2",
          "name": "Chengfei Lv",
          "hidden": false
        },
        {
          "_id": "689022980a411b3b8d28d6a3",
          "name": "Gang Yu",
          "hidden": false
        },
        {
          "_id": "689022980a411b3b8d28d6a4",
          "name": "Qiang Zhang",
          "hidden": false
        },
        {
          "_id": "689022980a411b3b8d28d6a5",
          "name": "Keyan Ding",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6479e038a26759caa62ea433/Fz7YpJV6QnC1THsv7jjSt.png"
      ],
      "publishedAt": "2025-08-01T09:26:01.000Z",
      "submittedOnDailyAt": "2025-08-04T01:39:47.788Z",
      "title": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges",
      "submittedOnDailyBy": {
        "_id": "6479e038a26759caa62ea433",
        "avatarUrl": "/avatars/7a9a023b1fae802eedbe90ec40c791d1.svg",
        "isPro": false,
        "fullname": "Feng",
        "user": "Kehua",
        "type": "user"
      },
      "summary": "Evaluating the conversational abilities of large language models (LLMs)\nremains a challenging task. Current mainstream approaches primarily rely on the\n``LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator\nto assess dialogue quality. However, such methods often suffer from various\nbiases, which undermine the reliability and consistency of the evaluation\nresults. To mitigate these biases, recent methods employ multiple LLMs as\njudges and aggregate their judgments to select the optimal assessment. Although\neffective, this multi-judge approach incurs significant computational overhead\nduring inference. In this paper, we propose an efficient multi-turn dialogue\nevaluator that captures the collective wisdom of multiple LLM judges by\naggregating their preference knowledge into a single model. Our approach\npreserves the advantages of diverse multi-judge feedback while drastically\nreducing the evaluation cost, enabling fast and flexible dialogue quality\nassessment. Extensive experiments on seven single rating and pairwise\ncomparison dialogue evaluation benchmarks demonstrate that our method\noutperforms existing baselines across diverse scenarios, showcasing its\nefficiency and robustness.",
      "upvotes": 1,
      "discussionId": "689022980a411b3b8d28d6a6",
      "githubRepo": "https://github.com/James-TYQ/MTDEval",
      "ai_summary": "An efficient multi-turn dialogue evaluator aggregates multiple LLM judgments into a single model to assess dialogue quality with reduced computational cost.",
      "ai_keywords": [
        "LLM-as-a-judge",
        "multi-judge approach",
        "preference knowledge",
        "multi-turn dialogue evaluator",
        "dialogue evaluation benchmarks"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-08-01T05:26:01.000Z",
    "title": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges",
    "summary": "Evaluating the conversational abilities of large language models (LLMs)\nremains a challenging task. Current mainstream approaches primarily rely on the\n``LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator\nto assess dialogue quality. However, such methods often suffer from various\nbiases, which undermine the reliability and consistency of the evaluation\nresults. To mitigate these biases, recent methods employ multiple LLMs as\njudges and aggregate their judgments to select the optimal assessment. Although\neffective, this multi-judge approach incurs significant computational overhead\nduring inference. In this paper, we propose an efficient multi-turn dialogue\nevaluator that captures the collective wisdom of multiple LLM judges by\naggregating their preference knowledge into a single model. Our approach\npreserves the advantages of diverse multi-judge feedback while drastically\nreducing the evaluation cost, enabling fast and flexible dialogue quality\nassessment. Extensive experiments on seven single rating and pairwise\ncomparison dialogue evaluation benchmarks demonstrate that our method\noutperforms existing baselines across diverse scenarios, showcasing its\nefficiency and robustness.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6479e038a26759caa62ea433/Fz7YpJV6QnC1THsv7jjSt.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.00454.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6479e038a26759caa62ea433",
      "avatarUrl": "/avatars/7a9a023b1fae802eedbe90ec40c791d1.svg",
      "fullname": "Feng",
      "name": "Kehua",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.22720",
      "authors": [
        {
          "_id": "689036f00a411b3b8d28d6d1",
          "name": "Amit Das",
          "hidden": false
        },
        {
          "_id": "689036f00a411b3b8d28d6d2",
          "name": "Md. Najib Hasan",
          "hidden": false
        },
        {
          "_id": "689036f00a411b3b8d28d6d3",
          "name": "Souvika Sarkar",
          "hidden": false
        },
        {
          "_id": "689036f00a411b3b8d28d6d4",
          "name": "Zheng Zhang",
          "hidden": false
        },
        {
          "_id": "689036f00a411b3b8d28d6d5",
          "name": "Fatemeh Jamshidi",
          "hidden": false
        },
        {
          "_id": "689036f00a411b3b8d28d6d6",
          "name": "Tathagata Bhattacharya",
          "hidden": false
        },
        {
          "_id": "689036f00a411b3b8d28d6d7",
          "name": "Nilanjana Raychawdhury",
          "hidden": false
        },
        {
          "_id": "689036f00a411b3b8d28d6d8",
          "name": "Dongji Feng",
          "hidden": false
        },
        {
          "_id": "689036f00a411b3b8d28d6d9",
          "name": "Vinija Jain",
          "hidden": false
        },
        {
          "_id": "689036f00a411b3b8d28d6da",
          "name": "Aman Chadha",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-30T14:39:51.000Z",
      "submittedOnDailyAt": "2025-08-04T02:59:25.205Z",
      "title": "Investigating Hallucination in Conversations for Low Resource Languages",
      "submittedOnDailyBy": {
        "_id": "63a4754927f1f64ed7238dac",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
        "isPro": false,
        "fullname": "Aman Chadha",
        "user": "amanchadha",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\ngenerating text that closely resemble human writing. However, they often\ngenerate factually incorrect statements, a problem typically referred to as\n'hallucination'. Addressing hallucination is crucial for enhancing the\nreliability and effectiveness of LLMs. While much research has focused on\nhallucinations in English, our study extends this investigation to\nconversational data in three languages: Hindi, Farsi, and Mandarin. We offer a\ncomprehensive analysis of a dataset to examine both factual and linguistic\nerrors in these languages for GPT-3.5, GPT-4o, Llama-3.1, Gemma-2.0,\nDeepSeek-R1 and Qwen-3. We found that LLMs produce very few hallucinated\nresponses in Mandarin but generate a significantly higher number of\nhallucinations in Hindi and Farsi.",
      "upvotes": 1,
      "discussionId": "689036f00a411b3b8d28d6db",
      "ai_summary": "LLMs generate fewer hallucinations in Mandarin compared to Hindi and Farsi across multiple models.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "hallucination",
        "GPT-3.5",
        "GPT-4o",
        "Llama-3.1",
        "Gemma-2.0",
        "DeepSeek-R1",
        "Qwen-3"
      ]
    },
    "publishedAt": "2025-07-30T10:39:51.000Z",
    "title": "Investigating Hallucination in Conversations for Low Resource Languages",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\ngenerating text that closely resemble human writing. However, they often\ngenerate factually incorrect statements, a problem typically referred to as\n'hallucination'. Addressing hallucination is crucial for enhancing the\nreliability and effectiveness of LLMs. While much research has focused on\nhallucinations in English, our study extends this investigation to\nconversational data in three languages: Hindi, Farsi, and Mandarin. We offer a\ncomprehensive analysis of a dataset to examine both factual and linguistic\nerrors in these languages for GPT-3.5, GPT-4o, Llama-3.1, Gemma-2.0,\nDeepSeek-R1 and Qwen-3. We found that LLMs produce very few hallucinated\nresponses in Mandarin but generate a significantly higher number of\nhallucinations in Hindi and Farsi.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.22720.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63a4754927f1f64ed7238dac",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
      "fullname": "Aman Chadha",
      "name": "amanchadha",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 9
    },
    "isAuthorParticipating": false
  }
]