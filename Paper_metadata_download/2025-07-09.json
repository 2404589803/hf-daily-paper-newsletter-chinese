[
  {
    "paper": {
      "id": "2507.06203",
      "authors": [
        {
          "_id": "686ddd7fcb5725779c60b444",
          "name": "Rui-Jie Zhu",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b445",
          "name": "Tianhao Peng",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b446",
          "name": "Tianhao Cheng",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b447",
          "name": "Xingwei Qu",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b448",
          "name": "Jinfa Huang",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b449",
          "name": "Dawei Zhu",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b44a",
          "name": "Hao Wang",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b44b",
          "name": "Kaiwen Xue",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b44c",
          "name": "Xuanliang Zhang",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b44d",
          "name": "Yong Shan",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b44e",
          "name": "Tianle Cai",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b44f",
          "name": "Taylor Kergan",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b450",
          "name": "Assel Kembay",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b451",
          "name": "Andrew Smith",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b452",
          "name": "Chenghua Lin",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b453",
          "name": "Binh Nguyen",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b454",
          "name": "Yuqi Pan",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b455",
          "name": "Yuhong Chou",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b456",
          "name": "Zefan Cai",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b457",
          "name": "Zhenhe Wu",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b458",
          "name": "Yongchi Zhao",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b459",
          "name": "Tianyu Liu",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b45a",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b45b",
          "name": "Wangchunshu Zhou",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b45c",
          "name": "Chujie Zheng",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b45d",
          "name": "Chongxuan Li",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b45e",
          "name": "Yuyin Zhou",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b45f",
          "name": "Zhoujun Li",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b460",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b461",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b462",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b463",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "686ddd7fcb5725779c60b464",
          "name": "Jason Eshraghian",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-08T17:29:07.000Z",
      "submittedOnDailyAt": "2025-07-09T01:45:08.087Z",
      "title": "A Survey on Latent Reasoning",
      "submittedOnDailyBy": {
        "_id": "63ff09f24852102d4871c19c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ff09f24852102d4871c19c/lyE3xemtZss3qebK5sEXw.png",
        "isPro": false,
        "fullname": "Rui-Jie Zhu",
        "user": "ridger",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have demonstrated impressive reasoning\ncapabilities, especially when guided by explicit chain-of-thought (CoT)\nreasoning that verbalizes intermediate steps. While CoT improves both\ninterpretability and accuracy, its dependence on natural language reasoning\nlimits the model's expressive bandwidth. Latent reasoning tackles this\nbottleneck by performing multi-step inference entirely in the model's\ncontinuous hidden state, eliminating token-level supervision. To advance latent\nreasoning research, this survey provides a comprehensive overview of the\nemerging field of latent reasoning. We begin by examining the foundational role\nof neural network layers as the computational substrate for reasoning,\nhighlighting how hierarchical representations support complex transformations.\nNext, we explore diverse latent reasoning methodologies, including\nactivation-based recurrence, hidden state propagation, and fine-tuning\nstrategies that compress or internalize explicit reasoning traces. Finally, we\ndiscuss advanced paradigms such as infinite-depth latent reasoning via masked\ndiffusion models, which enable globally consistent and reversible reasoning\nprocesses. By unifying these perspectives, we aim to clarify the conceptual\nlandscape of latent reasoning and chart future directions for research at the\nfrontier of LLM cognition. An associated GitHub repository collecting the\nlatest papers and repos is available at:\nhttps://github.com/multimodal-art-projection/LatentCoT-Horizon/.",
      "upvotes": 37,
      "discussionId": "686ddd7fcb5725779c60b465",
      "githubRepo": "https://github.com/multimodal-art-projection/LatentCoT-Horizon/",
      "ai_summary": "Latent reasoning enhances large language models by performing multi-step inference in continuous hidden states, improving efficiency and expressiveness beyond token-level supervision.",
      "ai_keywords": [
        "chain-of-thought reasoning",
        "latent reasoning",
        "neural network layers",
        "hierarchical representations",
        "activation-based recurrence",
        "hidden state propagation",
        "fine-tuning strategies",
        "infinite-depth latent reasoning",
        "masked diffusion models",
        "globally consistent reasoning",
        "reversible reasoning processes"
      ],
      "githubStars": 15
    },
    "publishedAt": "2025-07-08T13:29:07.000Z",
    "title": "A Survey on Latent Reasoning",
    "summary": "Large Language Models (LLMs) have demonstrated impressive reasoning\ncapabilities, especially when guided by explicit chain-of-thought (CoT)\nreasoning that verbalizes intermediate steps. While CoT improves both\ninterpretability and accuracy, its dependence on natural language reasoning\nlimits the model's expressive bandwidth. Latent reasoning tackles this\nbottleneck by performing multi-step inference entirely in the model's\ncontinuous hidden state, eliminating token-level supervision. To advance latent\nreasoning research, this survey provides a comprehensive overview of the\nemerging field of latent reasoning. We begin by examining the foundational role\nof neural network layers as the computational substrate for reasoning,\nhighlighting how hierarchical representations support complex transformations.\nNext, we explore diverse latent reasoning methodologies, including\nactivation-based recurrence, hidden state propagation, and fine-tuning\nstrategies that compress or internalize explicit reasoning traces. Finally, we\ndiscuss advanced paradigms such as infinite-depth latent reasoning via masked\ndiffusion models, which enable globally consistent and reversible reasoning\nprocesses. By unifying these perspectives, we aim to clarify the conceptual\nlandscape of latent reasoning and chart future directions for research at the\nfrontier of LLM cognition. An associated GitHub repository collecting the\nlatest papers and repos is available at:\nhttps://github.com/multimodal-art-projection/LatentCoT-Horizon/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06203.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63ff09f24852102d4871c19c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ff09f24852102d4871c19c/lyE3xemtZss3qebK5sEXw.png",
      "fullname": "Rui-Jie Zhu",
      "name": "ridger",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 23
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.06165",
      "authors": [
        {
          "_id": "686ddd5ccb5725779c60b430",
          "name": "Yunhan Yang",
          "hidden": false
        },
        {
          "_id": "686ddd5ccb5725779c60b431",
          "name": "Yufan Zhou",
          "hidden": false
        },
        {
          "_id": "686ddd5ccb5725779c60b432",
          "name": "Yuan-Chen Guo",
          "hidden": false
        },
        {
          "_id": "686ddd5ccb5725779c60b433",
          "name": "Zi-Xin Zou",
          "hidden": false
        },
        {
          "_id": "686ddd5ccb5725779c60b434",
          "name": "Yukun Huang",
          "hidden": false
        },
        {
          "_id": "686ddd5ccb5725779c60b435",
          "name": "Ying-Tian Liu",
          "hidden": false
        },
        {
          "_id": "686ddd5ccb5725779c60b436",
          "name": "Hao Xu",
          "hidden": false
        },
        {
          "_id": "686ddd5ccb5725779c60b437",
          "name": "Ding Liang",
          "hidden": false
        },
        {
          "_id": "686ddd5ccb5725779c60b438",
          "name": "Yan-Pei Cao",
          "hidden": false
        },
        {
          "_id": "686ddd5ccb5725779c60b439",
          "name": "Xihui Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6427e08288215cee63b1c44d/Y9wAFEXtYiDmbh7rRZDoz.mp4"
      ],
      "publishedAt": "2025-07-08T16:46:15.000Z",
      "submittedOnDailyAt": "2025-07-09T02:05:05.139Z",
      "title": "OmniPart: Part-Aware 3D Generation with Semantic Decoupling and\n  Structural Cohesion",
      "submittedOnDailyBy": {
        "_id": "6427e08288215cee63b1c44d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6427e08288215cee63b1c44d/rzaG978FF-ywzicWNl_xl.jpeg",
        "isPro": false,
        "fullname": "yao teng",
        "user": "tytyt",
        "type": "user"
      },
      "summary": "The creation of 3D assets with explicit, editable part structures is crucial\nfor advancing interactive applications, yet most generative methods produce\nonly monolithic shapes, limiting their utility. We introduce OmniPart, a novel\nframework for part-aware 3D object generation designed to achieve high semantic\ndecoupling among components while maintaining robust structural cohesion.\nOmniPart uniquely decouples this complex task into two synergistic stages: (1)\nan autoregressive structure planning module generates a controllable,\nvariable-length sequence of 3D part bounding boxes, critically guided by\nflexible 2D part masks that allow for intuitive control over part decomposition\nwithout requiring direct correspondences or semantic labels; and (2) a\nspatially-conditioned rectified flow model, efficiently adapted from a\npre-trained holistic 3D generator, synthesizes all 3D parts simultaneously and\nconsistently within the planned layout. Our approach supports user-defined part\ngranularity, precise localization, and enables diverse downstream applications.\nExtensive experiments demonstrate that OmniPart achieves state-of-the-art\nperformance, paving the way for more interpretable, editable, and versatile 3D\ncontent.",
      "upvotes": 28,
      "discussionId": "686ddd5ccb5725779c60b43a",
      "ai_summary": "OmniPart generates part-aware 3D objects with high semantic decoupling and robust structural cohesion using an autoregressive structure planning module and a spatially-conditioned rectified flow model.",
      "ai_keywords": [
        "autoregressive structure planning module",
        "3D part bounding boxes",
        "2D part masks",
        "spatially-conditioned rectified flow model",
        "holistic 3D generator"
      ]
    },
    "publishedAt": "2025-07-08T12:46:15.000Z",
    "title": "OmniPart: Part-Aware 3D Generation with Semantic Decoupling and\n  Structural Cohesion",
    "summary": "The creation of 3D assets with explicit, editable part structures is crucial\nfor advancing interactive applications, yet most generative methods produce\nonly monolithic shapes, limiting their utility. We introduce OmniPart, a novel\nframework for part-aware 3D object generation designed to achieve high semantic\ndecoupling among components while maintaining robust structural cohesion.\nOmniPart uniquely decouples this complex task into two synergistic stages: (1)\nan autoregressive structure planning module generates a controllable,\nvariable-length sequence of 3D part bounding boxes, critically guided by\nflexible 2D part masks that allow for intuitive control over part decomposition\nwithout requiring direct correspondences or semantic labels; and (2) a\nspatially-conditioned rectified flow model, efficiently adapted from a\npre-trained holistic 3D generator, synthesizes all 3D parts simultaneously and\nconsistently within the planned layout. Our approach supports user-defined part\ngranularity, precise localization, and enables diverse downstream applications.\nExtensive experiments demonstrate that OmniPart achieves state-of-the-art\nperformance, paving the way for more interpretable, editable, and versatile 3D\ncontent.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6427e08288215cee63b1c44d/Y9wAFEXtYiDmbh7rRZDoz.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06165.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6427e08288215cee63b1c44d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6427e08288215cee63b1c44d/rzaG978FF-ywzicWNl_xl.jpeg",
      "fullname": "yao teng",
      "name": "tytyt",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.05240",
      "authors": [
        {
          "_id": "686de9d4cb5725779c60b487",
          "name": "Meng Wei",
          "hidden": false
        },
        {
          "_id": "686de9d4cb5725779c60b488",
          "name": "Chenyang Wan",
          "hidden": false
        },
        {
          "_id": "686de9d4cb5725779c60b489",
          "name": "Xiqian Yu",
          "hidden": false
        },
        {
          "_id": "686de9d4cb5725779c60b48a",
          "name": "Tai Wang",
          "hidden": false
        },
        {
          "_id": "686de9d4cb5725779c60b48b",
          "name": "Yuqiang Yang",
          "hidden": false
        },
        {
          "_id": "686de9d4cb5725779c60b48c",
          "name": "Xiaohan Mao",
          "hidden": false
        },
        {
          "_id": "686de9d4cb5725779c60b48d",
          "name": "Chenming Zhu",
          "hidden": false
        },
        {
          "_id": "686de9d4cb5725779c60b48e",
          "name": "Wenzhe Cai",
          "hidden": false
        },
        {
          "_id": "686de9d4cb5725779c60b48f",
          "name": "Hanqing Wang",
          "hidden": false
        },
        {
          "_id": "686de9d4cb5725779c60b490",
          "name": "Yilun Chen",
          "hidden": false
        },
        {
          "_id": "686de9d4cb5725779c60b491",
          "name": "Xihui Liu",
          "hidden": false
        },
        {
          "_id": "686de9d4cb5725779c60b492",
          "name": "Jiangmiao Pang",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64e6d9d229a548f66aff6e5b/Vpc-LLN2y02mgIeZu43Mv.mp4"
      ],
      "publishedAt": "2025-07-07T17:49:41.000Z",
      "submittedOnDailyAt": "2025-07-09T03:18:43.255Z",
      "title": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling",
      "submittedOnDailyBy": {
        "_id": "64e6d9d229a548f66aff6e5b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6d9d229a548f66aff6e5b/yQ9E2TyzM4CfSjMPigcey.jpeg",
        "isPro": false,
        "fullname": "Tai Wang",
        "user": "taiwang",
        "type": "user"
      },
      "summary": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\nhttps://streamvln.github.io/{https://streamvln.github.io/}.",
      "upvotes": 26,
      "discussionId": "686de9d4cb5725779c60b493",
      "projectPage": "https://streamvln.github.io/",
      "githubRepo": "https://github.com/OpenRobotLab/StreamVLN",
      "ai_summary": "StreamVLN, a streaming VLN framework, uses a hybrid slow-fast context modeling strategy to balance fine-grained visual understanding, long-term context modeling, and computational efficiency in real-world settings.",
      "ai_keywords": [
        "Video-LLMs",
        "StreamVLN",
        "hybrid slow-fast context modeling",
        "multi-modal reasoning",
        "fast-streaming dialogue context",
        "slow-updating memory context",
        "3D-aware token pruning",
        "KV cache reuse",
        "VLN-CE benchmarks"
      ],
      "githubStars": 60
    },
    "publishedAt": "2025-07-07T13:49:41.000Z",
    "title": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling",
    "summary": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\nhttps://streamvln.github.io/{https://streamvln.github.io/}.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64e6d9d229a548f66aff6e5b/Vpc-LLN2y02mgIeZu43Mv.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05240.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e6d9d229a548f66aff6e5b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6d9d229a548f66aff6e5b/yQ9E2TyzM4CfSjMPigcey.jpeg",
      "fullname": "Tai Wang",
      "name": "taiwang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.06181",
      "authors": [
        {
          "_id": "686dcc36cb5725779c60b393",
          "name": "Zhongyuan Peng",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b394",
          "name": "Yifan Yao",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b395",
          "name": "Kaijing Ma",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b396",
          "name": "Shuyue Guo",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b397",
          "name": "Yizhe Li",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b398",
          "name": "Yichi Zhang",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b399",
          "name": "Chenchen Zhang",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b39a",
          "name": "Yifan Zhang",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b39b",
          "name": "Zhouliang Yu",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b39c",
          "name": "Luming Li",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b39d",
          "name": "Minghao Liu",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b39e",
          "name": "Yihang Xia",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b39f",
          "name": "Jiawei Shen",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b3a0",
          "name": "Yuchen Wu",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b3a1",
          "name": "Yixin Cao",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b3a2",
          "name": "Zhaoxiang Zhang",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b3a3",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b3a4",
          "name": "Jiaheng Liu",
          "hidden": false
        },
        {
          "_id": "686dcc36cb5725779c60b3a5",
          "name": "Ge Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-08T17:03:39.000Z",
      "submittedOnDailyAt": "2025-07-09T00:45:12.634Z",
      "title": "CriticLean: Critic-Guided Reinforcement Learning for Mathematical\n  Formalization",
      "submittedOnDailyBy": {
        "_id": "63299f93688ad82b783aaf20",
        "avatarUrl": "/avatars/7c11e60e551ef1c62aa2862529e357f5.svg",
        "isPro": false,
        "fullname": "zhongyuan peng",
        "user": "happzy2633",
        "type": "user"
      },
      "summary": "Translating natural language mathematical statements into formal, executable\ncode is a fundamental challenge in automated theorem proving. While prior work\nhas focused on generation and compilation success, little attention has been\npaid to the critic phase-the evaluation of whether generated formalizations\ntruly capture the semantic intent of the original problem. In this paper, we\nintroduce CriticLean, a novel critic-guided reinforcement learning framework\nthat elevates the role of the critic from a passive validator to an active\nlearning component. Specifically, first, we propose the CriticLeanGPT, trained\nvia supervised fine-tuning and reinforcement learning, to rigorously assess the\nsemantic fidelity of Lean 4 formalizations. Then, we introduce CriticLeanBench,\na benchmark designed to measure models' ability to distinguish semantically\ncorrect from incorrect formalizations, and demonstrate that our trained\nCriticLeanGPT models can significantly outperform strong open- and\nclosed-source baselines. Building on the CriticLean framework, we construct\nFineLeanCorpus, a dataset comprising over 285K problems that exhibits rich\ndomain diversity, broad difficulty coverage, and high correctness based on\nhuman evaluation. Overall, our findings highlight that optimizing the critic\nphase is essential for producing reliable formalizations, and we hope our\nCriticLean will provide valuable insights for future advances in formal\nmathematical reasoning.",
      "upvotes": 23,
      "discussionId": "686dcc36cb5725779c60b3a6",
      "githubRepo": "https://github.com/multimodal-art-projection/CriticLean",
      "ai_summary": "CriticLean, a reinforcement learning framework with CriticLeanGPT and CriticLeanBench, enhances semantic evaluation in automated theorem proving by actively learning to distinguish correct from incorrect formalizations.",
      "ai_keywords": [
        "critic phase",
        "reinforcement learning",
        "CriticLeanGPT",
        "supervised fine-tuning",
        "semantic fidelity",
        "Lean 4 formalizations",
        "CriticLeanBench",
        "FineLeanCorpus",
        "formal mathematical reasoning"
      ],
      "githubStars": 5
    },
    "publishedAt": "2025-07-08T13:03:39.000Z",
    "title": "CriticLean: Critic-Guided Reinforcement Learning for Mathematical\n  Formalization",
    "summary": "Translating natural language mathematical statements into formal, executable\ncode is a fundamental challenge in automated theorem proving. While prior work\nhas focused on generation and compilation success, little attention has been\npaid to the critic phase-the evaluation of whether generated formalizations\ntruly capture the semantic intent of the original problem. In this paper, we\nintroduce CriticLean, a novel critic-guided reinforcement learning framework\nthat elevates the role of the critic from a passive validator to an active\nlearning component. Specifically, first, we propose the CriticLeanGPT, trained\nvia supervised fine-tuning and reinforcement learning, to rigorously assess the\nsemantic fidelity of Lean 4 formalizations. Then, we introduce CriticLeanBench,\na benchmark designed to measure models' ability to distinguish semantically\ncorrect from incorrect formalizations, and demonstrate that our trained\nCriticLeanGPT models can significantly outperform strong open- and\nclosed-source baselines. Building on the CriticLean framework, we construct\nFineLeanCorpus, a dataset comprising over 285K problems that exhibits rich\ndomain diversity, broad difficulty coverage, and high correctness based on\nhuman evaluation. Overall, our findings highlight that optimizing the critic\nphase is essential for producing reliable formalizations, and we hope our\nCriticLean will provide valuable insights for future advances in formal\nmathematical reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06181.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63299f93688ad82b783aaf20",
      "avatarUrl": "/avatars/7c11e60e551ef1c62aa2862529e357f5.svg",
      "fullname": "zhongyuan peng",
      "name": "happzy2633",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.03112",
      "authors": [
        {
          "_id": "686c8b1c364e2ad167eb53b4",
          "name": "Peisong Wang",
          "hidden": false
        },
        {
          "_id": "686c8b1c364e2ad167eb53b5",
          "name": "Ruotian Ma",
          "hidden": false
        },
        {
          "_id": "686c8b1c364e2ad167eb53b6",
          "name": "Bang Zhang",
          "hidden": false
        },
        {
          "_id": "686c8b1c364e2ad167eb53b7",
          "name": "Xingyu Chen",
          "hidden": false
        },
        {
          "_id": "686c8b1c364e2ad167eb53b8",
          "name": "Zhiwei He",
          "hidden": false
        },
        {
          "_id": "686c8b1c364e2ad167eb53b9",
          "name": "Kang Luo",
          "hidden": false
        },
        {
          "_id": "686c8b1c364e2ad167eb53ba",
          "name": "Qingsong Lv",
          "hidden": false
        },
        {
          "_id": "686c8b1c364e2ad167eb53bb",
          "name": "Qingxuan Jiang",
          "hidden": false
        },
        {
          "_id": "686c8b1c364e2ad167eb53bc",
          "name": "Zheng Xie",
          "hidden": false
        },
        {
          "_id": "686c8b1c364e2ad167eb53bd",
          "name": "Shanyi Wang",
          "hidden": false
        },
        {
          "_id": "686c8b1c364e2ad167eb53be",
          "name": "Yuan Li",
          "hidden": false
        },
        {
          "_id": "686c8b1c364e2ad167eb53bf",
          "name": "Fanghua Ye",
          "hidden": false
        },
        {
          "_id": "686c8b1c364e2ad167eb53c0",
          "name": "Jian Li",
          "hidden": false
        },
        {
          "_id": "686c8b1c364e2ad167eb53c1",
          "name": "Yifan Yang",
          "hidden": false
        },
        {
          "_id": "686c8b1c364e2ad167eb53c2",
          "name": "Zhaopeng Tu",
          "hidden": false
        },
        {
          "_id": "686c8b1c364e2ad167eb53c3",
          "name": "Xiaolong Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-03T18:33:18.000Z",
      "submittedOnDailyAt": "2025-07-09T01:05:45.652Z",
      "title": "RLVER: Reinforcement Learning with Verifiable Emotion Rewards for\n  Empathetic Agents",
      "submittedOnDailyBy": {
        "_id": "61b859ddbdf1fac5ed499992",
        "avatarUrl": "/avatars/2387fb9b8a46840bfc75248462f0a410.svg",
        "isPro": false,
        "fullname": "Jiaqi Chen",
        "user": "judge",
        "type": "user"
      },
      "summary": "Large language models (LLMs) excel at logical and algorithmic reasoning, yet\ntheir emotional intelligence (EQ) still lags far behind their cognitive\nprowess. While reinforcement learning from verifiable rewards (RLVR) has\nadvanced in other domains, its application to dialogue-especially for emotional\nintelligence-remains underexplored. In this work, we introduce RLVER, the first\nend-to-end reinforcement learning framework that leverages verifiable emotion\nrewards from simulated users to cultivate higher-order empathetic abilities in\nLLMs. Within this framework, self-consistent affective simulated users engage\nin dialogue rollouts and produce deterministic emotion scores during\nconversations, serving as reward signals to guide the LLM's learning.\nFine-tuning publicly available Qwen2.5-7B-Instruct model with PPO boosts its\nSentient-Benchmark score from 13.3 to 79.2 while largely preserving\nmathematical and coding competence. Extensive experiments reveal that: (i)\nRLVER consistently improves multiple dialogue capabilities; (ii) Thinking and\nnon-thinking models show distinct trends--thinking models excel in empathy and\ninsight, while non-thinking models favor action; (iii) GRPO often yields stable\ngains, while PPO can push certain capabilities to a higher ceiling; (iv) More\nchallenging environments are not always better-moderate ones can yield stronger\noutcomes. Our results show that RLVER is a practical route toward emotionally\nintelligent and broadly capable language agents.",
      "upvotes": 22,
      "discussionId": "686c8b1c364e2ad167eb53c4",
      "ai_summary": "An end-to-end reinforcement learning framework using simulated user emotion rewards enhances emotional intelligence in large language models while maintaining cognitive skills.",
      "ai_keywords": [
        "large language models",
        "RLVR",
        "RLVER",
        "reinforcement learning",
        "affective simulated users",
        "dialogue rollouts",
        "deterministic emotion scores",
        "reward signals",
        "fine-tuning",
        "Qwen2.5-7B-Instruct",
        "PPO",
        "Sentient-Benchmark",
        "thinking models",
        "non-thinking models",
        "GRPO",
        "dialogue capabilities",
        "empathy",
        "insight",
        "action",
        "emotionally intelligent",
        "broadly capable language agents"
      ]
    },
    "publishedAt": "2025-07-03T14:33:18.000Z",
    "title": "RLVER: Reinforcement Learning with Verifiable Emotion Rewards for\n  Empathetic Agents",
    "summary": "Large language models (LLMs) excel at logical and algorithmic reasoning, yet\ntheir emotional intelligence (EQ) still lags far behind their cognitive\nprowess. While reinforcement learning from verifiable rewards (RLVR) has\nadvanced in other domains, its application to dialogue-especially for emotional\nintelligence-remains underexplored. In this work, we introduce RLVER, the first\nend-to-end reinforcement learning framework that leverages verifiable emotion\nrewards from simulated users to cultivate higher-order empathetic abilities in\nLLMs. Within this framework, self-consistent affective simulated users engage\nin dialogue rollouts and produce deterministic emotion scores during\nconversations, serving as reward signals to guide the LLM's learning.\nFine-tuning publicly available Qwen2.5-7B-Instruct model with PPO boosts its\nSentient-Benchmark score from 13.3 to 79.2 while largely preserving\nmathematical and coding competence. Extensive experiments reveal that: (i)\nRLVER consistently improves multiple dialogue capabilities; (ii) Thinking and\nnon-thinking models show distinct trends--thinking models excel in empathy and\ninsight, while non-thinking models favor action; (iii) GRPO often yields stable\ngains, while PPO can push certain capabilities to a higher ceiling; (iv) More\nchallenging environments are not always better-moderate ones can yield stronger\noutcomes. Our results show that RLVER is a practical route toward emotionally\nintelligent and broadly capable language agents.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.03112.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "61b859ddbdf1fac5ed499992",
      "avatarUrl": "/avatars/2387fb9b8a46840bfc75248462f0a410.svg",
      "fullname": "Jiaqi Chen",
      "name": "judge",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.05566",
      "authors": [
        {
          "_id": "686e0a5dcb5725779c60b4e6",
          "name": "David Bensaïd",
          "hidden": false
        },
        {
          "_id": "686e0a5dcb5725779c60b4e7",
          "name": "Noam Rotstein",
          "hidden": false
        },
        {
          "_id": "686e0a5dcb5725779c60b4e8",
          "name": "Roy Velich",
          "hidden": false
        },
        {
          "_id": "686e0a5dcb5725779c60b4e9",
          "name": "Daniel Bensaïd",
          "hidden": false
        },
        {
          "_id": "686e0a5dcb5725779c60b4ea",
          "name": "Ron Kimmel",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-08T01:11:30.000Z",
      "submittedOnDailyAt": "2025-07-09T04:55:03.373Z",
      "title": "SingLoRA: Low Rank Adaptation Using a Single Matrix",
      "submittedOnDailyBy": {
        "_id": "62b3e85bcbd2a402fc7804b1",
        "avatarUrl": "/avatars/63125ce8a1e20b8c6e836f223d24284f.svg",
        "isPro": false,
        "fullname": "noam rotstein",
        "user": "noamrot",
        "type": "user"
      },
      "summary": "Low-Rank Adaptation (LoRA) has significantly advanced parameter-efficient\nfine-tuning of large pretrained models. LoRA augments the pre-trained weights\nof a model by adding the product of two smaller matrices that together form a\nlow-rank matrix update. Recent research has shown that scale disparities\nbetween these two matrices often cause unstable training dynamics, leading to\nsuboptimal performance. In this paper, we propose SingLoRA, which reformulates\nlow-rank adaptation by learning the weights update as a decomposition of a\nsingle low-rank matrix multiplied by its transpose. This simple design\ninherently removes inter-matrix scale conflicts, ensuring stable optimization,\nand roughly halves the parameter count. We analyze SingLoRA within the\ninfinite-width neural network framework, showing that it guarantees stable\nfeature learning by construction. Extensive experiments on multiple tasks\nvalidate these benefits. In common sense reasoning, fine-tuning LLama 7B on\nMNLI with SingLoRA achieves 91.3% accuracy - surpassing LoRA (89.1%) and LoRA+\n(90.2%) - while using only 60% of their parameter budget. In image generation,\nfine-tuning Stable Diffusion with SingLoRA significantly improves image\nfidelity on DreamBooth, achieving a DINO similarity score of 0.151, compared to\nscores of 0.148 and 0.143 for DoRA and LoRA, respectively.",
      "upvotes": 21,
      "discussionId": "686e0a5dcb5725779c60b4eb",
      "ai_summary": "SingLoRA, a reformulated low-rank adaptation method, enhances parameter-efficient fine-tuning by learning a single low-rank matrix update, ensuring stable optimization and reduced parameter count.",
      "ai_keywords": [
        "Low-Rank Adaptation",
        "LoRA",
        "SingLoRA",
        "low-rank matrix",
        "infinite-width neural network",
        "feature learning",
        "common sense reasoning",
        "fine-tuning",
        "LLama 7B",
        "MNLI",
        "image generation",
        "Stable Diffusion",
        "DreamBooth",
        "DINO similarity score",
        "DoRA"
      ]
    },
    "publishedAt": "2025-07-07T21:11:30.000Z",
    "title": "SingLoRA: Low Rank Adaptation Using a Single Matrix",
    "summary": "Low-Rank Adaptation (LoRA) has significantly advanced parameter-efficient\nfine-tuning of large pretrained models. LoRA augments the pre-trained weights\nof a model by adding the product of two smaller matrices that together form a\nlow-rank matrix update. Recent research has shown that scale disparities\nbetween these two matrices often cause unstable training dynamics, leading to\nsuboptimal performance. In this paper, we propose SingLoRA, which reformulates\nlow-rank adaptation by learning the weights update as a decomposition of a\nsingle low-rank matrix multiplied by its transpose. This simple design\ninherently removes inter-matrix scale conflicts, ensuring stable optimization,\nand roughly halves the parameter count. We analyze SingLoRA within the\ninfinite-width neural network framework, showing that it guarantees stable\nfeature learning by construction. Extensive experiments on multiple tasks\nvalidate these benefits. In common sense reasoning, fine-tuning LLama 7B on\nMNLI with SingLoRA achieves 91.3% accuracy - surpassing LoRA (89.1%) and LoRA+\n(90.2%) - while using only 60% of their parameter budget. In image generation,\nfine-tuning Stable Diffusion with SingLoRA significantly improves image\nfidelity on DreamBooth, achieving a DINO similarity score of 0.151, compared to\nscores of 0.148 and 0.143 for DoRA and LoRA, respectively.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05566.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b3e85bcbd2a402fc7804b1",
      "avatarUrl": "/avatars/63125ce8a1e20b8c6e836f223d24284f.svg",
      "fullname": "noam rotstein",
      "name": "noamrot",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.05675",
      "authors": [
        {
          "_id": "686ddcfacb5725779c60b427",
          "name": "Rongsheng Wang",
          "hidden": false
        },
        {
          "_id": "686ddcfacb5725779c60b428",
          "name": "Junying Chen",
          "hidden": false
        },
        {
          "_id": "686ddcfacb5725779c60b429",
          "name": "Ke Ji",
          "hidden": false
        },
        {
          "_id": "686ddcfacb5725779c60b42a",
          "name": "Zhenyang Cai",
          "hidden": false
        },
        {
          "_id": "686ddcfacb5725779c60b42b",
          "name": "Shunian Chen",
          "hidden": false
        },
        {
          "_id": "686ddcfacb5725779c60b42c",
          "name": "Yunjin Yang",
          "hidden": false
        },
        {
          "_id": "686ddcfacb5725779c60b42d",
          "name": "Benyou Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-08T04:58:36.000Z",
      "submittedOnDailyAt": "2025-07-09T01:40:25.246Z",
      "title": "MedGen: Unlocking Medical Video Generation by Scaling\n  Granularly-annotated Medical Videos",
      "submittedOnDailyBy": {
        "_id": "63ca949b04c979828315389d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ca949b04c979828315389d/HS5xWNAYjjHeyAAwWJ11l.jpeg",
        "isPro": false,
        "fullname": "wangrongsheng",
        "user": "wangrongsheng",
        "type": "user"
      },
      "summary": "Recent advances in video generation have shown remarkable progress in\nopen-domain settings, yet medical video generation remains largely\nunderexplored. Medical videos are critical for applications such as clinical\ntraining, education, and simulation, requiring not only high visual fidelity\nbut also strict medical accuracy. However, current models often produce\nunrealistic or erroneous content when applied to medical prompts, largely due\nto the lack of large-scale, high-quality datasets tailored to the medical\ndomain. To address this gap, we introduce MedVideoCap-55K, the first\nlarge-scale, diverse, and caption-rich dataset for medical video generation. It\ncomprises over 55,000 curated clips spanning real-world medical scenarios,\nproviding a strong foundation for training generalist medical video generation\nmodels. Built upon this dataset, we develop MedGen, which achieves leading\nperformance among open-source models and rivals commercial systems across\nmultiple benchmarks in both visual quality and medical accuracy. We hope our\ndataset and model can serve as a valuable resource and help catalyze further\nresearch in medical video generation. Our code and data is available at\nhttps://github.com/FreedomIntelligence/MedGen",
      "upvotes": 18,
      "discussionId": "686ddcfbcb5725779c60b42e",
      "githubRepo": "https://github.com/FreedomIntelligence/MedGen",
      "ai_summary": "MedGen, a model trained on the large-scale MedVideoCap-55K dataset, achieves top performance in medical video generation by balancing visual quality and medical accuracy.",
      "ai_keywords": [
        "video generation",
        "medical video generation",
        "MedVideoCap-55K",
        "MedGen",
        "visual quality",
        "medical accuracy",
        "benchmarks"
      ],
      "githubStars": 9
    },
    "publishedAt": "2025-07-08T00:58:36.000Z",
    "title": "MedGen: Unlocking Medical Video Generation by Scaling\n  Granularly-annotated Medical Videos",
    "summary": "Recent advances in video generation have shown remarkable progress in\nopen-domain settings, yet medical video generation remains largely\nunderexplored. Medical videos are critical for applications such as clinical\ntraining, education, and simulation, requiring not only high visual fidelity\nbut also strict medical accuracy. However, current models often produce\nunrealistic or erroneous content when applied to medical prompts, largely due\nto the lack of large-scale, high-quality datasets tailored to the medical\ndomain. To address this gap, we introduce MedVideoCap-55K, the first\nlarge-scale, diverse, and caption-rich dataset for medical video generation. It\ncomprises over 55,000 curated clips spanning real-world medical scenarios,\nproviding a strong foundation for training generalist medical video generation\nmodels. Built upon this dataset, we develop MedGen, which achieves leading\nperformance among open-source models and rivals commercial systems across\nmultiple benchmarks in both visual quality and medical accuracy. We hope our\ndataset and model can serve as a valuable resource and help catalyze further\nresearch in medical video generation. Our code and data is available at\nhttps://github.com/FreedomIntelligence/MedGen",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05675.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63ca949b04c979828315389d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ca949b04c979828315389d/HS5xWNAYjjHeyAAwWJ11l.jpeg",
      "fullname": "wangrongsheng",
      "name": "wangrongsheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 60
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.06138",
      "authors": [
        {
          "_id": "686ddd6acb5725779c60b43c",
          "name": "Taolin Zhang",
          "hidden": false
        },
        {
          "_id": "686ddd6acb5725779c60b43d",
          "name": "Zihan Ma",
          "hidden": false
        },
        {
          "_id": "686ddd6acb5725779c60b43e",
          "name": "Maosong Cao",
          "hidden": false
        },
        {
          "_id": "686ddd6acb5725779c60b43f",
          "name": "Junnan Liu",
          "hidden": false
        },
        {
          "_id": "686ddd6acb5725779c60b440",
          "name": "Songyang Zhang",
          "hidden": false
        },
        {
          "_id": "686ddd6acb5725779c60b441",
          "name": "Kai Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-08T16:20:43.000Z",
      "submittedOnDailyAt": "2025-07-09T01:39:34.405Z",
      "title": "Coding Triangle: How Does Large Language Model Understand Code?",
      "submittedOnDailyBy": {
        "_id": "630716d11801ecc7d2595021",
        "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
        "isPro": false,
        "fullname": "Songyang Zhang",
        "user": "zsytony",
        "type": "user"
      },
      "summary": "Large language models (LLMs) have achieved remarkable progress in code\ngeneration, yet their true programming competence remains underexplored. We\nintroduce the Code Triangle framework, which systematically evaluates LLMs\nacross three fundamental dimensions: editorial analysis, code implementation,\nand test case generation. Through extensive experiments on competitive\nprogramming benchmarks, we reveal that while LLMs can form a self-consistent\nsystem across these dimensions, their solutions often lack the diversity and\nrobustness of human programmers. We identify a significant distribution shift\nbetween model cognition and human expertise, with model errors tending to\ncluster due to training data biases and limited reasoning transfer. Our study\ndemonstrates that incorporating human-generated editorials, solutions, and\ndiverse test cases, as well as leveraging model mixtures, can substantially\nenhance both the performance and robustness of LLMs. Furthermore, we reveal\nboth the consistency and inconsistency in the cognition of LLMs that may\nfacilitate self-reflection and self-improvement, providing a potential\ndirection for developing more powerful coding models.",
      "upvotes": 13,
      "discussionId": "686ddd6dcb5725779c60b442",
      "ai_summary": "The Code Triangle framework evaluates large language models across editorial analysis, code implementation, and test case generation, revealing limitations in diversity and robustness compared to human programmers and suggesting enhancements through human-generated content and model mixtures.",
      "ai_keywords": [
        "Code Triangle framework",
        "large language models",
        "editorial analysis",
        "code implementation",
        "test case generation",
        "competitive programming benchmarks",
        "self-consistent system",
        "distribution shift",
        "training data biases",
        "reasoning transfer",
        "human-generated editorials",
        "model mixtures",
        "self-reflection",
        "self-improvement"
      ]
    },
    "publishedAt": "2025-07-08T12:20:43.000Z",
    "title": "Coding Triangle: How Does Large Language Model Understand Code?",
    "summary": "Large language models (LLMs) have achieved remarkable progress in code\ngeneration, yet their true programming competence remains underexplored. We\nintroduce the Code Triangle framework, which systematically evaluates LLMs\nacross three fundamental dimensions: editorial analysis, code implementation,\nand test case generation. Through extensive experiments on competitive\nprogramming benchmarks, we reveal that while LLMs can form a self-consistent\nsystem across these dimensions, their solutions often lack the diversity and\nrobustness of human programmers. We identify a significant distribution shift\nbetween model cognition and human expertise, with model errors tending to\ncluster due to training data biases and limited reasoning transfer. Our study\ndemonstrates that incorporating human-generated editorials, solutions, and\ndiverse test cases, as well as leveraging model mixtures, can substantially\nenhance both the performance and robustness of LLMs. Furthermore, we reveal\nboth the consistency and inconsistency in the cognition of LLMs that may\nfacilitate self-reflection and self-improvement, providing a potential\ndirection for developing more powerful coding models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06138.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "630716d11801ecc7d2595021",
      "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
      "fullname": "Songyang Zhang",
      "name": "zsytony",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 18
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.06219",
      "authors": [
        {
          "_id": "686dcc7fcb5725779c60b3a8",
          "name": "Modi Shi",
          "hidden": false
        },
        {
          "_id": "686dcc7fcb5725779c60b3a9",
          "name": "Li Chen",
          "hidden": false
        },
        {
          "_id": "686dcc7fcb5725779c60b3aa",
          "name": "Jin Chen",
          "hidden": false
        },
        {
          "_id": "686dcc7fcb5725779c60b3ab",
          "name": "Yuxiang Lu",
          "hidden": false
        },
        {
          "_id": "686dcc7fcb5725779c60b3ac",
          "name": "Chiming Liu",
          "hidden": false
        },
        {
          "_id": "686dcc7fcb5725779c60b3ad",
          "name": "Guanghui Ren",
          "hidden": false
        },
        {
          "_id": "686dcc7fcb5725779c60b3ae",
          "name": "Ping Luo",
          "hidden": false
        },
        {
          "_id": "686dcc7fcb5725779c60b3af",
          "name": "Di Huang",
          "hidden": false
        },
        {
          "_id": "686dcc7fcb5725779c60b3b0",
          "name": "Maoqing Yao",
          "hidden": false
        },
        {
          "_id": "686dcc7fcb5725779c60b3b1",
          "name": "Hongyang Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-08T17:52:44.000Z",
      "submittedOnDailyAt": "2025-07-09T00:29:16.132Z",
      "title": "Is Diversity All You Need for Scalable Robotic Manipulation?",
      "submittedOnDailyBy": {
        "_id": "64b8faeb8b53fb5dbdfecae5",
        "avatarUrl": "/avatars/9f5919600ee69c38be896dd959bb8724.svg",
        "isPro": false,
        "fullname": "Yuxiang Lu",
        "user": "yxlu0",
        "type": "user"
      },
      "summary": "Data scaling has driven remarkable success in foundation models for Natural\nLanguage Processing (NLP) and Computer Vision (CV), yet the principles of\neffective data scaling in robotic manipulation remain insufficiently\nunderstood. In this work, we investigate the nuanced role of data diversity in\nrobot learning by examining three critical dimensions-task (what to do),\nembodiment (which robot to use), and expert (who demonstrates)-challenging the\nconventional intuition of \"more diverse is better\". Throughout extensive\nexperiments on various robot platforms, we reveal that (1) task diversity\nproves more critical than per-task demonstration quantity, benefiting transfer\nfrom diverse pre-training tasks to novel downstream scenarios; (2)\nmulti-embodiment pre-training data is optional for cross-embodiment\ntransfer-models trained on high-quality single-embodiment data can efficiently\ntransfer to different platforms, showing more desirable scaling property during\nfine-tuning than multi-embodiment pre-trained models; and (3) expert diversity,\narising from individual operational preferences and stochastic variations in\nhuman demonstrations, can be confounding to policy learning, with velocity\nmultimodality emerging as a key contributing factor. Based on this insight, we\npropose a distribution debiasing method to mitigate velocity ambiguity, the\nyielding GO-1-Pro achieves substantial performance gains of 15%, equivalent to\nusing 2.5 times pre-training data. Collectively, these findings provide new\nperspectives and offer practical guidance on how to scale robotic manipulation\ndatasets effectively.",
      "upvotes": 12,
      "discussionId": "686dcc7fcb5725779c60b3b2",
      "ai_summary": "Investigation into data diversity in robotic manipulation reveals that task diversity is crucial, multi-embodiment data is optional, and expert diversity can be confounding, leading to a distribution debiasing method for improved performance.",
      "ai_keywords": [
        "task diversity",
        "embodiment",
        "expert diversity",
        "multi-embodiment pre-training",
        "cross-embodiment transfer",
        "policy learning",
        "distribution debiasing"
      ]
    },
    "publishedAt": "2025-07-08T13:52:44.000Z",
    "title": "Is Diversity All You Need for Scalable Robotic Manipulation?",
    "summary": "Data scaling has driven remarkable success in foundation models for Natural\nLanguage Processing (NLP) and Computer Vision (CV), yet the principles of\neffective data scaling in robotic manipulation remain insufficiently\nunderstood. In this work, we investigate the nuanced role of data diversity in\nrobot learning by examining three critical dimensions-task (what to do),\nembodiment (which robot to use), and expert (who demonstrates)-challenging the\nconventional intuition of \"more diverse is better\". Throughout extensive\nexperiments on various robot platforms, we reveal that (1) task diversity\nproves more critical than per-task demonstration quantity, benefiting transfer\nfrom diverse pre-training tasks to novel downstream scenarios; (2)\nmulti-embodiment pre-training data is optional for cross-embodiment\ntransfer-models trained on high-quality single-embodiment data can efficiently\ntransfer to different platforms, showing more desirable scaling property during\nfine-tuning than multi-embodiment pre-trained models; and (3) expert diversity,\narising from individual operational preferences and stochastic variations in\nhuman demonstrations, can be confounding to policy learning, with velocity\nmultimodality emerging as a key contributing factor. Based on this insight, we\npropose a distribution debiasing method to mitigate velocity ambiguity, the\nyielding GO-1-Pro achieves substantial performance gains of 15%, equivalent to\nusing 2.5 times pre-training data. Collectively, these findings provide new\nperspectives and offer practical guidance on how to scale robotic manipulation\ndatasets effectively.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06219.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64b8faeb8b53fb5dbdfecae5",
      "avatarUrl": "/avatars/9f5919600ee69c38be896dd959bb8724.svg",
      "fullname": "Yuxiang Lu",
      "name": "yxlu0",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.05791",
      "authors": [
        {
          "_id": "686dcbe6cb5725779c60b37f",
          "name": "Yan Yang",
          "hidden": false
        },
        {
          "_id": "686dcbe6cb5725779c60b380",
          "name": "Dongxu Li",
          "hidden": false
        },
        {
          "_id": "686dcbe6cb5725779c60b381",
          "name": "Yutong Dai",
          "hidden": false
        },
        {
          "_id": "686dcbe6cb5725779c60b382",
          "name": "Yuhao Yang",
          "hidden": false
        },
        {
          "_id": "686dcbe6cb5725779c60b383",
          "name": "Ziyang Luo",
          "hidden": false
        },
        {
          "_id": "686dcbe6cb5725779c60b384",
          "name": "Zirui Zhao",
          "hidden": false
        },
        {
          "_id": "686dcbe6cb5725779c60b385",
          "name": "Zhiyuan Hu",
          "hidden": false
        },
        {
          "_id": "686dcbe6cb5725779c60b386",
          "name": "Junzhe Huang",
          "hidden": false
        },
        {
          "_id": "686dcbe6cb5725779c60b387",
          "name": "Amrita Saha",
          "hidden": false
        },
        {
          "_id": "686dcbe6cb5725779c60b388",
          "name": "Zeyuan Chen",
          "hidden": false
        },
        {
          "_id": "686dcbe6cb5725779c60b389",
          "name": "Ran Xu",
          "hidden": false
        },
        {
          "_id": "686dcbe6cb5725779c60b38a",
          "name": "Liyuan Pan",
          "hidden": false
        },
        {
          "_id": "686dcbe6cb5725779c60b38b",
          "name": "Caiming Xiong",
          "hidden": false
        },
        {
          "_id": "686dcbe6cb5725779c60b38c",
          "name": "Junnan Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-08T08:52:18.000Z",
      "submittedOnDailyAt": "2025-07-09T00:25:26.046Z",
      "title": "GTA1: GUI Test-time Scaling Agent",
      "submittedOnDailyBy": {
        "_id": "655b813476e4fad5529f3256",
        "avatarUrl": "/avatars/73d83e45d921531f9830a0ea80f76491.svg",
        "isPro": true,
        "fullname": "Yan Yang",
        "user": "HelloKKMe",
        "type": "user"
      },
      "summary": "Graphical user interface (GUI) agents autonomously operate across platforms\n(e.g., Linux) to complete tasks by interacting with visual elements.\nSpecifically, a user instruction is decomposed into a sequence of action\nproposals, each corresponding to an interaction with the GUI. After each\naction, the agent observes the updated GUI environment to plan the next step.\nHowever, two main challenges arise: i) resolving ambiguity in task planning\n(i.e., the action proposal sequence), where selecting an appropriate plan is\nnon-trivial, as many valid ones may exist; ii) accurately grounding actions in\ncomplex and high-resolution interfaces, i.e., precisely interacting with visual\ntargets.\n  This paper investigates the two aforementioned challenges with our GUI\nTest-time Scaling Agent, namely GTA1. First, to select the most appropriate\naction proposal, we introduce a test-time scaling method. At each step, we\nsample multiple candidate action proposals and leverage a judge model to\nevaluate and select the most suitable one. It trades off computation for better\ndecision quality by concurrent sampling, shortening task execution steps, and\nimproving overall performance. Second, we propose a model that achieves\nimproved accuracy when grounding the selected action proposal to its\ncorresponding visual elements. Our key insight is that reinforcement learning\n(RL) facilitates visual grounding through inherent objective alignments,\nrewarding successful clicks on interface elements.\n  Experimentally, our method establishes state-of-the-art performance across\ndiverse benchmarks. For example, GTA1-7B achieves 50.1%, 92.4%, and 67.7%\naccuracies on Screenspot-Pro, Screenspot-V2, and OSWorld-G, respectively. When\npaired with a planner applying our test-time scaling strategy, it exhibits\nstate-of-the-art agentic performance (e.g., 45.2% task success rate on\nOSWorld). We open-source our code and models here.",
      "upvotes": 11,
      "discussionId": "686dcbe6cb5725779c60b38d",
      "githubRepo": "https://github.com/Yan98/GTA1",
      "ai_summary": "GTA1 addresses task planning ambiguity and visual grounding in GUI interactions using test-time scaling and reinforcement learning, achieving state-of-the-art performance across benchmarks.",
      "ai_keywords": [
        "GUI",
        "test-time scaling",
        "action proposals",
        "judge model",
        "reinforcement learning",
        "visual grounding",
        "task planning",
        "state-of-the-art",
        "Screenspot-Pro",
        "Screenspot-V2",
        "OSWorld-G",
        "task success rate"
      ],
      "githubStars": 22
    },
    "publishedAt": "2025-07-08T04:52:18.000Z",
    "title": "GTA1: GUI Test-time Scaling Agent",
    "summary": "Graphical user interface (GUI) agents autonomously operate across platforms\n(e.g., Linux) to complete tasks by interacting with visual elements.\nSpecifically, a user instruction is decomposed into a sequence of action\nproposals, each corresponding to an interaction with the GUI. After each\naction, the agent observes the updated GUI environment to plan the next step.\nHowever, two main challenges arise: i) resolving ambiguity in task planning\n(i.e., the action proposal sequence), where selecting an appropriate plan is\nnon-trivial, as many valid ones may exist; ii) accurately grounding actions in\ncomplex and high-resolution interfaces, i.e., precisely interacting with visual\ntargets.\n  This paper investigates the two aforementioned challenges with our GUI\nTest-time Scaling Agent, namely GTA1. First, to select the most appropriate\naction proposal, we introduce a test-time scaling method. At each step, we\nsample multiple candidate action proposals and leverage a judge model to\nevaluate and select the most suitable one. It trades off computation for better\ndecision quality by concurrent sampling, shortening task execution steps, and\nimproving overall performance. Second, we propose a model that achieves\nimproved accuracy when grounding the selected action proposal to its\ncorresponding visual elements. Our key insight is that reinforcement learning\n(RL) facilitates visual grounding through inherent objective alignments,\nrewarding successful clicks on interface elements.\n  Experimentally, our method establishes state-of-the-art performance across\ndiverse benchmarks. For example, GTA1-7B achieves 50.1%, 92.4%, and 67.7%\naccuracies on Screenspot-Pro, Screenspot-V2, and OSWorld-G, respectively. When\npaired with a planner applying our test-time scaling strategy, it exhibits\nstate-of-the-art agentic performance (e.g., 45.2% task success rate on\nOSWorld). We open-source our code and models here.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05791.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655b813476e4fad5529f3256",
      "avatarUrl": "/avatars/73d83e45d921531f9830a0ea80f76491.svg",
      "fullname": "Yan Yang",
      "name": "HelloKKMe",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 16
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.06223",
      "authors": [
        {
          "_id": "686dc72ccb5725779c60b356",
          "name": "Zhiyuan Peng",
          "hidden": false
        },
        {
          "_id": "686dc72ccb5725779c60b357",
          "name": "Ting-ruen Wei",
          "hidden": false
        },
        {
          "_id": "686dc72ccb5725779c60b358",
          "name": "Tingyu Song",
          "hidden": false
        },
        {
          "_id": "686dc72ccb5725779c60b359",
          "name": "Yilun Zhao",
          "hidden": false
        },
        {
          "_id": "686dc72ccb5725779c60b35a",
          "name": "Yi Fang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-08T17:56:28.000Z",
      "submittedOnDailyAt": "2025-07-09T00:20:14.472Z",
      "title": "Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers",
      "submittedOnDailyBy": {
        "_id": "64dc29d9b5d625e0e9a6ecb9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/QxGBsnk1cNsBEPqSx4ae-.jpeg",
        "isPro": false,
        "fullname": "Tingyu Song",
        "user": "songtingyu",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have recently been applied to reranking tasks in\ninformation retrieval, achieving strong performance. However, their high\ncomputational demands often hinder practical deployment. Existing studies\nevaluate the efficiency of LLM-based rerankers using proxy metrics such as\nlatency, the number of forward passes, input tokens, and output tokens.\nHowever, these metrics depend on hardware and running-time choices (\\eg\nparallel or not, batch size, etc), and often fail to account for model size,\nmaking it difficult to interpret and obscuring the evaluation of the\nefficiency-effectiveness tradeoff. To address this issue, we propose\nE2R-FLOPs, for LLM-based rerankers: ranking metrics per\nPetaFLOP (RPP) for relevance per compute and queries per PetaFLOP (QPP) for\nhardware-agnostic throughput. Companied with the new metrics, an interpretable\nFLOPs estimator is built to estimate the FLOPs of an LLM-based reranker even\nwithout running any experiments. Based on the proposed metrics, we conduct\ncomprehensive experiments to evaluate a wide range of LLM-based rerankers with\ndifferent architecture, studying the efficiency-effectiveness trade-off and\nbringing this issue to the attention of the research community.",
      "upvotes": 9,
      "discussionId": "686dc72ccb5725779c60b35b",
      "ai_summary": "E\\textsuperscript{2}R-FLOPs evaluates LLM-based rerankers by measuring relevance and throughput per PetaFLOP, providing a hardware-agnostic metric for efficiency and effectiveness.",
      "ai_keywords": [
        "E\\textsuperscript{2}R-FLOPs",
        "ranking metrics per PetaFLOP",
        "queries per PetaFLOP",
        "FLOPs estimator",
        "LLM-based rerankers",
        "efficiency-effectiveness trade-off"
      ]
    },
    "publishedAt": "2025-07-08T13:56:28.000Z",
    "title": "Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers",
    "summary": "Large Language Models (LLMs) have recently been applied to reranking tasks in\ninformation retrieval, achieving strong performance. However, their high\ncomputational demands often hinder practical deployment. Existing studies\nevaluate the efficiency of LLM-based rerankers using proxy metrics such as\nlatency, the number of forward passes, input tokens, and output tokens.\nHowever, these metrics depend on hardware and running-time choices (\\eg\nparallel or not, batch size, etc), and often fail to account for model size,\nmaking it difficult to interpret and obscuring the evaluation of the\nefficiency-effectiveness tradeoff. To address this issue, we propose\nE2R-FLOPs, for LLM-based rerankers: ranking metrics per\nPetaFLOP (RPP) for relevance per compute and queries per PetaFLOP (QPP) for\nhardware-agnostic throughput. Companied with the new metrics, an interpretable\nFLOPs estimator is built to estimate the FLOPs of an LLM-based reranker even\nwithout running any experiments. Based on the proposed metrics, we conduct\ncomprehensive experiments to evaluate a wide range of LLM-based rerankers with\ndifferent architecture, studying the efficiency-effectiveness trade-off and\nbringing this issue to the attention of the research community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06223.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64dc29d9b5d625e0e9a6ecb9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/QxGBsnk1cNsBEPqSx4ae-.jpeg",
      "fullname": "Tingyu Song",
      "name": "songtingyu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.03698",
      "authors": [
        {
          "_id": "686dc7ebcb5725779c60b35d",
          "name": "Zhiling Yan",
          "hidden": false
        },
        {
          "_id": "686dc7ebcb5725779c60b35e",
          "name": "Sifan Song",
          "hidden": false
        },
        {
          "_id": "686dc7ebcb5725779c60b35f",
          "name": "Dingjie Song",
          "hidden": false
        },
        {
          "_id": "686dc7ebcb5725779c60b360",
          "name": "Yiwei Li",
          "hidden": false
        },
        {
          "_id": "686dc7ebcb5725779c60b361",
          "name": "Rong Zhou",
          "hidden": false
        },
        {
          "_id": "686dc7ebcb5725779c60b362",
          "name": "Weixiang Sun",
          "hidden": false
        },
        {
          "_id": "686dc7ebcb5725779c60b363",
          "name": "Zhennong Chen",
          "hidden": false
        },
        {
          "_id": "686dc7ebcb5725779c60b364",
          "name": "Sekeun Kim",
          "hidden": false
        },
        {
          "_id": "686dc7ebcb5725779c60b365",
          "name": "Hui Ren",
          "hidden": false
        },
        {
          "_id": "686dc7ebcb5725779c60b366",
          "name": "Tianming Liu",
          "hidden": false
        },
        {
          "_id": "686dc7ebcb5725779c60b367",
          "name": "Quanzheng Li",
          "hidden": false
        },
        {
          "_id": "686dc7ebcb5725779c60b368",
          "name": "Xiang Li",
          "hidden": false
        },
        {
          "_id": "686dc7ebcb5725779c60b369",
          "name": "Lifang He",
          "hidden": false
        },
        {
          "_id": "686dc7ebcb5725779c60b36a",
          "name": "Lichao Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-04T16:30:38.000Z",
      "submittedOnDailyAt": "2025-07-09T00:08:40.667Z",
      "title": "SAMed-2: Selective Memory Enhanced Medical Segment Anything Model",
      "submittedOnDailyBy": {
        "_id": "619f01b8cc04eadf54fa5d5d",
        "avatarUrl": "/avatars/928f3d1a6146e2e1ae4860445d929d5c.svg",
        "isPro": false,
        "fullname": "Song Dingjie",
        "user": "songdj",
        "type": "user"
      },
      "summary": "Recent \"segment anything\" efforts show promise by learning from large-scale\ndata, but adapting such models directly to medical images remains challenging\ndue to the complexity of medical data, noisy annotations, and continual\nlearning requirements across diverse modalities and anatomical structures. In\nthis work, we propose SAMed-2, a new foundation model for medical image\nsegmentation built upon the SAM-2 architecture. Specifically, we introduce a\ntemporal adapter into the image encoder to capture image correlations and a\nconfidence-driven memory mechanism to store high-certainty features for later\nretrieval. This memory-based strategy counters the pervasive noise in\nlarge-scale medical datasets and mitigates catastrophic forgetting when\nencountering new tasks or modalities. To train and evaluate SAMed-2, we curate\nMedBank-100k, a comprehensive dataset spanning seven imaging modalities and 21\nmedical segmentation tasks. Our experiments on both internal benchmarks and 10\nexternal datasets demonstrate superior performance over state-of-the-art\nbaselines in multi-task scenarios. The code is available at:\nhttps://github.com/ZhilingYan/Medical-SAM-Bench.",
      "upvotes": 8,
      "discussionId": "686dc7eccb5725779c60b36b",
      "ai_summary": "SAMed-2, an adaptation of SAM-2 for medical image segmentation, incorporates a temporal adapter and confidence-driven memory to improve performance across diverse medical datasets and tasks.",
      "ai_keywords": [
        "SAM-2",
        "temporal adapter",
        "confidence-driven memory",
        "MedBank-100k",
        "medical segmentation",
        "catastrophic forgetting",
        "multi-task scenarios"
      ]
    },
    "publishedAt": "2025-07-04T12:30:38.000Z",
    "title": "SAMed-2: Selective Memory Enhanced Medical Segment Anything Model",
    "summary": "Recent \"segment anything\" efforts show promise by learning from large-scale\ndata, but adapting such models directly to medical images remains challenging\ndue to the complexity of medical data, noisy annotations, and continual\nlearning requirements across diverse modalities and anatomical structures. In\nthis work, we propose SAMed-2, a new foundation model for medical image\nsegmentation built upon the SAM-2 architecture. Specifically, we introduce a\ntemporal adapter into the image encoder to capture image correlations and a\nconfidence-driven memory mechanism to store high-certainty features for later\nretrieval. This memory-based strategy counters the pervasive noise in\nlarge-scale medical datasets and mitigates catastrophic forgetting when\nencountering new tasks or modalities. To train and evaluate SAMed-2, we curate\nMedBank-100k, a comprehensive dataset spanning seven imaging modalities and 21\nmedical segmentation tasks. Our experiments on both internal benchmarks and 10\nexternal datasets demonstrate superior performance over state-of-the-art\nbaselines in multi-task scenarios. The code is available at:\nhttps://github.com/ZhilingYan/Medical-SAM-Bench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.03698.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "619f01b8cc04eadf54fa5d5d",
      "avatarUrl": "/avatars/928f3d1a6146e2e1ae4860445d929d5c.svg",
      "fullname": "Song Dingjie",
      "name": "songdj",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.05101",
      "authors": [
        {
          "_id": "686dc2d1cb5725779c60b342",
          "name": "Xinzhe Zheng",
          "hidden": false
        },
        {
          "_id": "686dc2d1cb5725779c60b343",
          "name": "Hao Du",
          "hidden": false
        },
        {
          "_id": "686dc2d1cb5725779c60b344",
          "name": "Fanding Xu",
          "hidden": false
        },
        {
          "_id": "686dc2d1cb5725779c60b345",
          "name": "Jinzhe Li",
          "hidden": false
        },
        {
          "_id": "686dc2d1cb5725779c60b346",
          "name": "Zhiyuan Liu",
          "hidden": false
        },
        {
          "_id": "686dc2d1cb5725779c60b347",
          "name": "Wenkang Wang",
          "hidden": false
        },
        {
          "_id": "686dc2d1cb5725779c60b348",
          "name": "Tao Chen",
          "hidden": false
        },
        {
          "_id": "686dc2d1cb5725779c60b349",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "686dc2d1cb5725779c60b34a",
          "name": "Stan Z. Li",
          "hidden": false
        },
        {
          "_id": "686dc2d1cb5725779c60b34b",
          "name": "Yan Lu",
          "hidden": false
        },
        {
          "_id": "686dc2d1cb5725779c60b34c",
          "name": "Nanqing Dong",
          "hidden": false
        },
        {
          "_id": "686dc2d1cb5725779c60b34d",
          "name": "Yang Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T15:21:05.000Z",
      "submittedOnDailyAt": "2025-07-09T01:01:03.312Z",
      "title": "PRING: Rethinking Protein-Protein Interaction Prediction from Pairs to\n  Graphs",
      "submittedOnDailyBy": {
        "_id": "6310a3cd531cc21f9e06de6a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6310a3cd531cc21f9e06de6a/aTGMx3O41lUARK9s3dAik.jpeg",
        "isPro": false,
        "fullname": "Zhiyuan Liu",
        "user": "acharkq",
        "type": "user"
      },
      "summary": "Deep learning-based computational methods have achieved promising results in\npredicting protein-protein interactions (PPIs). However, existing benchmarks\npredominantly focus on isolated pairwise evaluations, overlooking a model's\ncapability to reconstruct biologically meaningful PPI networks, which is\ncrucial for biology research. To address this gap, we introduce PRING, the\nfirst comprehensive benchmark that evaluates protein-protein interaction\nprediction from a graph-level perspective. PRING curates a high-quality,\nmulti-species PPI network dataset comprising 21,484 proteins and 186,818\ninteractions, with well-designed strategies to address both data redundancy and\nleakage. Building on this golden-standard dataset, we establish two\ncomplementary evaluation paradigms: (1) topology-oriented tasks, which assess\nintra and cross-species PPI network construction, and (2) function-oriented\ntasks, including protein complex pathway prediction, GO module analysis, and\nessential protein justification. These evaluations not only reflect the model's\ncapability to understand the network topology but also facilitate protein\nfunction annotation, biological module detection, and even disease mechanism\nanalysis. Extensive experiments on four representative model categories,\nconsisting of sequence similarity-based, naive sequence-based, protein language\nmodel-based, and structure-based approaches, demonstrate that current PPI\nmodels have potential limitations in recovering both structural and functional\nproperties of PPI networks, highlighting the gap in supporting real-world\nbiological applications. We believe PRING provides a reliable platform to guide\nthe development of more effective PPI prediction models for the community. The\ndataset and source code of PRING are available at\nhttps://github.com/SophieSarceau/PRING.",
      "upvotes": 6,
      "discussionId": "686dc2d1cb5725779c60b34e"
    },
    "publishedAt": "2025-07-07T11:21:05.000Z",
    "title": "PRING: Rethinking Protein-Protein Interaction Prediction from Pairs to\n  Graphs",
    "summary": "Deep learning-based computational methods have achieved promising results in\npredicting protein-protein interactions (PPIs). However, existing benchmarks\npredominantly focus on isolated pairwise evaluations, overlooking a model's\ncapability to reconstruct biologically meaningful PPI networks, which is\ncrucial for biology research. To address this gap, we introduce PRING, the\nfirst comprehensive benchmark that evaluates protein-protein interaction\nprediction from a graph-level perspective. PRING curates a high-quality,\nmulti-species PPI network dataset comprising 21,484 proteins and 186,818\ninteractions, with well-designed strategies to address both data redundancy and\nleakage. Building on this golden-standard dataset, we establish two\ncomplementary evaluation paradigms: (1) topology-oriented tasks, which assess\nintra and cross-species PPI network construction, and (2) function-oriented\ntasks, including protein complex pathway prediction, GO module analysis, and\nessential protein justification. These evaluations not only reflect the model's\ncapability to understand the network topology but also facilitate protein\nfunction annotation, biological module detection, and even disease mechanism\nanalysis. Extensive experiments on four representative model categories,\nconsisting of sequence similarity-based, naive sequence-based, protein language\nmodel-based, and structure-based approaches, demonstrate that current PPI\nmodels have potential limitations in recovering both structural and functional\nproperties of PPI networks, highlighting the gap in supporting real-world\nbiological applications. We believe PRING provides a reliable platform to guide\nthe development of more effective PPI prediction models for the community. The\ndataset and source code of PRING are available at\nhttps://github.com/SophieSarceau/PRING.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05101.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6310a3cd531cc21f9e06de6a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6310a3cd531cc21f9e06de6a/aTGMx3O41lUARK9s3dAik.jpeg",
      "fullname": "Zhiyuan Liu",
      "name": "acharkq",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.05963",
      "authors": [
        {
          "_id": "686dde62cb5725779c60b467",
          "name": "Zhenghao Zhang",
          "hidden": false
        },
        {
          "_id": "686dde62cb5725779c60b468",
          "name": "Junchao Liao",
          "hidden": false
        },
        {
          "_id": "686dde62cb5725779c60b469",
          "name": "Xiangyu Meng",
          "hidden": false
        },
        {
          "_id": "686dde62cb5725779c60b46a",
          "name": "Long Qin",
          "hidden": false
        },
        {
          "_id": "686dde62cb5725779c60b46b",
          "name": "Weizhi Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-08T13:11:40.000Z",
      "submittedOnDailyAt": "2025-07-09T01:43:57.686Z",
      "title": "Tora2: Motion and Appearance Customized Diffusion Transformer for\n  Multi-Entity Video Generation",
      "submittedOnDailyBy": {
        "_id": "63468720dd6d90d82ccf3450",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
        "isPro": false,
        "fullname": "YSH",
        "user": "BestWishYsh",
        "type": "user"
      },
      "summary": "Recent advances in diffusion transformer models for motion-guided video\ngeneration, such as Tora, have shown significant progress. In this paper, we\npresent Tora2, an enhanced version of Tora, which introduces several design\nimprovements to expand its capabilities in both appearance and motion\ncustomization. Specifically, we introduce a decoupled personalization extractor\nthat generates comprehensive personalization embeddings for multiple open-set\nentities, better preserving fine-grained visual details compared to previous\nmethods. Building on this, we design a gated self-attention mechanism to\nintegrate trajectory, textual description, and visual information for each\nentity. This innovation significantly reduces misalignment in multimodal\nconditioning during training. Moreover, we introduce a contrastive loss that\njointly optimizes trajectory dynamics and entity consistency through explicit\nmapping between motion and personalization embeddings. Tora2 is, to our best\nknowledge, the first method to achieve simultaneous multi-entity customization\nof appearance and motion for video generation. Experimental results demonstrate\nthat Tora2 achieves competitive performance with state-of-the-art customization\nmethods while providing advanced motion control capabilities, which marks a\ncritical advancement in multi-condition video generation. Project page:\nhttps://github.com/alibaba/Tora .",
      "upvotes": 5,
      "discussionId": "686dde75cb5725779c60b46c",
      "ai_summary": "Tora2 enhances motion-guided video generation by introducing a decoupled personalization extractor, gated self-attention mechanism, and contrastive loss, enabling simultaneous multi-entity customization and advanced motion control.",
      "ai_keywords": [
        "diffusion transformer models",
        "Tora",
        "decoupled personalization extractor",
        "personalization embeddings",
        "gated self-attention mechanism",
        "trajectory dynamics",
        "entity consistency",
        "contrastive loss",
        "multi-entity customization",
        "motion control"
      ]
    },
    "publishedAt": "2025-07-08T09:11:40.000Z",
    "title": "Tora2: Motion and Appearance Customized Diffusion Transformer for\n  Multi-Entity Video Generation",
    "summary": "Recent advances in diffusion transformer models for motion-guided video\ngeneration, such as Tora, have shown significant progress. In this paper, we\npresent Tora2, an enhanced version of Tora, which introduces several design\nimprovements to expand its capabilities in both appearance and motion\ncustomization. Specifically, we introduce a decoupled personalization extractor\nthat generates comprehensive personalization embeddings for multiple open-set\nentities, better preserving fine-grained visual details compared to previous\nmethods. Building on this, we design a gated self-attention mechanism to\nintegrate trajectory, textual description, and visual information for each\nentity. This innovation significantly reduces misalignment in multimodal\nconditioning during training. Moreover, we introduce a contrastive loss that\njointly optimizes trajectory dynamics and entity consistency through explicit\nmapping between motion and personalization embeddings. Tora2 is, to our best\nknowledge, the first method to achieve simultaneous multi-entity customization\nof appearance and motion for video generation. Experimental results demonstrate\nthat Tora2 achieves competitive performance with state-of-the-art customization\nmethods while providing advanced motion control capabilities, which marks a\ncritical advancement in multi-condition video generation. Project page:\nhttps://github.com/alibaba/Tora .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05963.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63468720dd6d90d82ccf3450",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
      "fullname": "YSH",
      "name": "BestWishYsh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 56
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.04723",
      "authors": [
        {
          "_id": "686df5fecb5725779c60b4b8",
          "name": "Zecheng Tang",
          "hidden": false
        },
        {
          "_id": "686df5fecb5725779c60b4b9",
          "name": "Haitian Wang",
          "hidden": false
        },
        {
          "_id": "686df5fecb5725779c60b4ba",
          "name": "Quantong Qiu",
          "hidden": false
        },
        {
          "_id": "686df5fecb5725779c60b4bb",
          "name": "Baibei Ji",
          "hidden": false
        },
        {
          "_id": "686df5fecb5725779c60b4bc",
          "name": "Ruoxi Sun",
          "hidden": false
        },
        {
          "_id": "686df5fecb5725779c60b4bd",
          "name": "Keyan Zhou",
          "hidden": false
        },
        {
          "_id": "686df5fecb5725779c60b4be",
          "name": "Juntao Li",
          "hidden": false
        },
        {
          "_id": "686df5fecb5725779c60b4bf",
          "name": "Min Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T07:33:24.000Z",
      "submittedOnDailyAt": "2025-07-09T03:25:02.560Z",
      "title": "LOOM-Scope: a comprehensive and efficient LOng-cOntext Model evaluation\n  framework",
      "submittedOnDailyBy": {
        "_id": "64096ef79e9f790c905b846d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64096ef79e9f790c905b846d/hVzw656lXdzbCxTtnheud.jpeg",
        "isPro": false,
        "fullname": "Zecheng Tang",
        "user": "ZetangForward",
        "type": "user"
      },
      "summary": "Long-context processing has become a fundamental capability for large\nlanguage models~(LLMs). To assess model's long-context performance, numerous\nlong-context evaluation benchmarks have been proposed. However, variations in\nevaluation settings across these benchmarks lead to inconsistent results,\nmaking it difficult to draw reliable comparisons. Besides, the high\ncomputational cost of long-context evaluation poses a significant barrier for\nthe community to conduct comprehensive assessments of long-context models. In\nthis paper, we propose LOOM-Scope, a comprehensive and efficient framework for\nlong-context evaluation. LOOM-Scope standardizes evaluation settings across\ndiverse benchmarks, supports deployment of efficient long-context inference\nacceleration methods, and introduces a holistic yet lightweight benchmark suite\nto evaluate models comprehensively. Homepage: https://loomscope.github.io",
      "upvotes": 4,
      "discussionId": "686df5fecb5725779c60b4c0",
      "projectPage": "https://loomscope.github.io/",
      "githubRepo": "https://github.com/LCM-Lab/LOOM-Scope",
      "githubStars": 3
    },
    "publishedAt": "2025-07-07T03:33:24.000Z",
    "title": "LOOM-Scope: a comprehensive and efficient LOng-cOntext Model evaluation\n  framework",
    "summary": "Long-context processing has become a fundamental capability for large\nlanguage models~(LLMs). To assess model's long-context performance, numerous\nlong-context evaluation benchmarks have been proposed. However, variations in\nevaluation settings across these benchmarks lead to inconsistent results,\nmaking it difficult to draw reliable comparisons. Besides, the high\ncomputational cost of long-context evaluation poses a significant barrier for\nthe community to conduct comprehensive assessments of long-context models. In\nthis paper, we propose LOOM-Scope, a comprehensive and efficient framework for\nlong-context evaluation. LOOM-Scope standardizes evaluation settings across\ndiverse benchmarks, supports deployment of efficient long-context inference\nacceleration methods, and introduces a holistic yet lightweight benchmark suite\nto evaluate models comprehensively. Homepage: https://loomscope.github.io",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04723.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64096ef79e9f790c905b846d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64096ef79e9f790c905b846d/hVzw656lXdzbCxTtnheud.jpeg",
      "fullname": "Zecheng Tang",
      "name": "ZetangForward",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.04103",
      "authors": [
        {
          "_id": "686dbfe2cb5725779c60b314",
          "name": "Dheeraj Vattikonda",
          "hidden": false
        },
        {
          "_id": "686dbfe2cb5725779c60b315",
          "name": "Santhoshi Ravichandran",
          "hidden": false
        },
        {
          "_id": "686dbfe2cb5725779c60b316",
          "name": "Emiliano Penaloza",
          "hidden": false
        },
        {
          "_id": "686dbfe2cb5725779c60b317",
          "name": "Hadi Nekoei",
          "hidden": false
        },
        {
          "_id": "686dbfe2cb5725779c60b318",
          "name": "Megh Thakkar",
          "hidden": false
        },
        {
          "_id": "686dbfe2cb5725779c60b319",
          "name": "Thibault Le Sellier de Chezelles",
          "hidden": false
        },
        {
          "_id": "686dbfe2cb5725779c60b31a",
          "name": "Nicolas Gontier",
          "hidden": false
        },
        {
          "_id": "686dbfe2cb5725779c60b31b",
          "name": "Miguel Muñoz-Mármol",
          "hidden": false
        },
        {
          "_id": "686dbfe2cb5725779c60b31c",
          "name": "Sahar Omidi Shayegan",
          "hidden": false
        },
        {
          "_id": "686dbfe2cb5725779c60b31d",
          "name": "Stefania Raimondo",
          "hidden": false
        },
        {
          "_id": "686dbfe2cb5725779c60b31e",
          "name": "Xue Liu",
          "hidden": false
        },
        {
          "_id": "686dbfe2cb5725779c60b31f",
          "name": "Alexandre Drouin",
          "hidden": false
        },
        {
          "_id": "686dbfe2cb5725779c60b320",
          "name": "Laurent Charlin",
          "hidden": false
        },
        {
          "_id": "686dbfe2cb5725779c60b321",
          "name": "Alexandre Piché",
          "hidden": false
        },
        {
          "_id": "686dbfe2cb5725779c60b322",
          "name": "Alexandre Lacoste",
          "hidden": false
        },
        {
          "_id": "686dbfe2cb5725779c60b323",
          "name": "Massimo Caccia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-05T17:12:33.000Z",
      "submittedOnDailyAt": "2025-07-09T02:00:38.643Z",
      "title": "How to Train Your LLM Web Agent: A Statistical Diagnosis",
      "submittedOnDailyBy": {
        "_id": "5fa9ff3ea13e063b8b2b60cb",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
        "isPro": false,
        "fullname": "Xing Han Lù",
        "user": "xhluca",
        "type": "user"
      },
      "summary": "LLM-based web agents have recently made significant progress, but much of it\nhas occurred in closed-source systems, widening the gap with open-source\nalternatives. Progress has been held back by two key challenges: first, a\nnarrow focus on single-step tasks that overlooks the complexity of multi-step\nweb interactions; and second, the high compute costs required to post-train\nLLM-based web agents. To address this, we present the first statistically\ngrounded study on compute allocation for LLM web-agent post-training. Our\napproach uses a two-stage pipeline, training a Llama 3.1 8B student to imitate\na Llama 3.3 70B teacher via supervised fine-tuning (SFT), followed by on-policy\nreinforcement learning. We find this process highly sensitive to hyperparameter\nchoices, making exhaustive sweeps impractical. To spare others from expensive\ntrial-and-error, we sample 1,370 configurations and use bootstrapping to\nestimate effective hyperparameters. Our results show that combining SFT with\non-policy RL consistently outperforms either approach alone on both WorkArena\nand MiniWob++. Further, this strategy requires only 55% of the compute to match\nthe peak performance of pure SFT on MiniWob++, effectively pushing the\ncompute-performance Pareto frontier, and is the only strategy that can close\nthe gap with closed-source models.",
      "upvotes": 4,
      "discussionId": "686dbfe2cb5725779c60b324",
      "ai_summary": "A study on compute allocation for post-training LLM-based web agents finds that combining supervised fine-tuning with on-policy reinforcement learning improves performance and reduces computational costs compared to either method alone.",
      "ai_keywords": [
        "LLM-based web agents",
        "supervised fine-tuning",
        "on-policy reinforcement learning",
        "hyperparameter choices",
        "bootstrapping",
        "WorkArena",
        "MiniWob++"
      ]
    },
    "publishedAt": "2025-07-05T13:12:33.000Z",
    "title": "How to Train Your LLM Web Agent: A Statistical Diagnosis",
    "summary": "LLM-based web agents have recently made significant progress, but much of it\nhas occurred in closed-source systems, widening the gap with open-source\nalternatives. Progress has been held back by two key challenges: first, a\nnarrow focus on single-step tasks that overlooks the complexity of multi-step\nweb interactions; and second, the high compute costs required to post-train\nLLM-based web agents. To address this, we present the first statistically\ngrounded study on compute allocation for LLM web-agent post-training. Our\napproach uses a two-stage pipeline, training a Llama 3.1 8B student to imitate\na Llama 3.3 70B teacher via supervised fine-tuning (SFT), followed by on-policy\nreinforcement learning. We find this process highly sensitive to hyperparameter\nchoices, making exhaustive sweeps impractical. To spare others from expensive\ntrial-and-error, we sample 1,370 configurations and use bootstrapping to\nestimate effective hyperparameters. Our results show that combining SFT with\non-policy RL consistently outperforms either approach alone on both WorkArena\nand MiniWob++. Further, this strategy requires only 55% of the compute to match\nthe peak performance of pure SFT on MiniWob++, effectively pushing the\ncompute-performance Pareto frontier, and is the only strategy that can close\nthe gap with closed-source models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04103.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5fa9ff3ea13e063b8b2b60cb",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
      "fullname": "Xing Han Lù",
      "name": "xhluca",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.05920",
      "authors": [
        {
          "_id": "686deb33cb5725779c60b49c",
          "name": "Xinyu Huang",
          "hidden": false
        },
        {
          "_id": "686deb33cb5725779c60b49d",
          "name": "Yuhao Dong",
          "hidden": false
        },
        {
          "_id": "686deb33cb5725779c60b49e",
          "name": "Weiwei Tian",
          "hidden": false
        },
        {
          "_id": "686deb33cb5725779c60b49f",
          "name": "Bo Li",
          "hidden": false
        },
        {
          "_id": "686deb33cb5725779c60b4a0",
          "name": "Rui Feng",
          "hidden": false
        },
        {
          "_id": "686deb33cb5725779c60b4a1",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-08T12:05:05.000Z",
      "submittedOnDailyAt": "2025-07-09T02:42:14.123Z",
      "title": "High-Resolution Visual Reasoning via Multi-Turn Grounding-Based\n  Reinforcement Learning",
      "submittedOnDailyBy": {
        "_id": "63bf7ba8da08ed0544ff20e9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673493776367-63bf7ba8da08ed0544ff20e9.jpeg",
        "isPro": false,
        "fullname": "Xinyu Huang",
        "user": "xinyu1205",
        "type": "user"
      },
      "summary": "State-of-the-art large multi-modal models (LMMs) face challenges when\nprocessing high-resolution images, as these inputs are converted into enormous\nvisual tokens, many of which are irrelevant to the downstream task. In this\npaper, we propose Multi-turn Grounding-based Policy Optimization (MGPO), an\nend-to-end reinforcement learning (RL) framework that enables LMMs to\niteratively focus on key visual regions by automatically cropping sub-images,\nbased on model-predicted grounding coordinates within a multi-turn conversation\nframework. Compared to supervised fine-tuning (SFT), which requires costly\nadditional grounding annotations, our approach highlights that LMMs can emerge\nrobust grounding abilities during the RL training process, leveraging only a\nbinary reward function derived from the correctness of the final answer.\nAdditionally, we observe that LMMs struggle to autonomously trigger visual\ngrounding during the rollout process. To address this cold start problem, we\ndesign a multi-turn conversational template and restrict policy loss\ncomputation to model outputs generated across multiple dialogue rounds, thereby\npromoting stable optimization. Extensive experiments demonstrate that, when\ntrained on standard visual-question-short answering data without grounding\nannotations, MGPO effectively elicits stronger grounding capabilities compared\nto GRPO, leading to 5.4\\% improvement on in-distribution MME-Realworld and\n5.2\\% improvement on the challenging out-of-distribution (OOD) V* Bench.\nNotably, MGPO post-training on Qwen2.5-VL-7B with 21K samples surpasses\nOpenAI's o1 and GPT-4o models on the OOD V* Bench. Codes are available at\nhttps://github.com/EvolvingLMMs-Lab/MGPO.",
      "upvotes": 3,
      "discussionId": "686deb34cb5725779c60b4a2",
      "ai_summary": "MGPO, an end-to-end reinforcement learning framework, enhances large multi-modal models' ability to focus on key visual regions without requiring additional grounding annotations, improving performance on both in-distribution and out-of-distribution benchmarks.",
      "ai_keywords": [
        "Multi-turn Grounding-based Policy Optimization",
        "reinforcement learning",
        "LMMs",
        "large multi-modal models",
        "high-resolution images",
        "visual tokens",
        "grounding coordinates",
        "multi-turn conversation",
        "supervised fine-tuning",
        "binary reward function",
        "visual grounding",
        "policy loss",
        "multi-turn conversational template",
        "stable optimization",
        "MME-Realworld",
        "V* Bench",
        "Qwen2.5-VL-7B",
        "OpenAI's o1",
        "GPT-4o"
      ]
    },
    "publishedAt": "2025-07-08T08:05:05.000Z",
    "title": "High-Resolution Visual Reasoning via Multi-Turn Grounding-Based\n  Reinforcement Learning",
    "summary": "State-of-the-art large multi-modal models (LMMs) face challenges when\nprocessing high-resolution images, as these inputs are converted into enormous\nvisual tokens, many of which are irrelevant to the downstream task. In this\npaper, we propose Multi-turn Grounding-based Policy Optimization (MGPO), an\nend-to-end reinforcement learning (RL) framework that enables LMMs to\niteratively focus on key visual regions by automatically cropping sub-images,\nbased on model-predicted grounding coordinates within a multi-turn conversation\nframework. Compared to supervised fine-tuning (SFT), which requires costly\nadditional grounding annotations, our approach highlights that LMMs can emerge\nrobust grounding abilities during the RL training process, leveraging only a\nbinary reward function derived from the correctness of the final answer.\nAdditionally, we observe that LMMs struggle to autonomously trigger visual\ngrounding during the rollout process. To address this cold start problem, we\ndesign a multi-turn conversational template and restrict policy loss\ncomputation to model outputs generated across multiple dialogue rounds, thereby\npromoting stable optimization. Extensive experiments demonstrate that, when\ntrained on standard visual-question-short answering data without grounding\nannotations, MGPO effectively elicits stronger grounding capabilities compared\nto GRPO, leading to 5.4\\% improvement on in-distribution MME-Realworld and\n5.2\\% improvement on the challenging out-of-distribution (OOD) V* Bench.\nNotably, MGPO post-training on Qwen2.5-VL-7B with 21K samples surpasses\nOpenAI's o1 and GPT-4o models on the OOD V* Bench. Codes are available at\nhttps://github.com/EvolvingLMMs-Lab/MGPO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05920.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63bf7ba8da08ed0544ff20e9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673493776367-63bf7ba8da08ed0544ff20e9.jpeg",
      "fullname": "Xinyu Huang",
      "name": "xinyu1205",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 42
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.05578",
      "authors": [
        {
          "_id": "686dca00cb5725779c60b379",
          "name": "Alexander Xiong",
          "hidden": false
        },
        {
          "_id": "686dca00cb5725779c60b37a",
          "name": "Xuandong Zhao",
          "hidden": false
        },
        {
          "_id": "686dca00cb5725779c60b37b",
          "name": "Aneesh Pappu",
          "hidden": false
        },
        {
          "_id": "686dca00cb5725779c60b37c",
          "name": "Dawn Song",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-08T01:30:46.000Z",
      "submittedOnDailyAt": "2025-07-09T00:17:12.585Z",
      "title": "The Landscape of Memorization in LLMs: Mechanisms, Measurement, and\n  Mitigation",
      "submittedOnDailyBy": {
        "_id": "6275a465597c70eb8949fce5",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png",
        "isPro": false,
        "fullname": "Xuandong Zhao",
        "user": "Xuandong",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks, yet they also exhibit memorization of their training\ndata. This phenomenon raises critical questions about model behavior, privacy\nrisks, and the boundary between learning and memorization. Addressing these\nconcerns, this paper synthesizes recent studies and investigates the landscape\nof memorization, the factors influencing it, and methods for its detection and\nmitigation. We explore key drivers, including training data duplication,\ntraining dynamics, and fine-tuning procedures that influence data memorization.\nIn addition, we examine methodologies such as prefix-based extraction,\nmembership inference, and adversarial prompting, assessing their effectiveness\nin detecting and measuring memorized content. Beyond technical analysis, we\nalso explore the broader implications of memorization, including the legal and\nethical implications. Finally, we discuss mitigation strategies, including data\ncleaning, differential privacy, and post-training unlearning, while\nhighlighting open challenges in balancing the minimization of harmful\nmemorization with utility. This paper provides a comprehensive overview of the\ncurrent state of research on LLM memorization across technical, privacy, and\nperformance dimensions, identifying critical directions for future work.",
      "upvotes": 3,
      "discussionId": "686dca01cb5725779c60b37d",
      "ai_summary": "The paper reviews recent studies on memorization in Large Language Models, exploring factors that influence memorization, detection methodologies, and mitigation strategies, while addressing privacy and ethical implications.",
      "ai_keywords": [
        "Large Language Models",
        "memorization",
        "training data duplication",
        "training dynamics",
        "fine-tuning procedures",
        "prefix-based extraction",
        "membership inference",
        "adversarial prompting",
        "data cleaning",
        "differential privacy",
        "post-training unlearning"
      ]
    },
    "publishedAt": "2025-07-07T21:30:46.000Z",
    "title": "The Landscape of Memorization in LLMs: Mechanisms, Measurement, and\n  Mitigation",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks, yet they also exhibit memorization of their training\ndata. This phenomenon raises critical questions about model behavior, privacy\nrisks, and the boundary between learning and memorization. Addressing these\nconcerns, this paper synthesizes recent studies and investigates the landscape\nof memorization, the factors influencing it, and methods for its detection and\nmitigation. We explore key drivers, including training data duplication,\ntraining dynamics, and fine-tuning procedures that influence data memorization.\nIn addition, we examine methodologies such as prefix-based extraction,\nmembership inference, and adversarial prompting, assessing their effectiveness\nin detecting and measuring memorized content. Beyond technical analysis, we\nalso explore the broader implications of memorization, including the legal and\nethical implications. Finally, we discuss mitigation strategies, including data\ncleaning, differential privacy, and post-training unlearning, while\nhighlighting open challenges in balancing the minimization of harmful\nmemorization with utility. This paper provides a comprehensive overview of the\ncurrent state of research on LLM memorization across technical, privacy, and\nperformance dimensions, identifying critical directions for future work.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05578.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6275a465597c70eb8949fce5",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png",
      "fullname": "Xuandong Zhao",
      "name": "Xuandong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.04610",
      "authors": [
        {
          "_id": "686dcc0bcb5725779c60b38f",
          "name": "Mostafa Elhoushi",
          "hidden": false
        },
        {
          "_id": "686dcc0bcb5725779c60b390",
          "name": "Jeff Johnson",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-07T01:59:47.000Z",
      "submittedOnDailyAt": "2025-07-09T00:32:16.318Z",
      "title": "any4: Learned 4-bit Numeric Representation for LLMs",
      "submittedOnDailyBy": {
        "_id": "63c9725ebedad7e2bf160bdc",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c9725ebedad7e2bf160bdc/wzPuyhOXCYBNGwZDshbnL.jpeg",
        "isPro": false,
        "fullname": "Mostafa Elhoushi",
        "user": "melhoushi",
        "type": "user"
      },
      "summary": "We present any4, a learned 4-bit weight quantization solution for large\nlanguage models (LLMs) providing arbitrary numeric representations without\nrequiring pre-processing of weights or activations. any4 yields higher accuracy\ncompared to other related 4-bit numeric representation types: int4, fp4 and\nnf4, as evaluated on a range of model sizes, generations and families (Llama 2,\nLlama 3, Mistral and Mixtral). While any4 does not require preprocessing of\nweights or activations, it is also competitive with orthogonal techniques that\nrequire such preprocessing (e.g., AWQ and GPTQ). We also experiment with any3\nand any2 and show competitiveness at lower bits. Additionally, we show that we\ncan calibrate using a single curated diverse sample rather than hundreds of\nsamples from a dataset as done in most quantization approaches. We also open\nsource tinygemm, a latency optimized GPU matrix multiplication library for\nLLMs, that implements any4 using a GPU-efficient lookup table strategy along\nwith other common quantization methods. We open source our code at\nhttps://github.com/facebookresearch/any4 .",
      "upvotes": 3,
      "discussionId": "686dcc0ccb5725779c60b391",
      "githubRepo": "https://github.com/facebookresearch/any4",
      "ai_summary": "any4 is a learned 4-bit weight quantization method for LLMs that achieves high accuracy without preprocessing and uses a GPU-efficient lookup table strategy.",
      "ai_keywords": [
        "weight quantization",
        "LLMs",
        "int4",
        "fp4",
        "nf4",
        "AWQ",
        "GPTQ",
        "calibration",
        "GPU matrix multiplication",
        "lookup table strategy"
      ],
      "githubStars": 16
    },
    "publishedAt": "2025-07-06T21:59:47.000Z",
    "title": "any4: Learned 4-bit Numeric Representation for LLMs",
    "summary": "We present any4, a learned 4-bit weight quantization solution for large\nlanguage models (LLMs) providing arbitrary numeric representations without\nrequiring pre-processing of weights or activations. any4 yields higher accuracy\ncompared to other related 4-bit numeric representation types: int4, fp4 and\nnf4, as evaluated on a range of model sizes, generations and families (Llama 2,\nLlama 3, Mistral and Mixtral). While any4 does not require preprocessing of\nweights or activations, it is also competitive with orthogonal techniques that\nrequire such preprocessing (e.g., AWQ and GPTQ). We also experiment with any3\nand any2 and show competitiveness at lower bits. Additionally, we show that we\ncan calibrate using a single curated diverse sample rather than hundreds of\nsamples from a dataset as done in most quantization approaches. We also open\nsource tinygemm, a latency optimized GPU matrix multiplication library for\nLLMs, that implements any4 using a GPU-efficient lookup table strategy along\nwith other common quantization methods. We open source our code at\nhttps://github.com/facebookresearch/any4 .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04610.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63c9725ebedad7e2bf160bdc",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c9725ebedad7e2bf160bdc/wzPuyhOXCYBNGwZDshbnL.jpeg",
      "fullname": "Mostafa Elhoushi",
      "name": "melhoushi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 34
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2507.06204",
      "authors": [
        {
          "_id": "686e0c68cb5725779c60b4ed",
          "name": "Nadav Schneider",
          "hidden": false
        },
        {
          "_id": "686e0c68cb5725779c60b4ee",
          "name": "Itamar Zimerman",
          "hidden": false
        },
        {
          "_id": "686e0c68cb5725779c60b4ef",
          "name": "Eliya Nachmani",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/65ce0b4a03a8179f5da5d4ef/D6ZhMrqrVT28Kh25ZySih.png",
        "https://cdn-uploads.huggingface.co/production/uploads/65ce0b4a03a8179f5da5d4ef/qqCHBOm_ZYidhb_WFlZ1-.png",
        "https://cdn-uploads.huggingface.co/production/uploads/65ce0b4a03a8179f5da5d4ef/kuXrDKc48bZzbeC1l1Si8.png",
        "https://cdn-uploads.huggingface.co/production/uploads/65ce0b4a03a8179f5da5d4ef/e5wW0wTXLV1FOgG0pzEcl.png"
      ],
      "publishedAt": "2025-07-08T17:30:14.000Z",
      "submittedOnDailyAt": "2025-07-09T05:06:48.447Z",
      "title": "Differential Mamba",
      "submittedOnDailyBy": {
        "_id": "65ce0b4a03a8179f5da5d4ef",
        "avatarUrl": "/avatars/c0b68efb885486c5180d1d69c4e317ac.svg",
        "isPro": false,
        "fullname": "Nadav Schneider",
        "user": "nadavsc",
        "type": "user"
      },
      "summary": "Sequence models like Transformers and RNNs often overallocate attention to\nirrelevant context, leading to noisy intermediate representations. This\ndegrades LLM capabilities by promoting hallucinations, weakening long-range and\nretrieval abilities, and reducing robustness. Recent work has shown that\ndifferential design can mitigate this issue in Transformers, improving their\neffectiveness across various applications. In this paper, we explore whether\nthese techniques, originally developed for Transformers, can be applied to\nMamba, a recent architecture based on selective state-space layers that\nachieves Transformer-level performance with greater efficiency. We show that a\nnaive adaptation of differential design to Mamba is insufficient and requires\ncareful architectural modifications. To address this, we introduce a novel\ndifferential mechanism for Mamba, empirically validated on language modeling\nbenchmarks, demonstrating improved retrieval capabilities and superior\nperformance over vanilla Mamba. Finally, we conduct extensive ablation studies\nand empirical analyses to justify our design choices and provide evidence that\nour approach effectively mitigates the overallocation problem in Mamba-based\nmodels. Our code is publicly available.",
      "upvotes": 2,
      "discussionId": "686e0c68cb5725779c60b4f0",
      "ai_summary": "A novel differential mechanism for Mamba, a selective state-space layer architecture, improves retrieval capabilities and performance by addressing overallocation issues.",
      "ai_keywords": [
        "Transformers",
        "RNNs",
        "differential design",
        "Mamba",
        "selective state-space layers",
        "language modeling benchmarks",
        "ablation studies",
        "empirical analyses"
      ]
    },
    "publishedAt": "2025-07-08T13:30:14.000Z",
    "title": "Differential Mamba",
    "summary": "Sequence models like Transformers and RNNs often overallocate attention to\nirrelevant context, leading to noisy intermediate representations. This\ndegrades LLM capabilities by promoting hallucinations, weakening long-range and\nretrieval abilities, and reducing robustness. Recent work has shown that\ndifferential design can mitigate this issue in Transformers, improving their\neffectiveness across various applications. In this paper, we explore whether\nthese techniques, originally developed for Transformers, can be applied to\nMamba, a recent architecture based on selective state-space layers that\nachieves Transformer-level performance with greater efficiency. We show that a\nnaive adaptation of differential design to Mamba is insufficient and requires\ncareful architectural modifications. To address this, we introduce a novel\ndifferential mechanism for Mamba, empirically validated on language modeling\nbenchmarks, demonstrating improved retrieval capabilities and superior\nperformance over vanilla Mamba. Finally, we conduct extensive ablation studies\nand empirical analyses to justify our design choices and provide evidence that\nour approach effectively mitigates the overallocation problem in Mamba-based\nmodels. Our code is publicly available.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/65ce0b4a03a8179f5da5d4ef/D6ZhMrqrVT28Kh25ZySih.png",
      "https://cdn-uploads.huggingface.co/production/uploads/65ce0b4a03a8179f5da5d4ef/qqCHBOm_ZYidhb_WFlZ1-.png",
      "https://cdn-uploads.huggingface.co/production/uploads/65ce0b4a03a8179f5da5d4ef/kuXrDKc48bZzbeC1l1Si8.png",
      "https://cdn-uploads.huggingface.co/production/uploads/65ce0b4a03a8179f5da5d4ef/e5wW0wTXLV1FOgG0pzEcl.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06204.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65ce0b4a03a8179f5da5d4ef",
      "avatarUrl": "/avatars/c0b68efb885486c5180d1d69c4e317ac.svg",
      "fullname": "Nadav Schneider",
      "name": "nadavsc",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]