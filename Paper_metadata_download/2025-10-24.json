[
  {
    "paper": {
      "id": "2510.19779",
      "authors": [
        {
          "_id": "68f983f6b9b2e4ae04673741",
          "user": {
            "_id": "67dc66fe55c24fc4f981a4ab",
            "avatarUrl": "/avatars/7bd900ade802d99db7c562ad6c2f6661.svg",
            "isPro": false,
            "fullname": "Yuezhou Hu",
            "user": "yuezhouhu",
            "type": "user"
          },
          "name": "Yuezhou Hu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-23T14:51:20.772Z",
          "hidden": false
        },
        {
          "_id": "68f983f6b9b2e4ae04673742",
          "name": "Jiaxin Guo",
          "hidden": false
        },
        {
          "_id": "68f983f6b9b2e4ae04673743",
          "name": "Xinyu Feng",
          "hidden": false
        },
        {
          "_id": "68f983f6b9b2e4ae04673744",
          "name": "Tuo Zhao",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67dc66fe55c24fc4f981a4ab/ysq4UzVT_dquWCpvozNWh.png"
      ],
      "publishedAt": "2025-10-22T17:13:00.000Z",
      "submittedOnDailyAt": "2025-10-24T02:14:45.526Z",
      "title": "AdaSPEC: Selective Knowledge Distillation for Efficient Speculative\n  Decoders",
      "submittedOnDailyBy": {
        "_id": "67dc66fe55c24fc4f981a4ab",
        "avatarUrl": "/avatars/7bd900ade802d99db7c562ad6c2f6661.svg",
        "isPro": false,
        "fullname": "Yuezhou Hu",
        "user": "yuezhouhu",
        "type": "user"
      },
      "summary": "Speculative Decoding (SD) accelerates large language model inference by\nemploying a small draft model to generate predictions, which are then verified\nby a larger target model. The effectiveness of SD hinges on the alignment\nbetween these models, which is typically enhanced by Knowledge Distillation\n(KD). However, conventional KD methods aim to minimize the KL divergence\nbetween the draft and target models across all tokens, a goal that is\nmisaligned with the true objective of SD, which is to maximize token acceptance\nrate. Therefore, draft models often struggle to fully assimilate the target\nmodel's knowledge due to capacity constraints, leading to suboptimal\nperformance. To address this challenge, we propose AdaSPEC, a novel method that\nincorporates selective token filtering into the KD process. AdaSPEC utilizes a\nreference model to identify and filter out difficult-to-fit tokens, enabling\nthe distillation of a draft model that better aligns with the target model on\nsimpler tokens. This approach improves the overall token acceptance rate\nwithout compromising generation quality. We evaluate AdaSPEC across diverse\ntasks, including arithmetic reasoning, instruction-following, coding, and\nsummarization, using model configurations of 31M/1.4B and 350M/2.7B parameters.\nOur results demonstrate that AdaSPEC consistently outperforms the\nstate-of-the-art DistillSpec method, achieving higher acceptance rates across\nall tasks (up to 15\\%). The code is publicly available at\nhttps://github.com/yuezhouhu/adaspec.",
      "upvotes": 39,
      "discussionId": "68f983f7b9b2e4ae04673745",
      "githubRepo": "https://github.com/yuezhouhu/adaspec",
      "ai_summary": "AdaSPEC enhances speculative decoding by selectively filtering tokens during knowledge distillation, improving token acceptance rates without sacrificing generation quality.",
      "ai_keywords": [
        "Speculative Decoding",
        "Knowledge Distillation",
        "KL divergence",
        "token acceptance rate",
        "selective token filtering",
        "reference model",
        "arithmetic reasoning",
        "instruction-following",
        "coding",
        "summarization"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "64155eaa95fb6f824b237c3d",
        "name": "GeorgiaTech",
        "fullname": "Georgia Institute of Technology",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64155e8abe60230f2f40b03a/3i-AL3LrNkaTarSKnaGy8.png"
      }
    },
    "publishedAt": "2025-10-22T13:13:00.000Z",
    "title": "AdaSPEC: Selective Knowledge Distillation for Efficient Speculative\n  Decoders",
    "summary": "Speculative Decoding (SD) accelerates large language model inference by\nemploying a small draft model to generate predictions, which are then verified\nby a larger target model. The effectiveness of SD hinges on the alignment\nbetween these models, which is typically enhanced by Knowledge Distillation\n(KD). However, conventional KD methods aim to minimize the KL divergence\nbetween the draft and target models across all tokens, a goal that is\nmisaligned with the true objective of SD, which is to maximize token acceptance\nrate. Therefore, draft models often struggle to fully assimilate the target\nmodel's knowledge due to capacity constraints, leading to suboptimal\nperformance. To address this challenge, we propose AdaSPEC, a novel method that\nincorporates selective token filtering into the KD process. AdaSPEC utilizes a\nreference model to identify and filter out difficult-to-fit tokens, enabling\nthe distillation of a draft model that better aligns with the target model on\nsimpler tokens. This approach improves the overall token acceptance rate\nwithout compromising generation quality. We evaluate AdaSPEC across diverse\ntasks, including arithmetic reasoning, instruction-following, coding, and\nsummarization, using model configurations of 31M/1.4B and 350M/2.7B parameters.\nOur results demonstrate that AdaSPEC consistently outperforms the\nstate-of-the-art DistillSpec method, achieving higher acceptance rates across\nall tasks (up to 15\\%). The code is publicly available at\nhttps://github.com/yuezhouhu/adaspec.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67dc66fe55c24fc4f981a4ab/ysq4UzVT_dquWCpvozNWh.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19779.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67dc66fe55c24fc4f981a4ab",
      "avatarUrl": "/avatars/7bd900ade802d99db7c562ad6c2f6661.svg",
      "fullname": "Yuezhou Hu",
      "name": "yuezhouhu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "64155eaa95fb6f824b237c3d",
      "name": "GeorgiaTech",
      "fullname": "Georgia Institute of Technology",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64155e8abe60230f2f40b03a/3i-AL3LrNkaTarSKnaGy8.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.20579",
      "authors": [
        {
          "_id": "68fadb71f158a71c5a2f582a",
          "name": "Jiahao Meng",
          "hidden": false
        },
        {
          "_id": "68fadb71f158a71c5a2f582b",
          "name": "Xiangtai Li",
          "hidden": false
        },
        {
          "_id": "68fadb71f158a71c5a2f582c",
          "name": "Haochen Wang",
          "hidden": false
        },
        {
          "_id": "68fadb71f158a71c5a2f582d",
          "name": "Yue Tan",
          "hidden": false
        },
        {
          "_id": "68fadb71f158a71c5a2f582e",
          "name": "Tao Zhang",
          "hidden": false
        },
        {
          "_id": "68fadb71f158a71c5a2f582f",
          "name": "Lingdong Kong",
          "hidden": false
        },
        {
          "_id": "68fadb71f158a71c5a2f5830",
          "name": "Yunhai Tong",
          "hidden": false
        },
        {
          "_id": "68fadb71f158a71c5a2f5831",
          "name": "Anran Wang",
          "hidden": false
        },
        {
          "_id": "68fadb71f158a71c5a2f5832",
          "name": "Zhiyang Teng",
          "hidden": false
        },
        {
          "_id": "68fadb71f158a71c5a2f5833",
          "name": "Yujing Wang",
          "hidden": false
        },
        {
          "_id": "68fadb71f158a71c5a2f5834",
          "name": "Zhuochen Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-23T14:05:56.000Z",
      "submittedOnDailyAt": "2025-10-24T00:20:51.680Z",
      "title": "Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal\n  Evidence",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Most video reasoning models only generate textual reasoning traces without\nindicating when and where key evidence appears. Recent models such as OpenAI-o3\nhave sparked wide interest in evidence-centered reasoning for images, yet\nextending this ability to videos is more challenging, as it requires joint\ntemporal tracking and spatial localization across dynamic scenes. We introduce\nOpen-o3 Video, a non-agent framework that integrates explicit spatio-temporal\nevidence into video reasoning, and carefully collect training data and design\ntraining strategies to address the aforementioned challenges. The model\nhighlights key timestamps, objects, and bounding boxes alongside its answers,\nallowing reasoning to be grounded in concrete visual observations. To enable\nthis functionality, we first curate and build two high-quality datasets,\nSTGR-CoT-30k for SFT and STGR-RL-36k for RL, with carefully constructed\ntemporal and spatial annotations, since most existing datasets offer either\ntemporal spans for videos or spatial boxes on images, lacking unified\nspatio-temporal supervision and reasoning traces. Then, we adopt a cold-start\nreinforcement learning strategy with multiple specially designed rewards that\njointly encourage answer accuracy, temporal alignment, and spatial precision.\nOn V-STAR benchmark, Open-o3 Video achieves state-of-the-art performance,\nraising mAM by 14.4% and mLGM by 24.2% on the Qwen2.5-VL baseline. Consistent\nimprovements are also observed on a broad range of video understanding\nbenchmarks, including VideoMME, WorldSense, VideoMMMU, and TVGBench. Beyond\naccuracy, the reasoning traces produced by Open-o3 Video also provide valuable\nsignals for test-time scaling, enabling confidence-aware verification and\nimproving answer reliability.",
      "upvotes": 27,
      "discussionId": "68fadb72f158a71c5a2f5835",
      "projectPage": "https://marinero4972.github.io/projects/Open-o3-Video/",
      "githubRepo": "https://github.com/marinero4972/Open-o3-Video",
      "ai_summary": "Open-o3 Video integrates spatio-temporal evidence into video reasoning, achieving state-of-the-art performance on multiple benchmarks and providing valuable reasoning traces for test-time scaling.",
      "ai_keywords": [
        "spatio-temporal evidence",
        "video reasoning",
        "temporal tracking",
        "spatial localization",
        "non-agent framework",
        "SFT",
        "RL",
        "cold-start reinforcement learning",
        "V-STAR benchmark",
        "mAM",
        "mLGM",
        "VideoMME",
        "WorldSense",
        "VideoMMMU",
        "TVGBench",
        "reasoning traces",
        "confidence-aware verification"
      ],
      "githubStars": 15,
      "organization": {
        "_id": "653b817d32c97d0655575872",
        "name": "ByteDance",
        "fullname": "ByteDance",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
      }
    },
    "publishedAt": "2025-10-23T10:05:56.000Z",
    "title": "Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal\n  Evidence",
    "summary": "Most video reasoning models only generate textual reasoning traces without\nindicating when and where key evidence appears. Recent models such as OpenAI-o3\nhave sparked wide interest in evidence-centered reasoning for images, yet\nextending this ability to videos is more challenging, as it requires joint\ntemporal tracking and spatial localization across dynamic scenes. We introduce\nOpen-o3 Video, a non-agent framework that integrates explicit spatio-temporal\nevidence into video reasoning, and carefully collect training data and design\ntraining strategies to address the aforementioned challenges. The model\nhighlights key timestamps, objects, and bounding boxes alongside its answers,\nallowing reasoning to be grounded in concrete visual observations. To enable\nthis functionality, we first curate and build two high-quality datasets,\nSTGR-CoT-30k for SFT and STGR-RL-36k for RL, with carefully constructed\ntemporal and spatial annotations, since most existing datasets offer either\ntemporal spans for videos or spatial boxes on images, lacking unified\nspatio-temporal supervision and reasoning traces. Then, we adopt a cold-start\nreinforcement learning strategy with multiple specially designed rewards that\njointly encourage answer accuracy, temporal alignment, and spatial precision.\nOn V-STAR benchmark, Open-o3 Video achieves state-of-the-art performance,\nraising mAM by 14.4% and mLGM by 24.2% on the Qwen2.5-VL baseline. Consistent\nimprovements are also observed on a broad range of video understanding\nbenchmarks, including VideoMME, WorldSense, VideoMMMU, and TVGBench. Beyond\naccuracy, the reasoning traces produced by Open-o3 Video also provide valuable\nsignals for test-time scaling, enabling confidence-aware verification and\nimproving answer reliability.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20579.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 142
    },
    "organization": {
      "_id": "653b817d32c97d0655575872",
      "name": "ByteDance",
      "fullname": "ByteDance",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.19600",
      "authors": [
        {
          "_id": "68faed18f158a71c5a2f5883",
          "name": "Qianli Ma",
          "hidden": false
        },
        {
          "_id": "68faed18f158a71c5a2f5884",
          "name": "Siyu Wang",
          "hidden": false
        },
        {
          "_id": "68faed18f158a71c5a2f5885",
          "name": "Yilin Chen",
          "hidden": false
        },
        {
          "_id": "68faed18f158a71c5a2f5886",
          "name": "Yinhao Tang",
          "hidden": false
        },
        {
          "_id": "68faed18f158a71c5a2f5887",
          "name": "Yixiang Yang",
          "hidden": false
        },
        {
          "_id": "68faed18f158a71c5a2f5888",
          "name": "Chang Guo",
          "hidden": false
        },
        {
          "_id": "68faed18f158a71c5a2f5889",
          "name": "Bingjie Gao",
          "hidden": false
        },
        {
          "_id": "68faed18f158a71c5a2f588a",
          "name": "Zhening Xing",
          "hidden": false
        },
        {
          "_id": "68faed18f158a71c5a2f588b",
          "name": "Yanan Sun",
          "hidden": false
        },
        {
          "_id": "68faed18f158a71c5a2f588c",
          "name": "Zhipeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-22T13:53:57.000Z",
      "submittedOnDailyAt": "2025-10-24T01:37:25.642Z",
      "title": "Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1",
      "submittedOnDailyBy": {
        "_id": "6448b2f53e7b3c11be684348",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6448b2f53e7b3c11be684348/QvlUQG3pWf8ZyEVBV6F7w.jpeg",
        "isPro": true,
        "fullname": "Qianli Ma",
        "user": "Mqleet",
        "type": "user"
      },
      "summary": "In the quest for scientific progress, communicating research is as vital as\nthe discovery itself. Yet, researchers are often sidetracked by the manual,\nrepetitive chore of building project webpages to make their dense papers\naccessible. While automation has tackled static slides and posters, the\ndynamic, interactive nature of webpages has remained an unaddressed challenge.\nTo bridge this gap, we reframe the problem, arguing that the solution lies not\nin a single command, but in a collaborative, hierarchical process. We introduce\nAutoPage, a novel multi-agent system that embodies this philosophy.\nAutoPage deconstructs paper-to-page creation into a coarse-to-fine pipeline\nfrom narrative planning to multimodal content generation and interactive\nrendering. To combat AI hallucination, dedicated \"Checker\" agents verify each\nstep against the source paper, while optional human checkpoints ensure the\nfinal product aligns perfectly with the author's vision, transforming the\nsystem from a mere tool into a powerful collaborative assistant. To rigorously\nvalidate our approach, we also construct PageBench, the first\nbenchmark for this new task. Experiments show AutoPage not only generates\nhigh-quality, visually appealing pages but does so with remarkable efficiency\nin under 15 minutes for less than \\0.1. Code and dataset will be released at\nhttps://mqleet.github.io/AutoPage_ProjectPage/{Webpage}$.",
      "upvotes": 16,
      "discussionId": "68faed18f158a71c5a2f588d",
      "projectPage": "https://mqleet.github.io/AutoPage_ProjectPage",
      "githubRepo": "https://github.com/AutoLab-SAI-SJTU/AutoPage",
      "ai_summary": "AutoPage, a multi-agent system, automates the creation of interactive research webpages through a hierarchical process, ensuring high-quality and efficient results.",
      "ai_keywords": [
        "multi-agent system",
        "narrative planning",
        "multimodal content generation",
        "interactive rendering",
        "AI hallucination",
        "Checker agents",
        "human checkpoints",
        "PageBench",
        "benchmark"
      ],
      "githubStars": 42,
      "organization": {
        "_id": "68ee0edd23dc954f7744ac27",
        "name": "AutoLab-SJTU",
        "fullname": "AutoLab"
      }
    },
    "publishedAt": "2025-10-22T09:53:57.000Z",
    "title": "Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1",
    "summary": "In the quest for scientific progress, communicating research is as vital as\nthe discovery itself. Yet, researchers are often sidetracked by the manual,\nrepetitive chore of building project webpages to make their dense papers\naccessible. While automation has tackled static slides and posters, the\ndynamic, interactive nature of webpages has remained an unaddressed challenge.\nTo bridge this gap, we reframe the problem, arguing that the solution lies not\nin a single command, but in a collaborative, hierarchical process. We introduce\nAutoPage, a novel multi-agent system that embodies this philosophy.\nAutoPage deconstructs paper-to-page creation into a coarse-to-fine pipeline\nfrom narrative planning to multimodal content generation and interactive\nrendering. To combat AI hallucination, dedicated \"Checker\" agents verify each\nstep against the source paper, while optional human checkpoints ensure the\nfinal product aligns perfectly with the author's vision, transforming the\nsystem from a mere tool into a powerful collaborative assistant. To rigorously\nvalidate our approach, we also construct PageBench, the first\nbenchmark for this new task. Experiments show AutoPage not only generates\nhigh-quality, visually appealing pages but does so with remarkable efficiency\nin under 15 minutes for less than \\0.1. Code and dataset will be released at\nhttps://mqleet.github.io/AutoPage_ProjectPage/{Webpage}$.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19600.png",
    "numComments": 3,
    "submittedBy": {
      "_id": "6448b2f53e7b3c11be684348",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6448b2f53e7b3c11be684348/QvlUQG3pWf8ZyEVBV6F7w.jpeg",
      "fullname": "Qianli Ma",
      "name": "Mqleet",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "68ee0edd23dc954f7744ac27",
      "name": "AutoLab-SJTU",
      "fullname": "AutoLab"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.20822",
      "authors": [
        {
          "_id": "68fada12f158a71c5a2f5804",
          "name": "Yihao Meng",
          "hidden": false
        },
        {
          "_id": "68fada12f158a71c5a2f5805",
          "name": "Hao Ouyang",
          "hidden": false
        },
        {
          "_id": "68fada12f158a71c5a2f5806",
          "name": "Yue Yu",
          "hidden": false
        },
        {
          "_id": "68fada12f158a71c5a2f5807",
          "name": "Qiuyu Wang",
          "hidden": false
        },
        {
          "_id": "68fada12f158a71c5a2f5808",
          "name": "Wen Wang",
          "hidden": false
        },
        {
          "_id": "68fada12f158a71c5a2f5809",
          "name": "Ka Leong Cheng",
          "hidden": false
        },
        {
          "_id": "68fada12f158a71c5a2f580a",
          "name": "Hanlin Wang",
          "hidden": false
        },
        {
          "_id": "68fada12f158a71c5a2f580b",
          "name": "Yixuan Li",
          "hidden": false
        },
        {
          "_id": "68fada12f158a71c5a2f580c",
          "name": "Cheng Chen",
          "hidden": false
        },
        {
          "_id": "68fada12f158a71c5a2f580d",
          "name": "Yanhong Zeng",
          "hidden": false
        },
        {
          "_id": "68fada12f158a71c5a2f580e",
          "name": "Yujun Shen",
          "hidden": false
        },
        {
          "_id": "68fada12f158a71c5a2f580f",
          "name": "Huamin Qu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-23T17:59:59.000Z",
      "submittedOnDailyAt": "2025-10-24T00:15:58.230Z",
      "title": "HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video\n  Narratives",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "State-of-the-art text-to-video models excel at generating isolated clips but\nfall short of creating the coherent, multi-shot narratives, which are the\nessence of storytelling. We bridge this \"narrative gap\" with HoloCine, a model\nthat generates entire scenes holistically to ensure global consistency from the\nfirst shot to the last. Our architecture achieves precise directorial control\nthrough a Window Cross-Attention mechanism that localizes text prompts to\nspecific shots, while a Sparse Inter-Shot Self-Attention pattern (dense within\nshots but sparse between them) ensures the efficiency required for minute-scale\ngeneration. Beyond setting a new state-of-the-art in narrative coherence,\nHoloCine develops remarkable emergent abilities: a persistent memory for\ncharacters and scenes, and an intuitive grasp of cinematic techniques. Our work\nmarks a pivotal shift from clip synthesis towards automated filmmaking, making\nend-to-end cinematic creation a tangible future. Our code is available at:\nhttps://holo-cine.github.io/.",
      "upvotes": 15,
      "discussionId": "68fada12f158a71c5a2f5810",
      "projectPage": "https://holo-cine.github.io",
      "ai_summary": "HoloCine generates coherent multi-shot narratives using a Window Cross-Attention mechanism and Sparse Inter-Shot Self-Attention, enabling end-to-end cinematic creation.",
      "ai_keywords": [
        "Window Cross-Attention",
        "Sparse Inter-Shot Self-Attention",
        "text-to-video models",
        "narrative coherence",
        "automated filmmaking"
      ],
      "organization": {
        "_id": "67c1d682826160b28f778510",
        "name": "antgroup",
        "fullname": "Ant Group",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
      }
    },
    "publishedAt": "2025-10-23T13:59:59.000Z",
    "title": "HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video\n  Narratives",
    "summary": "State-of-the-art text-to-video models excel at generating isolated clips but\nfall short of creating the coherent, multi-shot narratives, which are the\nessence of storytelling. We bridge this \"narrative gap\" with HoloCine, a model\nthat generates entire scenes holistically to ensure global consistency from the\nfirst shot to the last. Our architecture achieves precise directorial control\nthrough a Window Cross-Attention mechanism that localizes text prompts to\nspecific shots, while a Sparse Inter-Shot Self-Attention pattern (dense within\nshots but sparse between them) ensures the efficiency required for minute-scale\ngeneration. Beyond setting a new state-of-the-art in narrative coherence,\nHoloCine develops remarkable emergent abilities: a persistent memory for\ncharacters and scenes, and an intuitive grasp of cinematic techniques. Our work\nmarks a pivotal shift from clip synthesis towards automated filmmaking, making\nend-to-end cinematic creation a tangible future. Our code is available at:\nhttps://holo-cine.github.io/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20822.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 142
    },
    "organization": {
      "_id": "67c1d682826160b28f778510",
      "name": "antgroup",
      "fullname": "Ant Group",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.19304",
      "authors": [
        {
          "_id": "68fa31fef158a71c5a2f5608",
          "user": {
            "_id": "64c37ee87d89024360937d81",
            "avatarUrl": "/avatars/cc9733b0862bbdca5e00f61a7ff7bb94.svg",
            "isPro": false,
            "fullname": "mingyu jo",
            "user": "jojo0217",
            "type": "user"
          },
          "name": "Mingyu Jo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-24T01:07:20.272Z",
          "hidden": false
        },
        {
          "_id": "68fa31fef158a71c5a2f5609",
          "name": "Jaesik Yoon",
          "hidden": false
        },
        {
          "_id": "68fa31fef158a71c5a2f560a",
          "name": "Justin Deschenaux",
          "hidden": false
        },
        {
          "_id": "68fa31fef158a71c5a2f560b",
          "name": "Caglar Gulcehre",
          "hidden": false
        },
        {
          "_id": "68fa31fef158a71c5a2f560c",
          "name": "Sungjin Ahn",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-22T07:08:47.000Z",
      "submittedOnDailyAt": "2025-10-24T00:27:36.960Z",
      "title": "Loopholing Discrete Diffusion: Deterministic Bypass of the Sampling Wall",
      "submittedOnDailyBy": {
        "_id": "64c37ee87d89024360937d81",
        "avatarUrl": "/avatars/cc9733b0862bbdca5e00f61a7ff7bb94.svg",
        "isPro": false,
        "fullname": "mingyu jo",
        "user": "jojo0217",
        "type": "user"
      },
      "summary": "Discrete diffusion models offer a promising alternative to autoregressive\ngeneration through parallel decoding, but they suffer from a sampling wall:\nonce categorical sampling occurs, rich distributional information collapses\ninto one-hot vectors and cannot be propagated across steps, forcing subsequent\nsteps to operate with limited information. To mitigate this problem, we\nintroduce Loopholing, a novel and simple mechanism that preserves this\ninformation via a deterministic latent pathway, leading to Loopholing Discrete\nDiffusion Models (LDDMs). Trained efficiently with a self-conditioning\nstrategy, LDDMs achieve substantial gains-reducing generative perplexity by up\nto 61% over prior baselines, closing (and in some cases surpassing) the gap\nwith autoregressive models, and producing more coherent text. Applied to\nreasoning tasks, LDDMs also improve performance on arithmetic benchmarks such\nas Countdown and Game of 24. These results also indicate that loopholing\nmitigates idle steps and oscillations, providing a scalable path toward\nhigh-quality non-autoregressive text generation.",
      "upvotes": 15,
      "discussionId": "68fa31fef158a71c5a2f560d",
      "projectPage": "https://sites.google.com/view/lddms/home",
      "ai_summary": "Loopholing Discrete Diffusion Models (LDDMs) enhance text generation by preserving distributional information through a deterministic latent pathway, reducing perplexity and improving coherence and performance on reasoning tasks.",
      "ai_keywords": [
        "discrete diffusion models",
        "parallel decoding",
        "sampling wall",
        "one-hot vectors",
        "Loopholing",
        "deterministic latent pathway",
        "Loopholing Discrete Diffusion Models (LDDMs)",
        "self-conditioning strategy",
        "generative perplexity",
        "autoregressive models",
        "coherent text",
        "arithmetic benchmarks",
        "Countdown",
        "Game of 24",
        "idle steps",
        "oscillations",
        "non-autoregressive text generation"
      ]
    },
    "publishedAt": "2025-10-22T03:08:47.000Z",
    "title": "Loopholing Discrete Diffusion: Deterministic Bypass of the Sampling Wall",
    "summary": "Discrete diffusion models offer a promising alternative to autoregressive\ngeneration through parallel decoding, but they suffer from a sampling wall:\nonce categorical sampling occurs, rich distributional information collapses\ninto one-hot vectors and cannot be propagated across steps, forcing subsequent\nsteps to operate with limited information. To mitigate this problem, we\nintroduce Loopholing, a novel and simple mechanism that preserves this\ninformation via a deterministic latent pathway, leading to Loopholing Discrete\nDiffusion Models (LDDMs). Trained efficiently with a self-conditioning\nstrategy, LDDMs achieve substantial gains-reducing generative perplexity by up\nto 61% over prior baselines, closing (and in some cases surpassing) the gap\nwith autoregressive models, and producing more coherent text. Applied to\nreasoning tasks, LDDMs also improve performance on arithmetic benchmarks such\nas Countdown and Game of 24. These results also indicate that loopholing\nmitigates idle steps and oscillations, providing a scalable path toward\nhigh-quality non-autoregressive text generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19304.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64c37ee87d89024360937d81",
      "avatarUrl": "/avatars/cc9733b0862bbdca5e00f61a7ff7bb94.svg",
      "fullname": "mingyu jo",
      "name": "jojo0217",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.20187",
      "authors": [
        {
          "_id": "68fad821f158a71c5a2f57f4",
          "name": "Dian Yu",
          "hidden": false
        },
        {
          "_id": "68fad821f158a71c5a2f57f5",
          "name": "Yulai Zhao",
          "hidden": false
        },
        {
          "_id": "68fad821f158a71c5a2f57f6",
          "name": "Kishan Panaganti",
          "hidden": false
        },
        {
          "_id": "68fad821f158a71c5a2f57f7",
          "name": "Linfeng Song",
          "hidden": false
        },
        {
          "_id": "68fad821f158a71c5a2f57f8",
          "name": "Haitao Mi",
          "hidden": false
        },
        {
          "_id": "68fad821f158a71c5a2f57f9",
          "name": "Dong Yu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-23T04:15:22.000Z",
      "submittedOnDailyAt": "2025-10-24T00:18:52.427Z",
      "title": "Every Question Has Its Own Value: Reinforcement Learning with Explicit\n  Human Values",
      "submittedOnDailyBy": {
        "_id": "62d58fd53bf5e059f7cc3245",
        "avatarUrl": "/avatars/7a4f3ee4a37245f67efd26749d66a706.svg",
        "isPro": false,
        "fullname": "Dian Yu",
        "user": "yudian",
        "type": "user"
      },
      "summary": "We propose Reinforcement Learning with Explicit Human Values (RLEV), a method\nthat aligns Large Language Model (LLM) optimization directly with quantifiable\nhuman value signals. While Reinforcement Learning with Verifiable Rewards\n(RLVR) effectively trains models in objective domains using binary correctness\nrewards, it overlooks that not all tasks are equally significant. RLEV extends\nthis framework by incorporating human-defined value signals directly into the\nreward function. Using exam-style data with explicit ground-truth value labels,\nRLEV consistently outperforms correctness-only baselines across multiple RL\nalgorithms and model scales. Crucially, RLEV policies not only improve\nvalue-weighted accuracy but also learn a value-sensitive termination policy:\nconcise for low-value prompts, thorough for high-value ones. We demonstrate\nthis behavior stems from value-weighted gradient amplification on\nend-of-sequence tokens. Ablation studies confirm the gain is causally linked to\nvalue alignment. RLEV remains robust under noisy value signals, such as\ndifficulty-based labels, demonstrating that optimizing for an explicit utility\nfunction offers a practical path to aligning LLMs with human priorities.",
      "upvotes": 12,
      "discussionId": "68fad821f158a71c5a2f57fa",
      "ai_summary": "RLEV, a reinforcement learning method, aligns LLM optimization with human value signals, improving value-weighted accuracy and learning value-sensitive termination policies.",
      "ai_keywords": [
        "Reinforcement Learning with Explicit Human Values (RLEV)",
        "Large Language Model (LLM)",
        "Reinforcement Learning with Verifiable Rewards (RLVR)",
        "value signals",
        "reward function",
        "exam-style data",
        "value-weighted accuracy",
        "value-sensitive termination policy",
        "gradient amplification",
        "end-of-sequence tokens",
        "ablation studies",
        "value alignment",
        "noisy value signals",
        "utility function"
      ],
      "organization": {
        "_id": "66543b6e420092799d2f625c",
        "name": "tencent",
        "fullname": "Tencent",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
      }
    },
    "publishedAt": "2025-10-23T00:15:22.000Z",
    "title": "Every Question Has Its Own Value: Reinforcement Learning with Explicit\n  Human Values",
    "summary": "We propose Reinforcement Learning with Explicit Human Values (RLEV), a method\nthat aligns Large Language Model (LLM) optimization directly with quantifiable\nhuman value signals. While Reinforcement Learning with Verifiable Rewards\n(RLVR) effectively trains models in objective domains using binary correctness\nrewards, it overlooks that not all tasks are equally significant. RLEV extends\nthis framework by incorporating human-defined value signals directly into the\nreward function. Using exam-style data with explicit ground-truth value labels,\nRLEV consistently outperforms correctness-only baselines across multiple RL\nalgorithms and model scales. Crucially, RLEV policies not only improve\nvalue-weighted accuracy but also learn a value-sensitive termination policy:\nconcise for low-value prompts, thorough for high-value ones. We demonstrate\nthis behavior stems from value-weighted gradient amplification on\nend-of-sequence tokens. Ablation studies confirm the gain is causally linked to\nvalue alignment. RLEV remains robust under noisy value signals, such as\ndifficulty-based labels, demonstrating that optimizing for an explicit utility\nfunction offers a practical path to aligning LLMs with human priorities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20187.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d58fd53bf5e059f7cc3245",
      "avatarUrl": "/avatars/7a4f3ee4a37245f67efd26749d66a706.svg",
      "fullname": "Dian Yu",
      "name": "yudian",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "organization": {
      "_id": "66543b6e420092799d2f625c",
      "name": "tencent",
      "fullname": "Tencent",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.19365",
      "authors": [
        {
          "_id": "68f98172b9b2e4ae04673725",
          "user": {
            "_id": "6497ffbf2a997a45e987e139",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6497ffbf2a997a45e987e139/LUipMEC-wC1QeIXHSMnsx.png",
            "isPro": true,
            "fullname": "Umar Butler",
            "user": "umarbutler",
            "type": "user"
          },
          "name": "Umar Butler",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-10-23T14:53:11.283Z",
          "hidden": false
        },
        {
          "_id": "68f98172b9b2e4ae04673726",
          "user": {
            "_id": "63568098ef1d4c9191533b3f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63568098ef1d4c9191533b3f/o9IGAr_XBBiMtGRGeO3V3.png",
            "isPro": false,
            "fullname": "Abdur-Rahman Butler",
            "user": "abdurrahmanbutler",
            "type": "user"
          },
          "name": "Abdur-Rahman Butler",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-23T14:51:30.656Z",
          "hidden": false
        },
        {
          "_id": "68f98172b9b2e4ae04673727",
          "user": {
            "_id": "65839aaab6a60815eefed378",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65839aaab6a60815eefed378/a-Xym6IxpYy0b1B6CJpEj.jpeg",
            "isPro": false,
            "fullname": "Adrian Lucas Malec",
            "user": "adlumal",
            "type": "user"
          },
          "name": "Adrian Lucas Malec",
          "status": "admin_assigned",
          "statusLastChangedAt": "2025-10-23T14:53:47.869Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6497ffbf2a997a45e987e139/zowG0N3HVQ6RkF_ekAbz2.webp"
      ],
      "publishedAt": "2025-10-22T08:38:44.000Z",
      "submittedOnDailyAt": "2025-10-24T02:37:18.738Z",
      "title": "The Massive Legal Embedding Benchmark (MLEB)",
      "submittedOnDailyBy": {
        "_id": "6497ffbf2a997a45e987e139",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6497ffbf2a997a45e987e139/LUipMEC-wC1QeIXHSMnsx.png",
        "isPro": true,
        "fullname": "Umar Butler",
        "user": "umarbutler",
        "type": "user"
      },
      "summary": "We present the Massive Legal Embedding Benchmark (MLEB), the largest, most\ndiverse, and most comprehensive open-source benchmark for legal information\nretrieval to date. MLEB consists of ten expert-annotated datasets spanning\nmultiple jurisdictions (the US, UK, EU, Australia, Ireland, and Singapore),\ndocument types (cases, legislation, regulatory guidance, contracts, and\nliterature), and task types (search, zero-shot classification, and question\nanswering). Seven of the datasets in MLEB were newly constructed in order to\nfill domain and jurisdictional gaps in the open-source legal information\nretrieval landscape. We document our methodology in building MLEB and creating\nthe new constituent datasets, and release our code, results, and data openly to\nassist with reproducible evaluations.",
      "upvotes": 12,
      "discussionId": "68f98172b9b2e4ae04673728",
      "projectPage": "https://isaacus.com/mleb",
      "githubRepo": "https://github.com/isaacus-dev/mleb",
      "ai_summary": "MLEB is the largest open-source benchmark for legal information retrieval, encompassing multiple jurisdictions, document types, and task types.",
      "ai_keywords": [
        "Massive Legal Embedding Benchmark",
        "MLEB",
        "legal information retrieval",
        "expert-annotated datasets",
        "jurisdictions",
        "document types",
        "task types",
        "zero-shot classification",
        "question answering"
      ],
      "githubStars": 16,
      "organization": {
        "_id": "671f7ee82174ca42d524f1d3",
        "name": "isaacus",
        "fullname": "Isaacus",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6497ffbf2a997a45e987e139/HCfH2lC0OMQWC5PiwZQya.png"
      }
    },
    "publishedAt": "2025-10-22T04:38:44.000Z",
    "title": "The Massive Legal Embedding Benchmark (MLEB)",
    "summary": "We present the Massive Legal Embedding Benchmark (MLEB), the largest, most\ndiverse, and most comprehensive open-source benchmark for legal information\nretrieval to date. MLEB consists of ten expert-annotated datasets spanning\nmultiple jurisdictions (the US, UK, EU, Australia, Ireland, and Singapore),\ndocument types (cases, legislation, regulatory guidance, contracts, and\nliterature), and task types (search, zero-shot classification, and question\nanswering). Seven of the datasets in MLEB were newly constructed in order to\nfill domain and jurisdictional gaps in the open-source legal information\nretrieval landscape. We document our methodology in building MLEB and creating\nthe new constituent datasets, and release our code, results, and data openly to\nassist with reproducible evaluations.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6497ffbf2a997a45e987e139/zowG0N3HVQ6RkF_ekAbz2.webp"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19365.png",
    "numComments": 4,
    "submittedBy": {
      "_id": "6497ffbf2a997a45e987e139",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6497ffbf2a997a45e987e139/LUipMEC-wC1QeIXHSMnsx.png",
      "fullname": "Umar Butler",
      "name": "umarbutler",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 42
    },
    "organization": {
      "_id": "671f7ee82174ca42d524f1d3",
      "name": "isaacus",
      "fullname": "Isaacus",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6497ffbf2a997a45e987e139/HCfH2lC0OMQWC5PiwZQya.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.20766",
      "authors": [
        {
          "_id": "68faf9f2f158a71c5a2f58ae",
          "name": "Noam Issachar",
          "hidden": false
        },
        {
          "_id": "68faf9f2f158a71c5a2f58af",
          "name": "Guy Yariv",
          "hidden": false
        },
        {
          "_id": "68faf9f2f158a71c5a2f58b0",
          "name": "Sagie Benaim",
          "hidden": false
        },
        {
          "_id": "68faf9f2f158a71c5a2f58b1",
          "name": "Yossi Adi",
          "hidden": false
        },
        {
          "_id": "68faf9f2f158a71c5a2f58b2",
          "name": "Dani Lischinski",
          "hidden": false
        },
        {
          "_id": "68faf9f2f158a71c5a2f58b3",
          "name": "Raanan Fattal",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/646d239f4220471ca0c6471c/1YjzbsoNi3ExGkpg8YQym.png"
      ],
      "publishedAt": "2025-10-23T17:42:14.000Z",
      "submittedOnDailyAt": "2025-10-24T02:35:27.483Z",
      "title": "DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion",
      "submittedOnDailyBy": {
        "_id": "646d239f4220471ca0c6471c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d239f4220471ca0c6471c/sRwzko8XEUVCkeD7jXceH.jpeg",
        "isPro": false,
        "fullname": "Guy Yariv",
        "user": "GuyYariv",
        "type": "user"
      },
      "summary": "Diffusion Transformer models can generate images with remarkable fidelity and\ndetail, yet training them at ultra-high resolutions remains extremely costly\ndue to the self-attention mechanism's quadratic scaling with the number of\nimage tokens. In this paper, we introduce Dynamic Position Extrapolation\n(DyPE), a novel, training-free method that enables pre-trained diffusion\ntransformers to synthesize images at resolutions far beyond their training\ndata, with no additional sampling cost. DyPE takes advantage of the spectral\nprogression inherent to the diffusion process, where low-frequency structures\nconverge early, while high-frequencies take more steps to resolve.\nSpecifically, DyPE dynamically adjusts the model's positional encoding at each\ndiffusion step, matching their frequency spectrum with the current stage of the\ngenerative process. This approach allows us to generate images at resolutions\nthat exceed the training resolution dramatically, e.g., 16 million pixels using\nFLUX. On multiple benchmarks, DyPE consistently improves performance and\nachieves state-of-the-art fidelity in ultra-high-resolution image generation,\nwith gains becoming even more pronounced at higher resolutions. Project page is\navailable at https://noamissachar.github.io/DyPE/.",
      "upvotes": 10,
      "discussionId": "68faf9f2f158a71c5a2f58b4",
      "projectPage": "https://noamissachar.github.io/DyPE/",
      "githubRepo": "https://github.com/guyyariv/DyPE",
      "ai_summary": "Dynamic Position Extrapolation (DyPE) enhances ultra-high-resolution image generation by dynamically adjusting positional encodings in pre-trained diffusion transformers, achieving state-of-the-art fidelity without additional sampling cost.",
      "ai_keywords": [
        "Diffusion Transformer models",
        "self-attention mechanism",
        "Dynamic Position Extrapolation (DyPE)",
        "positional encoding",
        "spectral progression",
        "low-frequency structures",
        "high-frequencies",
        "generative process",
        "ultra-high-resolution image generation",
        "FLUX"
      ],
      "githubStars": 0,
      "organization": {
        "_id": "65157bc51e7b9224c9c6d460",
        "name": "HUJI-IL",
        "fullname": "The Hebrew University of Jerusalem"
      }
    },
    "publishedAt": "2025-10-23T13:42:14.000Z",
    "title": "DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion",
    "summary": "Diffusion Transformer models can generate images with remarkable fidelity and\ndetail, yet training them at ultra-high resolutions remains extremely costly\ndue to the self-attention mechanism's quadratic scaling with the number of\nimage tokens. In this paper, we introduce Dynamic Position Extrapolation\n(DyPE), a novel, training-free method that enables pre-trained diffusion\ntransformers to synthesize images at resolutions far beyond their training\ndata, with no additional sampling cost. DyPE takes advantage of the spectral\nprogression inherent to the diffusion process, where low-frequency structures\nconverge early, while high-frequencies take more steps to resolve.\nSpecifically, DyPE dynamically adjusts the model's positional encoding at each\ndiffusion step, matching their frequency spectrum with the current stage of the\ngenerative process. This approach allows us to generate images at resolutions\nthat exceed the training resolution dramatically, e.g., 16 million pixels using\nFLUX. On multiple benchmarks, DyPE consistently improves performance and\nachieves state-of-the-art fidelity in ultra-high-resolution image generation,\nwith gains becoming even more pronounced at higher resolutions. Project page is\navailable at https://noamissachar.github.io/DyPE/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/646d239f4220471ca0c6471c/1YjzbsoNi3ExGkpg8YQym.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20766.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "646d239f4220471ca0c6471c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d239f4220471ca0c6471c/sRwzko8XEUVCkeD7jXceH.jpeg",
      "fullname": "Guy Yariv",
      "name": "GuyYariv",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "65157bc51e7b9224c9c6d460",
      "name": "HUJI-IL",
      "fullname": "The Hebrew University of Jerusalem"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.18821",
      "authors": [
        {
          "_id": "68f85c9b7669bcaeecce0d9a",
          "name": "Hongliang Lu",
          "hidden": false
        },
        {
          "_id": "68f85c9b7669bcaeecce0d9b",
          "user": {
            "_id": "669ffabefbfc379ab3d8fbb0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xCjsWtFvbZPLRrOv_1zZD.jpeg",
            "isPro": false,
            "fullname": "Yuhang Wen",
            "user": "Necolizer",
            "type": "user"
          },
          "name": "Yuhang Wen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-10-22T15:43:23.308Z",
          "hidden": false
        },
        {
          "_id": "68f85c9b7669bcaeecce0d9c",
          "name": "Pengyu Cheng",
          "hidden": false
        },
        {
          "_id": "68f85c9b7669bcaeecce0d9d",
          "name": "Ruijin Ding",
          "hidden": false
        },
        {
          "_id": "68f85c9b7669bcaeecce0d9e",
          "name": "Haotian Xu",
          "hidden": false
        },
        {
          "_id": "68f85c9b7669bcaeecce0d9f",
          "name": "Jiaqi Guo",
          "hidden": false
        },
        {
          "_id": "68f85c9b7669bcaeecce0da0",
          "name": "Chutian Wang",
          "hidden": false
        },
        {
          "_id": "68f85c9b7669bcaeecce0da1",
          "name": "Haonan Chen",
          "hidden": false
        },
        {
          "_id": "68f85c9b7669bcaeecce0da2",
          "name": "Xiaoxi Jiang",
          "hidden": false
        },
        {
          "_id": "68f85c9b7669bcaeecce0da3",
          "name": "Guanjun Jiang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-21T17:19:35.000Z",
      "submittedOnDailyAt": "2025-10-24T03:42:50.965Z",
      "title": "Search Self-play: Pushing the Frontier of Agent Capability without\n  Supervision",
      "submittedOnDailyBy": {
        "_id": "669ffabefbfc379ab3d8fbb0",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xCjsWtFvbZPLRrOv_1zZD.jpeg",
        "isPro": false,
        "fullname": "Yuhang Wen",
        "user": "Necolizer",
        "type": "user"
      },
      "summary": "Reinforcement learning with verifiable rewards (RLVR) has become the\nmainstream technique for training LLM agents. However, RLVR highly depends on\nwell-crafted task queries and corresponding ground-truth answers to provide\naccurate rewards, which requires massive human efforts and hinders the RL\nscaling processes, especially under agentic scenarios. Although a few recent\nworks explore task synthesis methods, the difficulty of generated agentic tasks\ncan hardly be controlled to provide effective RL training advantages. To\nachieve agentic RLVR with higher scalability, we explore self-play training for\ndeep search agents, in which the learning LLM utilizes multi-turn search engine\ncalling and acts simultaneously as both a task proposer and a problem solver.\nThe task proposer aims to generate deep search queries with well-defined\nground-truth answers and increasing task difficulty. The problem solver tries\nto handle the generated search queries and output the correct answer\npredictions. To ensure that each generated search query has accurate ground\ntruth, we collect all the searching results from the proposer's trajectory as\nexternal knowledge, then conduct retrieval-augmentation generation (RAG) to\ntest whether the proposed query can be correctly answered with all necessary\nsearch documents provided. In this search self-play (SSP) game, the proposer\nand the solver co-evolve their agent capabilities through both competition and\ncooperation. With substantial experimental results, we find that SSP can\nsignificantly improve search agents' performance uniformly on various\nbenchmarks without any supervision under both from-scratch and continuous RL\ntraining setups. The code is at https://github.com/Alibaba-Quark/SSP.",
      "upvotes": 5,
      "discussionId": "68f85c9c7669bcaeecce0da4",
      "githubRepo": "https://github.com/Alibaba-Quark/SSP",
      "ai_summary": "Self-play training for deep search agents improves performance through co-evolution of task generation and problem solving without supervision.",
      "ai_keywords": [
        "reinforcement learning with verifiable rewards",
        "RLVR",
        "self-play training",
        "deep search agents",
        "multi-turn search engine calling",
        "task proposer",
        "problem solver",
        "retrieval-augmentation generation",
        "RAG",
        "search self-play",
        "SSP",
        "from-scratch",
        "continuous RL training"
      ],
      "githubStars": 5,
      "organization": {
        "_id": "6765550c990bcf161cc7e94e",
        "name": "Quark-LLM",
        "fullname": "Quark",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67655397f7ada4603851ddb3/v51dcR1-2Z-ndlUi-1jQI.png"
      }
    },
    "publishedAt": "2025-10-21T13:19:35.000Z",
    "title": "Search Self-play: Pushing the Frontier of Agent Capability without\n  Supervision",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has become the\nmainstream technique for training LLM agents. However, RLVR highly depends on\nwell-crafted task queries and corresponding ground-truth answers to provide\naccurate rewards, which requires massive human efforts and hinders the RL\nscaling processes, especially under agentic scenarios. Although a few recent\nworks explore task synthesis methods, the difficulty of generated agentic tasks\ncan hardly be controlled to provide effective RL training advantages. To\nachieve agentic RLVR with higher scalability, we explore self-play training for\ndeep search agents, in which the learning LLM utilizes multi-turn search engine\ncalling and acts simultaneously as both a task proposer and a problem solver.\nThe task proposer aims to generate deep search queries with well-defined\nground-truth answers and increasing task difficulty. The problem solver tries\nto handle the generated search queries and output the correct answer\npredictions. To ensure that each generated search query has accurate ground\ntruth, we collect all the searching results from the proposer's trajectory as\nexternal knowledge, then conduct retrieval-augmentation generation (RAG) to\ntest whether the proposed query can be correctly answered with all necessary\nsearch documents provided. In this search self-play (SSP) game, the proposer\nand the solver co-evolve their agent capabilities through both competition and\ncooperation. With substantial experimental results, we find that SSP can\nsignificantly improve search agents' performance uniformly on various\nbenchmarks without any supervision under both from-scratch and continuous RL\ntraining setups. The code is at https://github.com/Alibaba-Quark/SSP.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.18821.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "669ffabefbfc379ab3d8fbb0",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xCjsWtFvbZPLRrOv_1zZD.jpeg",
      "fullname": "Yuhang Wen",
      "name": "Necolizer",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "6765550c990bcf161cc7e94e",
      "name": "Quark-LLM",
      "fullname": "Quark",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67655397f7ada4603851ddb3/v51dcR1-2Z-ndlUi-1jQI.png"
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2510.20820",
      "authors": [
        {
          "_id": "68fada3af158a71c5a2f5812",
          "name": "Guocheng Gordon Qian",
          "hidden": false
        },
        {
          "_id": "68fada3af158a71c5a2f5813",
          "name": "Ruihang Zhang",
          "hidden": false
        },
        {
          "_id": "68fada3af158a71c5a2f5814",
          "name": "Tsai-Shien Chen",
          "hidden": false
        },
        {
          "_id": "68fada3af158a71c5a2f5815",
          "name": "Yusuf Dalva",
          "hidden": false
        },
        {
          "_id": "68fada3af158a71c5a2f5816",
          "name": "Anujraaj Argo Goyal",
          "hidden": false
        },
        {
          "_id": "68fada3af158a71c5a2f5817",
          "name": "Willi Menapace",
          "hidden": false
        },
        {
          "_id": "68fada3af158a71c5a2f5818",
          "name": "Ivan Skorokhodov",
          "hidden": false
        },
        {
          "_id": "68fada3af158a71c5a2f5819",
          "name": "Meng Dong",
          "hidden": false
        },
        {
          "_id": "68fada3af158a71c5a2f581a",
          "name": "Arpit Sahni",
          "hidden": false
        },
        {
          "_id": "68fada3af158a71c5a2f581b",
          "name": "Daniil Ostashev",
          "hidden": false
        },
        {
          "_id": "68fada3af158a71c5a2f581c",
          "name": "Ju Hu",
          "hidden": false
        },
        {
          "_id": "68fada3af158a71c5a2f581d",
          "name": "Sergey Tulyakov",
          "hidden": false
        },
        {
          "_id": "68fada3af158a71c5a2f581e",
          "name": "Kuan-Chieh Jackson Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-23T17:59:55.000Z",
      "submittedOnDailyAt": "2025-10-24T00:16:05.053Z",
      "title": "LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered\n  Canvas",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Despite their impressive visual fidelity, existing personalized generative\nmodels lack interactive control over spatial composition and scale poorly to\nmultiple subjects. To address these limitations, we present LayerComposer, an\ninteractive framework for personalized, multi-subject text-to-image generation.\nOur approach introduces two main contributions: (1) a layered canvas, a novel\nrepresentation in which each subject is placed on a distinct layer, enabling\nocclusion-free composition; and (2) a locking mechanism that preserves selected\nlayers with high fidelity while allowing the remaining layers to adapt flexibly\nto the surrounding context. Similar to professional image-editing software, the\nproposed layered canvas allows users to place, resize, or lock input subjects\nthrough intuitive layer manipulation. Our versatile locking mechanism requires\nno architectural changes, relying instead on inherent positional embeddings\ncombined with a new complementary data sampling strategy. Extensive experiments\ndemonstrate that LayerComposer achieves superior spatial control and identity\npreservation compared to the state-of-the-art methods in multi-subject\npersonalized image generation.",
      "upvotes": 4,
      "discussionId": "68fada3af158a71c5a2f581f",
      "projectPage": "https://snap-research.github.io/layercomposer/",
      "ai_summary": "LayerComposer provides interactive control over spatial composition and scalability in multi-subject text-to-image generation through a layered canvas and locking mechanism.",
      "ai_keywords": [
        "layered canvas",
        "locking mechanism",
        "positional embeddings",
        "data sampling strategy",
        "text-to-image generation",
        "spatial control",
        "identity preservation"
      ],
      "organization": {
        "_id": "63c87c41cd6a490608ce31d1",
        "name": "snap-research",
        "fullname": "Snap Research",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1674083325534-61f19829233c91cbd2f79e70.png"
      }
    },
    "publishedAt": "2025-10-23T13:59:55.000Z",
    "title": "LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered\n  Canvas",
    "summary": "Despite their impressive visual fidelity, existing personalized generative\nmodels lack interactive control over spatial composition and scale poorly to\nmultiple subjects. To address these limitations, we present LayerComposer, an\ninteractive framework for personalized, multi-subject text-to-image generation.\nOur approach introduces two main contributions: (1) a layered canvas, a novel\nrepresentation in which each subject is placed on a distinct layer, enabling\nocclusion-free composition; and (2) a locking mechanism that preserves selected\nlayers with high fidelity while allowing the remaining layers to adapt flexibly\nto the surrounding context. Similar to professional image-editing software, the\nproposed layered canvas allows users to place, resize, or lock input subjects\nthrough intuitive layer manipulation. Our versatile locking mechanism requires\nno architectural changes, relying instead on inherent positional embeddings\ncombined with a new complementary data sampling strategy. Extensive experiments\ndemonstrate that LayerComposer achieves superior spatial control and identity\npreservation compared to the state-of-the-art methods in multi-subject\npersonalized image generation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20820.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 142
    },
    "organization": {
      "_id": "63c87c41cd6a490608ce31d1",
      "name": "snap-research",
      "fullname": "Snap Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1674083325534-61f19829233c91cbd2f79e70.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.20803",
      "authors": [
        {
          "_id": "68fae536f158a71c5a2f5848",
          "name": "Xiaolong Wang",
          "hidden": false
        },
        {
          "_id": "68fae536f158a71c5a2f5849",
          "name": "Lixiang Ru",
          "hidden": false
        },
        {
          "_id": "68fae536f158a71c5a2f584a",
          "name": "Ziyuan Huang",
          "hidden": false
        },
        {
          "_id": "68fae536f158a71c5a2f584b",
          "name": "Kaixiang Ji",
          "hidden": false
        },
        {
          "_id": "68fae536f158a71c5a2f584c",
          "name": "Dandan Zheng",
          "hidden": false
        },
        {
          "_id": "68fae536f158a71c5a2f584d",
          "name": "Jingdong Chen",
          "hidden": false
        },
        {
          "_id": "68fae536f158a71c5a2f584e",
          "name": "Jun Zhou",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-23T17:58:26.000Z",
      "submittedOnDailyAt": "2025-10-24T01:04:30.815Z",
      "title": "ARGenSeg: Image Segmentation with Autoregressive Image Generation Model",
      "submittedOnDailyBy": {
        "_id": "64e848dd9a3cd93b371166cf",
        "avatarUrl": "/avatars/6fcee1fe59624113fcf233caf0197729.svg",
        "isPro": false,
        "fullname": "Xiaolong Wang",
        "user": "Xiaolong-Wang",
        "type": "user"
      },
      "summary": "We propose a novel AutoRegressive Generation-based paradigm for image\nSegmentation (ARGenSeg), achieving multimodal understanding and pixel-level\nperception within a unified framework. Prior works integrating image\nsegmentation into multimodal large language models (MLLMs) typically employ\neither boundary points representation or dedicated segmentation heads. These\nmethods rely on discrete representations or semantic prompts fed into\ntask-specific decoders, which limits the ability of the MLLM to capture\nfine-grained visual details. To address these challenges, we introduce a\nsegmentation framework for MLLM based on image generation, which naturally\nproduces dense masks for target objects. We leverage MLLM to output visual\ntokens and detokenize them into images using an universal VQ-VAE, making the\nsegmentation fully dependent on the pixel-level understanding of the MLLM. To\nreduce inference latency, we employ a next-scale-prediction strategy to\ngenerate required visual tokens in parallel. Extensive experiments demonstrate\nthat our method surpasses prior state-of-the-art approaches on multiple\nsegmentation datasets with a remarkable boost in inference speed, while\nmaintaining strong understanding capabilities.",
      "upvotes": 3,
      "discussionId": "68fae536f158a71c5a2f584f",
      "ai_summary": "A novel AutoRegressive Generation-based paradigm for image segmentation leverages multimodal large language models and VQ-VAE for dense mask generation, achieving high performance and fast inference.",
      "ai_keywords": [
        "AutoRegressive Generation",
        "image segmentation",
        "multimodal large language models",
        "boundary points representation",
        "segmentation heads",
        "discrete representations",
        "semantic prompts",
        "task-specific decoders",
        "visual tokens",
        "universal VQ-VAE",
        "next-scale-prediction strategy"
      ],
      "organization": {
        "_id": "67aea5c8f086ab0f70ed97c9",
        "name": "inclusionAI",
        "fullname": "inclusionAI",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"
      }
    },
    "publishedAt": "2025-10-23T13:58:26.000Z",
    "title": "ARGenSeg: Image Segmentation with Autoregressive Image Generation Model",
    "summary": "We propose a novel AutoRegressive Generation-based paradigm for image\nSegmentation (ARGenSeg), achieving multimodal understanding and pixel-level\nperception within a unified framework. Prior works integrating image\nsegmentation into multimodal large language models (MLLMs) typically employ\neither boundary points representation or dedicated segmentation heads. These\nmethods rely on discrete representations or semantic prompts fed into\ntask-specific decoders, which limits the ability of the MLLM to capture\nfine-grained visual details. To address these challenges, we introduce a\nsegmentation framework for MLLM based on image generation, which naturally\nproduces dense masks for target objects. We leverage MLLM to output visual\ntokens and detokenize them into images using an universal VQ-VAE, making the\nsegmentation fully dependent on the pixel-level understanding of the MLLM. To\nreduce inference latency, we employ a next-scale-prediction strategy to\ngenerate required visual tokens in parallel. Extensive experiments demonstrate\nthat our method surpasses prior state-of-the-art approaches on multiple\nsegmentation datasets with a remarkable boost in inference speed, while\nmaintaining strong understanding capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20803.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64e848dd9a3cd93b371166cf",
      "avatarUrl": "/avatars/6fcee1fe59624113fcf233caf0197729.svg",
      "fullname": "Xiaolong Wang",
      "name": "Xiaolong-Wang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "67aea5c8f086ab0f70ed97c9",
      "name": "inclusionAI",
      "fullname": "inclusionAI",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.19944",
      "authors": [
        {
          "_id": "68fb1742f158a71c5a2f592e",
          "name": "Jiashi Feng",
          "hidden": false
        },
        {
          "_id": "68fb1742f158a71c5a2f592f",
          "name": "Xiu Li",
          "hidden": false
        },
        {
          "_id": "68fb1742f158a71c5a2f5930",
          "name": "Jing Lin",
          "hidden": false
        },
        {
          "_id": "68fb1742f158a71c5a2f5931",
          "name": "Jiahang Liu",
          "hidden": false
        },
        {
          "_id": "68fb1742f158a71c5a2f5932",
          "name": "Gaohong Liu",
          "hidden": false
        },
        {
          "_id": "68fb1742f158a71c5a2f5933",
          "name": "Weiqiang Lou",
          "hidden": false
        },
        {
          "_id": "68fb1742f158a71c5a2f5934",
          "name": "Su Ma",
          "hidden": false
        },
        {
          "_id": "68fb1742f158a71c5a2f5935",
          "name": "Guang Shi",
          "hidden": false
        },
        {
          "_id": "68fb1742f158a71c5a2f5936",
          "name": "Qinlong Wang",
          "hidden": false
        },
        {
          "_id": "68fb1742f158a71c5a2f5937",
          "name": "Jun Wang",
          "hidden": false
        },
        {
          "_id": "68fb1742f158a71c5a2f5938",
          "name": "Zhongcong Xu",
          "hidden": false
        },
        {
          "_id": "68fb1742f158a71c5a2f5939",
          "name": "Xuanyu Yi",
          "hidden": false
        },
        {
          "_id": "68fb1742f158a71c5a2f593a",
          "name": "Zihao Yu",
          "hidden": false
        },
        {
          "_id": "68fb1742f158a71c5a2f593b",
          "name": "Jianfeng Zhang",
          "hidden": false
        },
        {
          "_id": "68fb1742f158a71c5a2f593c",
          "name": "Yifan Zhu",
          "hidden": false
        },
        {
          "_id": "68fb1742f158a71c5a2f593d",
          "name": "Rui Chen",
          "hidden": false
        },
        {
          "_id": "68fb1742f158a71c5a2f593e",
          "name": "Jinxin Chi",
          "hidden": false
        },
        {
          "_id": "68fb1742f158a71c5a2f593f",
          "name": "Zixian Du",
          "hidden": false
        },
        {
          "_id": "68fb1742f158a71c5a2f5940",
          "name": "Li Han",
          "hidden": false
        },
        {
          "_id": "68fb1742f158a71c5a2f5941",
          "name": "Lixin Huang",
          "hidden": false
        },
        {
          "_id": "68fb1742f158a71c5a2f5942",
          "name": "Kaihua Jiang",
          "hidden": false
        },
        {
          "_id": "68fb1742f158a71c5a2f5943",
          "name": "Yuhan Li",
          "hidden": false
        },
        {
          "_id": "68fb1742f158a71c5a2f5944",
          "name": "Guan Luo",
          "hidden": false
        },
        {
          "_id": "68fb1742f158a71c5a2f5945",
          "name": "Shuguang Wang",
          "hidden": false
        },
        {
          "_id": "68fb1742f158a71c5a2f5946",
          "name": "Qianyi Wu",
          "hidden": false
        },
        {
          "_id": "68fb1742f158a71c5a2f5947",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "68fb1742f158a71c5a2f5948",
          "name": "Junyang Zhang",
          "hidden": false
        },
        {
          "_id": "68fb1742f158a71c5a2f5949",
          "name": "Xuanmeng Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-22T18:16:32.000Z",
      "submittedOnDailyAt": "2025-10-24T04:38:54.664Z",
      "title": "Seed3D 1.0: From Images to High-Fidelity Simulation-Ready 3D Assets",
      "submittedOnDailyBy": {
        "_id": "631c2bed7f7a1b6cb9f6b114",
        "avatarUrl": "/avatars/97d2c0b6123691ea27157ebf8da59b45.svg",
        "isPro": false,
        "fullname": "Zhongcong Xu",
        "user": "zcxu-eric",
        "type": "user"
      },
      "summary": "Developing embodied AI agents requires scalable training environments that\nbalance content diversity with physics accuracy. World simulators provide such\nenvironments but face distinct limitations: video-based methods generate\ndiverse content but lack real-time physics feedback for interactive learning,\nwhile physics-based engines provide accurate dynamics but face scalability\nlimitations from costly manual asset creation. We present Seed3D 1.0, a\nfoundation model that generates simulation-ready 3D assets from single images,\naddressing the scalability challenge while maintaining physics rigor. Unlike\nexisting 3D generation models, our system produces assets with accurate\ngeometry, well-aligned textures, and realistic physically-based materials.\nThese assets can be directly integrated into physics engines with minimal\nconfiguration, enabling deployment in robotic manipulation and simulation\ntraining. Beyond individual objects, the system scales to complete scene\ngeneration through assembling objects into coherent environments. By enabling\nscalable simulation-ready content creation, Seed3D 1.0 provides a foundation\nfor advancing physics-based world simulators. Seed3D 1.0 is now available on\nhttps://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?modelId=doubao-seed3d-1-0-250928&tab=Gen3D",
      "upvotes": 3,
      "discussionId": "68fb1742f158a71c5a2f594a",
      "projectPage": "https://seed.bytedance.com/seed3d",
      "ai_summary": "Seed3D 1.0 generates scalable, physics-accurate 3D assets from images for use in simulation environments, enhancing both content diversity and real-time physics feedback.",
      "ai_keywords": [
        "world simulators",
        "video-based methods",
        "physics-based engines",
        "simulation-ready 3D assets",
        "accurate geometry",
        "well-aligned textures",
        "physically-based materials",
        "physics engines",
        "robotic manipulation",
        "scene generation",
        "coherent environments"
      ],
      "organization": {
        "_id": "67d1140985ea0644e2f14b99",
        "name": "ByteDance-Seed",
        "fullname": "ByteDance Seed",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
      }
    },
    "publishedAt": "2025-10-22T14:16:32.000Z",
    "title": "Seed3D 1.0: From Images to High-Fidelity Simulation-Ready 3D Assets",
    "summary": "Developing embodied AI agents requires scalable training environments that\nbalance content diversity with physics accuracy. World simulators provide such\nenvironments but face distinct limitations: video-based methods generate\ndiverse content but lack real-time physics feedback for interactive learning,\nwhile physics-based engines provide accurate dynamics but face scalability\nlimitations from costly manual asset creation. We present Seed3D 1.0, a\nfoundation model that generates simulation-ready 3D assets from single images,\naddressing the scalability challenge while maintaining physics rigor. Unlike\nexisting 3D generation models, our system produces assets with accurate\ngeometry, well-aligned textures, and realistic physically-based materials.\nThese assets can be directly integrated into physics engines with minimal\nconfiguration, enabling deployment in robotic manipulation and simulation\ntraining. Beyond individual objects, the system scales to complete scene\ngeneration through assembling objects into coherent environments. By enabling\nscalable simulation-ready content creation, Seed3D 1.0 provides a foundation\nfor advancing physics-based world simulators. Seed3D 1.0 is now available on\nhttps://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?modelId=doubao-seed3d-1-0-250928&tab=Gen3D",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19944.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "631c2bed7f7a1b6cb9f6b114",
      "avatarUrl": "/avatars/97d2c0b6123691ea27157ebf8da59b45.svg",
      "fullname": "Zhongcong Xu",
      "name": "zcxu-eric",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 78
    },
    "organization": {
      "_id": "67d1140985ea0644e2f14b99",
      "name": "ByteDance-Seed",
      "fullname": "ByteDance Seed",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.20771",
      "authors": [
        {
          "_id": "68fadaeaf158a71c5a2f5821",
          "name": "Huijie Zhang",
          "hidden": false
        },
        {
          "_id": "68fadaeaf158a71c5a2f5822",
          "name": "Aliaksandr Siarohin",
          "hidden": false
        },
        {
          "_id": "68fadaeaf158a71c5a2f5823",
          "name": "Willi Menapace",
          "hidden": false
        },
        {
          "_id": "68fadaeaf158a71c5a2f5824",
          "name": "Michael Vasilkovsky",
          "hidden": false
        },
        {
          "_id": "68fadaeaf158a71c5a2f5825",
          "name": "Sergey Tulyakov",
          "hidden": false
        },
        {
          "_id": "68fadaeaf158a71c5a2f5826",
          "name": "Qing Qu",
          "hidden": false
        },
        {
          "_id": "68fadaeaf158a71c5a2f5827",
          "name": "Ivan Skorokhodov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-23T17:45:06.000Z",
      "submittedOnDailyAt": "2025-10-24T00:19:24.773Z",
      "title": "AlphaFlow: Understanding and Improving MeanFlow Models",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "MeanFlow has recently emerged as a powerful framework for few-step generative\nmodeling trained from scratch, but its success is not yet fully understood. In\nthis work, we show that the MeanFlow objective naturally decomposes into two\nparts: trajectory flow matching and trajectory consistency. Through gradient\nanalysis, we find that these terms are strongly negatively correlated, causing\noptimization conflict and slow convergence. Motivated by these insights, we\nintroduce alpha-Flow, a broad family of objectives that unifies trajectory\nflow matching, Shortcut Model, and MeanFlow under one formulation. By adopting\na curriculum strategy that smoothly anneals from trajectory flow matching to\nMeanFlow, alpha-Flow disentangles the conflicting objectives, and achieves\nbetter convergence. When trained from scratch on class-conditional ImageNet-1K\n256x256 with vanilla DiT backbones, alpha-Flow consistently outperforms\nMeanFlow across scales and settings. Our largest alpha-Flow-XL/2+ model\nachieves new state-of-the-art results using vanilla DiT backbones, with FID\nscores of 2.58 (1-NFE) and 2.15 (2-NFE).",
      "upvotes": 2,
      "discussionId": "68fadaebf158a71c5a2f5828",
      "ai_summary": "The $\\alpha$-Flow framework improves few-step generative modeling by unifying and disentangling conflicting objectives, leading to better convergence and state-of-the-art performance on ImageNet-1K.",
      "ai_keywords": [
        "MeanFlow",
        "trajectory flow matching",
        "trajectory consistency",
        "gradient analysis",
        "$\\alpha$-Flow",
        "Shortcut Model",
        "curriculum strategy",
        "class-conditional",
        "ImageNet-1K",
        "DiT backbones",
        "FID scores"
      ],
      "organization": {
        "_id": "63c87c41cd6a490608ce31d1",
        "name": "snap-research",
        "fullname": "Snap Research",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1674083325534-61f19829233c91cbd2f79e70.png"
      }
    },
    "publishedAt": "2025-10-23T13:45:06.000Z",
    "title": "AlphaFlow: Understanding and Improving MeanFlow Models",
    "summary": "MeanFlow has recently emerged as a powerful framework for few-step generative\nmodeling trained from scratch, but its success is not yet fully understood. In\nthis work, we show that the MeanFlow objective naturally decomposes into two\nparts: trajectory flow matching and trajectory consistency. Through gradient\nanalysis, we find that these terms are strongly negatively correlated, causing\noptimization conflict and slow convergence. Motivated by these insights, we\nintroduce alpha-Flow, a broad family of objectives that unifies trajectory\nflow matching, Shortcut Model, and MeanFlow under one formulation. By adopting\na curriculum strategy that smoothly anneals from trajectory flow matching to\nMeanFlow, alpha-Flow disentangles the conflicting objectives, and achieves\nbetter convergence. When trained from scratch on class-conditional ImageNet-1K\n256x256 with vanilla DiT backbones, alpha-Flow consistently outperforms\nMeanFlow across scales and settings. Our largest alpha-Flow-XL/2+ model\nachieves new state-of-the-art results using vanilla DiT backbones, with FID\nscores of 2.58 (1-NFE) and 2.15 (2-NFE).",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20771.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 142
    },
    "organization": {
      "_id": "63c87c41cd6a490608ce31d1",
      "name": "snap-research",
      "fullname": "Snap Research",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1674083325534-61f19829233c91cbd2f79e70.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.20668",
      "authors": [
        {
          "_id": "68fb1095f158a71c5a2f58dd",
          "name": "Jinbin Bai",
          "hidden": false
        },
        {
          "_id": "68fb1095f158a71c5a2f58de",
          "name": "Yu Lei",
          "hidden": false
        },
        {
          "_id": "68fb1095f158a71c5a2f58df",
          "name": "Hecong Wu",
          "hidden": false
        },
        {
          "_id": "68fb1095f158a71c5a2f58e0",
          "name": "Yuchen Zhu",
          "hidden": false
        },
        {
          "_id": "68fb1095f158a71c5a2f58e1",
          "name": "Shufan Li",
          "hidden": false
        },
        {
          "_id": "68fb1095f158a71c5a2f58e2",
          "name": "Yi Xin",
          "hidden": false
        },
        {
          "_id": "68fb1095f158a71c5a2f58e3",
          "name": "Xiangtai Li",
          "hidden": false
        },
        {
          "_id": "68fb1095f158a71c5a2f58e4",
          "name": "Molei Tao",
          "hidden": false
        },
        {
          "_id": "68fb1095f158a71c5a2f58e5",
          "name": "Aditya Grover",
          "hidden": false
        },
        {
          "_id": "68fb1095f158a71c5a2f58e6",
          "name": "Ming-Hsuan Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-23T15:46:44.000Z",
      "submittedOnDailyAt": "2025-10-24T04:09:21.195Z",
      "title": "From Masks to Worlds: A Hitchhiker's Guide to World Models",
      "submittedOnDailyBy": {
        "_id": "63fccdac93b993a4ebd7789a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
        "isPro": false,
        "fullname": "Jinbin Bai",
        "user": "BryanW",
        "type": "user"
      },
      "summary": "This is not a typical survey of world models; it is a guide for those who\nwant to build worlds. We do not aim to catalog every paper that has ever\nmentioned a ``world model\". Instead, we follow one clear road: from early\nmasked models that unified representation learning across modalities, to\nunified architectures that share a single paradigm, then to interactive\ngenerative models that close the action-perception loop, and finally to\nmemory-augmented systems that sustain consistent worlds over time. We bypass\nloosely related branches to focus on the core: the generative heart, the\ninteractive loop, and the memory system. We show that this is the most\npromising path towards true world models.",
      "upvotes": 2,
      "discussionId": "68fb1095f158a71c5a2f58e7",
      "githubRepo": "https://github.com/M-E-AGI-Lab/Awesome-World-Models",
      "ai_summary": "The guide outlines a progression from early masked models to memory-augmented systems, emphasizing generative capabilities, interactive loops, and memory for building world models.",
      "ai_keywords": [
        "masked models",
        "representation learning",
        "unified architectures",
        "interactive generative models",
        "action-perception loop",
        "memory-augmented systems",
        "world models"
      ],
      "githubStars": 7
    },
    "publishedAt": "2025-10-23T11:46:44.000Z",
    "title": "From Masks to Worlds: A Hitchhiker's Guide to World Models",
    "summary": "This is not a typical survey of world models; it is a guide for those who\nwant to build worlds. We do not aim to catalog every paper that has ever\nmentioned a ``world model\". Instead, we follow one clear road: from early\nmasked models that unified representation learning across modalities, to\nunified architectures that share a single paradigm, then to interactive\ngenerative models that close the action-perception loop, and finally to\nmemory-augmented systems that sustain consistent worlds over time. We bypass\nloosely related branches to focus on the core: the generative heart, the\ninteractive loop, and the memory system. We show that this is the most\npromising path towards true world models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20668.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63fccdac93b993a4ebd7789a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
      "fullname": "Jinbin Bai",
      "name": "BryanW",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.20470",
      "authors": [
        {
          "_id": "68faec7af158a71c5a2f5879",
          "name": "Kun Ouyang",
          "hidden": false
        },
        {
          "_id": "68faec7af158a71c5a2f587a",
          "name": "Yuanxin Liu",
          "hidden": false
        },
        {
          "_id": "68faec7af158a71c5a2f587b",
          "name": "Linli Yao",
          "hidden": false
        },
        {
          "_id": "68faec7af158a71c5a2f587c",
          "name": "Yishuo Cai",
          "hidden": false
        },
        {
          "_id": "68faec7af158a71c5a2f587d",
          "name": "Hao Zhou",
          "hidden": false
        },
        {
          "_id": "68faec7af158a71c5a2f587e",
          "name": "Jie Zhou",
          "hidden": false
        },
        {
          "_id": "68faec7af158a71c5a2f587f",
          "name": "Fandong Meng",
          "hidden": false
        },
        {
          "_id": "68faec7af158a71c5a2f5880",
          "name": "Xu Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-23T12:11:46.000Z",
      "submittedOnDailyAt": "2025-10-24T01:36:56.303Z",
      "title": "Conan: Progressive Learning to Reason Like a Detective over Multi-Scale\n  Visual Evidence",
      "submittedOnDailyBy": {
        "_id": "62cd7aca7a036fc9941bb2b0",
        "avatarUrl": "/avatars/17a4d27af0243fd7dccf06066f671461.svg",
        "isPro": false,
        "fullname": "kun ouyang",
        "user": "RUBBISHLIKE",
        "type": "user"
      },
      "summary": "Video reasoning, which requires multi-step deduction across frames, remains a\nmajor challenge for multimodal large language models (MLLMs). While\nreinforcement learning (RL)-based methods enhance reasoning capabilities, they\noften rely on text-only chains that yield ungrounded or hallucinated\nconclusions. Conversely, frame-retrieval approaches introduce visual grounding\nbut still struggle with inaccurate evidence localization. To address these\nchallenges, we present Conan, a framework for evidence-grounded multi-step\nvideo reasoning. Conan identifies contextual and evidence frames, reasons over\ncross-frame clues, and adaptively decides when to conclude or explore further.\nTo achieve this, we (1) construct Conan-91K, a large-scale dataset of\nautomatically generated reasoning traces that includes frame identification,\nevidence reasoning, and action decision, and (2) design a multi-stage\nprogressive cold-start strategy combined with an\nIdentification-Reasoning-Action (AIR) RLVR training framework to jointly\nenhance multi-step visual reasoning. Extensive experiments on six multi-step\nreasoning benchmarks demonstrate that Conan surpasses the baseline\nQwen2.5-VL-7B-Instruct by an average of over 10% in accuracy, achieving\nstate-of-the-art performance. Furthermore, Conan generalizes effectively to\nlong-video understanding tasks, validating its strong scalability and\nrobustness.",
      "upvotes": 2,
      "discussionId": "68faec7af158a71c5a2f5881",
      "githubRepo": "https://github.com/OuyangKun10/Conan",
      "ai_summary": "Conan, a framework for evidence-grounded multi-step video reasoning, enhances visual grounding and reasoning accuracy through a multi-stage training strategy and outperforms existing models on various benchmarks.",
      "ai_keywords": [
        "video reasoning",
        "multimodal large language models",
        "reinforcement learning",
        "frame-retrieval",
        "contextual frames",
        "evidence frames",
        "cross-frame clues",
        "multi-stage progressive cold-start strategy",
        "Identification-Reasoning-Action (AIR) RLVR training framework",
        "long-video understanding"
      ],
      "githubStars": 3,
      "organization": {
        "_id": "61dcd8e344f59573371b5cb6",
        "name": "PekingUniversity",
        "fullname": "Peking University",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
      }
    },
    "publishedAt": "2025-10-23T08:11:46.000Z",
    "title": "Conan: Progressive Learning to Reason Like a Detective over Multi-Scale\n  Visual Evidence",
    "summary": "Video reasoning, which requires multi-step deduction across frames, remains a\nmajor challenge for multimodal large language models (MLLMs). While\nreinforcement learning (RL)-based methods enhance reasoning capabilities, they\noften rely on text-only chains that yield ungrounded or hallucinated\nconclusions. Conversely, frame-retrieval approaches introduce visual grounding\nbut still struggle with inaccurate evidence localization. To address these\nchallenges, we present Conan, a framework for evidence-grounded multi-step\nvideo reasoning. Conan identifies contextual and evidence frames, reasons over\ncross-frame clues, and adaptively decides when to conclude or explore further.\nTo achieve this, we (1) construct Conan-91K, a large-scale dataset of\nautomatically generated reasoning traces that includes frame identification,\nevidence reasoning, and action decision, and (2) design a multi-stage\nprogressive cold-start strategy combined with an\nIdentification-Reasoning-Action (AIR) RLVR training framework to jointly\nenhance multi-step visual reasoning. Extensive experiments on six multi-step\nreasoning benchmarks demonstrate that Conan surpasses the baseline\nQwen2.5-VL-7B-Instruct by an average of over 10% in accuracy, achieving\nstate-of-the-art performance. Furthermore, Conan generalizes effectively to\nlong-video understanding tasks, validating its strong scalability and\nrobustness.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20470.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62cd7aca7a036fc9941bb2b0",
      "avatarUrl": "/avatars/17a4d27af0243fd7dccf06066f671461.svg",
      "fullname": "kun ouyang",
      "name": "RUBBISHLIKE",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "organization": {
      "_id": "61dcd8e344f59573371b5cb6",
      "name": "PekingUniversity",
      "fullname": "Peking University",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.20270",
      "authors": [
        {
          "_id": "68faded5f158a71c5a2f583b",
          "name": "Ziqian Zhong",
          "hidden": false
        },
        {
          "_id": "68faded5f158a71c5a2f583c",
          "name": "Aditi Raghunathan",
          "hidden": false
        },
        {
          "_id": "68faded5f158a71c5a2f583d",
          "name": "Nicholas Carlini",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-23T06:58:32.000Z",
      "submittedOnDailyAt": "2025-10-24T00:35:23.999Z",
      "title": "ImpossibleBench: Measuring LLMs' Propensity of Exploiting Test Cases",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "The tendency to find and exploit \"shortcuts\" to complete tasks poses\nsignificant risks for reliable assessment and deployment of large language\nmodels (LLMs). For example, an LLM agent with access to unit tests may delete\nfailing tests rather than fix the underlying bug. Such behavior undermines both\nthe validity of benchmark results and the reliability of real-world LLM coding\nassistant deployments.\n  To quantify, study, and mitigate such behavior, we introduce ImpossibleBench,\na benchmark framework that systematically measures LLM agents' propensity to\nexploit test cases. ImpossibleBench creates \"impossible\" variants of tasks from\nexisting benchmarks like LiveCodeBench and SWE-bench by introducing direct\nconflicts between the natural-language specification and the unit tests. We\nmeasure an agent's \"cheating rate\" as its pass rate on these impossible tasks,\nwhere any pass necessarily implies a specification-violating shortcut.\n  As a practical framework, ImpossibleBench is not just an evaluation but a\nversatile tool. We demonstrate its utility for: (1) studying model behaviors,\nrevealing more fine-grained details of cheating behaviors from simple test\nmodification to complex operator overloading; (2) context engineering, showing\nhow prompt, test access and feedback loop affect cheating rates; and (3)\ndeveloping monitoring tools, providing a testbed with verified deceptive\nsolutions. We hope ImpossibleBench serves as a useful framework for building\nmore robust and reliable LLM systems.\n  Our implementation can be found at\nhttps://github.com/safety-research/impossiblebench.",
      "upvotes": 2,
      "discussionId": "68faded5f158a71c5a2f583e",
      "ai_summary": "ImpossibleBench is a benchmark framework that measures and mitigates LLMs' tendency to exploit test cases by introducing impossible task variants, thereby enhancing model reliability.",
      "ai_keywords": [
        "large language models",
        "LLMs",
        "unit tests",
        "benchmark framework",
        "ImpossibleBench",
        "LiveCodeBench",
        "SWE-bench",
        "cheating rate",
        "specification-violating shortcuts",
        "context engineering",
        "monitoring tools"
      ]
    },
    "publishedAt": "2025-10-23T02:58:32.000Z",
    "title": "ImpossibleBench: Measuring LLMs' Propensity of Exploiting Test Cases",
    "summary": "The tendency to find and exploit \"shortcuts\" to complete tasks poses\nsignificant risks for reliable assessment and deployment of large language\nmodels (LLMs). For example, an LLM agent with access to unit tests may delete\nfailing tests rather than fix the underlying bug. Such behavior undermines both\nthe validity of benchmark results and the reliability of real-world LLM coding\nassistant deployments.\n  To quantify, study, and mitigate such behavior, we introduce ImpossibleBench,\na benchmark framework that systematically measures LLM agents' propensity to\nexploit test cases. ImpossibleBench creates \"impossible\" variants of tasks from\nexisting benchmarks like LiveCodeBench and SWE-bench by introducing direct\nconflicts between the natural-language specification and the unit tests. We\nmeasure an agent's \"cheating rate\" as its pass rate on these impossible tasks,\nwhere any pass necessarily implies a specification-violating shortcut.\n  As a practical framework, ImpossibleBench is not just an evaluation but a\nversatile tool. We demonstrate its utility for: (1) studying model behaviors,\nrevealing more fine-grained details of cheating behaviors from simple test\nmodification to complex operator overloading; (2) context engineering, showing\nhow prompt, test access and feedback loop affect cheating rates; and (3)\ndeveloping monitoring tools, providing a testbed with verified deceptive\nsolutions. We hope ImpossibleBench serves as a useful framework for building\nmore robust and reliable LLM systems.\n  Our implementation can be found at\nhttps://github.com/safety-research/impossiblebench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20270.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 142
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.20733",
      "authors": [
        {
          "_id": "68fb1e70f158a71c5a2f596f",
          "name": "Yujia Zheng",
          "hidden": false
        },
        {
          "_id": "68fb1e70f158a71c5a2f5970",
          "name": "Zhuokai Zhao",
          "hidden": false
        },
        {
          "_id": "68fb1e70f158a71c5a2f5971",
          "name": "Zijian Li",
          "hidden": false
        },
        {
          "_id": "68fb1e70f158a71c5a2f5972",
          "name": "Yaqi Xie",
          "hidden": false
        },
        {
          "_id": "68fb1e70f158a71c5a2f5973",
          "name": "Mingze Gao",
          "hidden": false
        },
        {
          "_id": "68fb1e70f158a71c5a2f5974",
          "name": "Lizhu Zhang",
          "hidden": false
        },
        {
          "_id": "68fb1e70f158a71c5a2f5975",
          "name": "Kun Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-23T16:48:02.000Z",
      "submittedOnDailyAt": "2025-10-24T05:12:17.178Z",
      "title": "Thought Communication in Multiagent Collaboration",
      "submittedOnDailyBy": {
        "_id": "6804a894ea0734d6b26f19c3",
        "avatarUrl": "/avatars/b3f5321366bbbb2fd91624289fbea958.svg",
        "isPro": false,
        "fullname": "Yujia Zheng",
        "user": "yujiazheng",
        "type": "user"
      },
      "summary": "Natural language has long enabled human cooperation, but its lossy,\nambiguous, and indirect nature limits the potential of collective intelligence.\nWhile machines are not subject to these constraints, most LLM-based multi-agent\nsystems still rely solely on natural language, exchanging tokens or their\nembeddings. To go beyond language, we introduce a new paradigm, thought\ncommunication, which enables agents to interact directly mind-to-mind, akin to\ntelepathy. To uncover these latent thoughts in a principled way, we formalize\nthe process as a general latent variable model, where agent states are\ngenerated by an unknown function of underlying thoughts. We prove that, in a\nnonparametric setting without auxiliary information, both shared and private\nlatent thoughts between any pair of agents can be identified. Moreover, the\nglobal structure of thought sharing, including which agents share which\nthoughts and how these relationships are structured, can also be recovered with\ntheoretical guarantees. Guided by the established theory, we develop a\nframework that extracts latent thoughts from all agents prior to communication\nand assigns each agent the relevant thoughts, along with their sharing\npatterns. This paradigm naturally extends beyond LLMs to all modalities, as\nmost observational data arise from hidden generative processes. Experiments on\nboth synthetic and real-world benchmarks validate the theory and demonstrate\nthe collaborative advantages of thought communication. We hope this work\nilluminates the potential of leveraging the hidden world, as many challenges\nremain unsolvable through surface-level observation alone, regardless of\ncompute or data scale.",
      "upvotes": 1,
      "discussionId": "68fb1e70f158a71c5a2f5976",
      "ai_summary": "Thought communication enables direct mind-to-mind interaction between agents, uncovering latent thoughts and improving collaborative intelligence beyond natural language.",
      "ai_keywords": [
        "thought communication",
        "latent variable model",
        "agent states",
        "latent thoughts",
        "shared thoughts",
        "private thoughts",
        "global structure",
        "thought sharing",
        "generative processes"
      ]
    },
    "publishedAt": "2025-10-23T12:48:02.000Z",
    "title": "Thought Communication in Multiagent Collaboration",
    "summary": "Natural language has long enabled human cooperation, but its lossy,\nambiguous, and indirect nature limits the potential of collective intelligence.\nWhile machines are not subject to these constraints, most LLM-based multi-agent\nsystems still rely solely on natural language, exchanging tokens or their\nembeddings. To go beyond language, we introduce a new paradigm, thought\ncommunication, which enables agents to interact directly mind-to-mind, akin to\ntelepathy. To uncover these latent thoughts in a principled way, we formalize\nthe process as a general latent variable model, where agent states are\ngenerated by an unknown function of underlying thoughts. We prove that, in a\nnonparametric setting without auxiliary information, both shared and private\nlatent thoughts between any pair of agents can be identified. Moreover, the\nglobal structure of thought sharing, including which agents share which\nthoughts and how these relationships are structured, can also be recovered with\ntheoretical guarantees. Guided by the established theory, we develop a\nframework that extracts latent thoughts from all agents prior to communication\nand assigns each agent the relevant thoughts, along with their sharing\npatterns. This paradigm naturally extends beyond LLMs to all modalities, as\nmost observational data arise from hidden generative processes. Experiments on\nboth synthetic and real-world benchmarks validate the theory and demonstrate\nthe collaborative advantages of thought communication. We hope this work\nilluminates the potential of leveraging the hidden world, as many challenges\nremain unsolvable through surface-level observation alone, regardless of\ncompute or data scale.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.20733.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6804a894ea0734d6b26f19c3",
      "avatarUrl": "/avatars/b3f5321366bbbb2fd91624289fbea958.svg",
      "fullname": "Yujia Zheng",
      "name": "yujiazheng",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.15804",
      "authors": [
        {
          "_id": "68fb0385f158a71c5a2f58b6",
          "name": "Shauli Ravfogel",
          "hidden": false
        },
        {
          "_id": "68fb0385f158a71c5a2f58b7",
          "name": "Gilad Yehudai",
          "hidden": false
        },
        {
          "_id": "68fb0385f158a71c5a2f58b8",
          "name": "Tal Linzen",
          "hidden": false
        },
        {
          "_id": "68fb0385f158a71c5a2f58b9",
          "name": "Joan Bruna",
          "hidden": false
        },
        {
          "_id": "68fb0385f158a71c5a2f58ba",
          "name": "Alberto Bietti",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/635686ec5aeb69011c7d1abd/6itgs9RmcaqaIyJcx1dde.jpeg"
      ],
      "publishedAt": "2025-10-17T16:30:07.000Z",
      "submittedOnDailyAt": "2025-10-24T03:41:20.084Z",
      "title": "Emergence of Linear Truth Encodings in Language Models",
      "submittedOnDailyBy": {
        "_id": "635686ec5aeb69011c7d1abd",
        "avatarUrl": "/avatars/c59034ad2c9c2daf4b4a8d3c56449f5e.svg",
        "isPro": false,
        "fullname": "Shauli Ravfogel",
        "user": "ravfogs",
        "type": "user"
      },
      "summary": "Recent probing studies reveal that large language models exhibit linear\nsubspaces that separate true from false statements, yet the mechanism behind\ntheir emergence is unclear. We introduce a transparent, one-layer transformer\ntoy model that reproduces such truth subspaces end-to-end and exposes one\nconcrete route by which they can arise. We study one simple setting in which\ntruth encoding can emerge: a data distribution where factual statements\nco-occur with other factual statements (and vice-versa), encouraging the model\nto learn this distinction in order to lower the LM loss on future tokens. We\ncorroborate this pattern with experiments in pretrained language models.\nFinally, in the toy setting we observe a two-phase learning dynamic: networks\nfirst memorize individual factual associations in a few steps, then -- over a\nlonger horizon -- learn to linearly separate true from false, which in turn\nlowers language-modeling loss. Together, these results provide both a\nmechanistic demonstration and an empirical motivation for how and why linear\ntruth representations can emerge in language models.",
      "upvotes": 0,
      "discussionId": "68fb0385f158a71c5a2f58bb",
      "ai_summary": "A one-layer transformer model demonstrates how linear subspaces separating true from false statements can emerge in language models through memorization and subsequent linear separation.",
      "ai_keywords": [
        "large language models",
        "linear subspaces",
        "truth subspaces",
        "one-layer transformer",
        "truth encoding",
        "factual statements",
        "LM loss",
        "language-modeling loss"
      ]
    },
    "publishedAt": "2025-10-17T12:30:07.000Z",
    "title": "Emergence of Linear Truth Encodings in Language Models",
    "summary": "Recent probing studies reveal that large language models exhibit linear\nsubspaces that separate true from false statements, yet the mechanism behind\ntheir emergence is unclear. We introduce a transparent, one-layer transformer\ntoy model that reproduces such truth subspaces end-to-end and exposes one\nconcrete route by which they can arise. We study one simple setting in which\ntruth encoding can emerge: a data distribution where factual statements\nco-occur with other factual statements (and vice-versa), encouraging the model\nto learn this distinction in order to lower the LM loss on future tokens. We\ncorroborate this pattern with experiments in pretrained language models.\nFinally, in the toy setting we observe a two-phase learning dynamic: networks\nfirst memorize individual factual associations in a few steps, then -- over a\nlonger horizon -- learn to linearly separate true from false, which in turn\nlowers language-modeling loss. Together, these results provide both a\nmechanistic demonstration and an empirical motivation for how and why linear\ntruth representations can emerge in language models.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/635686ec5aeb69011c7d1abd/6itgs9RmcaqaIyJcx1dde.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.15804.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "635686ec5aeb69011c7d1abd",
      "avatarUrl": "/avatars/c59034ad2c9c2daf4b4a8d3c56449f5e.svg",
      "fullname": "Shauli Ravfogel",
      "name": "ravfogs",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  }
]