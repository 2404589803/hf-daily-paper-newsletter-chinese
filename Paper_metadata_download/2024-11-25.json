[
    {
        "paper": {
            "id": "2411.14793",
            "authors": [
                {
                    "_id": "6743d8895689f9445a5da3e9",
                    "user": {
                        "_id": "62f9e54defd2de85492a4dde",
                        "avatarUrl": "/avatars/9925de0d02b2f2ed03cbafda44fd7de4.svg",
                        "isPro": false,
                        "fullname": "Jooyoung Choi",
                        "user": "jychoi",
                        "type": "user"
                    },
                    "name": "Jooyoung Choi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-25T08:34:18.831Z",
                    "hidden": false
                },
                {
                    "_id": "6743d8895689f9445a5da3ea",
                    "user": {
                        "_id": "631842124d24c00131a66656",
                        "avatarUrl": "/avatars/590633e163da425bf041e3988d8dd015.svg",
                        "isPro": false,
                        "fullname": "shin",
                        "user": "chaehun",
                        "type": "user"
                    },
                    "name": "Chaehun Shin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-25T08:34:16.800Z",
                    "hidden": false
                },
                {
                    "_id": "6743d8895689f9445a5da3eb",
                    "user": {
                        "_id": "636f6e8a31af06da86499ebc",
                        "avatarUrl": "/avatars/9430fbc05774aad8e46c0861769b3c30.svg",
                        "isPro": false,
                        "fullname": "Yeongtak",
                        "user": "Yeongtak",
                        "type": "user"
                    },
                    "name": "Yeongtak Oh",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-25T08:34:15.050Z",
                    "hidden": false
                },
                {
                    "_id": "6743d8895689f9445a5da3ec",
                    "name": "Heeseung Kim",
                    "hidden": false
                },
                {
                    "_id": "6743d8895689f9445a5da3ed",
                    "name": "Sungroh Yoon",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-22T08:29:25.000Z",
            "title": "Style-Friendly SNR Sampler for Style-Driven Generation",
            "summary": "Recent large-scale diffusion models generate high-quality images but struggle\nto learn new, personalized artistic styles, which limits the creation of unique\nstyle templates. Fine-tuning with reference images is the most promising\napproach, but it often blindly utilizes objectives and noise level\ndistributions used for pre-training, leading to suboptimal style alignment. We\npropose the Style-friendly SNR sampler, which aggressively shifts the\nsignal-to-noise ratio (SNR) distribution toward higher noise levels during\nfine-tuning to focus on noise levels where stylistic features emerge. This\nenables models to better capture unique styles and generate images with higher\nstyle alignment. Our method allows diffusion models to learn and share new\n\"style templates\", enhancing personalized content creation. We demonstrate the\nability to generate styles such as personal watercolor paintings, minimal flat\ncartoons, 3D renderings, multi-panel images, and memes with text, thereby\nbroadening the scope of style-driven generation.",
            "upvotes": 25,
            "discussionId": "6743d88d5689f9445a5da4ec"
        },
        "publishedAt": "2024-11-25T00:29:31.872Z",
        "title": "Style-Friendly SNR Sampler for Style-Driven Generation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14793.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/9925de0d02b2f2ed03cbafda44fd7de4.svg",
            "fullname": "Jooyoung Choi",
            "name": "jychoi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        }
    },
    {
        "paper": {
            "id": "2411.15124",
            "authors": [
                {
                    "_id": "674414c8d3ad4510c1a26134",
                    "name": "Nathan Lambert",
                    "hidden": false
                },
                {
                    "_id": "674414c8d3ad4510c1a26135",
                    "name": "Jacob Morrison",
                    "hidden": false
                },
                {
                    "_id": "674414c8d3ad4510c1a26136",
                    "name": "Valentina Pyatkin",
                    "hidden": false
                },
                {
                    "_id": "674414c8d3ad4510c1a26137",
                    "name": "Shengyi Huang",
                    "hidden": false
                },
                {
                    "_id": "674414c8d3ad4510c1a26138",
                    "name": "Hamish Ivison",
                    "hidden": false
                },
                {
                    "_id": "674414c8d3ad4510c1a26139",
                    "name": "Faeze Brahman",
                    "hidden": false
                },
                {
                    "_id": "674414c8d3ad4510c1a2613a",
                    "name": "Lester James V. Miranda",
                    "hidden": false
                },
                {
                    "_id": "674414c8d3ad4510c1a2613b",
                    "name": "Alisa Liu",
                    "hidden": false
                },
                {
                    "_id": "674414c8d3ad4510c1a2613c",
                    "name": "Nouha Dziri",
                    "hidden": false
                },
                {
                    "_id": "674414c8d3ad4510c1a2613d",
                    "name": "Shane Lyu",
                    "hidden": false
                },
                {
                    "_id": "674414c8d3ad4510c1a2613e",
                    "name": "Yuling Gu",
                    "hidden": false
                },
                {
                    "_id": "674414c8d3ad4510c1a2613f",
                    "name": "Saumya Malik",
                    "hidden": false
                },
                {
                    "_id": "674414c8d3ad4510c1a26140",
                    "name": "Victoria Graf",
                    "hidden": false
                },
                {
                    "_id": "674414c8d3ad4510c1a26141",
                    "name": "Jena D. Hwang",
                    "hidden": false
                },
                {
                    "_id": "674414c8d3ad4510c1a26142",
                    "name": "Jiangjiang Yang",
                    "hidden": false
                },
                {
                    "_id": "674414c8d3ad4510c1a26143",
                    "name": "Ronan Le Bras",
                    "hidden": false
                },
                {
                    "_id": "674414c8d3ad4510c1a26144",
                    "name": "Oyvind Tafjord",
                    "hidden": false
                },
                {
                    "_id": "674414c8d3ad4510c1a26145",
                    "name": "Chris Wilhelm",
                    "hidden": false
                },
                {
                    "_id": "674414c8d3ad4510c1a26146",
                    "name": "Luca Soldaini",
                    "hidden": false
                },
                {
                    "_id": "674414c8d3ad4510c1a26147",
                    "name": "Noah A. Smith",
                    "hidden": false
                },
                {
                    "_id": "674414c8d3ad4510c1a26148",
                    "name": "Yizhong Wang",
                    "hidden": false
                },
                {
                    "_id": "674414c8d3ad4510c1a26149",
                    "name": "Pradeep Dasigi",
                    "hidden": false
                },
                {
                    "_id": "674414c8d3ad4510c1a2614a",
                    "name": "Hannaneh Hajishirzi",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-22T18:44:04.000Z",
            "title": "TÜLU 3: Pushing Frontiers in Open Language Model Post-Training",
            "summary": "Language model post-training is applied to refine behaviors and unlock new\nskills across a wide range of recent language models, but open recipes for\napplying these techniques lag behind proprietary ones. The underlying training\ndata and recipes for post-training are simultaneously the most important pieces\nof the puzzle and the portion with the least transparency. To bridge this gap,\nwe introduce T\\\"ULU 3, a family of fully-open state-of-the-art post-trained\nmodels, alongside its data, code, and training recipes, serving as a\ncomprehensive guide for modern post-training techniques. T\\\"ULU 3, which builds\non Llama 3.1 base models, achieves results surpassing the instruct versions of\nLlama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and\nClaude 3.5-Haiku. The training algorithms for our models include supervised\nfinetuning (SFT), Direct Preference Optimization (DPO), and a novel method we\ncall Reinforcement Learning with Verifiable Rewards (RLVR). With T\\\"ULU 3, we\nintroduce a multi-task evaluation scheme for post-training recipes with\ndevelopment and unseen evaluations, standard benchmark implementations, and\nsubstantial decontamination of existing open datasets on said benchmarks. We\nconclude with analysis and discussion of training methods that did not reliably\nimprove performance.\n  In addition to the T\\\"ULU 3 model weights and demo, we release the complete\nrecipe -- including datasets for diverse core skills, a robust toolkit for data\ncuration and evaluation, the training code and infrastructure, and, most\nimportantly, a detailed report for reproducing and further adapting the T\\\"ULU\n3 approach to more domains.",
            "upvotes": 24,
            "discussionId": "674414c9d3ad4510c1a26196"
        },
        "publishedAt": "2024-11-25T04:40:46.649Z",
        "title": "TÜLU 3: Pushing Frontiers in Open Language Model Post-Training",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.15124.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5217
        }
    },
    {
        "paper": {
            "id": "2411.15098",
            "authors": [
                {
                    "_id": "6743e90bed93f631f50d711b",
                    "name": "Zhenxiong Tan",
                    "hidden": false
                },
                {
                    "_id": "6743e90bed93f631f50d711c",
                    "name": "Songhua Liu",
                    "hidden": false
                },
                {
                    "_id": "6743e90bed93f631f50d711d",
                    "name": "Xingyi Yang",
                    "hidden": false
                },
                {
                    "_id": "6743e90bed93f631f50d711e",
                    "name": "Qiaochu Xue",
                    "hidden": false
                },
                {
                    "_id": "6743e90bed93f631f50d711f",
                    "name": "Xinchao Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-22T17:55:15.000Z",
            "title": "OminiControl: Minimal and Universal Control for Diffusion Transformer",
            "summary": "In this paper, we introduce OminiControl, a highly versatile and\nparameter-efficient framework that integrates image conditions into pre-trained\nDiffusion Transformer (DiT) models. At its core, OminiControl leverages a\nparameter reuse mechanism, enabling the DiT to encode image conditions using\nitself as a powerful backbone and process them with its flexible multi-modal\nattention processors. Unlike existing methods, which rely heavily on additional\nencoder modules with complex architectures, OminiControl (1) effectively and\nefficiently incorporates injected image conditions with only ~0.1% additional\nparameters, and (2) addresses a wide range of image conditioning tasks in a\nunified manner, including subject-driven generation and spatially-aligned\nconditions such as edges, depth, and more. Remarkably, these capabilities are\nachieved by training on images generated by the DiT itself, which is\nparticularly beneficial for subject-driven generation. Extensive evaluations\ndemonstrate that OminiControl outperforms existing UNet-based and DiT-adapted\nmodels in both subject-driven and spatially-aligned conditional generation.\nAdditionally, we release our training dataset, Subjects200K, a diverse\ncollection of over 200,000 identity-consistent images, along with an efficient\ndata synthesis pipeline to advance research in subject-consistent generation.",
            "upvotes": 15,
            "discussionId": "6743e910ed93f631f50d72cf"
        },
        "publishedAt": "2024-11-25T03:11:29.653Z",
        "title": "OminiControl: Minimal and Universal Control for Diffusion Transformer",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.15098.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
            "fullname": "Xingyi Yang",
            "name": "adamdad",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 7
        }
    },
    {
        "paper": {
            "id": "2411.12946",
            "authors": [
                {
                    "_id": "673e9a33ed6ba7ea687cb7ae",
                    "user": {
                        "_id": "655f4ff710e5c5fbef30fd97",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655f4ff710e5c5fbef30fd97/bI_KQnFof50HRqYBWpyu2.jpeg",
                        "isPro": false,
                        "fullname": "Gabriel C",
                        "user": "gabrielchua",
                        "type": "user"
                    },
                    "name": "Gabriel Chua",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-21T08:09:39.171Z",
                    "hidden": false
                },
                {
                    "_id": "673e9a33ed6ba7ea687cb7af",
                    "user": {
                        "_id": "66e39d4311e4f40aa760f063",
                        "avatarUrl": "/avatars/1d95e1ab088caeb4366f052fd42d426d.svg",
                        "isPro": false,
                        "fullname": "Chan Shing Yee",
                        "user": "shingurding",
                        "type": "user"
                    },
                    "name": "Shing Yee Chan",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-11-21T02:25:56.265Z",
                    "hidden": false
                },
                {
                    "_id": "673e9a33ed6ba7ea687cb7b0",
                    "name": "Shaun Khoo",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-20T00:31:23.000Z",
            "title": "A Flexible Large Language Models Guardrail Development Methodology\n  Applied to Off-Topic Prompt Detection",
            "summary": "Large Language Models are prone to off-topic misuse, where users may prompt\nthese models to perform tasks beyond their intended scope. Current guardrails,\nwhich often rely on curated examples or custom classifiers, suffer from high\nfalse-positive rates, limited adaptability, and the impracticality of requiring\nreal-world data that is not available in pre-production. In this paper, we\nintroduce a flexible, data-free guardrail development methodology that\naddresses these challenges. By thoroughly defining the problem space\nqualitatively and passing this to an LLM to generate diverse prompts, we\nconstruct a synthetic dataset to benchmark and train off-topic guardrails that\noutperform heuristic approaches. Additionally, by framing the task as\nclassifying whether the user prompt is relevant with respect to the system\nprompt, our guardrails effectively generalize to other misuse categories,\nincluding jailbreak and harmful prompts. Lastly, we further contribute to the\nfield by open-sourcing both the synthetic dataset and the off-topic guardrail\nmodels, providing valuable resources for developing guardrails in\npre-production environments and supporting future research and development in\nLLM safety.",
            "upvotes": 12,
            "discussionId": "673e9a34ed6ba7ea687cb7d4"
        },
        "publishedAt": "2024-11-25T04:50:49.719Z",
        "title": "A Flexible Large Language Models Guardrail Development Methodology Applied to Off-Topic Prompt Detection",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/655f4ff710e5c5fbef30fd97/npdk6KjBt_cU3AtsvkhtO.png",
            "https://cdn-uploads.huggingface.co/production/uploads/655f4ff710e5c5fbef30fd97/1J7C8ze4CO3X-cJ6FeU0P.png",
            "https://cdn-uploads.huggingface.co/production/uploads/655f4ff710e5c5fbef30fd97/iUKQdm7UnaQOcJ5tGUiIG.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.12946.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655f4ff710e5c5fbef30fd97/bI_KQnFof50HRqYBWpyu2.jpeg",
            "fullname": "Gabriel C",
            "name": "gabrielchua",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 80
        }
    },
    {
        "paper": {
            "id": "2411.14982",
            "authors": [
                {
                    "_id": "67444e5070ee90e92d3ba59d",
                    "name": "Kaichen Zhang",
                    "hidden": false
                },
                {
                    "_id": "67444e5070ee90e92d3ba59e",
                    "name": "Yifei Shen",
                    "hidden": false
                },
                {
                    "_id": "67444e5070ee90e92d3ba59f",
                    "name": "Bo Li",
                    "hidden": false
                },
                {
                    "_id": "67444e5070ee90e92d3ba5a0",
                    "name": "Ziwei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-22T14:41:36.000Z",
            "title": "Large Multi-modal Models Can Interpret Features in Large Multi-modal\n  Models",
            "summary": "Recent advances in Large Multimodal Models (LMMs) lead to significant\nbreakthroughs in both academia and industry. One question that arises is how\nwe, as humans, can understand their internal neural representations. This paper\ntakes an initial step towards addressing this question by presenting a\nversatile framework to identify and interpret the semantics within LMMs.\nSpecifically, 1) we first apply a Sparse Autoencoder(SAE) to disentangle the\nrepresentations into human understandable features. 2) We then present an\nautomatic interpretation framework to interpreted the open-semantic features\nlearned in SAE by the LMMs themselves. We employ this framework to analyze the\nLLaVA-NeXT-8B model using the LLaVA-OV-72B model, demonstrating that these\nfeatures can effectively steer the model's behavior. Our results contribute to\na deeper understanding of why LMMs excel in specific tasks, including EQ tests,\nand illuminate the nature of their mistakes along with potential strategies for\ntheir rectification. These findings offer new insights into the internal\nmechanisms of LMMs and suggest parallels with the cognitive processes of the\nhuman brain.",
            "upvotes": 9,
            "discussionId": "67444e5170ee90e92d3ba61d"
        },
        "publishedAt": "2024-11-25T08:46:19.165Z",
        "title": "Large Multi-modal Models Can Interpret Features in Large Multi-modal Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14982.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/4c1720db323082e644babeb827c6f5d4.svg",
            "fullname": "kcz",
            "name": "kcz358",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 7
        }
    },
    {
        "paper": {
            "id": "2411.14794",
            "authors": [
                {
                    "_id": "6743eb8cde9997dd2682a74f",
                    "name": "Songhao Han",
                    "hidden": false
                },
                {
                    "_id": "6743eb8cde9997dd2682a750",
                    "name": "Wei Huang",
                    "hidden": false
                },
                {
                    "_id": "6743eb8cde9997dd2682a751",
                    "name": "Hairong Shi",
                    "hidden": false
                },
                {
                    "_id": "6743eb8cde9997dd2682a752",
                    "name": "Le Zhuo",
                    "hidden": false
                },
                {
                    "_id": "6743eb8cde9997dd2682a753",
                    "name": "Xiu Su",
                    "hidden": false
                },
                {
                    "_id": "6743eb8cde9997dd2682a754",
                    "name": "Shifeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "6743eb8cde9997dd2682a755",
                    "name": "Xu Zhou",
                    "hidden": false
                },
                {
                    "_id": "6743eb8cde9997dd2682a756",
                    "name": "Xiaojuan Qi",
                    "hidden": false
                },
                {
                    "_id": "6743eb8cde9997dd2682a757",
                    "name": "Yue Liao",
                    "hidden": false
                },
                {
                    "_id": "6743eb8cde9997dd2682a758",
                    "name": "Si Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-22T08:33:36.000Z",
            "title": "VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained\n  Video Reasoning via Core Frame Selection",
            "summary": "The advancement of Large Vision Language Models (LVLMs) has significantly\nimproved multimodal understanding, yet challenges remain in video reasoning\ntasks due to the scarcity of high-quality, large-scale datasets. Existing video\nquestion-answering (VideoQA) datasets often rely on costly manual annotations\nwith insufficient granularity or automatic construction methods with redundant\nframe-by-frame analysis, limiting their scalability and effectiveness for\ncomplex reasoning. To address these challenges, we introduce VideoEspresso, a\nnovel dataset that features VideoQA pairs preserving essential spatial details\nand temporal coherence, along with multimodal annotations of intermediate\nreasoning steps. Our construction pipeline employs a semantic-aware method to\nreduce redundancy, followed by generating QA pairs using GPT-4o. We further\ndevelop video Chain-of-Thought (CoT) annotations to enrich reasoning processes,\nguiding GPT-4o in extracting logical relationships from QA pairs and video\ncontent. To exploit the potential of high-quality VideoQA pairs, we propose a\nHybrid LVLMs Collaboration framework, featuring a Frame Selector and a\ntwo-stage instruction fine-tuned reasoning LVLM. This framework adaptively\nselects core frames and performs CoT reasoning using multimodal evidence.\nEvaluated on our proposed benchmark with 14 tasks against 9 popular LVLMs, our\nmethod outperforms existing baselines on most tasks, demonstrating superior\nvideo reasoning capabilities. Our code and dataset will be released at:\nhttps://github.com/hshjerry/VideoEspresso",
            "upvotes": 7,
            "discussionId": "6743eb8dde9997dd2682a7df"
        },
        "publishedAt": "2024-11-25T06:11:28.558Z",
        "title": "VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14794.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/e54ea7bf0c240cf76d538296efb3976c.svg",
            "fullname": "Le Zhuo",
            "name": "JackyZhuo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 3
        }
    },
    {
        "paper": {
            "id": "2411.13543",
            "authors": [
                {
                    "_id": "673fa529128c57b2f2996fdc",
                    "user": {
                        "_id": "6564d9a83f025ef9f11b28bb",
                        "avatarUrl": "/avatars/ab19f5f339e80c3b0a4b8d70abaced94.svg",
                        "isPro": false,
                        "fullname": "Davide Paglieri",
                        "user": "pagli98",
                        "type": "user"
                    },
                    "name": "Davide Paglieri",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-21T21:32:39.052Z",
                    "hidden": false
                },
                {
                    "_id": "673fa529128c57b2f2996fdd",
                    "name": "Bartłomiej Cupiał",
                    "hidden": false
                },
                {
                    "_id": "673fa529128c57b2f2996fde",
                    "name": "Samuel Coward",
                    "hidden": false
                },
                {
                    "_id": "673fa529128c57b2f2996fdf",
                    "name": "Ulyana Piterbarg",
                    "hidden": false
                },
                {
                    "_id": "673fa529128c57b2f2996fe0",
                    "name": "Maciej Wolczyk",
                    "hidden": false
                },
                {
                    "_id": "673fa529128c57b2f2996fe1",
                    "name": "Akbir Khan",
                    "hidden": false
                },
                {
                    "_id": "673fa529128c57b2f2996fe2",
                    "name": "Eduardo Pignatelli",
                    "hidden": false
                },
                {
                    "_id": "673fa529128c57b2f2996fe3",
                    "name": "Łukasz Kuciński",
                    "hidden": false
                },
                {
                    "_id": "673fa529128c57b2f2996fe4",
                    "name": "Lerrel Pinto",
                    "hidden": false
                },
                {
                    "_id": "673fa529128c57b2f2996fe5",
                    "name": "Rob Fergus",
                    "hidden": false
                },
                {
                    "_id": "673fa529128c57b2f2996fe6",
                    "name": "Jakob Nicolaus Foerster",
                    "hidden": false
                },
                {
                    "_id": "673fa529128c57b2f2996fe7",
                    "name": "Jack Parker-Holder",
                    "hidden": false
                },
                {
                    "_id": "673fa529128c57b2f2996fe8",
                    "name": "Tim Rocktäschel",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-20T18:54:32.000Z",
            "title": "BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games",
            "summary": "Large Language Models (LLMs) and Vision Language Models (VLMs) possess\nextensive knowledge and exhibit promising reasoning abilities; however, they\nstill struggle to perform well in complex, dynamic environments. Real-world\ntasks require handling intricate interactions, advanced spatial reasoning,\nlong-term planning, and continuous exploration of new strategies-areas in which\nwe lack effective methodologies for comprehensively evaluating these\ncapabilities. To address this gap, we introduce BALROG, a novel benchmark\ndesigned to assess the agentic capabilities of LLMs and VLMs through a diverse\nset of challenging games. Our benchmark incorporates a range of existing\nreinforcement learning environments with varying levels of difficulty,\nincluding tasks that are solvable by non-expert humans in seconds to extremely\nchallenging ones that may take years to master (e.g., the NetHack Learning\nEnvironment). We devise fine-grained metrics to measure performance and conduct\nan extensive evaluation of several popular open-source and closed-source LLMs\nand VLMs. Our findings indicate that while current models achieve partial\nsuccess in the easier games, they struggle significantly with more challenging\ntasks. Notably, we observe severe deficiencies in vision-based decision-making,\nas models perform worse when visual representations of the environments are\nprovided. We release BALROG as an open and user-friendly benchmark to\nfacilitate future research and development in the agentic community.",
            "upvotes": 6,
            "discussionId": "673fa52a128c57b2f2997031"
        },
        "publishedAt": "2024-11-25T09:50:55.836Z",
        "title": "BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6564d9a83f025ef9f11b28bb/GrLIxshbxFEJewVN0qH1s.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6564d9a83f025ef9f11b28bb/Il4_0ZPtuPP7rIX8vYQVS.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6564d9a83f025ef9f11b28bb/L6rsNooQqhLMW_osnxCAH.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6564d9a83f025ef9f11b28bb/Kf8vye8iNHp8m1YErD3RY.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.13543.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/ab19f5f339e80c3b0a4b8d70abaced94.svg",
            "fullname": "Davide Paglieri",
            "name": "pagli98",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2411.14762",
            "authors": [
                {
                    "_id": "674416d8388e030c2597afab",
                    "name": "Huiwon Jang",
                    "hidden": false
                },
                {
                    "_id": "674416d8388e030c2597afac",
                    "name": "Sihyun Yu",
                    "hidden": false
                },
                {
                    "_id": "674416d8388e030c2597afad",
                    "name": "Jinwoo Shin",
                    "hidden": false
                },
                {
                    "_id": "674416d8388e030c2597afae",
                    "name": "Pieter Abbeel",
                    "hidden": false
                },
                {
                    "_id": "674416d8388e030c2597afaf",
                    "user": {
                        "_id": "6744164fe61fe2cad3ce3dc3",
                        "avatarUrl": "/avatars/c9ffeb09f4a62bf99869b3481e0d60a3.svg",
                        "isPro": false,
                        "fullname": "Younggyo Seo",
                        "user": "younggyoseo",
                        "type": "user"
                    },
                    "name": "Younggyo Seo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-25T08:33:55.730Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-22T06:50:44.000Z",
            "title": "Efficient Long Video Tokenization via Coordinated-based Patch\n  Reconstruction",
            "summary": "Efficient tokenization of videos remains a challenge in training vision\nmodels that can process long videos. One promising direction is to develop a\ntokenizer that can encode long video clips, as it would enable the tokenizer to\nleverage the temporal coherence of videos better for tokenization. However,\ntraining existing tokenizers on long videos often incurs a huge training cost\nas they are trained to reconstruct all the frames at once. In this paper, we\nintroduce CoordTok, a video tokenizer that learns a mapping from\ncoordinate-based representations to the corresponding patches of input videos,\ninspired by recent advances in 3D generative models. In particular, CoordTok\nencodes a video into factorized triplane representations and reconstructs\npatches that correspond to randomly sampled (x,y,t) coordinates. This allows\nfor training large tokenizer models directly on long videos without requiring\nexcessive training resources. Our experiments show that CoordTok can\ndrastically reduce the number of tokens for encoding long video clips. For\ninstance, CoordTok can encode a 128-frame video with 128times128 resolution\ninto 1280 tokens, while baselines need 6144 or 8192 tokens to achieve similar\nreconstruction quality. We further show that this efficient video tokenization\nenables memory-efficient training of a diffusion transformer that can generate\n128 frames at once.",
            "upvotes": 6,
            "discussionId": "674416d9388e030c2597b034"
        },
        "publishedAt": "2024-11-25T04:49:36.463Z",
        "title": "Efficient Long Video Tokenization via Coordinated-based Patch Reconstruction",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14762.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/c9ffeb09f4a62bf99869b3481e0d60a3.svg",
            "fullname": "Younggyo Seo",
            "name": "younggyoseo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2411.14521",
            "authors": [
                {
                    "_id": "6744115b33e10fc6d427c915",
                    "name": "Luchao Qi",
                    "hidden": false
                },
                {
                    "_id": "6744115b33e10fc6d427c916",
                    "name": "Jiaye Wu",
                    "hidden": false
                },
                {
                    "_id": "6744115b33e10fc6d427c917",
                    "name": "Bang Gong",
                    "hidden": false
                },
                {
                    "_id": "6744115b33e10fc6d427c918",
                    "name": "Annie N. Wang",
                    "hidden": false
                },
                {
                    "_id": "6744115b33e10fc6d427c919",
                    "name": "David W. Jacobs",
                    "hidden": false
                },
                {
                    "_id": "6744115b33e10fc6d427c91a",
                    "name": "Roni Sengupta",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-21T18:54:13.000Z",
            "title": "MyTimeMachine: Personalized Facial Age Transformation",
            "summary": "Facial aging is a complex process, highly dependent on multiple factors like\ngender, ethnicity, lifestyle, etc., making it extremely challenging to learn a\nglobal aging prior to predict aging for any individual accurately. Existing\ntechniques often produce realistic and plausible aging results, but the re-aged\nimages often do not resemble the person's appearance at the target age and thus\nneed personalization. In many practical applications of virtual aging, e.g. VFX\nin movies and TV shows, access to a personal photo collection of the user\ndepicting aging in a small time interval (20sim40 years) is often available.\nHowever, naive attempts to personalize global aging techniques on personal\nphoto collections often fail. Thus, we propose MyTimeMachine (MyTM), which\ncombines a global aging prior with a personal photo collection (using as few as\n50 images) to learn a personalized age transformation. We introduce a novel\nAdapter Network that combines personalized aging features with global aging\nfeatures and generates a re-aged image with StyleGAN2. We also introduce three\nloss functions to personalize the Adapter Network with personalized aging loss,\nextrapolation regularization, and adaptive w-norm regularization. Our approach\ncan also be extended to videos, achieving high-quality, identity-preserving,\nand temporally consistent aging effects that resemble actual appearances at\ntarget ages, demonstrating its superiority over state-of-the-art approaches.",
            "upvotes": 5,
            "discussionId": "6744115e33e10fc6d427ca07"
        },
        "publishedAt": "2024-11-25T04:25:43.825Z",
        "title": "MyTimeMachine: Personalized Facial Age Transformation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14521.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5217
        }
    },
    {
        "paper": {
            "id": "2411.14208",
            "authors": [
                {
                    "_id": "6740bb4ad3ad4510c18306ed",
                    "user": {
                        "_id": "660b79aa822b119f215c3488",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660b79aa822b119f215c3488/le_EuzAz6mhW0FQ9J0knV.jpeg",
                        "isPro": false,
                        "fullname": "Kunhao Liu",
                        "user": "KunhaoLiu",
                        "type": "user"
                    },
                    "name": "Kunhao Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-24T22:06:52.908Z",
                    "hidden": false
                },
                {
                    "_id": "6740bb4ad3ad4510c18306ee",
                    "name": "Ling Shao",
                    "hidden": false
                },
                {
                    "_id": "6740bb4ad3ad4510c18306ef",
                    "name": "Shijian Lu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-21T15:16:48.000Z",
            "title": "Novel View Extrapolation with Video Diffusion Priors",
            "summary": "The field of novel view synthesis has made significant strides thanks to the\ndevelopment of radiance field methods. However, most radiance field techniques\nare far better at novel view interpolation than novel view extrapolation where\nthe synthesis novel views are far beyond the observed training views. We design\nViewExtrapolator, a novel view synthesis approach that leverages the generative\npriors of Stable Video Diffusion (SVD) for realistic novel view extrapolation.\nBy redesigning the SVD denoising process, ViewExtrapolator refines the\nartifact-prone views rendered by radiance fields, greatly enhancing the clarity\nand realism of the synthesized novel views. ViewExtrapolator is a generic novel\nview extrapolator that can work with different types of 3D rendering such as\nviews rendered from point clouds when only a single view or monocular video is\navailable. Additionally, ViewExtrapolator requires no fine-tuning of SVD,\nmaking it both data-efficient and computation-efficient. Extensive experiments\ndemonstrate the superiority of ViewExtrapolator in novel view extrapolation.\nProject page: https://kunhao-liu.github.io/ViewExtrapolator/.",
            "upvotes": 4,
            "discussionId": "6740bb4cd3ad4510c18307c9"
        },
        "publishedAt": "2024-11-25T04:29:22.161Z",
        "title": "Novel View Extrapolation with Video Diffusion Priors",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/660b79aa822b119f215c3488/XBL78VbobXUDD2r25z8Qk.gif",
            "https://cdn-uploads.huggingface.co/production/uploads/660b79aa822b119f215c3488/3PaQVyVME_3lXggxl3FP1.gif",
            "https://cdn-uploads.huggingface.co/production/uploads/660b79aa822b119f215c3488/7x0dpagKUnrlegsL1jqAL.gif",
            "https://cdn-uploads.huggingface.co/production/uploads/660b79aa822b119f215c3488/XSJRqcRpyd_V3DIDzMOCc.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.14208.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660b79aa822b119f215c3488/le_EuzAz6mhW0FQ9J0knV.jpeg",
            "fullname": "Kunhao Liu",
            "name": "KunhaoLiu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2411.15115",
            "authors": [
                {
                    "_id": "6743d80aa05eb026d723f89f",
                    "name": "Daeun Lee",
                    "hidden": false
                },
                {
                    "_id": "6743d80aa05eb026d723f8a0",
                    "name": "Jaehong Yoon",
                    "hidden": false
                },
                {
                    "_id": "6743d80aa05eb026d723f8a1",
                    "user": {
                        "_id": "5ffe32d8942cf3533d364449",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654821969191-5ffe32d8942cf3533d364449.jpeg",
                        "isPro": false,
                        "fullname": "Jaemin Cho",
                        "user": "j-min",
                        "type": "user"
                    },
                    "name": "Jaemin Cho",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-25T08:34:20.962Z",
                    "hidden": false
                },
                {
                    "_id": "6743d80aa05eb026d723f8a2",
                    "name": "Mohit Bansal",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-22T18:31:47.000Z",
            "title": "VideoRepair: Improving Text-to-Video Generation via Misalignment\n  Evaluation and Localized Refinement",
            "summary": "Recent text-to-video (T2V) diffusion models have demonstrated impressive\ngeneration capabilities across various domains. However, these models often\ngenerate videos that have misalignments with text prompts, especially when the\nprompts describe complex scenes with multiple objects and attributes. To\naddress this, we introduce VideoRepair, a novel model-agnostic, training-free\nvideo refinement framework that automatically identifies fine-grained\ntext-video misalignments and generates explicit spatial and textual feedback,\nenabling a T2V diffusion model to perform targeted, localized refinements.\nVideoRepair consists of four stages: In (1) video evaluation, we detect\nmisalignments by generating fine-grained evaluation questions and answering\nthose questions with MLLM. In (2) refinement planning, we identify accurately\ngenerated objects and then create localized prompts to refine other areas in\nthe video. Next, in (3) region decomposition, we segment the correctly\ngenerated area using a combined grounding module. We regenerate the video by\nadjusting the misaligned regions while preserving the correct regions in (4)\nlocalized refinement. On two popular video generation benchmarks (EvalCrafter\nand T2V-CompBench), VideoRepair substantially outperforms recent baselines\nacross various text-video alignment metrics. We provide a comprehensive\nanalysis of VideoRepair components and qualitative examples.",
            "upvotes": 3,
            "discussionId": "6743d80ca05eb026d723fa82"
        },
        "publishedAt": "2024-11-25T11:31:44.223Z",
        "title": "VideoRepair: Improving Text-to-Video Generation via Misalignment Evaluation and Localized Refinement",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.15115.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654821969191-5ffe32d8942cf3533d364449.jpeg",
            "fullname": "Jaemin Cho",
            "name": "j-min",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 2
        }
    },
    {
        "paper": {
            "id": "2411.13127",
            "authors": [
                {
                    "_id": "67416de111bf5a76d6eb47ca",
                    "user": {
                        "_id": "6617af2beab5eef6b1e8bb9e",
                        "avatarUrl": "/avatars/d939c02027916331d4c44119565f2ca6.svg",
                        "isPro": false,
                        "fullname": "XavierJiezou",
                        "user": "XavierJiezou",
                        "type": "user"
                    },
                    "name": "Xuechao Zou",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-11-23T05:53:38.924Z",
                    "hidden": false
                },
                {
                    "_id": "67416de111bf5a76d6eb47cb",
                    "name": "Shun Zhang",
                    "hidden": false
                },
                {
                    "_id": "67416de111bf5a76d6eb47cc",
                    "user": {
                        "_id": "6387676c23da90491eb9fb16",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669818175965-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Kai Li",
                        "user": "JusperLee",
                        "type": "user"
                    },
                    "name": "Kai Li",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2024-11-23T05:53:38.924Z",
                    "hidden": false
                },
                {
                    "_id": "67416de111bf5a76d6eb47cd",
                    "name": "Shiying Wang",
                    "hidden": false
                },
                {
                    "_id": "67416de111bf5a76d6eb47ce",
                    "name": "Junliang Xing",
                    "hidden": false
                },
                {
                    "_id": "67416de111bf5a76d6eb47cf",
                    "name": "Lei Jin",
                    "hidden": false
                },
                {
                    "_id": "67416de111bf5a76d6eb47d0",
                    "name": "Congyan Lang",
                    "hidden": false
                },
                {
                    "_id": "67416de111bf5a76d6eb47d1",
                    "name": "Pin Tao",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-20T08:37:39.000Z",
            "title": "Adapting Vision Foundation Models for Robust Cloud Segmentation in\n  Remote Sensing Images",
            "summary": "Cloud segmentation is a critical challenge in remote sensing image\ninterpretation, as its accuracy directly impacts the effectiveness of\nsubsequent data processing and analysis. Recently, vision foundation models\n(VFM) have demonstrated powerful generalization capabilities across various\nvisual tasks. In this paper, we present a parameter-efficient adaptive\napproach, termed Cloud-Adapter, designed to enhance the accuracy and robustness\nof cloud segmentation. Our method leverages a VFM pretrained on general domain\ndata, which remains frozen, eliminating the need for additional training.\nCloud-Adapter incorporates a lightweight spatial perception module that\ninitially utilizes a convolutional neural network (ConvNet) to extract dense\nspatial representations. These multi-scale features are then aggregated and\nserve as contextual inputs to an adapting module, which modulates the frozen\ntransformer layers within the VFM. Experimental results demonstrate that the\nCloud-Adapter approach, utilizing only 0.6% of the trainable parameters of the\nfrozen backbone, achieves substantial performance gains. Cloud-Adapter\nconsistently attains state-of-the-art (SOTA) performance across a wide variety\nof cloud segmentation datasets from multiple satellite sources, sensor series,\ndata processing levels, land cover scenarios, and annotation granularities. We\nhave released the source code and pretrained models at\nhttps://github.com/XavierJiezou/Cloud-Adapter to support further research.",
            "upvotes": 2,
            "discussionId": "67416de211bf5a76d6eb485f"
        },
        "publishedAt": "2024-11-25T12:58:08.585Z",
        "title": "Adapting Vision Foundation Models for Robust Cloud Segmentation in Remote Sensing Images",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.13127.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669818175965-noauth.jpeg",
            "fullname": "Kai Li",
            "name": "JusperLee",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false,
            "followerCount": 5
        }
    },
    {
        "paper": {
            "id": "2411.15033",
            "authors": [
                {
                    "_id": "67442e8d28cc74d48002625a",
                    "user": {
                        "_id": "65ddf03dd53ad584a7df9dac",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ddf03dd53ad584a7df9dac/oi0pvfpCJmnODErAfF_9o.jpeg",
                        "isPro": false,
                        "fullname": "Simone Colombani",
                        "user": "colo286",
                        "type": "user"
                    },
                    "name": "Simone Colombani",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-11-25T08:33:54.023Z",
                    "hidden": false
                },
                {
                    "_id": "67442e8d28cc74d48002625b",
                    "name": "Dimitri Ognibene",
                    "hidden": false
                },
                {
                    "_id": "67442e8d28cc74d48002625c",
                    "name": "Giuseppe Boccignone",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-22T16:05:54.000Z",
            "title": "One to rule them all: natural language to bind communication, perception\n  and action",
            "summary": "In recent years, research in the area of human-robot interaction has focused\non developing robots capable of understanding complex human instructions and\nperforming tasks in dynamic and diverse environments. These systems have a wide\nrange of applications, from personal assistance to industrial robotics,\nemphasizing the importance of robots interacting flexibly, naturally and safely\nwith humans. This paper presents an advanced architecture for robotic action\nplanning that integrates communication, perception, and planning with Large\nLanguage Models (LLMs). Our system is designed to translate commands expressed\nin natural language into executable robot actions, incorporating environmental\ninformation and dynamically updating plans based on real-time feedback. The\nPlanner Module is the core of the system where LLMs embedded in a modified\nReAct framework are employed to interpret and carry out user commands. By\nleveraging their extensive pre-trained knowledge, LLMs can effectively process\nuser requests without the need to introduce new knowledge on the changing\nenvironment. The modified ReAct framework further enhances the execution space\nby providing real-time environmental perception and the outcomes of physical\nactions. By combining robust and dynamic semantic map representations as graphs\nwith control components and failure explanations, this architecture enhances a\nrobot adaptability, task execution, and seamless collaboration with human users\nin shared and dynamic environments. Through the integration of continuous\nfeedback loops with the environment the system can dynamically adjusts the plan\nto accommodate unexpected changes, optimizing the robot ability to perform\ntasks. Using a dataset of previous experience is possible to provide detailed\nfeedback about the failure. Updating the LLMs context of the next iteration\nwith suggestion on how to overcame the issue.",
            "upvotes": 2,
            "discussionId": "67442e8e28cc74d4800262b0"
        },
        "publishedAt": "2024-11-25T07:10:01.815Z",
        "title": "One to rule them all: natural language to bind communication, perception and action",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.15033.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ddf03dd53ad584a7df9dac/oi0pvfpCJmnODErAfF_9o.jpeg",
            "fullname": "Simone Colombani",
            "name": "colo286",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2411.15131",
            "authors": [
                {
                    "_id": "674413c5b5259d64b54af07b",
                    "name": "Ri-Zhao Qiu",
                    "hidden": false
                },
                {
                    "_id": "674413c5b5259d64b54af07c",
                    "name": "Yuchen Song",
                    "hidden": false
                },
                {
                    "_id": "674413c5b5259d64b54af07d",
                    "name": "Xuanbin Peng",
                    "hidden": false
                },
                {
                    "_id": "674413c5b5259d64b54af07e",
                    "name": "Sai Aneesh Suryadevara",
                    "hidden": false
                },
                {
                    "_id": "674413c5b5259d64b54af07f",
                    "name": "Ge Yang",
                    "hidden": false
                },
                {
                    "_id": "674413c5b5259d64b54af080",
                    "name": "Minghuan Liu",
                    "hidden": false
                },
                {
                    "_id": "674413c5b5259d64b54af081",
                    "name": "Mazeyu Ji",
                    "hidden": false
                },
                {
                    "_id": "674413c5b5259d64b54af082",
                    "name": "Chengzhe Jia",
                    "hidden": false
                },
                {
                    "_id": "674413c5b5259d64b54af083",
                    "name": "Ruihan Yang",
                    "hidden": false
                },
                {
                    "_id": "674413c5b5259d64b54af084",
                    "name": "Xueyan Zou",
                    "hidden": false
                },
                {
                    "_id": "674413c5b5259d64b54af085",
                    "name": "Xiaolong Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-11-22T18:56:56.000Z",
            "title": "WildLMa: Long Horizon Loco-Manipulation in the Wild",
            "summary": "`In-the-wild' mobile manipulation aims to deploy robots in diverse real-world\nenvironments, which requires the robot to (1) have skills that generalize\nacross object configurations; (2) be capable of long-horizon task execution in\ndiverse environments; and (3) perform complex manipulation beyond\npick-and-place. Quadruped robots with manipulators hold promise for extending\nthe workspace and enabling robust locomotion, but existing results do not\ninvestigate such a capability. This paper proposes WildLMa with three\ncomponents to address these issues: (1) adaptation of learned low-level\ncontroller for VR-enabled whole-body teleoperation and traversability; (2)\nWildLMa-Skill -- a library of generalizable visuomotor skills acquired via\nimitation learning or heuristics and (3) WildLMa-Planner -- an interface of\nlearned skills that allow LLM planners to coordinate skills for long-horizon\ntasks. We demonstrate the importance of high-quality training data by achieving\nhigher grasping success rate over existing RL baselines using only tens of\ndemonstrations. WildLMa exploits CLIP for language-conditioned imitation\nlearning that empirically generalizes to objects unseen in training\ndemonstrations. Besides extensive quantitative evaluation, we qualitatively\ndemonstrate practical robot applications, such as cleaning up trash in\nuniversity hallways or outdoor terrains, operating articulated objects, and\nrearranging items on a bookshelf.",
            "upvotes": 2,
            "discussionId": "674413c7b5259d64b54af0da"
        },
        "publishedAt": "2024-11-25T04:36:02.848Z",
        "title": "WildLMa: Long Horizon Loco-Manipulation in the Wild",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.15131.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false,
            "followerCount": 5217
        }
    }
]