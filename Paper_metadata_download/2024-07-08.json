[
    {
        "paper": {
            "id": "2406.11832",
            "authors": [
                {
                    "_id": "66714eedcb8c849f4316e1c9",
                    "user": {
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b4a717aa03b6520839e9b8/_rcsJh0ummwK-oAMAT4oY.jpeg",
                        "isPro": false,
                        "fullname": "Haiwen Diao",
                        "user": "Paranioar",
                        "type": "user"
                    },
                    "name": "Haiwen Diao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-07-08T08:51:10.327Z",
                    "hidden": false
                },
                {
                    "_id": "66714eedcb8c849f4316e1ca",
                    "name": "Yufeng Cui",
                    "hidden": false
                },
                {
                    "_id": "66714eedcb8c849f4316e1cb",
                    "name": "Xiaotong Li",
                    "hidden": false
                },
                {
                    "_id": "66714eedcb8c849f4316e1cc",
                    "name": "Yueze Wang",
                    "hidden": false
                },
                {
                    "_id": "66714eedcb8c849f4316e1cd",
                    "name": "Huchuan Lu",
                    "hidden": false
                },
                {
                    "_id": "66714eedcb8c849f4316e1ce",
                    "name": "Xinlong Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-17T17:59:44.000Z",
            "title": "Unveiling Encoder-Free Vision-Language Models",
            "summary": "Existing vision-language models (VLMs) mostly rely on vision encoders to\nextract visual features followed by large language models (LLMs) for\nvisual-language tasks. However, the vision encoders set a strong inductive bias\nin abstracting visual representation, e.g., resolution, aspect ratio, and\nsemantic priors, which could impede the flexibility and efficiency of the VLMs.\nTraining pure VLMs that accept the seamless vision and language inputs, i.e.,\nwithout vision encoders, remains challenging and rarely explored. Empirical\nobservations reveal that direct training without encoders results in slow\nconvergence and large performance gaps. In this work, we bridge the gap between\nencoder-based and encoder-free models, and present a simple yet effective\ntraining recipe towards pure VLMs. Specifically, we unveil the key aspects of\ntraining encoder-free VLMs efficiently via thorough experiments: (1) Bridging\nvision-language representation inside one unified decoder; (2) Enhancing visual\nrecognition capability via extra supervision. With these strategies, we launch\nEVE, an encoder-free vision-language model that can be trained and forwarded\nefficiently. Notably, solely utilizing 35M publicly accessible data, EVE can\nimpressively rival the encoder-based VLMs of similar capacities across multiple\nvision-language benchmarks. It significantly outperforms the counterpart\nFuyu-8B with mysterious training procedures and undisclosed training data. We\nbelieve that EVE provides a transparent and efficient route for developing a\npure decoder-only architecture across modalities. Our code and models are\npublicly available at: https://github.com/baaivision/EVE.",
            "upvotes": 24
        },
        "publishedAt": "2024-07-08T03:00:01.473Z",
        "title": "Unveiling Encoder-Free Vision-Language Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.11832.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.04363",
            "authors": [
                {
                    "_id": "668b8e236ce40bc8352c5169",
                    "name": "Petr Anokhin",
                    "hidden": false
                },
                {
                    "_id": "668b8e236ce40bc8352c516a",
                    "name": "Nikita Semenov",
                    "hidden": false
                },
                {
                    "_id": "668b8e236ce40bc8352c516b",
                    "name": "Artyom Sorokin",
                    "hidden": false
                },
                {
                    "_id": "668b8e236ce40bc8352c516c",
                    "name": "Dmitry Evseev",
                    "hidden": false
                },
                {
                    "_id": "668b8e236ce40bc8352c516d",
                    "name": "Mikhail Burtsev",
                    "hidden": false
                },
                {
                    "_id": "668b8e236ce40bc8352c516e",
                    "name": "Evgeny Burnaev",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-05T09:06:47.000Z",
            "title": "AriGraph: Learning Knowledge Graph World Models with Episodic Memory for\n  LLM Agents",
            "summary": "Advancements in generative AI have broadened the potential applications of\nLarge Language Models (LLMs) in the development of autonomous agents. Achieving\ntrue autonomy requires accumulating and updating knowledge gained from\ninteractions with the environment and effectively utilizing it. Current\nLLM-based approaches leverage past experiences using a full history of\nobservations, summarization or retrieval augmentation. However, these\nunstructured memory representations do not facilitate the reasoning and\nplanning essential for complex decision-making. In our study, we introduce\nAriGraph, a novel method wherein the agent constructs a memory graph that\nintegrates semantic and episodic memories while exploring the environment. This\ngraph structure facilitates efficient associative retrieval of interconnected\nconcepts, relevant to the agent's current state and goals, thus serving as an\neffective environmental model that enhances the agent's exploratory and\nplanning capabilities. We demonstrate that our Ariadne LLM agent, equipped with\nthis proposed memory architecture augmented with planning and decision-making,\neffectively handles complex tasks on a zero-shot basis in the TextWorld\nenvironment. Our approach markedly outperforms established methods such as\nfull-history, summarization, and Retrieval-Augmented Generation in various\ntasks, including the cooking challenge from the First TextWorld Problems\ncompetition and novel tasks like house cleaning and puzzle Treasure Hunting.",
            "upvotes": 15
        },
        "publishedAt": "2024-07-08T05:48:53.158Z",
        "title": "AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64f734b08e7fa6529fb693f1/YEVXSLv5n-b-nl-JjSYay.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.04363.png",
        "numComments": 2,
        "submittedBy": {
            "avatarUrl": "/avatars/a50c7e1d22174f06fbaf99e8caca7c37.svg",
            "fullname": "Petr Anokhin",
            "name": "petranokhin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.04051",
            "authors": [
                {
                    "_id": "668b611fa9ee6373c751c9b6",
                    "name": "Tongyi SpeechTeam",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-04T16:49:02.000Z",
            "title": "FunAudioLLM: Voice Understanding and Generation Foundation Models for\n  Natural Interaction Between Humans and LLMs",
            "summary": "This report introduces FunAudioLLM, a model family designed to enhance\nnatural voice interactions between humans and large language models (LLMs). At\nits core are two innovative models: SenseVoice, which handles multilingual\nspeech recognition, emotion recognition, and audio event detection; and\nCosyVoice, which facilitates natural speech generation with control over\nmultiple languages, timbre, speaking style, and speaker identity.\nSenseVoice-Small delivers exceptionally low-latency ASR for 5 languages, and\nSenseVoice-Large supports high-precision ASR for over 50 languages, while\nCosyVoice excels in multi-lingual voice generation, zero-shot in-context\nlearning, cross-lingual voice cloning, and instruction-following capabilities.\nThe models related to SenseVoice and CosyVoice have been open-sourced on\nModelscope and Huggingface, along with the corresponding training, inference,\nand fine-tuning codes released on GitHub. By integrating these models with\nLLMs, FunAudioLLM enables applications such as speech-to-speech translation,\nemotional voice chat, interactive podcasts, and expressive audiobook narration,\nthereby pushing the boundaries of voice interaction technology. Demos are\navailable at https://fun-audio-llm.github.io, and the code can be accessed at\nhttps://github.com/FunAudioLLM.",
            "upvotes": 13
        },
        "publishedAt": "2024-07-08T02:16:49.947Z",
        "title": "FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.04051.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.03958",
            "authors": [
                {
                    "_id": "668ba1fcb424c825f7b2e9ee",
                    "name": "Young-Jun Lee",
                    "hidden": false
                },
                {
                    "_id": "668ba1fcb424c825f7b2e9ef",
                    "name": "Dokyong Lee",
                    "hidden": false
                },
                {
                    "_id": "668ba1fcb424c825f7b2e9f0",
                    "name": "Junyoung Youn",
                    "hidden": false
                },
                {
                    "_id": "668ba1fcb424c825f7b2e9f1",
                    "name": "Kyeongjin Oh",
                    "hidden": false
                },
                {
                    "_id": "668ba1fcb424c825f7b2e9f2",
                    "name": "Byungsoo Ko",
                    "hidden": false
                },
                {
                    "_id": "668ba1fcb424c825f7b2e9f3",
                    "name": "Jonghwan Hyeon",
                    "hidden": false
                },
                {
                    "_id": "668ba1fcb424c825f7b2e9f4",
                    "name": "Ho-Jin Choi",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-04T14:26:49.000Z",
            "title": "Stark: Social Long-Term Multi-Modal Conversation with Persona\n  Commonsense Knowledge",
            "summary": "Humans share a wide variety of images related to their personal experiences\nwithin conversations via instant messaging tools. However, existing works focus\non (1) image-sharing behavior in singular sessions, leading to limited\nlong-term social interaction, and (2) a lack of personalized image-sharing\nbehavior. In this work, we introduce Stark, a large-scale long-term multi-modal\nconversation dataset that covers a wide range of social personas in a\nmulti-modality format, time intervals, and images. To construct Stark\nautomatically, we propose a novel multi-modal contextualization framework, Mcu,\nthat generates long-term multi-modal dialogue distilled from ChatGPT and our\nproposed Plan-and-Execute image aligner. Using our Stark, we train a\nmulti-modal conversation model, Ultron 7B, which demonstrates impressive visual\nimagination ability. Furthermore, we demonstrate the effectiveness of our\ndataset in human evaluation. We make our source code and dataset publicly\navailable.",
            "upvotes": 7
        },
        "publishedAt": "2024-07-08T06:54:39.442Z",
        "title": "Stark: Social Long-Term Multi-Modal Conversation with Persona Commonsense Knowledge",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.03958.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657152eb12f162153b50ec9d/qnldHP35PclV0pDz_05q8.jpeg",
            "fullname": "Byung-Kwan Lee",
            "name": "BK-Lee",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.04078",
            "authors": [
                {
                    "_id": "668b943035da6da695a9a392",
                    "name": "Chengpeng Li",
                    "hidden": false
                },
                {
                    "_id": "668b943035da6da695a9a393",
                    "name": "Guanting Dong",
                    "hidden": false
                },
                {
                    "_id": "668b943035da6da695a9a394",
                    "user": {
                        "avatarUrl": "/avatars/14246aae3b1f8b7ad050f8ff2c8b260e.svg",
                        "isPro": false,
                        "fullname": "Mingfeng Xue",
                        "user": "mingfengxue",
                        "type": "user"
                    },
                    "name": "Mingfeng Xue",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-07-08T08:49:40.844Z",
                    "hidden": false
                },
                {
                    "_id": "668b943035da6da695a9a395",
                    "name": "Ru Peng",
                    "hidden": false
                },
                {
                    "_id": "668b943035da6da695a9a396",
                    "name": "Xiang Wang",
                    "hidden": false
                },
                {
                    "_id": "668b943035da6da695a9a397",
                    "name": "Dayiheng Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-04T17:39:16.000Z",
            "title": "DotaMath: Decomposition of Thought with Code Assistance and\n  Self-correction for Mathematical Reasoning",
            "summary": "Large language models (LLMs) have made impressive progress in handling simple\nmath problems, yet they still struggle with more challenging and complex\nmathematical tasks. In this paper, we introduce a series of LLMs that employs\nthe Decomposition of thought with code assistance and self-correction for\nmathematical reasoning, dubbed as DotaMath. DotaMath models tackle complex\nmathematical tasks by decomposing them into simpler logical subtasks,\nleveraging code to solve these subtasks, obtaining fine-grained feedback from\nthe code interpreter, and engaging in self-reflection and correction. By\nannotating diverse interactive tool-use trajectories and employing query\nevolution on GSM8K and MATH datasets, we generate an instruction fine-tuning\ndataset called DotaMathQA with 574K query-response pairs. We train a series of\nbase LLMs using imitation learning on DotaMathQA, resulting in DotaMath models\nthat achieve remarkable performance compared to open-source LLMs across various\nin-domain and out-of-domain benchmarks. Notably, DotaMath-deepseek-7B showcases\nan outstanding performance of 64.8% on the competitive MATH dataset and 86.7%\non GSM8K. Besides, DotaMath-deepseek-7B maintains strong competitiveness on a\nseries of in-domain and out-of-domain benchmarks (Avg. 80.1%). Looking forward,\nwe anticipate that the DotaMath paradigm will open new pathways for addressing\nintricate mathematical problems. Our code is publicly available at\nhttps://github.com/ChengpengLi1003/DotaMath.",
            "upvotes": 6
        },
        "publishedAt": "2024-07-08T05:56:10.008Z",
        "title": "DotaMath: Decomposition of Thought with Code Assistance and Self-correction for Mathematical Reasoning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.04078.png",
        "numComments": 3,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
            "fullname": "KABI",
            "name": "dongguanting",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.04172",
            "authors": [
                {
                    "_id": "668b4dc02c83140a9b2d539c",
                    "name": "Ahmed Masry",
                    "hidden": false
                },
                {
                    "_id": "668b4dc02c83140a9b2d539d",
                    "name": "Megh Thakkar",
                    "hidden": false
                },
                {
                    "_id": "668b4dc02c83140a9b2d539e",
                    "name": "Aayush Bajaj",
                    "hidden": false
                },
                {
                    "_id": "668b4dc02c83140a9b2d539f",
                    "name": "Aaryaman Kartha",
                    "hidden": false
                },
                {
                    "_id": "668b4dc02c83140a9b2d53a0",
                    "name": "Enamul Hoque",
                    "hidden": false
                },
                {
                    "_id": "668b4dc02c83140a9b2d53a1",
                    "name": "Shafiq Joty",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-04T22:16:40.000Z",
            "title": "ChartGemma: Visual Instruction-tuning for Chart Reasoning in the Wild",
            "summary": "Given the ubiquity of charts as a data analysis, visualization, and\ndecision-making tool across industries and sciences, there has been a growing\ninterest in developing pre-trained foundation models as well as general purpose\ninstruction-tuned models for chart understanding and reasoning. However,\nexisting methods suffer crucial drawbacks across two critical axes affecting\nthe performance of chart representation models: they are trained on data\ngenerated from underlying data tables of the charts, ignoring the visual trends\nand patterns in chart images, and use weakly aligned vision-language backbone\nmodels for domain-specific training, limiting their generalizability when\nencountering charts in the wild. We address these important drawbacks and\nintroduce ChartGemma, a novel chart understanding and reasoning model developed\nover PaliGemma. Rather than relying on underlying data tables, ChartGemma is\ntrained on instruction-tuning data generated directly from chart images, thus\ncapturing both high-level trends and low-level visual information from a\ndiverse set of charts. Our simple approach achieves state-of-the-art results\nacross 5 benchmarks spanning chart summarization, question answering, and\nfact-checking, and our elaborate qualitative studies on real-world charts show\nthat ChartGemma generates more realistic and factually correct summaries\ncompared to its contemporaries. We release the code, model checkpoints,\ndataset, and demos at https://github.com/vis-nlp/ChartGemma.",
            "upvotes": 6
        },
        "publishedAt": "2024-07-08T05:29:49.471Z",
        "title": "ChartGemma: Visual Instruction-tuning for Chart Reasoning in the Wild",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.04172.png",
        "numComments": 5,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.04620",
            "authors": [
                {
                    "_id": "668b5e98d358e8fd17762b9a",
                    "name": "Yu Sun",
                    "hidden": false
                },
                {
                    "_id": "668b5e98d358e8fd17762b9b",
                    "name": "Xinhao Li",
                    "hidden": false
                },
                {
                    "_id": "668b5e98d358e8fd17762b9c",
                    "user": {
                        "avatarUrl": "/avatars/5a76c9343451c24e9697d3165cdc0af6.svg",
                        "isPro": false,
                        "fullname": "Karan Dalal",
                        "user": "karansdalal",
                        "type": "user"
                    },
                    "name": "Karan Dalal",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2024-07-08T04:11:37.802Z",
                    "hidden": false
                },
                {
                    "_id": "668b5e98d358e8fd17762b9d",
                    "name": "Jiarui Xu",
                    "hidden": false
                },
                {
                    "_id": "668b5e98d358e8fd17762b9e",
                    "name": "Arjun Vikram",
                    "hidden": false
                },
                {
                    "_id": "668b5e98d358e8fd17762b9f",
                    "name": "Genghan Zhang",
                    "hidden": false
                },
                {
                    "_id": "668b5e98d358e8fd17762ba0",
                    "name": "Yann Dubois",
                    "hidden": false
                },
                {
                    "_id": "668b5e98d358e8fd17762ba1",
                    "name": "Xinlei Chen",
                    "hidden": false
                },
                {
                    "_id": "668b5e98d358e8fd17762ba2",
                    "name": "Xiaolong Wang",
                    "hidden": false
                },
                {
                    "_id": "668b5e98d358e8fd17762ba3",
                    "name": "Sanmi Koyejo",
                    "hidden": false
                },
                {
                    "_id": "668b5e98d358e8fd17762ba4",
                    "name": "Tatsunori Hashimoto",
                    "hidden": false
                },
                {
                    "_id": "668b5e98d358e8fd17762ba5",
                    "name": "Carlos Guestrin",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-05T16:23:20.000Z",
            "title": "Learning to (Learn at Test Time): RNNs with Expressive Hidden States",
            "summary": "Self-attention performs well in long context but has quadratic complexity.\nExisting RNN layers have linear complexity, but their performance in long\ncontext is limited by the expressive power of their hidden state. We propose a\nnew class of sequence modeling layers with linear complexity and an expressive\nhidden state. The key idea is to make the hidden state a machine learning model\nitself, and the update rule a step of self-supervised learning. Since the\nhidden state is updated by training even on test sequences, our layers are\ncalled Test-Time Training (TTT) layers. We consider two instantiations:\nTTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer\nMLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B\nparameters, comparing with a strong Transformer and Mamba, a modern RNN. Both\nTTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer,\nthey can keep reducing perplexity by conditioning on more tokens, while Mamba\ncannot after 16k context. With preliminary systems optimization, TTT-Linear is\nalready faster than Transformer at 8k context and matches Mamba in wall-clock\ntime. TTT-MLP still faces challenges in memory I/O, but shows larger potential\nin long context, pointing to a promising direction for future research.",
            "upvotes": 6
        },
        "publishedAt": "2024-07-08T02:05:53.820Z",
        "title": "Learning to (Learn at Test Time): RNNs with Expressive Hidden States",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.04620.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2406.08085",
            "authors": [
                {
                    "_id": "666aa4f277d119acc29d4ba0",
                    "name": "Haoji Zhang",
                    "hidden": false
                },
                {
                    "_id": "666aa4f277d119acc29d4ba1",
                    "name": "Yiqin Wang",
                    "hidden": false
                },
                {
                    "_id": "666aa4f277d119acc29d4ba2",
                    "name": "Yansong Tang",
                    "hidden": false
                },
                {
                    "_id": "666aa4f277d119acc29d4ba3",
                    "name": "Yong Liu",
                    "hidden": false
                },
                {
                    "_id": "666aa4f277d119acc29d4ba4",
                    "name": "Jiashi Feng",
                    "hidden": false
                },
                {
                    "_id": "666aa4f277d119acc29d4ba5",
                    "name": "Jifeng Dai",
                    "hidden": false
                },
                {
                    "_id": "666aa4f277d119acc29d4ba6",
                    "name": "Xiaojie Jin",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-06-12T11:07:55.000Z",
            "title": "Flash-VStream: Memory-Based Real-Time Understanding for Long Video\n  Streams",
            "summary": "Benefiting from the advancements in large language models and cross-modal\nalignment, existing multi-modal video understanding methods have achieved\nprominent performance in offline scenario. However, online video streams, as\none of the most common media forms in the real world, have seldom received\nattention. Compared to offline videos, the 'dynamic' nature of online video\nstreams poses challenges for the direct application of existing models and\nintroduces new problems, such as the storage of extremely long-term\ninformation, interaction between continuous visual content and 'asynchronous'\nuser questions. Therefore, in this paper we present Flash-VStream, a\nvideo-language model that simulates the memory mechanism of human. Our model is\nable to process extremely long video streams in real-time and respond to user\nqueries simultaneously. Compared to existing models, Flash-VStream achieves\nsignificant reductions in inference latency and VRAM consumption, which is\nintimately related to performing understanding of online streaming video. In\naddition, given that existing video understanding benchmarks predominantly\nconcentrate on offline scenario, we propose VStream-QA, a novel question\nanswering benchmark specifically designed for online video streaming\nunderstanding. Comparisons with popular existing methods on the proposed\nbenchmark demonstrate the superiority of our method for such challenging\nsetting. To verify the generalizability of our approach, we further evaluate it\non existing video understanding benchmarks and achieves state-of-the-art\nperformance in offline scenarios as well. All code, models, and datasets are\navailable at the https://invinciblewyq.github.io/vstream-page/",
            "upvotes": 5
        },
        "publishedAt": "2024-07-08T09:26:59.924Z",
        "title": "Flash-VStream: Memory-Based Real-Time Understanding for Long Video Streams",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.08085.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.02855",
            "authors": [
                {
                    "_id": "668a8f6517212ba35e0f77ec",
                    "user": {
                        "avatarUrl": "/avatars/aefd9271b891abc6dd2ded1a30eebca4.svg",
                        "isPro": false,
                        "fullname": "Zhexin Zhang",
                        "user": "nonstopfor",
                        "type": "user"
                    },
                    "name": "Zhexin Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-07-08T08:50:09.660Z",
                    "hidden": false
                },
                {
                    "_id": "668a8f6517212ba35e0f77ed",
                    "name": "Junxiao Yang",
                    "hidden": false
                },
                {
                    "_id": "668a8f6517212ba35e0f77ee",
                    "name": "Pei Ke",
                    "hidden": false
                },
                {
                    "_id": "668a8f6517212ba35e0f77ef",
                    "name": "Shiyao Cui",
                    "hidden": false
                },
                {
                    "_id": "668a8f6517212ba35e0f77f0",
                    "name": "Chujie Zheng",
                    "hidden": false
                },
                {
                    "_id": "668a8f6517212ba35e0f77f1",
                    "name": "Hongning Wang",
                    "hidden": false
                },
                {
                    "_id": "668a8f6517212ba35e0f77f2",
                    "name": "Minlie Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-03T07:14:05.000Z",
            "title": "Safe Unlearning: A Surprisingly Effective and Generalizable Solution to\n  Defend Against Jailbreak Attacks",
            "summary": "LLMs are known to be vulnerable to jailbreak attacks, even after safety\nalignment. An important observation is that, while different types of jailbreak\nattacks can generate significantly different queries, they mostly result in\nsimilar responses that are rooted in the same harmful knowledge (e.g., detailed\nsteps to make a bomb). Therefore, we conjecture that directly unlearn the\nharmful knowledge in the LLM can be a more effective way to defend against\njailbreak attacks than the mainstream supervised fine-tuning (SFT) based\napproaches. Our extensive experiments confirmed our insight and suggested\nsurprising generalizability of our unlearning-based approach: using only 20 raw\nharmful questions without any jailbreak prompt during training, our\nsolution reduced the Attack Success Rate (ASR) in Vicuna-7B on\nout-of-distribution (OOD) harmful questions wrapped with various complex\njailbreak prompts from 82.6\\% to 7.7\\%. This significantly outperforms\nLlama2-7B-Chat, which is fine-tuned on about 0.1M safety alignment samples but\nstill has an ASR of 21.9\\% even under the help of an additional safety system\nprompt. Further analysis reveals that the generalization ability of our\nsolution stems from the intrinsic relatedness among harmful responses across\nharmful questions (e.g., response patterns, shared steps and actions, and\nsimilarity among their learned representations in the LLM). Our code is\navailable at https://github.com/thu-coai/SafeUnlearning.",
            "upvotes": 5
        },
        "publishedAt": "2024-07-08T07:59:05.909Z",
        "title": "Safe Unlearning: A Surprisingly Effective and Generalizable Solution to Defend Against Jailbreak Attacks",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.02855.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "/avatars/aefd9271b891abc6dd2ded1a30eebca4.svg",
            "fullname": "Zhexin Zhang",
            "name": "nonstopfor",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.04622",
            "authors": [
                {
                    "_id": "668b5ea8c2759a7bde1ea83f",
                    "name": "Zachary Kenton",
                    "hidden": false
                },
                {
                    "_id": "668b5ea8c2759a7bde1ea840",
                    "name": "Noah Y. Siegel",
                    "hidden": false
                },
                {
                    "_id": "668b5ea8c2759a7bde1ea841",
                    "name": "János Kramár",
                    "hidden": false
                },
                {
                    "_id": "668b5ea8c2759a7bde1ea842",
                    "name": "Jonah Brown-Cohen",
                    "hidden": false
                },
                {
                    "_id": "668b5ea8c2759a7bde1ea843",
                    "name": "Samuel Albanie",
                    "hidden": false
                },
                {
                    "_id": "668b5ea8c2759a7bde1ea844",
                    "name": "Jannis Bulian",
                    "hidden": false
                },
                {
                    "_id": "668b5ea8c2759a7bde1ea845",
                    "name": "Rishabh Agarwal",
                    "hidden": false
                },
                {
                    "_id": "668b5ea8c2759a7bde1ea846",
                    "name": "David Lindner",
                    "hidden": false
                },
                {
                    "_id": "668b5ea8c2759a7bde1ea847",
                    "name": "Yunhao Tang",
                    "hidden": false
                },
                {
                    "_id": "668b5ea8c2759a7bde1ea848",
                    "name": "Noah D. Goodman",
                    "hidden": false
                },
                {
                    "_id": "668b5ea8c2759a7bde1ea849",
                    "name": "Rohin Shah",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-05T16:29:15.000Z",
            "title": "On scalable oversight with weak LLMs judging strong LLMs",
            "summary": "Scalable oversight protocols aim to enable humans to accurately supervise\nsuperhuman AI. In this paper we study debate, where two AI's compete to\nconvince a judge; consultancy, where a single AI tries to convince a judge that\nasks questions; and compare to a baseline of direct question-answering, where\nthe judge just answers outright without the AI. We use large language models\n(LLMs) as both AI agents and as stand-ins for human judges, taking the judge\nmodels to be weaker than agent models. We benchmark on a diverse range of\nasymmetries between judges and agents, extending previous work on a single\nextractive QA task with information asymmetry, to also include mathematics,\ncoding, logic and multimodal reasoning asymmetries. We find that debate\noutperforms consultancy across all tasks when the consultant is randomly\nassigned to argue for the correct/incorrect answer. Comparing debate to direct\nquestion answering, the results depend on the type of task: in extractive QA\ntasks with information asymmetry debate outperforms direct question answering,\nbut in other tasks without information asymmetry the results are mixed.\nPrevious work assigned debaters/consultants an answer to argue for. When we\nallow them to instead choose which answer to argue for, we find judges are less\nfrequently convinced by the wrong answer in debate than in consultancy.\nFurther, we find that stronger debater models increase judge accuracy, though\nmore modestly than in previous studies.",
            "upvotes": 4
        },
        "publishedAt": "2024-07-08T02:09:23.845Z",
        "title": "On scalable oversight with weak LLMs judging strong LLMs",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.04622.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.03963",
            "authors": [
                {
                    "_id": "668b603f35da6da695980a87",
                    "name": "LLM-jp",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980a89",
                    "name": "Akiko Aizawa",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980a8a",
                    "name": "Eiji Aramaki",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980a8b",
                    "name": "Bowen Chen",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980a8c",
                    "name": "Fei Cheng",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980a8d",
                    "name": "Hiroyuki Deguchi",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980a8e",
                    "name": "Rintaro Enomoto",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980a8f",
                    "name": "Kazuki Fujii",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980a90",
                    "name": "Kensuke Fukumoto",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980a91",
                    "name": "Takuya Fukushima",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980a92",
                    "name": "Namgi Han",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980a93",
                    "name": "Yuto Harada",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980a94",
                    "name": "Chikara Hashimoto",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980a95",
                    "name": "Tatsuya Hiraoka",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980a96",
                    "name": "Shohei Hisada",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980a97",
                    "name": "Sosuke Hosokawa",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980a98",
                    "name": "Lu Jie",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980a99",
                    "name": "Keisuke Kamata",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980a9a",
                    "name": "Teruhito Kanazawa",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980a9b",
                    "name": "Hiroki Kanezashi",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980a9c",
                    "name": "Hiroshi Kataoka",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980a9d",
                    "name": "Satoru Katsumata",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980a9e",
                    "name": "Daisuke Kawahara",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980a9f",
                    "name": "Seiya Kawano",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980aa0",
                    "name": "Atsushi Keyaki",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980aa1",
                    "name": "Keisuke Kiryu",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980aa2",
                    "name": "Hirokazu Kiyomaru",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980aa3",
                    "name": "Takashi Kodama",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980aa4",
                    "name": "Takahiro Kubo",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980aa5",
                    "name": "Yohei Kuga",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980aa6",
                    "name": "Ryoma Kumon",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980aa7",
                    "name": "Shuhei Kurita",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980aa8",
                    "name": "Sadao Kurohashi",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980aa9",
                    "name": "Conglong Li",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980aaa",
                    "name": "Taiki Maekawa",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980aab",
                    "name": "Hiroshi Matsuda",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980aac",
                    "name": "Yusuke Miyao",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980aad",
                    "name": "Kentaro Mizuki",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980aae",
                    "name": "Sakae Mizuki",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980aaf",
                    "name": "Yugo Murawaki",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980ab0",
                    "name": "Ryo Nakamura",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980ab1",
                    "name": "Taishi Nakamura",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980ab2",
                    "name": "Kouta Nakayama",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980ab3",
                    "name": "Tomoka Nakazato",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980ab4",
                    "name": "Takuro Niitsuma",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980ab5",
                    "name": "Jiro Nishitoba",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980ab6",
                    "name": "Yusuke Oda",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980ab7",
                    "name": "Hayato Ogawa",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980ab8",
                    "name": "Takumi Okamoto",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980ab9",
                    "name": "Naoaki Okazaki",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980aba",
                    "name": "Yohei Oseki",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980abb",
                    "name": "Shintaro Ozaki",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980abc",
                    "name": "Koki Ryu",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980abd",
                    "name": "Rafal Rzepka",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980abe",
                    "name": "Keisuke Sakaguchi",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980abf",
                    "name": "Shota Sasaki",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980ac0",
                    "name": "Satoshi Sekine",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980ac1",
                    "name": "Kohei Suda",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980ac2",
                    "name": "Saku Sugawara",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980ac3",
                    "name": "Issa Sugiura",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980ac4",
                    "name": "Hiroaki Sugiyama",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980ac5",
                    "name": "Hisami Suzuki",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980ac6",
                    "name": "Jun Suzuki",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980ac7",
                    "name": "Toyotaro Suzumura",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980ac8",
                    "name": "Kensuke Tachibana",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980ac9",
                    "name": "Yu Takagi",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980aca",
                    "name": "Kyosuke Takami",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980acb",
                    "name": "Koichi Takeda",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980acc",
                    "name": "Masashi Takeshita",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980acd",
                    "name": "Masahiro Tanaka",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980ace",
                    "name": "Kenjiro Taura",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980acf",
                    "name": "Arseny Tolmachev",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980ad0",
                    "name": "Nobuhiro Ueda",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980ad1",
                    "name": "Zhen Wan",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980ad2",
                    "name": "Shuntaro Yada",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980ad3",
                    "name": "Sakiko Yahata",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980ad4",
                    "name": "Yuya Yamamoto",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980ad5",
                    "name": "Yusuke Yamauchi",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980ad6",
                    "name": "Hitomi Yanaka",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980ad7",
                    "name": "Rio Yokota",
                    "hidden": false
                },
                {
                    "_id": "668b603f35da6da695980ad8",
                    "name": "Koichiro Yoshino",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-04T14:33:03.000Z",
            "title": "LLM-jp: A Cross-organizational Project for the Research and Development\n  of Fully Open Japanese LLMs",
            "summary": "This paper introduces LLM-jp, a cross-organizational project for the research\nand development of Japanese large language models (LLMs). LLM-jp aims to\ndevelop open-source and strong Japanese LLMs, and as of this writing, more than\n1,500 participants from academia and industry are working together for this\npurpose. This paper presents the background of the establishment of LLM-jp,\nsummaries of its activities, and technical reports on the LLMs developed by\nLLM-jp. For the latest activities, visit https://llm-jp.nii.ac.jp/en/.",
            "upvotes": 3
        },
        "publishedAt": "2024-07-08T02:13:13.420Z",
        "title": "LLM-jp: A Cross-organizational Project for the Research and Development of Fully Open Japanese LLMs",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.03963.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.03923",
            "authors": [
                {
                    "_id": "668b8dedcc47c4aec57bc0eb",
                    "name": "Junghe Lee",
                    "hidden": false
                },
                {
                    "_id": "668b8dedcc47c4aec57bc0ec",
                    "name": "Donghyeong Kim",
                    "hidden": false
                },
                {
                    "_id": "668b8dedcc47c4aec57bc0ed",
                    "name": "Dogyoon Lee",
                    "hidden": false
                },
                {
                    "_id": "668b8dedcc47c4aec57bc0ee",
                    "name": "Suhwan Cho",
                    "hidden": false
                },
                {
                    "_id": "668b8dedcc47c4aec57bc0ef",
                    "name": "Sangyoun Lee",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-04T13:37:04.000Z",
            "title": "CRiM-GS: Continuous Rigid Motion-Aware Gaussian Splatting from Motion\n  Blur Images",
            "summary": "Neural radiance fields (NeRFs) have received significant attention due to\ntheir high-quality novel view rendering ability, prompting research to address\nvarious real-world cases. One critical challenge is the camera motion blur\ncaused by camera movement during exposure time, which prevents accurate 3D\nscene reconstruction. In this study, we propose continuous rigid motion-aware\ngaussian splatting (CRiM-GS) to reconstruct accurate 3D scene from blurry\nimages with real-time rendering speed. Considering the actual camera motion\nblurring process, which consists of complex motion patterns, we predict the\ncontinuous movement of the camera based on neural ordinary differential\nequations (ODEs). Specifically, we leverage rigid body transformations to model\nthe camera motion with proper regularization, preserving the shape and size of\nthe object. Furthermore, we introduce a continuous deformable 3D transformation\nin the SE(3) field to adapt the rigid body transformation to\nreal-world problems by ensuring a higher degree of freedom. By revisiting\nfundamental camera theory and employing advanced neural network training\ntechniques, we achieve accurate modeling of continuous camera trajectories. We\nconduct extensive experiments, demonstrating state-of-the-art performance both\nquantitatively and qualitatively on benchmark datasets.",
            "upvotes": 2
        },
        "publishedAt": "2024-07-08T05:27:51.985Z",
        "title": "CRiM-GS: Continuous Rigid Motion-Aware Gaussian Splatting from Motion Blur Images",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.03923.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    },
    {
        "paper": {
            "id": "2407.03418",
            "authors": [
                {
                    "_id": "668b8960ccbf926e742808b7",
                    "name": "Paul Pu Liang",
                    "hidden": false
                },
                {
                    "_id": "668b8960ccbf926e742808b8",
                    "name": "Akshay Goindani",
                    "hidden": false
                },
                {
                    "_id": "668b8960ccbf926e742808b9",
                    "name": "Talha Chafekar",
                    "hidden": false
                },
                {
                    "_id": "668b8960ccbf926e742808ba",
                    "name": "Leena Mathur",
                    "hidden": false
                },
                {
                    "_id": "668b8960ccbf926e742808bb",
                    "name": "Haofei Yu",
                    "hidden": false
                },
                {
                    "_id": "668b8960ccbf926e742808bc",
                    "name": "Ruslan Salakhutdinov",
                    "hidden": false
                },
                {
                    "_id": "668b8960ccbf926e742808bd",
                    "name": "Louis-Philippe Morency",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-07-03T18:00:48.000Z",
            "title": "HEMM: Holistic Evaluation of Multimodal Foundation Models",
            "summary": "Multimodal foundation models that can holistically process text alongside\nimages, video, audio, and other sensory modalities are increasingly used in a\nvariety of real-world applications. However, it is challenging to characterize\nand study progress in multimodal foundation models, given the range of possible\nmodeling decisions, tasks, and domains. In this paper, we introduce Holistic\nEvaluation of Multimodal Models (HEMM) to systematically evaluate the\ncapabilities of multimodal foundation models across a set of 3 dimensions:\nbasic skills, information flow, and real-world use cases. Basic multimodal\nskills are internal abilities required to solve problems, such as learning\ninteractions across modalities, fine-grained alignment, multi-step reasoning,\nand the ability to handle external knowledge. Information flow studies how\nmultimodal content changes during a task through querying, translation,\nediting, and fusion. Use cases span domain-specific challenges introduced in\nreal-world multimedia, affective computing, natural sciences, healthcare, and\nhuman-computer interaction applications. Through comprehensive experiments\nacross the 30 tasks in HEMM, we (1) identify key dataset dimensions (e.g.,\nbasic skills, information flows, and use cases) that pose challenges to today's\nmodels, and (2) distill performance trends regarding how different modeling\ndimensions (e.g., scale, pre-training data, multimodal alignment, pre-training,\nand instruction tuning objectives) influence performance. Our conclusions\nregarding challenging multimodal interactions, use cases, and tasks requiring\nreasoning and external knowledge, the benefits of data and model scale, and the\nimpacts of instruction tuning yield actionable insights for future work in\nmultimodal foundation models.",
            "upvotes": 2
        },
        "publishedAt": "2024-07-08T05:09:59.806Z",
        "title": "HEMM: Holistic Evaluation of Multimodal Foundation Models",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.03418.png",
        "numComments": 1,
        "submittedBy": {
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isMod": false
        }
    }
]