[
  {
    "paper": {
      "id": "2508.06471",
      "authors": [
        {
          "_id": "68996c86f022d141f5d4354d",
          "name": "GLM-4. 5 Team",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d4354f",
          "name": "Aohan Zeng",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43550",
          "name": "Xin Lv",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43551",
          "user": {
            "_id": "6231576e92e83fd1179ac3f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1664543160657-6231576e92e83fd1179ac3f0.jpeg",
            "isPro": false,
            "fullname": "Qinkai Zheng",
            "user": "Stanislas",
            "type": "user"
          },
          "name": "Qinkai Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-11T06:45:31.330Z",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43552",
          "name": "Zhenyu Hou",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43553",
          "name": "Bin Chen",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43554",
          "name": "Chengxing Xie",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43555",
          "name": "Cunxiang Wang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43556",
          "name": "Da Yin",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43557",
          "name": "Hao Zeng",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43558",
          "name": "Jiajie Zhang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43559",
          "name": "Kedong Wang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d4355a",
          "name": "Lucen Zhong",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d4355b",
          "name": "Mingdao Liu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d4355c",
          "name": "Rui Lu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d4355d",
          "name": "Shulin Cao",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d4355e",
          "name": "Xiaohan Zhang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d4355f",
          "name": "Xuancheng Huang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43560",
          "name": "Yao Wei",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43561",
          "user": {
            "_id": "65acc5afe2a2c8635614de43",
            "avatarUrl": "/avatars/c5fce792792cc0b52ed7475d72460c58.svg",
            "isPro": false,
            "fullname": "Yean Cheng",
            "user": "LiquidAmmonia",
            "type": "user"
          },
          "name": "Yean Cheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-11T06:45:35.718Z",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43562",
          "name": "Yifan An",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43563",
          "name": "Yilin Niu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43564",
          "name": "Yuanhao Wen",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43565",
          "name": "Yushi Bai",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43566",
          "name": "Zhengxiao Du",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43567",
          "name": "Zihan Wang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43568",
          "name": "Zilin Zhu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43569",
          "name": "Bohan Zhang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d4356a",
          "name": "Bosi Wen",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d4356b",
          "name": "Bowen Wu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d4356c",
          "name": "Bowen Xu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d4356d",
          "name": "Can Huang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d4356e",
          "name": "Casey Zhao",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d4356f",
          "name": "Changpeng Cai",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43570",
          "name": "Chao Yu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43571",
          "name": "Chen Li",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43572",
          "name": "Chendi Ge",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43573",
          "name": "Chenghua Huang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43574",
          "name": "Chenhui Zhang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43575",
          "name": "Chenxi Xu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43576",
          "name": "Chenzheng Zhu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43577",
          "name": "Chuang Li",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43578",
          "name": "Congfeng Yin",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43579",
          "name": "Daoyan Lin",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d4357a",
          "name": "Dayong Yang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d4357b",
          "name": "Dazhi Jiang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d4357c",
          "name": "Ding Ai",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d4357d",
          "name": "Erle Zhu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d4357e",
          "name": "Fei Wang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d4357f",
          "name": "Gengzheng Pan",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43580",
          "name": "Guo Wang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43581",
          "name": "Hailong Sun",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43582",
          "name": "Haitao Li",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43583",
          "name": "Haiyang Li",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43584",
          "name": "Haiyi Hu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43585",
          "name": "Hanyu Zhang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43586",
          "name": "Hao Peng",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43587",
          "name": "Hao Tai",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43588",
          "name": "Haoke Zhang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43589",
          "name": "Haoran Wang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d4358a",
          "name": "Haoyu Yang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d4358b",
          "name": "He Liu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d4358c",
          "name": "He Zhao",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d4358d",
          "name": "Hongwei Liu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d4358e",
          "name": "Hongxi Yan",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d4358f",
          "name": "Huan Liu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43590",
          "name": "Huilong Chen",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43591",
          "name": "Ji Li",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43592",
          "name": "Jiajing Zhao",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43593",
          "name": "Jiamin Ren",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43594",
          "name": "Jian Jiao",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43595",
          "name": "Jiani Zhao",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43596",
          "name": "Jianyang Yan",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43597",
          "name": "Jiaqi Wang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43598",
          "name": "Jiayi Gui",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d43599",
          "name": "Jiayue Zhao",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d4359a",
          "name": "Jie Liu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d4359b",
          "name": "Jijie Li",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d4359c",
          "name": "Jing Li",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d4359d",
          "name": "Jing Lu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d4359e",
          "name": "Jingsen Wang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d4359f",
          "name": "Jingwei Yuan",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435a0",
          "name": "Jingxuan Li",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435a1",
          "name": "Jingzhao Du",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435a2",
          "name": "Jinhua Du",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435a3",
          "name": "Jinxin Liu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435a4",
          "name": "Junkai Zhi",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435a5",
          "name": "Junli Gao",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435a6",
          "name": "Ke Wang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435a7",
          "name": "Lekang Yang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435a8",
          "name": "Liang Xu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435a9",
          "name": "Lin Fan",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435aa",
          "name": "Lindong Wu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435ab",
          "name": "Lintao Ding",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435ac",
          "name": "Lu Wang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435ad",
          "name": "Man Zhang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435ae",
          "name": "Minghao Li",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435af",
          "name": "Minghuan Xu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435b0",
          "name": "Mingming Zhao",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435b1",
          "name": "Mingshu Zhai",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435b2",
          "name": "Pengfan Du",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435b3",
          "name": "Qian Dong",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435b4",
          "name": "Shangde Lei",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435b5",
          "name": "Shangqing Tu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435b6",
          "name": "Shangtong Yang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435b7",
          "name": "Shaoyou Lu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435b8",
          "name": "Shijie Li",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435b9",
          "name": "Shuang Li",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435ba",
          "name": "Shuang-Li",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435bb",
          "name": "Shuxun Yang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435bc",
          "name": "Sibo Yi",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435bd",
          "name": "Tianshu Yu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435be",
          "name": "Wei Tian",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435bf",
          "name": "Weihan Wang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435c0",
          "name": "Wenbo Yu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435c1",
          "name": "Weng Lam Tam",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435c2",
          "name": "Wenjie Liang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435c3",
          "name": "Wentao Liu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435c4",
          "name": "Xiao Wang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435c5",
          "name": "Xiaohan Jia",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435c6",
          "name": "Xiaotao Gu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435c7",
          "name": "Xiaoying Ling",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435c8",
          "name": "Xin Wang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435c9",
          "name": "Xing Fan",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435ca",
          "name": "Xingru Pan",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435cb",
          "name": "Xinyuan Zhang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435cc",
          "name": "Xinze Zhang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435cd",
          "name": "Xiuqing Fu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435ce",
          "name": "Xunkai Zhang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435cf",
          "name": "Yabo Xu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435d0",
          "name": "Yandong Wu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435d1",
          "name": "Yida Lu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435d2",
          "name": "Yidong Wang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435d3",
          "name": "Yilin Zhou",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435d4",
          "name": "Yiming Pan",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435d5",
          "name": "Ying Zhang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435d6",
          "name": "Yingli Wang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435d7",
          "name": "Yingru Li",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435d8",
          "name": "Yinpei Su",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435d9",
          "name": "Yipeng Geng",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435da",
          "name": "Yitong Zhu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435db",
          "name": "Yongkun Yang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435dc",
          "name": "Yuhang Li",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435dd",
          "name": "Yuhao Wu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435de",
          "name": "Yujiang Li",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435df",
          "name": "Yunan Liu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435e0",
          "name": "Yunqing Wang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435e1",
          "name": "Yuntao Li",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435e2",
          "user": {
            "_id": "643507d1ce04fdb57e9d7e05",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643507d1ce04fdb57e9d7e05/QuCFAO3v7G3LrVGusqFj1.png",
            "isPro": false,
            "fullname": "zR",
            "user": "ZAHNGYUXUAN",
            "type": "user"
          },
          "name": "Yuxuan Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-11T06:45:21.258Z",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435e3",
          "name": "Zezhen Liu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435e4",
          "name": "Zhen Yang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435e5",
          "name": "Zhengda Zhou",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435e6",
          "name": "Zhongpei Qiao",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435e7",
          "name": "Zhuoer Feng",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435e8",
          "name": "Zhuorui Liu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435e9",
          "name": "Zichen Zhang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435ea",
          "name": "Zihan Wang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435eb",
          "name": "Zijun Yao",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435ec",
          "name": "Zikang Wang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435ed",
          "name": "Ziqiang Liu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435ee",
          "name": "Ziwei Chai",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435ef",
          "user": {
            "_id": "68806066345a5d85e2494aba",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/AkrQMIp4kQm3Zn1giMa5I.png",
            "isPro": false,
            "fullname": "Zixuan Li",
            "user": "zixuanlimit",
            "type": "user"
          },
          "name": "Zixuan Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-11T06:45:26.163Z",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435f0",
          "name": "Zuodong Zhao",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435f1",
          "name": "Wenguang Chen",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435f2",
          "name": "Jidong Zhai",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435f3",
          "name": "Bin Xu",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435f4",
          "name": "Minlie Huang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435f5",
          "name": "Hongning Wang",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435f6",
          "name": "Juanzi Li",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435f7",
          "name": "Yuxiao Dong",
          "hidden": false
        },
        {
          "_id": "68996c86f022d141f5d435f8",
          "name": "Jie Tang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-08T17:21:06.000Z",
      "submittedOnDailyAt": "2025-08-11T02:39:27.170Z",
      "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",
      "submittedOnDailyBy": {
        "_id": "62d22496c58f969c152bcefd",
        "avatarUrl": "/avatars/76c3b70e312f25e1e610473475553c5c.svg",
        "isPro": false,
        "fullname": "Tiezhen WANG",
        "user": "xianbao",
        "type": "user"
      },
      "summary": "We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language\nmodel with 355B total parameters and 32B activated parameters, featuring a\nhybrid reasoning method that supports both thinking and direct response modes.\nThrough multi-stage training on 23T tokens and comprehensive post-training with\nexpert model iteration and reinforcement learning, GLM-4.5 achieves strong\nperformance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on\nTAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer\nparameters than several competitors, GLM-4.5 ranks 3rd overall among all\nevaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B\nparameters) and a compact version, GLM-4.5-Air (106B parameters), to advance\nresearch in reasoning and agentic AI systems. Code, models, and more\ninformation are available at https://github.com/zai-org/GLM-4.5.",
      "upvotes": 30,
      "discussionId": "68996c87f022d141f5d435f9",
      "ai_summary": "GLM-4.5, a Mixture-of-Experts large language model with 355B parameters, achieves strong performance across agentic, reasoning, and coding tasks using multi-stage training and reinforcement learning.",
      "ai_keywords": [
        "Mixture-of-Experts",
        "hybrid reasoning method",
        "multi-stage training",
        "expert model iteration",
        "reinforcement learning",
        "TAU-Bench",
        "AIME 24",
        "SWE-bench Verified",
        "agentic benchmarks"
      ]
    },
    "publishedAt": "2025-08-08T13:21:06.000Z",
    "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",
    "summary": "We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language\nmodel with 355B total parameters and 32B activated parameters, featuring a\nhybrid reasoning method that supports both thinking and direct response modes.\nThrough multi-stage training on 23T tokens and comprehensive post-training with\nexpert model iteration and reinforcement learning, GLM-4.5 achieves strong\nperformance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on\nTAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer\nparameters than several competitors, GLM-4.5 ranks 3rd overall among all\nevaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B\nparameters) and a compact version, GLM-4.5-Air (106B parameters), to advance\nresearch in reasoning and agentic AI systems. Code, models, and more\ninformation are available at https://github.com/zai-org/GLM-4.5.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.06471.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62d22496c58f969c152bcefd",
      "avatarUrl": "/avatars/76c3b70e312f25e1e610473475553c5c.svg",
      "fullname": "Tiezhen WANG",
      "name": "xianbao",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 127
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.04825",
      "authors": [
        {
          "_id": "6895558948b0ae5ca2710cf7",
          "user": {
            "_id": "660eb276a0de79aa07e754df",
            "avatarUrl": "/avatars/48ee2d484e70714516589a8b13137036.svg",
            "isPro": false,
            "fullname": "Seungyong Lee",
            "user": "RyanL22",
            "type": "user"
          },
          "name": "Seungyong Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-08T16:20:16.112Z",
          "hidden": false
        },
        {
          "_id": "6895558948b0ae5ca2710cf8",
          "user": {
            "_id": "635035fedc10894a3f66ea79",
            "avatarUrl": "/avatars/bd9c373705d29ad8f14987a2864e5956.svg",
            "isPro": false,
            "fullname": "Jeong-gi Kwak",
            "user": "jgkwak",
            "type": "user"
          },
          "name": "Jeong-gi Kwak",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-08T16:20:12.557Z",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/660eb276a0de79aa07e754df/kVH3Bi0LdvAD7H_HuIGRm.png"
      ],
      "publishedAt": "2025-08-06T19:10:58.000Z",
      "submittedOnDailyAt": "2025-08-11T00:37:43.016Z",
      "title": "Voost: A Unified and Scalable Diffusion Transformer for Bidirectional\n  Virtual Try-On and Try-Off",
      "submittedOnDailyBy": {
        "_id": "660eb276a0de79aa07e754df",
        "avatarUrl": "/avatars/48ee2d484e70714516589a8b13137036.svg",
        "isPro": false,
        "fullname": "Seungyong Lee",
        "user": "RyanL22",
        "type": "user"
      },
      "summary": "Virtual try-on aims to synthesize a realistic image of a person wearing a\ntarget garment, but accurately modeling garment-body correspondence remains a\npersistent challenge, especially under pose and appearance variation. In this\npaper, we propose Voost - a unified and scalable framework that jointly learns\nvirtual try-on and try-off with a single diffusion transformer. By modeling\nboth tasks jointly, Voost enables each garment-person pair to supervise both\ndirections and supports flexible conditioning over generation direction and\ngarment category, enhancing garment-body relational reasoning without\ntask-specific networks, auxiliary losses, or additional labels. In addition, we\nintroduce two inference-time techniques: attention temperature scaling for\nrobustness to resolution or mask variation, and self-corrective sampling that\nleverages bidirectional consistency between tasks. Extensive experiments\ndemonstrate that Voost achieves state-of-the-art results on both try-on and\ntry-off benchmarks, consistently outperforming strong baselines in alignment\naccuracy, visual fidelity, and generalization.",
      "upvotes": 14,
      "discussionId": "6895558948b0ae5ca2710cf9",
      "projectPage": "https://nxnai.github.io/Voost/",
      "githubRepo": "https://github.com/nxnai/Voost",
      "ai_summary": "Voost, a unified diffusion transformer framework, jointly learns virtual try-on and try-off, enhancing garment-body correspondence and achieving state-of-the-art results across benchmarks.",
      "ai_keywords": [
        "diffusion transformer",
        "garment-body correspondence",
        "virtual try-on",
        "virtual try-off",
        "attention temperature scaling",
        "self-corrective sampling",
        "bidirectional consistency"
      ],
      "githubStars": 10
    },
    "publishedAt": "2025-08-06T15:10:58.000Z",
    "title": "Voost: A Unified and Scalable Diffusion Transformer for Bidirectional\n  Virtual Try-On and Try-Off",
    "summary": "Virtual try-on aims to synthesize a realistic image of a person wearing a\ntarget garment, but accurately modeling garment-body correspondence remains a\npersistent challenge, especially under pose and appearance variation. In this\npaper, we propose Voost - a unified and scalable framework that jointly learns\nvirtual try-on and try-off with a single diffusion transformer. By modeling\nboth tasks jointly, Voost enables each garment-person pair to supervise both\ndirections and supports flexible conditioning over generation direction and\ngarment category, enhancing garment-body relational reasoning without\ntask-specific networks, auxiliary losses, or additional labels. In addition, we\nintroduce two inference-time techniques: attention temperature scaling for\nrobustness to resolution or mask variation, and self-corrective sampling that\nleverages bidirectional consistency between tasks. Extensive experiments\ndemonstrate that Voost achieves state-of-the-art results on both try-on and\ntry-off benchmarks, consistently outperforming strong baselines in alignment\naccuracy, visual fidelity, and generalization.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/660eb276a0de79aa07e754df/kVH3Bi0LdvAD7H_HuIGRm.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.04825.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "660eb276a0de79aa07e754df",
      "avatarUrl": "/avatars/48ee2d484e70714516589a8b13137036.svg",
      "fullname": "Seungyong Lee",
      "name": "RyanL22",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.05731",
      "authors": [
        {
          "_id": "689945f3f022d141f5d434a5",
          "user": {
            "_id": "62722849517c0ca41f7cd13d",
            "avatarUrl": "/avatars/bb1f8f2f2665944930cb5a7ce19c47d4.svg",
            "isPro": false,
            "fullname": "Yuhang Liu",
            "user": "SiriusL",
            "type": "user"
          },
          "name": "Yuhang Liu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-11T06:52:51.989Z",
          "hidden": false
        },
        {
          "_id": "689945f3f022d141f5d434a6",
          "name": "Zeyu Liu",
          "hidden": false
        },
        {
          "_id": "689945f3f022d141f5d434a7",
          "name": "Shuanghe Zhu",
          "hidden": false
        },
        {
          "_id": "689945f3f022d141f5d434a8",
          "name": "Pengxiang Li",
          "hidden": false
        },
        {
          "_id": "689945f3f022d141f5d434a9",
          "name": "Congkai Xie",
          "hidden": false
        },
        {
          "_id": "689945f3f022d141f5d434aa",
          "name": "Jiasheng Wang",
          "hidden": false
        },
        {
          "_id": "689945f3f022d141f5d434ab",
          "name": "Xueyu Hu",
          "hidden": false
        },
        {
          "_id": "689945f3f022d141f5d434ac",
          "user": {
            "_id": "650dde4ce14eeb01d42b37a1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650dde4ce14eeb01d42b37a1/n5Yv24uofZ2XJjXdYCrKd.png",
            "isPro": false,
            "fullname": "Xiaotian Han",
            "user": "xiaotianhan",
            "type": "user"
          },
          "name": "Xiaotian Han",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-11T06:48:38.769Z",
          "hidden": false
        },
        {
          "_id": "689945f3f022d141f5d434ad",
          "name": "Jianbo Yuan",
          "hidden": false
        },
        {
          "_id": "689945f3f022d141f5d434ae",
          "name": "Xinyao Wang",
          "hidden": false
        },
        {
          "_id": "689945f3f022d141f5d434af",
          "name": "Shengyu Zhang",
          "hidden": false
        },
        {
          "_id": "689945f3f022d141f5d434b0",
          "name": "Hongxia Yang",
          "hidden": false
        },
        {
          "_id": "689945f3f022d141f5d434b1",
          "name": "Fei Wu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-07T17:49:56.000Z",
      "submittedOnDailyAt": "2025-08-11T00:34:02.328Z",
      "title": "InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy\n  Optimization",
      "submittedOnDailyBy": {
        "_id": "62722849517c0ca41f7cd13d",
        "avatarUrl": "/avatars/bb1f8f2f2665944930cb5a7ce19c47d4.svg",
        "isPro": false,
        "fullname": "Yuhang Liu",
        "user": "SiriusL",
        "type": "user"
      },
      "summary": "The emergence of Multimodal Large Language Models (MLLMs) has propelled the\ndevelopment of autonomous agents that operate on Graphical User Interfaces\n(GUIs) using pure visual input. A fundamental challenge is robustly grounding\nnatural language instructions. This requires a precise spatial alignment, which\naccurately locates the coordinates of each element, and, more critically, a\ncorrect semantic alignment, which matches the instructions to the functionally\nappropriate UI element. Although Reinforcement Learning with Verifiable Rewards\n(RLVR) has proven to be effective at improving spatial alignment for these\nMLLMs, we find that inefficient exploration bottlenecks semantic alignment,\nwhich prevent models from learning difficult semantic associations. To address\nthis exploration problem, we present Adaptive Exploration Policy Optimization\n(AEPO), a new policy optimization framework. AEPO employs a multi-answer\ngeneration strategy to enforce broader exploration, which is then guided by a\ntheoretically grounded Adaptive Exploration Reward (AER) function derived from\nfirst principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B\nand InfiGUI-G1-7B, establish new state-of-the-art results across multiple\nchallenging GUI grounding benchmarks, achieving significant relative\nimprovements of up to 9.0% against the naive RLVR baseline on benchmarks\ndesigned to test generalization and semantic understanding. Resources are\navailable at https://github.com/InfiXAI/InfiGUI-G1.",
      "upvotes": 10,
      "discussionId": "689945f3f022d141f5d434b2",
      "githubRepo": "https://github.com/InfiXAI/InfiGUI-G1",
      "ai_summary": "Adaptive Exploration Policy Optimization (AEPO) enhances semantic alignment in Multimodal Large Language Models (MLLMs) for GUI interaction, improving performance on benchmarks by up to 9.0% compared to RLVR.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "GUIs",
        "Reinforcement Learning with Verifiable Rewards",
        "Adaptive Exploration Policy Optimization",
        "multi-answer generation",
        "Adaptive Exploration Reward",
        "efficiency eta=U/C",
        "InfiGUI-G1-3B",
        "InfiGUI-G1-7B"
      ],
      "githubStars": 7
    },
    "publishedAt": "2025-08-07T13:49:56.000Z",
    "title": "InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy\n  Optimization",
    "summary": "The emergence of Multimodal Large Language Models (MLLMs) has propelled the\ndevelopment of autonomous agents that operate on Graphical User Interfaces\n(GUIs) using pure visual input. A fundamental challenge is robustly grounding\nnatural language instructions. This requires a precise spatial alignment, which\naccurately locates the coordinates of each element, and, more critically, a\ncorrect semantic alignment, which matches the instructions to the functionally\nappropriate UI element. Although Reinforcement Learning with Verifiable Rewards\n(RLVR) has proven to be effective at improving spatial alignment for these\nMLLMs, we find that inefficient exploration bottlenecks semantic alignment,\nwhich prevent models from learning difficult semantic associations. To address\nthis exploration problem, we present Adaptive Exploration Policy Optimization\n(AEPO), a new policy optimization framework. AEPO employs a multi-answer\ngeneration strategy to enforce broader exploration, which is then guided by a\ntheoretically grounded Adaptive Exploration Reward (AER) function derived from\nfirst principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B\nand InfiGUI-G1-7B, establish new state-of-the-art results across multiple\nchallenging GUI grounding benchmarks, achieving significant relative\nimprovements of up to 9.0% against the naive RLVR baseline on benchmarks\ndesigned to test generalization and semantic understanding. Resources are\navailable at https://github.com/InfiXAI/InfiGUI-G1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.05731.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62722849517c0ca41f7cd13d",
      "avatarUrl": "/avatars/bb1f8f2f2665944930cb5a7ce19c47d4.svg",
      "fullname": "Yuhang Liu",
      "name": "SiriusL",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 7
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.05988",
      "authors": [
        {
          "_id": "68994f80f022d141f5d434cf",
          "name": "Wenhao Zeng",
          "hidden": false
        },
        {
          "_id": "68994f80f022d141f5d434d0",
          "name": "Yaoning Wang",
          "hidden": false
        },
        {
          "_id": "68994f80f022d141f5d434d1",
          "name": "Chao Hu",
          "hidden": false
        },
        {
          "_id": "68994f80f022d141f5d434d2",
          "user": {
            "_id": "645b0c3ec35da9c7afd95421",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
            "isPro": false,
            "fullname": "Yuling",
            "user": "YerbaPage",
            "type": "user"
          },
          "name": "Yuling Shi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-11T06:45:44.939Z",
          "hidden": false
        },
        {
          "_id": "68994f80f022d141f5d434d3",
          "name": "Chengcheng Wan",
          "hidden": false
        },
        {
          "_id": "68994f80f022d141f5d434d4",
          "name": "Hongyu Zhang",
          "hidden": false
        },
        {
          "_id": "68994f80f022d141f5d434d5",
          "name": "Xiaodong Gu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-08T03:46:21.000Z",
      "submittedOnDailyAt": "2025-08-11T00:37:13.589Z",
      "title": "Pruning the Unsurprising: Efficient Code Reasoning via First-Token\n  Surprisal",
      "submittedOnDailyBy": {
        "_id": "645b0c3ec35da9c7afd95421",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
        "isPro": false,
        "fullname": "Yuling",
        "user": "YerbaPage",
        "type": "user"
      },
      "summary": "Recently, Large Reasoning Models (LRMs) have demonstrated remarkable\ncapabilities in code reasoning by scaling up the length of Chain-of-Thought\n(CoT). However, excessively long reasoning traces introduce substantial\nchallenges in terms of training cost, inference latency, and deployment\nfeasibility. While various CoT compression approaches have emerged to address\nthis challenge, they face inherent trade-offs: token-level methods often\ndisrupt syntactic and logical coherence, while step-level methods based on\nperplexity fail to reliably capture the logically critical reasoning steps. In\nthis paper, we propose ASAP (Anchor-guided, Surprisal-based Pruning), a novel\ncoarse-to-fine framework for CoT compression. ASAP first performs anchor-guided\npruning to preserve the core reasoning structure, which efficiently reduces the\nsearch space for subsequent processing. It then enables a logic-aware pruning\nby selecting logically essential reasoning steps based on a novel first-token\nsurprisal metric. Finally, ASAP teaches models to autonomously generate and\nleverage these concise CoTs at inference time, enabling efficient reasoning in\ncoding tasks. Experiments show that ASAP achieves state-of-the-art accuracy\nacross multiple code generation benchmarks while substantially reducing\ntraining and inference costs. On the challenging LiveCodeBench v4_v5 benchmark,\nour approach reduces token generation by 23.5% and inference latency by 43.5%\ncompared to the strongest baseline, while achieving a competitive accuracy of\n36.19% in Pass@1. Our results highlight a promising direction for building\npowerful and efficient LRMs.",
      "upvotes": 9,
      "discussionId": "68994f80f022d141f5d434d6",
      "githubRepo": "https://github.com/Zengwh02/ASAP",
      "ai_summary": "ASAP, a novel coarse-to-fine framework, compresses Chain-of-Thought in code reasoning by preserving core structure and essential steps, reducing costs and improving efficiency.",
      "ai_keywords": [
        "Large Reasoning Models",
        "Chain-of-Thought",
        "CoT compression",
        "anchor-guided pruning",
        "logic-aware pruning",
        "first-token surprisal metric",
        "LiveCodeBench",
        "Pass@1"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-08-07T23:46:21.000Z",
    "title": "Pruning the Unsurprising: Efficient Code Reasoning via First-Token\n  Surprisal",
    "summary": "Recently, Large Reasoning Models (LRMs) have demonstrated remarkable\ncapabilities in code reasoning by scaling up the length of Chain-of-Thought\n(CoT). However, excessively long reasoning traces introduce substantial\nchallenges in terms of training cost, inference latency, and deployment\nfeasibility. While various CoT compression approaches have emerged to address\nthis challenge, they face inherent trade-offs: token-level methods often\ndisrupt syntactic and logical coherence, while step-level methods based on\nperplexity fail to reliably capture the logically critical reasoning steps. In\nthis paper, we propose ASAP (Anchor-guided, Surprisal-based Pruning), a novel\ncoarse-to-fine framework for CoT compression. ASAP first performs anchor-guided\npruning to preserve the core reasoning structure, which efficiently reduces the\nsearch space for subsequent processing. It then enables a logic-aware pruning\nby selecting logically essential reasoning steps based on a novel first-token\nsurprisal metric. Finally, ASAP teaches models to autonomously generate and\nleverage these concise CoTs at inference time, enabling efficient reasoning in\ncoding tasks. Experiments show that ASAP achieves state-of-the-art accuracy\nacross multiple code generation benchmarks while substantially reducing\ntraining and inference costs. On the challenging LiveCodeBench v4_v5 benchmark,\nour approach reduces token generation by 23.5% and inference latency by 43.5%\ncompared to the strongest baseline, while achieving a competitive accuracy of\n36.19% in Pass@1. Our results highlight a promising direction for building\npowerful and efficient LRMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.05988.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "645b0c3ec35da9c7afd95421",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
      "fullname": "Yuling",
      "name": "YerbaPage",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 276
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2507.22025",
      "authors": [
        {
          "_id": "6895adf548b0ae5ca2710e71",
          "user": {
            "_id": "669f53549a21428ccda89fab",
            "avatarUrl": "/avatars/957c8251615ab4552f2e286ef7445c58.svg",
            "isPro": false,
            "fullname": "LianShuQuan",
            "user": "LianShuQuan",
            "type": "user"
          },
          "name": "Shuquan Lian",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-08T16:19:02.176Z",
          "hidden": false
        },
        {
          "_id": "6895adf548b0ae5ca2710e72",
          "name": "Yuhang Wu",
          "hidden": false
        },
        {
          "_id": "6895adf548b0ae5ca2710e73",
          "name": "Jia Ma",
          "hidden": false
        },
        {
          "_id": "6895adf548b0ae5ca2710e74",
          "name": "Zihan Song",
          "hidden": false
        },
        {
          "_id": "6895adf548b0ae5ca2710e75",
          "name": "Bingqi Chen",
          "hidden": false
        },
        {
          "_id": "6895adf548b0ae5ca2710e76",
          "name": "Xiawu Zheng",
          "hidden": false
        },
        {
          "_id": "6895adf548b0ae5ca2710e77",
          "name": "Hui Li",
          "hidden": false
        }
      ],
      "publishedAt": "2025-07-29T17:22:07.000Z",
      "submittedOnDailyAt": "2025-08-11T03:39:00.218Z",
      "title": "UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and\n  Precise Inference-Time Grounding",
      "submittedOnDailyBy": {
        "_id": "669f53549a21428ccda89fab",
        "avatarUrl": "/avatars/957c8251615ab4552f2e286ef7445c58.svg",
        "isPro": false,
        "fullname": "LianShuQuan",
        "user": "LianShuQuan",
        "type": "user"
      },
      "summary": "The emergence of Multimodal Large Language Models (MLLMs) has driven\nsignificant advances in Graphical User Interface (GUI) agent capabilities.\nNevertheless, existing GUI agent training and inference techniques still suffer\nfrom a dilemma for reasoning designs, ineffective reward, and visual noise. To\naddress these issues, we introduce UI-AGILE, a comprehensive framework\nenhancing GUI agents at both the training and inference stages. For training,\nwe propose a suite of improvements to the Supervised Fine-Tuning (SFT) process:\n1) a Continuous Reward function to incentivize high-precision grounding; 2) a\n\"Simple Thinking\" reward to balance planning with speed and grounding accuracy;\nand 3) a Cropping-based Resampling strategy to mitigate the sparse reward\nproblem and improve learning on complex tasks. For inference, we present\nDecomposed Grounding with Selection, a novel method that dramatically improves\ngrounding accuracy on high-resolution displays by breaking the image into\nsmaller, manageable parts. Experiments show that UI-AGILE achieves the\nstate-of-the-art performance on two benchmarks ScreenSpot-Pro and\nScreenSpot-v2. For instance, using both our proposed training and inference\nenhancement methods brings 23% grounding accuracy improvement over the best\nbaseline on ScreenSpot-Pro.",
      "upvotes": 2,
      "discussionId": "6895adf648b0ae5ca2710e78",
      "ai_summary": "UI-AGILE enhances GUI agents through improved training with a Continuous Reward function, Simple Thinking reward, and Cropping-based Resampling, and inference with Decomposed Grounding with Selection, achieving state-of-the-art performance on GUI benchmarks.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "GUI agents",
        "Supervised Fine-Tuning",
        "Continuous Reward function",
        "Simple Thinking reward",
        "Cropping-based Resampling",
        "Decomposed Grounding with Selection",
        "ScreenSpot-Pro",
        "ScreenSpot-v2"
      ]
    },
    "publishedAt": "2025-07-29T13:22:07.000Z",
    "title": "UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and\n  Precise Inference-Time Grounding",
    "summary": "The emergence of Multimodal Large Language Models (MLLMs) has driven\nsignificant advances in Graphical User Interface (GUI) agent capabilities.\nNevertheless, existing GUI agent training and inference techniques still suffer\nfrom a dilemma for reasoning designs, ineffective reward, and visual noise. To\naddress these issues, we introduce UI-AGILE, a comprehensive framework\nenhancing GUI agents at both the training and inference stages. For training,\nwe propose a suite of improvements to the Supervised Fine-Tuning (SFT) process:\n1) a Continuous Reward function to incentivize high-precision grounding; 2) a\n\"Simple Thinking\" reward to balance planning with speed and grounding accuracy;\nand 3) a Cropping-based Resampling strategy to mitigate the sparse reward\nproblem and improve learning on complex tasks. For inference, we present\nDecomposed Grounding with Selection, a novel method that dramatically improves\ngrounding accuracy on high-resolution displays by breaking the image into\nsmaller, manageable parts. Experiments show that UI-AGILE achieves the\nstate-of-the-art performance on two benchmarks ScreenSpot-Pro and\nScreenSpot-v2. For instance, using both our proposed training and inference\nenhancement methods brings 23% grounding accuracy improvement over the best\nbaseline on ScreenSpot-Pro.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.22025.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "669f53549a21428ccda89fab",
      "avatarUrl": "/avatars/957c8251615ab4552f2e286ef7445c58.svg",
      "fullname": "LianShuQuan",
      "name": "LianShuQuan",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.05547",
      "authors": [
        {
          "_id": "689979a7f022d141f5d4360e",
          "name": "Hao Dong",
          "hidden": false
        },
        {
          "_id": "689979a7f022d141f5d4360f",
          "name": "Lijun Sheng",
          "hidden": false
        },
        {
          "_id": "689979a7f022d141f5d43610",
          "name": "Jian Liang",
          "hidden": false
        },
        {
          "_id": "689979a7f022d141f5d43611",
          "name": "Ran He",
          "hidden": false
        },
        {
          "_id": "689979a7f022d141f5d43612",
          "name": "Eleni Chatzi",
          "hidden": false
        },
        {
          "_id": "689979a7f022d141f5d43613",
          "name": "Olga Fink",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-07T16:27:37.000Z",
      "submittedOnDailyAt": "2025-08-11T03:35:44.570Z",
      "title": "Adapting Vision-Language Models Without Labels: A Comprehensive Survey",
      "submittedOnDailyBy": {
        "_id": "6649fb62a460da1da20f66d0",
        "avatarUrl": "/avatars/6afa26922ba8abe8c9603e6f7222531b.svg",
        "isPro": false,
        "fullname": "Hao Dong",
        "user": "hdong51",
        "type": "user"
      },
      "summary": "Vision-Language Models (VLMs) have demonstrated remarkable generalization\ncapabilities across a wide range of tasks. However, their performance often\nremains suboptimal when directly applied to specific downstream scenarios\nwithout task-specific adaptation. To enhance their utility while preserving\ndata efficiency, recent research has increasingly focused on unsupervised\nadaptation methods that do not rely on labeled data. Despite the growing\ninterest in this area, there remains a lack of a unified, task-oriented survey\ndedicated to unsupervised VLM adaptation. To bridge this gap, we present a\ncomprehensive and structured overview of the field. We propose a taxonomy based\non the availability and nature of unlabeled visual data, categorizing existing\napproaches into four key paradigms: Data-Free Transfer (no data), Unsupervised\nDomain Transfer (abundant data), Episodic Test-Time Adaptation (batch data),\nand Online Test-Time Adaptation (streaming data). Within this framework, we\nanalyze core methodologies and adaptation strategies associated with each\nparadigm, aiming to establish a systematic understanding of the field.\nAdditionally, we review representative benchmarks across diverse applications\nand highlight open challenges and promising directions for future research. An\nactively maintained repository of relevant literature is available at\nhttps://github.com/tim-learn/Awesome-LabelFree-VLMs.",
      "upvotes": 1,
      "discussionId": "689979a8f022d141f5d43614",
      "projectPage": "https://github.com/tim-learn/Awesome-LabelFree-VLMs",
      "githubRepo": "https://github.com/tim-learn/Awesome-LabelFree-VLMs",
      "ai_summary": "A comprehensive survey of unsupervised adaptation methods for Vision-Language Models (VLMs) categorizes approaches based on the availability of unlabeled visual data and discusses methodologies, benchmarks, and future research directions.",
      "ai_keywords": [
        "Vision-Language Models",
        "unsupervised adaptation",
        "Data-Free Transfer",
        "Unsupervised Domain Transfer",
        "Episodic Test-Time Adaptation",
        "Online Test-Time Adaptation"
      ],
      "githubStars": 13
    },
    "publishedAt": "2025-08-07T12:27:37.000Z",
    "title": "Adapting Vision-Language Models Without Labels: A Comprehensive Survey",
    "summary": "Vision-Language Models (VLMs) have demonstrated remarkable generalization\ncapabilities across a wide range of tasks. However, their performance often\nremains suboptimal when directly applied to specific downstream scenarios\nwithout task-specific adaptation. To enhance their utility while preserving\ndata efficiency, recent research has increasingly focused on unsupervised\nadaptation methods that do not rely on labeled data. Despite the growing\ninterest in this area, there remains a lack of a unified, task-oriented survey\ndedicated to unsupervised VLM adaptation. To bridge this gap, we present a\ncomprehensive and structured overview of the field. We propose a taxonomy based\non the availability and nature of unlabeled visual data, categorizing existing\napproaches into four key paradigms: Data-Free Transfer (no data), Unsupervised\nDomain Transfer (abundant data), Episodic Test-Time Adaptation (batch data),\nand Online Test-Time Adaptation (streaming data). Within this framework, we\nanalyze core methodologies and adaptation strategies associated with each\nparadigm, aiming to establish a systematic understanding of the field.\nAdditionally, we review representative benchmarks across diverse applications\nand highlight open challenges and promising directions for future research. An\nactively maintained repository of relevant literature is available at\nhttps://github.com/tim-learn/Awesome-LabelFree-VLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.05547.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6649fb62a460da1da20f66d0",
      "avatarUrl": "/avatars/6afa26922ba8abe8c9603e6f7222531b.svg",
      "fullname": "Hao Dong",
      "name": "hdong51",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.05502",
      "authors": [
        {
          "_id": "68994b7bf022d141f5d434be",
          "name": "Yufei Gao",
          "hidden": false
        },
        {
          "_id": "68994b7bf022d141f5d434bf",
          "name": "Jiaying Fei",
          "hidden": false
        },
        {
          "_id": "68994b7bf022d141f5d434c0",
          "name": "Nuo Chen",
          "hidden": false
        },
        {
          "_id": "68994b7bf022d141f5d434c1",
          "name": "Ruirui Chen",
          "hidden": false
        },
        {
          "_id": "68994b7bf022d141f5d434c2",
          "name": "Guohang Yan",
          "hidden": false
        },
        {
          "_id": "68994b7bf022d141f5d434c3",
          "name": "Yunshi Lan",
          "hidden": false
        },
        {
          "_id": "68994b7bf022d141f5d434c4",
          "name": "Botian Shi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-07T15:36:24.000Z",
      "submittedOnDailyAt": "2025-08-11T00:17:30.049Z",
      "title": "MELLA: Bridging Linguistic Capability and Cultural Groundedness for\n  Low-Resource Language MLLMs",
      "submittedOnDailyBy": {
        "_id": "655469586bc4180700cf7a34",
        "avatarUrl": "/avatars/252392d0c45783d8f149feac7a6215ec.svg",
        "isPro": false,
        "fullname": "Kejia Zhang",
        "user": "KejiaRobust",
        "type": "user"
      },
      "summary": "Multimodal Large Language Models (MLLMs) have shown remarkable performance in\nhigh-resource languages. However, their effectiveness diminishes significantly\nin the contexts of low-resource languages. Current multilingual enhancement\nmethods are often limited to text modality or rely solely on machine\ntranslation. While such approaches help models acquire basic linguistic\ncapabilities and produce \"thin descriptions\", they neglect the importance of\nmultimodal informativeness and cultural groundedness, both of which are crucial\nfor serving low-resource language users effectively. To bridge this gap, in\nthis study, we identify two significant objectives for a truly effective MLLM\nin low-resource language settings, namely 1) linguistic capability and 2)\ncultural groundedness, placing special emphasis on cultural awareness. To\nachieve these dual objectives, we propose a dual-source strategy that guides\nthe collection of data tailored to each goal, sourcing native web alt-text for\nculture and MLLM-generated captions for linguistics. As a concrete\nimplementation, we introduce MELLA, a multimodal, multilingual dataset.\nExperiment results show that after fine-tuning on MELLA, there is a general\nperformance improvement for the eight languages on various MLLM backbones, with\nmodels producing \"thick descriptions\". We verify that the performance gains are\nfrom both cultural knowledge enhancement and linguistic capability enhancement.\nOur dataset can be found at https://opendatalab.com/applyMultilingualCorpus.",
      "upvotes": 1,
      "discussionId": "68994b7bf022d141f5d434c5",
      "ai_summary": "MELLA, a multimodal, multilingual dataset, enhances MLLMs in low-resource languages by improving linguistic capability and cultural groundedness through native web alt-text and MLLM-generated captions.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "MLLMs",
        "low-resource languages",
        "multilingual enhancement",
        "native web alt-text",
        "MLLM-generated captions",
        "cultural awareness",
        "thick descriptions"
      ]
    },
    "publishedAt": "2025-08-07T11:36:24.000Z",
    "title": "MELLA: Bridging Linguistic Capability and Cultural Groundedness for\n  Low-Resource Language MLLMs",
    "summary": "Multimodal Large Language Models (MLLMs) have shown remarkable performance in\nhigh-resource languages. However, their effectiveness diminishes significantly\nin the contexts of low-resource languages. Current multilingual enhancement\nmethods are often limited to text modality or rely solely on machine\ntranslation. While such approaches help models acquire basic linguistic\ncapabilities and produce \"thin descriptions\", they neglect the importance of\nmultimodal informativeness and cultural groundedness, both of which are crucial\nfor serving low-resource language users effectively. To bridge this gap, in\nthis study, we identify two significant objectives for a truly effective MLLM\nin low-resource language settings, namely 1) linguistic capability and 2)\ncultural groundedness, placing special emphasis on cultural awareness. To\nachieve these dual objectives, we propose a dual-source strategy that guides\nthe collection of data tailored to each goal, sourcing native web alt-text for\nculture and MLLM-generated captions for linguistics. As a concrete\nimplementation, we introduce MELLA, a multimodal, multilingual dataset.\nExperiment results show that after fine-tuning on MELLA, there is a general\nperformance improvement for the eight languages on various MLLM backbones, with\nmodels producing \"thick descriptions\". We verify that the performance gains are\nfrom both cultural knowledge enhancement and linguistic capability enhancement.\nOur dataset can be found at https://opendatalab.com/applyMultilingualCorpus.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.05502.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "655469586bc4180700cf7a34",
      "avatarUrl": "/avatars/252392d0c45783d8f149feac7a6215ec.svg",
      "fullname": "Kejia Zhang",
      "name": "KejiaRobust",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.02831",
      "authors": [
        {
          "_id": "689473b7741a16f544fbd06f",
          "user": {
            "_id": "653064c7800ac851b9b6ac29",
            "avatarUrl": "/avatars/f8167661d1c80e8a1bcf409c618d63a0.svg",
            "isPro": false,
            "fullname": "Mikołaj Zieliński",
            "user": "MikolajZ",
            "type": "user"
          },
          "name": "Mikołaj Zieliński",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-07T10:37:26.983Z",
          "hidden": false
        },
        {
          "_id": "689473b7741a16f544fbd070",
          "name": "Krzysztof Byrski",
          "hidden": false
        },
        {
          "_id": "689473b7741a16f544fbd071",
          "name": "Tomasz Szczepanik",
          "hidden": false
        },
        {
          "_id": "689473b7741a16f544fbd072",
          "name": "Przemysław Spurek",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-04T18:59:23.000Z",
      "submittedOnDailyAt": "2025-08-11T05:05:38.669Z",
      "title": "GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing",
      "submittedOnDailyBy": {
        "_id": "653064c7800ac851b9b6ac29",
        "avatarUrl": "/avatars/f8167661d1c80e8a1bcf409c618d63a0.svg",
        "isPro": false,
        "fullname": "Mikołaj Zieliński",
        "user": "MikolajZ",
        "type": "user"
      },
      "summary": "Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) have recently\ntransformed 3D scene representation and rendering. NeRF achieves high-fidelity\nnovel view synthesis by learning volumetric representations through neural\nnetworks, but its implicit encoding makes editing and physical interaction\nchallenging. In contrast, GS represents scenes as explicit collections of\nGaussian primitives, enabling real-time rendering, faster training, and more\nintuitive manipulation. This explicit structure has made GS particularly\nwell-suited for interactive editing and integration with physics-based\nsimulation. In this paper, we introduce GENIE (Gaussian Encoding for Neural\nRadiance Fields Interactive Editing), a hybrid model that combines the\nphotorealistic rendering quality of NeRF with the editable and structured\nrepresentation of GS. Instead of using spherical harmonics for appearance\nmodeling, we assign each Gaussian a trainable feature embedding. These\nembeddings are used to condition a NeRF network based on the k nearest\nGaussians to each query point. To make this conditioning efficient, we\nintroduce Ray-Traced Gaussian Proximity Search (RT-GPS), a fast nearest\nGaussian search based on a modified ray-tracing pipeline. We also integrate a\nmulti-resolution hash grid to initialize and update Gaussian features.\nTogether, these components enable real-time, locality-aware editing: as\nGaussian primitives are repositioned or modified, their interpolated influence\nis immediately reflected in the rendered output. By combining the strengths of\nimplicit and explicit representations, GENIE supports intuitive scene\nmanipulation, dynamic interaction, and compatibility with physical simulation,\nbridging the gap between geometry-based editing and neural rendering. The code\ncan be found under (https://github.com/MikolajZielinski/genie)",
      "upvotes": 1,
      "discussionId": "689473b7741a16f544fbd073",
      "projectPage": "https://mikolajzielinski.github.io/genie.github.io/",
      "githubRepo": "https://github.com/MikolajZielinski/genie",
      "ai_summary": "GENIE combines NeRF's photorealistic rendering with Gaussian Splatting's editable and structured representation, enabling real-time, locality-aware editing and integration with physics-based simulation.",
      "ai_keywords": [
        "Neural Radiance Fields",
        "Gaussian Splatting",
        "volumetric representations",
        "neural networks",
        "implicit encoding",
        "explicit collections",
        "Gaussian primitives",
        "real-time rendering",
        "faster training",
        "intuitive manipulation",
        "interactive editing",
        "physics-based simulation",
        "Gaussian Encoding for Neural Radiance Fields Interactive Editing",
        "feature embedding",
        "NeRF network",
        "Ray-Traced Gaussian Proximity Search",
        "RT-GPS",
        "multi-resolution hash grid",
        "locality-aware editing",
        "scene manipulation",
        "dynamic interaction"
      ],
      "githubStars": 8
    },
    "publishedAt": "2025-08-04T14:59:23.000Z",
    "title": "GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing",
    "summary": "Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) have recently\ntransformed 3D scene representation and rendering. NeRF achieves high-fidelity\nnovel view synthesis by learning volumetric representations through neural\nnetworks, but its implicit encoding makes editing and physical interaction\nchallenging. In contrast, GS represents scenes as explicit collections of\nGaussian primitives, enabling real-time rendering, faster training, and more\nintuitive manipulation. This explicit structure has made GS particularly\nwell-suited for interactive editing and integration with physics-based\nsimulation. In this paper, we introduce GENIE (Gaussian Encoding for Neural\nRadiance Fields Interactive Editing), a hybrid model that combines the\nphotorealistic rendering quality of NeRF with the editable and structured\nrepresentation of GS. Instead of using spherical harmonics for appearance\nmodeling, we assign each Gaussian a trainable feature embedding. These\nembeddings are used to condition a NeRF network based on the k nearest\nGaussians to each query point. To make this conditioning efficient, we\nintroduce Ray-Traced Gaussian Proximity Search (RT-GPS), a fast nearest\nGaussian search based on a modified ray-tracing pipeline. We also integrate a\nmulti-resolution hash grid to initialize and update Gaussian features.\nTogether, these components enable real-time, locality-aware editing: as\nGaussian primitives are repositioned or modified, their interpolated influence\nis immediately reflected in the rendered output. By combining the strengths of\nimplicit and explicit representations, GENIE supports intuitive scene\nmanipulation, dynamic interaction, and compatibility with physical simulation,\nbridging the gap between geometry-based editing and neural rendering. The code\ncan be found under (https://github.com/MikolajZielinski/genie)",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02831.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "653064c7800ac851b9b6ac29",
      "avatarUrl": "/avatars/f8167661d1c80e8a1bcf409c618d63a0.svg",
      "fullname": "Mikołaj Zieliński",
      "name": "MikolajZ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.01242",
      "authors": [
        {
          "_id": "68997c1bf022d141f5d43616",
          "name": "Shuangkang Fang",
          "hidden": false
        },
        {
          "_id": "68997c1bf022d141f5d43617",
          "name": "I-Chao Shen",
          "hidden": false
        },
        {
          "_id": "68997c1bf022d141f5d43618",
          "name": "Yufeng Wang",
          "hidden": false
        },
        {
          "_id": "68997c1bf022d141f5d43619",
          "name": "Yi-Hsuan Tsai",
          "hidden": false
        },
        {
          "_id": "68997c1bf022d141f5d4361a",
          "name": "Yi Yang",
          "hidden": false
        },
        {
          "_id": "68997c1bf022d141f5d4361b",
          "name": "Shuchang Zhou",
          "hidden": false
        },
        {
          "_id": "68997c1bf022d141f5d4361c",
          "name": "Wenrui Ding",
          "hidden": false
        },
        {
          "_id": "68997c1bf022d141f5d4361d",
          "name": "Takeo Igarashi",
          "hidden": false
        },
        {
          "_id": "68997c1bf022d141f5d4361e",
          "name": "Ming-Hsuan Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-02T07:37:37.000Z",
      "submittedOnDailyAt": "2025-08-11T03:44:55.489Z",
      "title": "MeshLLM: Empowering Large Language Models to Progressively Understand\n  and Generate 3D Mesh",
      "submittedOnDailyBy": {
        "_id": "66ee3663518ccb4e62be37c4",
        "avatarUrl": "/avatars/8d423c9bf63af13969300711a9efa870.svg",
        "isPro": false,
        "fullname": "Shuangkang Fang",
        "user": "fsk515",
        "type": "user"
      },
      "summary": "We present MeshLLM, a novel framework that leverages large language models\n(LLMs) to understand and generate text-serialized 3D meshes. Our approach\naddresses key limitations in existing methods, including the limited dataset\nscale when catering to LLMs' token length and the loss of 3D structural\ninformation during mesh serialization. We introduce a Primitive-Mesh\ndecomposition strategy, which divides 3D meshes into structurally meaningful\nsubunits. This enables the creation of a large-scale dataset with 1500k+\nsamples, almost 50 times larger than previous methods, which aligns better with\nthe LLM scaling law principles. Furthermore, we propose inferring face\nconnectivity from vertices and local mesh assembly training strategies,\nsignificantly enhancing the LLMs' ability to capture mesh topology and spatial\nstructures. Experiments show that MeshLLM outperforms the state-of-the-art\nLLaMA-Mesh in both mesh generation quality and shape understanding,\nhighlighting its great potential in processing text-serialized 3D meshes.",
      "upvotes": 1,
      "discussionId": "68997c2bf022d141f5d4361f",
      "ai_summary": "MeshLLM uses large language models to generate and understand text-serialized 3D meshes by decomposing them into meaningful subunits and training with local mesh assembly strategies.",
      "ai_keywords": [
        "MeshLLM",
        "large language models",
        "LLMs",
        "3D meshes",
        "Primitive-Mesh decomposition",
        "face connectivity",
        "local mesh assembly",
        "mesh topology",
        "spatial structures",
        "LLaMA-Mesh"
      ]
    },
    "publishedAt": "2025-08-02T03:37:37.000Z",
    "title": "MeshLLM: Empowering Large Language Models to Progressively Understand\n  and Generate 3D Mesh",
    "summary": "We present MeshLLM, a novel framework that leverages large language models\n(LLMs) to understand and generate text-serialized 3D meshes. Our approach\naddresses key limitations in existing methods, including the limited dataset\nscale when catering to LLMs' token length and the loss of 3D structural\ninformation during mesh serialization. We introduce a Primitive-Mesh\ndecomposition strategy, which divides 3D meshes into structurally meaningful\nsubunits. This enables the creation of a large-scale dataset with 1500k+\nsamples, almost 50 times larger than previous methods, which aligns better with\nthe LLM scaling law principles. Furthermore, we propose inferring face\nconnectivity from vertices and local mesh assembly training strategies,\nsignificantly enhancing the LLMs' ability to capture mesh topology and spatial\nstructures. Experiments show that MeshLLM outperforms the state-of-the-art\nLLaMA-Mesh in both mesh generation quality and shape understanding,\nhighlighting its great potential in processing text-serialized 3D meshes.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01242.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66ee3663518ccb4e62be37c4",
      "avatarUrl": "/avatars/8d423c9bf63af13969300711a9efa870.svg",
      "fullname": "Shuangkang Fang",
      "name": "fsk515",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]