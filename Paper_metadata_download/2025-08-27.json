[
  {
    "paper": {
      "id": "2508.18124",
      "authors": [
        {
          "_id": "68ae7da2364411bea07df7d1",
          "user": {
            "_id": "661b9d96c153e4a0a25adc3e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661b9d96c153e4a0a25adc3e/VRt7kCQ0KdJp-lhPLOajO.jpeg",
            "isPro": false,
            "fullname": "Weida Wang",
            "user": "weidawang",
            "type": "user"
          },
          "name": "Weida Wang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-27T07:10:33.081Z",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7d2",
          "name": "Dongchen Huang",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7d3",
          "name": "Jiatong Li",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7d4",
          "name": "Tengchao Yang",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7d5",
          "name": "Ziyang Zheng",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7d6",
          "user": {
            "_id": "64bce15bafd1e46c5504ad38",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
            "isPro": false,
            "fullname": "Di Zhang",
            "user": "di-zhang-fdu",
            "type": "user"
          },
          "name": "Di Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-27T07:10:30.932Z",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7d7",
          "name": "Dong Han",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7d8",
          "name": "Benteng Chen",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7d9",
          "name": "Binzhao Luo",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7da",
          "name": "Zhiyu Liu",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7db",
          "name": "Kunling Liu",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7dc",
          "name": "Zhiyuan Gao",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7dd",
          "name": "Shiqi Geng",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7de",
          "user": {
            "_id": "689d58bb635465616d75d302",
            "avatarUrl": "/avatars/68797cb28dc87478c791d5152e62c52a.svg",
            "isPro": false,
            "fullname": "Wei Ma",
            "user": "BoringMarsh",
            "type": "user"
          },
          "name": "Wei Ma",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-27T07:10:28.667Z",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7df",
          "name": "Jiaming Su",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7e0",
          "name": "Xin Li",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7e1",
          "name": "Shuchen Pu",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7e2",
          "name": "Yuhan Shui",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7e3",
          "name": "Qianjia Cheng",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7e4",
          "name": "Zhihao Dou",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7e5",
          "name": "Dongfei Cui",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7e6",
          "name": "Changyong He",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7e7",
          "name": "Jin Zeng",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7e8",
          "name": "Zeke Xie",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7e9",
          "name": "Mao Su",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7ea",
          "name": "Dongzhan Zhou",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7eb",
          "name": "Yuqiang Li",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7ec",
          "name": "Wanli Ouyang",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7ed",
          "name": "Yunqi Cai",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7ee",
          "name": "Xi Dai",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7ef",
          "name": "Shufei Zhang",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7f0",
          "name": "Lei Bai",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7f1",
          "name": "Jinguang Cheng",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7f2",
          "name": "Zhong Fang",
          "hidden": false
        },
        {
          "_id": "68ae7da2364411bea07df7f3",
          "name": "Hongming Weng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-25T15:32:22.000Z",
      "submittedOnDailyAt": "2025-08-27T02:15:01.675Z",
      "title": "CMPhysBench: A Benchmark for Evaluating Large Language Models in\n  Condensed Matter Physics",
      "submittedOnDailyBy": {
        "_id": "661b9d96c153e4a0a25adc3e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661b9d96c153e4a0a25adc3e/VRt7kCQ0KdJp-lhPLOajO.jpeg",
        "isPro": false,
        "fullname": "Weida Wang",
        "user": "weidawang",
        "type": "user"
      },
      "summary": "We introduce CMPhysBench, designed to assess the proficiency of Large\nLanguage Models (LLMs) in Condensed Matter Physics, as a novel Benchmark.\nCMPhysBench is composed of more than 520 graduate-level meticulously curated\nquestions covering both representative subfields and foundational theoretical\nframeworks of condensed matter physics, such as magnetism, superconductivity,\nstrongly correlated systems, etc. To ensure a deep understanding of the\nproblem-solving process,we focus exclusively on calculation problems, requiring\nLLMs to independently generate comprehensive solutions. Meanwhile, leveraging\ntree-based representations of expressions, we introduce the Scalable Expression\nEdit Distance (SEED) score, which provides fine-grained (non-binary) partial\ncredit and yields a more accurate assessment of similarity between prediction\nand ground-truth. Our results show that even the best models, Grok-4, reach\nonly 36 average SEED score and 28% accuracy on CMPhysBench, underscoring a\nsignificant capability gap, especially for this practical and frontier domain\nrelative to traditional physics. The code anddataset are publicly available at\nhttps://github.com/CMPhysBench/CMPhysBench.",
      "upvotes": 30,
      "discussionId": "68ae7da2364411bea07df7f4",
      "githubRepo": "https://github.com/CMPhysBench/CMPhysBench",
      "ai_summary": "CMPhysBench evaluates LLMs in condensed matter physics using calculation problems and a new SEED score for partial credit assessment, revealing significant capability gaps.",
      "ai_keywords": [
        "Large Language Models (LLMs)",
        "Condensed Matter Physics",
        "Scalable Expression Edit Distance (SEED)",
        "tree-based representations",
        "Grok-4"
      ],
      "githubStars": 10
    },
    "publishedAt": "2025-08-25T11:32:22.000Z",
    "title": "CMPhysBench: A Benchmark for Evaluating Large Language Models in\n  Condensed Matter Physics",
    "summary": "We introduce CMPhysBench, designed to assess the proficiency of Large\nLanguage Models (LLMs) in Condensed Matter Physics, as a novel Benchmark.\nCMPhysBench is composed of more than 520 graduate-level meticulously curated\nquestions covering both representative subfields and foundational theoretical\nframeworks of condensed matter physics, such as magnetism, superconductivity,\nstrongly correlated systems, etc. To ensure a deep understanding of the\nproblem-solving process,we focus exclusively on calculation problems, requiring\nLLMs to independently generate comprehensive solutions. Meanwhile, leveraging\ntree-based representations of expressions, we introduce the Scalable Expression\nEdit Distance (SEED) score, which provides fine-grained (non-binary) partial\ncredit and yields a more accurate assessment of similarity between prediction\nand ground-truth. Our results show that even the best models, Grok-4, reach\nonly 36 average SEED score and 28% accuracy on CMPhysBench, underscoring a\nsignificant capability gap, especially for this practical and frontier domain\nrelative to traditional physics. The code anddataset are publicly available at\nhttps://github.com/CMPhysBench/CMPhysBench.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18124.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "661b9d96c153e4a0a25adc3e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661b9d96c153e4a0a25adc3e/VRt7kCQ0KdJp-lhPLOajO.jpeg",
      "fullname": "Weida Wang",
      "name": "weidawang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 0
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.19205",
      "authors": [
        {
          "_id": "68ae6a0f364411bea07df70f",
          "name": "Zhiliang Peng",
          "hidden": false
        },
        {
          "_id": "68ae6a0f364411bea07df710",
          "name": "Jianwei Yu",
          "hidden": false
        },
        {
          "_id": "68ae6a0f364411bea07df711",
          "name": "Wenhui Wang",
          "hidden": false
        },
        {
          "_id": "68ae6a0f364411bea07df712",
          "name": "Yaoyao Chang",
          "hidden": false
        },
        {
          "_id": "68ae6a0f364411bea07df713",
          "name": "Yutao Sun",
          "hidden": false
        },
        {
          "_id": "68ae6a0f364411bea07df714",
          "user": {
            "_id": "5df85abada6d0311fd3d5408",
            "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
            "isPro": false,
            "fullname": "Li Dong",
            "user": "unilm",
            "type": "user"
          },
          "name": "Li Dong",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-27T07:10:54.323Z",
          "hidden": false
        },
        {
          "_id": "68ae6a0f364411bea07df715",
          "name": "Yi Zhu",
          "hidden": false
        },
        {
          "_id": "68ae6a0f364411bea07df716",
          "name": "Weijiang Xu",
          "hidden": false
        },
        {
          "_id": "68ae6a0f364411bea07df717",
          "name": "Hangbo Bao",
          "hidden": false
        },
        {
          "_id": "68ae6a0f364411bea07df718",
          "name": "Zehua Wang",
          "hidden": false
        },
        {
          "_id": "68ae6a0f364411bea07df719",
          "name": "Shaohan Huang",
          "hidden": false
        },
        {
          "_id": "68ae6a0f364411bea07df71a",
          "name": "Yan Xia",
          "hidden": false
        },
        {
          "_id": "68ae6a0f364411bea07df71b",
          "name": "Furu Wei",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/5df85abada6d0311fd3d5408/6tNh2DlU1e_nu7eznNjy1.png"
      ],
      "publishedAt": "2025-08-26T17:09:12.000Z",
      "submittedOnDailyAt": "2025-08-27T00:47:28.459Z",
      "title": "VibeVoice Technical Report",
      "submittedOnDailyBy": {
        "_id": "5df85abada6d0311fd3d5408",
        "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
        "isPro": false,
        "fullname": "Li Dong",
        "user": "unilm",
        "type": "user"
      },
      "summary": "This report presents VibeVoice, a novel model designed to synthesize\nlong-form speech with multiple speakers by employing next-token diffusion,\nwhich is a unified method for modeling continuous data by autoregressively\ngenerating latent vectors via diffusion. To enable this, we introduce a novel\ncontinuous speech tokenizer that, when compared to the popular Encodec model,\nimproves data compression by 80 times while maintaining comparable performance.\nThe tokenizer effectively preserves audio fidelity while significantly boosting\ncomputational efficiency for processing long sequences. Thus, VibeVoice can\nsynthesize long-form speech for up to 90 minutes (in a 64K context window\nlength) with a maximum of 4 speakers, capturing the authentic conversational\n``vibe'' and surpassing open-source and proprietary dialogue models.",
      "upvotes": 25,
      "discussionId": "68ae6a0f364411bea07df71c",
      "projectPage": "https://microsoft.github.io/VibeVoice/",
      "githubRepo": "https://github.com/microsoft/VibeVoice",
      "ai_summary": "VibeVoice synthesizes long-form multi-speaker speech using next-token diffusion and a highly efficient continuous speech tokenizer, achieving superior performance and fidelity.",
      "ai_keywords": [
        "next-token diffusion",
        "continuous speech tokenizer",
        "Encodec",
        "audio fidelity",
        "computational efficiency",
        "long-form speech",
        "multi-speaker synthesis",
        "conversational vibe",
        "dialogue models"
      ],
      "githubStars": 1982
    },
    "publishedAt": "2025-08-26T13:09:12.000Z",
    "title": "VibeVoice Technical Report",
    "summary": "This report presents VibeVoice, a novel model designed to synthesize\nlong-form speech with multiple speakers by employing next-token diffusion,\nwhich is a unified method for modeling continuous data by autoregressively\ngenerating latent vectors via diffusion. To enable this, we introduce a novel\ncontinuous speech tokenizer that, when compared to the popular Encodec model,\nimproves data compression by 80 times while maintaining comparable performance.\nThe tokenizer effectively preserves audio fidelity while significantly boosting\ncomputational efficiency for processing long sequences. Thus, VibeVoice can\nsynthesize long-form speech for up to 90 minutes (in a 64K context window\nlength) with a maximum of 4 speakers, capturing the authentic conversational\n``vibe'' and surpassing open-source and proprietary dialogue models.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/5df85abada6d0311fd3d5408/6tNh2DlU1e_nu7eznNjy1.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.19205.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5df85abada6d0311fd3d5408",
      "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg",
      "fullname": "Li Dong",
      "name": "unilm",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 36
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.17661",
      "authors": [
        {
          "_id": "68ad25ab86b21a0e2e358dbe",
          "user": {
            "_id": "68ad3865c08be6c3fc6d3eda",
            "avatarUrl": "/avatars/2b9850c4af1ef08b1ee8a72a32cfaf5a.svg",
            "isPro": false,
            "fullname": "Minhyeong Lee",
            "user": "mhlee1022",
            "type": "user"
          },
          "name": "Minhyeong Lee",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-26T09:49:42.549Z",
          "hidden": false
        },
        {
          "_id": "68ad25ab86b21a0e2e358dbf",
          "name": "Suyoung Hwang",
          "hidden": false
        },
        {
          "_id": "68ad25ab86b21a0e2e358dc0",
          "user": {
            "_id": "683802887db23fa7271ac77d",
            "avatarUrl": "/avatars/c5acf9e2bc039b9d78429b0d1f34575d.svg",
            "isPro": false,
            "fullname": "Seunghyun Moon",
            "user": "MoonRainy21",
            "type": "user"
          },
          "name": "Seunghyun Moon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-26T09:49:46.298Z",
          "hidden": false
        },
        {
          "_id": "68ad25ab86b21a0e2e358dc1",
          "user": {
            "_id": "68ad28861ef76caa8a336f4a",
            "avatarUrl": "/avatars/e8d7aac8fd592eaf25968124de4a4dfa.svg",
            "isPro": false,
            "fullname": "Geonho Nah",
            "user": "rallyduck1005",
            "type": "user"
          },
          "name": "Geonho Nah",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-27T07:11:18.807Z",
          "hidden": false
        },
        {
          "_id": "68ad25ab86b21a0e2e358dc2",
          "user": {
            "_id": "656184b8738013bf302cc8ba",
            "avatarUrl": "/avatars/9a97be3ff98b77ab0008d1b7a46317eb.svg",
            "isPro": false,
            "fullname": "Donghyun Koh",
            "user": "kohandy",
            "type": "user"
          },
          "name": "Donghyun Koh",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-26T09:49:56.446Z",
          "hidden": false
        },
        {
          "_id": "68ad25ab86b21a0e2e358dc3",
          "user": {
            "_id": "68ad266bb9f6659790765107",
            "avatarUrl": "/avatars/9370a89b717bf2124d3d0795032de474.svg",
            "isPro": false,
            "fullname": "Youngjun Cho",
            "user": "zerojun48",
            "type": "user"
          },
          "name": "Youngjun Cho",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-26T09:49:35.313Z",
          "hidden": false
        },
        {
          "_id": "68ad25ab86b21a0e2e358dc4",
          "user": {
            "_id": "68ad26522166e79cec22a0de",
            "avatarUrl": "/avatars/fc14ed569cb8f1b242ad5d663c7a8689.svg",
            "isPro": false,
            "fullname": "Johyun Park",
            "user": "jstring",
            "type": "user"
          },
          "name": "Johyun Park",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-26T09:49:37.713Z",
          "hidden": false
        },
        {
          "_id": "68ad25ab86b21a0e2e358dc5",
          "user": {
            "_id": "68ad29b78711ce479b260096",
            "avatarUrl": "/avatars/8d2eccb6ee5385032e72f6e1ed20eafa.svg",
            "isPro": false,
            "fullname": "Hojin Yoo",
            "user": "smileyhojin",
            "type": "user"
          },
          "name": "Hojin Yoo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-26T09:49:51.088Z",
          "hidden": false
        },
        {
          "_id": "68ad25ab86b21a0e2e358dc6",
          "user": {
            "_id": "644f9b4633ac8f46fa0c5e43",
            "avatarUrl": "/avatars/d641b2ec1bebe449e1b3faa0be280dc4.svg",
            "isPro": false,
            "fullname": "Jiho Park",
            "user": "coinmoles",
            "type": "user"
          },
          "name": "Jiho Park",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-27T07:11:16.743Z",
          "hidden": false
        },
        {
          "_id": "68ad25ab86b21a0e2e358dc7",
          "user": {
            "_id": "654587385cd5692b3a80c780",
            "avatarUrl": "/avatars/6e9e533574651f678ab67f8b98e921dd.svg",
            "isPro": false,
            "fullname": "Haneul Choi",
            "user": "caelum02",
            "type": "user"
          },
          "name": "Haneul Choi",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-26T09:49:48.884Z",
          "hidden": false
        },
        {
          "_id": "68ad25ab86b21a0e2e358dc8",
          "user": {
            "_id": "644b8e407c95629d87da4952",
            "avatarUrl": "/avatars/83f83ec454973cb52454c8c6f6fcafe2.svg",
            "isPro": false,
            "fullname": "Seunghyun Moon",
            "user": "Rainy21",
            "type": "user"
          },
          "name": "Sungbin Moon",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-26T09:49:53.953Z",
          "hidden": false
        },
        {
          "_id": "68ad25ab86b21a0e2e358dc9",
          "name": "Taehoon Hwang",
          "hidden": false
        },
        {
          "_id": "68ad25ab86b21a0e2e358dca",
          "name": "Seungwon Kim",
          "hidden": false
        },
        {
          "_id": "68ad25ab86b21a0e2e358dcb",
          "user": {
            "_id": "66f3f780169b85adf963508d",
            "avatarUrl": "/avatars/495ce69c478c332b5fafe897bf1ee80e.svg",
            "isPro": false,
            "fullname": "Jaeyeong Kim",
            "user": "jy9394",
            "type": "user"
          },
          "name": "Jaeyeong Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-26T09:49:39.585Z",
          "hidden": false
        },
        {
          "_id": "68ad25ab86b21a0e2e358dcc",
          "user": {
            "_id": "68ad27d8321728c884e0643a",
            "avatarUrl": "/avatars/38d00e607604495ba23d8547d0c2669c.svg",
            "isPro": false,
            "fullname": "Seongjun Kim",
            "user": "sungjune222",
            "type": "user"
          },
          "name": "Seongjun Kim",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-26T10:34:43.089Z",
          "hidden": false
        },
        {
          "_id": "68ad25ab86b21a0e2e358dcd",
          "name": "Juneau Jung",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-25T04:49:16.000Z",
      "submittedOnDailyAt": "2025-08-27T00:58:59.804Z",
      "title": "Spacer: Towards Engineered Scientific Inspiration",
      "submittedOnDailyBy": {
        "_id": "683802887db23fa7271ac77d",
        "avatarUrl": "/avatars/c5acf9e2bc039b9d78429b0d1f34575d.svg",
        "isPro": false,
        "fullname": "Seunghyun Moon",
        "user": "MoonRainy21",
        "type": "user"
      },
      "summary": "Recent advances in LLMs have made automated scientific research the next\nfrontline in the path to artificial superintelligence. However, these systems\nare bound either to tasks of narrow scope or the limited creative capabilities\nof LLMs. We propose Spacer, a scientific discovery system that develops\ncreative and factually grounded concepts without external intervention. Spacer\nattempts to achieve this via 'deliberate decontextualization,' an approach that\ndisassembles information into atomic units - keywords - and draws creativity\nfrom unexplored connections between them. Spacer consists of (i) Nuri, an\ninspiration engine that builds keyword sets, and (ii) the Manifesting Pipeline\nthat refines these sets into elaborate scientific statements. Nuri extracts\nnovel, high-potential keyword sets from a keyword graph built with 180,000\nacademic publications in biological fields. The Manifesting Pipeline finds\nlinks between keywords, analyzes their logical structure, validates their\nplausibility, and ultimately drafts original scientific concepts. According to\nour experiments, the evaluation metric of Nuri accurately classifies\nhigh-impact publications with an AUROC score of 0.737. Our Manifesting Pipeline\nalso successfully reconstructs core concepts from the latest top-journal\narticles solely from their keyword sets. An LLM-based scoring system estimates\nthat this reconstruction was sound for over 85% of the cases. Finally, our\nembedding space analysis shows that outputs from Spacer are significantly more\nsimilar to leading publications compared with those from SOTA LLMs.",
      "upvotes": 19,
      "discussionId": "68ad25ab86b21a0e2e358dce",
      "githubRepo": "https://github.com/Asteromorph-corp/Spacer",
      "ai_summary": "Spacer, a scientific discovery system, uses deliberate decontextualization to generate creative and factually grounded scientific concepts from keyword sets, achieving high accuracy and similarity to leading publications.",
      "ai_keywords": [
        "LLMs",
        "scientific discovery system",
        "deliberate decontextualization",
        "keyword sets",
        "inspiration engine",
        "Manifesting Pipeline",
        "keyword graph",
        "logical structure",
        "plausibility",
        "embedding space analysis",
        "SOTA LLMs"
      ],
      "githubStars": 19
    },
    "publishedAt": "2025-08-25T00:49:16.000Z",
    "title": "Spacer: Towards Engineered Scientific Inspiration",
    "summary": "Recent advances in LLMs have made automated scientific research the next\nfrontline in the path to artificial superintelligence. However, these systems\nare bound either to tasks of narrow scope or the limited creative capabilities\nof LLMs. We propose Spacer, a scientific discovery system that develops\ncreative and factually grounded concepts without external intervention. Spacer\nattempts to achieve this via 'deliberate decontextualization,' an approach that\ndisassembles information into atomic units - keywords - and draws creativity\nfrom unexplored connections between them. Spacer consists of (i) Nuri, an\ninspiration engine that builds keyword sets, and (ii) the Manifesting Pipeline\nthat refines these sets into elaborate scientific statements. Nuri extracts\nnovel, high-potential keyword sets from a keyword graph built with 180,000\nacademic publications in biological fields. The Manifesting Pipeline finds\nlinks between keywords, analyzes their logical structure, validates their\nplausibility, and ultimately drafts original scientific concepts. According to\nour experiments, the evaluation metric of Nuri accurately classifies\nhigh-impact publications with an AUROC score of 0.737. Our Manifesting Pipeline\nalso successfully reconstructs core concepts from the latest top-journal\narticles solely from their keyword sets. An LLM-based scoring system estimates\nthat this reconstruction was sound for over 85% of the cases. Finally, our\nembedding space analysis shows that outputs from Spacer are significantly more\nsimilar to leading publications compared with those from SOTA LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.17661.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "683802887db23fa7271ac77d",
      "avatarUrl": "/avatars/c5acf9e2bc039b9d78429b0d1f34575d.svg",
      "fullname": "Seunghyun Moon",
      "name": "MoonRainy21",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.19209",
      "authors": [
        {
          "_id": "68ae7568364411bea07df736",
          "name": "Jianwen Jiang",
          "hidden": false
        },
        {
          "_id": "68ae7568364411bea07df737",
          "name": "Weihong Zeng",
          "hidden": false
        },
        {
          "_id": "68ae7568364411bea07df738",
          "name": "Zerong Zheng",
          "hidden": false
        },
        {
          "_id": "68ae7568364411bea07df739",
          "name": "Jiaqi Yang",
          "hidden": false
        },
        {
          "_id": "68ae7568364411bea07df73a",
          "user": {
            "_id": "66d7ca4c0429a62c38c46bf5",
            "avatarUrl": "/avatars/45c7859627ac4be70c40f5b1fd02b18a.svg",
            "isPro": false,
            "fullname": "liangdebugger",
            "user": "chao0412",
            "type": "user"
          },
          "name": "Chao Liang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-27T07:10:50.020Z",
          "hidden": false
        },
        {
          "_id": "68ae7568364411bea07df73b",
          "name": "Wang Liao",
          "hidden": false
        },
        {
          "_id": "68ae7568364411bea07df73c",
          "name": "Han Liang",
          "hidden": false
        },
        {
          "_id": "68ae7568364411bea07df73d",
          "name": "Yuan Zhang",
          "hidden": false
        },
        {
          "_id": "68ae7568364411bea07df73e",
          "name": "Mingyuan Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-26T17:15:26.000Z",
      "submittedOnDailyAt": "2025-08-27T01:33:19.940Z",
      "title": "OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive\n  Simulation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Existing video avatar models can produce fluid human animations, yet they\nstruggle to move beyond mere physical likeness to capture a character's\nauthentic essence. Their motions typically synchronize with low-level cues like\naudio rhythm, lacking a deeper semantic understanding of emotion, intent, or\ncontext. To bridge this gap, we propose a framework designed to\ngenerate character animations that are not only physically plausible but also\nsemantically coherent and expressive. Our model, OmniHuman-1.5, is\nbuilt upon two key technical contributions. First, we leverage Multimodal Large\nLanguage Models to synthesize a structured textual representation of conditions\nthat provides high-level semantic guidance. This guidance steers our motion\ngenerator beyond simplistic rhythmic synchronization, enabling the production\nof actions that are contextually and emotionally resonant. Second, to ensure\nthe effective fusion of these multimodal inputs and mitigate inter-modality\nconflicts, we introduce a specialized Multimodal DiT architecture with a novel\nPseudo Last Frame design. The synergy of these components allows our model to\naccurately interpret the joint semantics of audio, images, and text, thereby\ngenerating motions that are deeply coherent with the character, scene, and\nlinguistic content. Extensive experiments demonstrate that our model achieves\nleading performance across a comprehensive set of metrics, including lip-sync\naccuracy, video quality, motion naturalness and semantic consistency with\ntextual prompts. Furthermore, our approach shows remarkable extensibility to\ncomplex scenarios, such as those involving multi-person and non-human subjects.\nHomepage: https://omnihuman-lab.github.io/v1_5/",
      "upvotes": 17,
      "discussionId": "68ae7568364411bea07df73f",
      "projectPage": "https://omnihuman-lab.github.io/v1_5/",
      "ai_summary": "A framework using Multimodal Large Language Models and a specialized Multimodal DiT architecture generates semantically coherent and expressive character animations from multimodal inputs.",
      "ai_keywords": [
        "Multimodal Large Language Models",
        "Multimodal DiT architecture",
        "Pseudo Last Frame design",
        "semantic coherence",
        "multimodal fusion",
        "inter-modality conflicts",
        "lip-sync accuracy",
        "video quality",
        "motion naturalness",
        "semantic consistency"
      ]
    },
    "publishedAt": "2025-08-26T13:15:26.000Z",
    "title": "OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive\n  Simulation",
    "summary": "Existing video avatar models can produce fluid human animations, yet they\nstruggle to move beyond mere physical likeness to capture a character's\nauthentic essence. Their motions typically synchronize with low-level cues like\naudio rhythm, lacking a deeper semantic understanding of emotion, intent, or\ncontext. To bridge this gap, we propose a framework designed to\ngenerate character animations that are not only physically plausible but also\nsemantically coherent and expressive. Our model, OmniHuman-1.5, is\nbuilt upon two key technical contributions. First, we leverage Multimodal Large\nLanguage Models to synthesize a structured textual representation of conditions\nthat provides high-level semantic guidance. This guidance steers our motion\ngenerator beyond simplistic rhythmic synchronization, enabling the production\nof actions that are contextually and emotionally resonant. Second, to ensure\nthe effective fusion of these multimodal inputs and mitigate inter-modality\nconflicts, we introduce a specialized Multimodal DiT architecture with a novel\nPseudo Last Frame design. The synergy of these components allows our model to\naccurately interpret the joint semantics of audio, images, and text, thereby\ngenerating motions that are deeply coherent with the character, scene, and\nlinguistic content. Extensive experiments demonstrate that our model achieves\nleading performance across a comprehensive set of metrics, including lip-sync\naccuracy, video quality, motion naturalness and semantic consistency with\ntextual prompts. Furthermore, our approach shows remarkable extensibility to\ncomplex scenarios, such as those involving multi-person and non-human subjects.\nHomepage: https://omnihuman-lab.github.io/v1_5/",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.19209.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 95
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.18756",
      "authors": [
        {
          "_id": "68ae770e364411bea07df75d",
          "user": {
            "_id": "65a62085576772f531e13856",
            "avatarUrl": "/avatars/72c67a60422e333ea4e323f7480ae0b7.svg",
            "isPro": false,
            "fullname": "Huang Zihao",
            "user": "FetchFortune",
            "type": "user"
          },
          "name": "Zihao Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-27T07:10:44.445Z",
          "hidden": false
        },
        {
          "_id": "68ae770e364411bea07df75e",
          "name": "Yu Bao",
          "hidden": false
        },
        {
          "_id": "68ae770e364411bea07df75f",
          "name": "Qiyang Min",
          "hidden": false
        },
        {
          "_id": "68ae770e364411bea07df760",
          "name": "Siyan Chen",
          "hidden": false
        },
        {
          "_id": "68ae770e364411bea07df761",
          "name": "Ran Guo",
          "hidden": false
        },
        {
          "_id": "68ae770e364411bea07df762",
          "name": "Hongzhi Huang",
          "hidden": false
        },
        {
          "_id": "68ae770e364411bea07df763",
          "name": "Defa Zhu",
          "hidden": false
        },
        {
          "_id": "68ae770e364411bea07df764",
          "name": "Yutao Zeng",
          "hidden": false
        },
        {
          "_id": "68ae770e364411bea07df765",
          "name": "Banggu Wu",
          "hidden": false
        },
        {
          "_id": "68ae770e364411bea07df766",
          "name": "Xun Zhou",
          "hidden": false
        },
        {
          "_id": "68ae770e364411bea07df767",
          "name": "Siyuan Qiao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-26T07:33:11.000Z",
      "submittedOnDailyAt": "2025-08-27T01:40:15.570Z",
      "title": "UltraMemV2: Memory Networks Scaling to 120B Parameters with Superior\n  Long-Context Learning",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "While Mixture of Experts (MoE) models achieve remarkable efficiency by\nactivating only subsets of parameters, they suffer from high memory access\ncosts during inference. Memory-layer architectures offer an appealing\nalternative with very few memory access, but previous attempts like UltraMem\nhave only matched the performance of 2-expert MoE models, falling significantly\nshort of state-of-the-art 8-expert configurations. We present UltraMemV2, a\nredesigned memory-layer architecture that closes this performance gap. Our\napproach introduces five key improvements: integrating memory layers into every\ntransformer block, simplifying value expansion with single linear projections,\nadopting FFN-based value processing from PEER, implementing principled\nparameter initialization, and rebalancing memory-to-FFN computation ratios.\nThrough extensive evaluation, we demonstrate that UltraMemV2 achieves\nperformance parity with 8-expert MoE models under same computation and\nparameters but significantly low memory access. Notably, UltraMemV2 shows\nsuperior performance on memory-intensive tasks, with improvements of +1.6\npoints on long-context memorization, +6.2 points on multi-round memorization,\nand +7.9 points on in-context learning. We validate our approach at scale with\nmodels up to 2.5B activated parameters from 120B total parameters, and\nestablish that activation density has greater impact on performance than total\nsparse parameter count. Our work brings memory-layer architectures to\nperformance parity with state-of-the-art MoE models, presenting a compelling\nalternative for efficient sparse computation.",
      "upvotes": 16,
      "discussionId": "68ae770f364411bea07df768",
      "githubRepo": "https://github.com/ZihaoHuang-notabot/Ultra-Sparse-Memory-Network",
      "ai_summary": "UltraMemV2, a redesigned memory-layer architecture, achieves performance parity with 8-expert MoE models while significantly reducing memory access costs.",
      "ai_keywords": [
        "Mixture of Experts",
        "MoE",
        "memory-layer architectures",
        "UltraMem",
        "UltraMemV2",
        "transformer block",
        "value expansion",
        "FFN-based value processing",
        "PEER",
        "parameter initialization",
        "memory-to-FFN computation ratios",
        "long-context memorization",
        "multi-round memorization",
        "in-context learning",
        "activation density",
        "sparse computation"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-08-26T03:33:11.000Z",
    "title": "UltraMemV2: Memory Networks Scaling to 120B Parameters with Superior\n  Long-Context Learning",
    "summary": "While Mixture of Experts (MoE) models achieve remarkable efficiency by\nactivating only subsets of parameters, they suffer from high memory access\ncosts during inference. Memory-layer architectures offer an appealing\nalternative with very few memory access, but previous attempts like UltraMem\nhave only matched the performance of 2-expert MoE models, falling significantly\nshort of state-of-the-art 8-expert configurations. We present UltraMemV2, a\nredesigned memory-layer architecture that closes this performance gap. Our\napproach introduces five key improvements: integrating memory layers into every\ntransformer block, simplifying value expansion with single linear projections,\nadopting FFN-based value processing from PEER, implementing principled\nparameter initialization, and rebalancing memory-to-FFN computation ratios.\nThrough extensive evaluation, we demonstrate that UltraMemV2 achieves\nperformance parity with 8-expert MoE models under same computation and\nparameters but significantly low memory access. Notably, UltraMemV2 shows\nsuperior performance on memory-intensive tasks, with improvements of +1.6\npoints on long-context memorization, +6.2 points on multi-round memorization,\nand +7.9 points on in-context learning. We validate our approach at scale with\nmodels up to 2.5B activated parameters from 120B total parameters, and\nestablish that activation density has greater impact on performance than total\nsparse parameter count. Our work brings memory-layer architectures to\nperformance parity with state-of-the-art MoE models, presenting a compelling\nalternative for efficient sparse computation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18756.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 95
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.17445",
      "authors": [
        {
          "_id": "68ae7d1a364411bea07df7be",
          "user": {
            "_id": "6382252f54421460665ec501",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6382252f54421460665ec501/gW9fev3T5QPcNq4f9hqB1.jpeg",
            "isPro": false,
            "fullname": "Yizhi Li",
            "user": "yizhilll",
            "type": "user"
          },
          "name": "Yizhi Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-27T07:10:35.933Z",
          "hidden": false
        },
        {
          "_id": "68ae7d1a364411bea07df7bf",
          "name": "Qingshui Gu",
          "hidden": false
        },
        {
          "_id": "68ae7d1a364411bea07df7c0",
          "name": "Zhoufutu Wen",
          "hidden": false
        },
        {
          "_id": "68ae7d1a364411bea07df7c1",
          "name": "Ziniu Li",
          "hidden": false
        },
        {
          "_id": "68ae7d1a364411bea07df7c2",
          "name": "Tianshun Xing",
          "hidden": false
        },
        {
          "_id": "68ae7d1a364411bea07df7c3",
          "name": "Shuyue Guo",
          "hidden": false
        },
        {
          "_id": "68ae7d1a364411bea07df7c4",
          "user": {
            "_id": "64ab99dcb76bfd863eba64c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ab99dcb76bfd863eba64c1/UBXwDPx17X-gl-SzBPvrc.jpeg",
            "isPro": false,
            "fullname": "TY.Zheng",
            "user": "aaabiao",
            "type": "user"
          },
          "name": "Tianyu Zheng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-27T07:10:39.951Z",
          "hidden": false
        },
        {
          "_id": "68ae7d1a364411bea07df7c5",
          "name": "Xin Zhou",
          "hidden": false
        },
        {
          "_id": "68ae7d1a364411bea07df7c6",
          "name": "Xingwei Qu",
          "hidden": false
        },
        {
          "_id": "68ae7d1a364411bea07df7c7",
          "name": "Wangchunshu Zhou",
          "hidden": false
        },
        {
          "_id": "68ae7d1a364411bea07df7c8",
          "name": "Zheng Zhang",
          "hidden": false
        },
        {
          "_id": "68ae7d1a364411bea07df7c9",
          "name": "Wei Shen",
          "hidden": false
        },
        {
          "_id": "68ae7d1a364411bea07df7ca",
          "name": "Qian Liu",
          "hidden": false
        },
        {
          "_id": "68ae7d1a364411bea07df7cb",
          "name": "Chenghua Lin",
          "hidden": false
        },
        {
          "_id": "68ae7d1a364411bea07df7cc",
          "name": "Jian Yang",
          "hidden": false
        },
        {
          "_id": "68ae7d1a364411bea07df7cd",
          "user": {
            "_id": "638efcf4c67af472d316d424",
            "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
            "isPro": false,
            "fullname": "Ge Zhang",
            "user": "zhangysk",
            "type": "user"
          },
          "name": "Ge Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-27T07:10:37.920Z",
          "hidden": false
        },
        {
          "_id": "68ae7d1a364411bea07df7ce",
          "name": "Wenhao Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-24T16:52:37.000Z",
      "submittedOnDailyAt": "2025-08-27T02:08:57.110Z",
      "title": "TreePO: Bridging the Gap of Policy Optimization and Efficacy and\n  Inference Efficiency with Heuristic Tree-based Modeling",
      "submittedOnDailyBy": {
        "_id": "638efcf4c67af472d316d424",
        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
        "isPro": false,
        "fullname": "Ge Zhang",
        "user": "zhangysk",
        "type": "user"
      },
      "summary": "Recent advancements in aligning large language models via reinforcement\nlearning have achieved remarkable gains in solving complex reasoning problems,\nbut at the cost of expensive on-policy rollouts and limited exploration of\ndiverse reasoning paths. In this work, we introduce TreePO, involving a\nself-guided rollout algorithm that views sequence generation as a\ntree-structured searching process. Composed of dynamic tree sampling policy and\nfixed-length segment decoding, TreePO leverages local uncertainty to warrant\nadditional branches. By amortizing computation across common prefixes and\npruning low-value paths early, TreePO essentially reduces the per-update\ncompute burden while preserving or enhancing exploration diversity. Key\ncontributions include: (1) a segment-wise sampling algorithm that alleviates\nthe KV cache burden through contiguous segments and spawns new branches along\nwith an early-stop mechanism; (2) a tree-based segment-level advantage\nestimation that considers both global and local proximal policy optimization.\nand (3) analysis on the effectiveness of probability and quality-driven dynamic\ndivergence and fallback strategy. We empirically validate the performance gain\nof TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours\nfrom 22\\% up to 43\\% of the sampling design for the trained models, meanwhile\nshowing up to 40\\% reduction at trajectory-level and 35\\% at token-level\nsampling compute for the existing models. While offering a free lunch of\ninference efficiency, TreePO reveals a practical path toward scaling RL-based\npost-training with fewer samples and less compute. Home page locates at\nhttps://m-a-p.ai/TreePO.",
      "upvotes": 14,
      "discussionId": "68ae7d1a364411bea07df7cf",
      "ai_summary": "TreePO, a self-guided rollout algorithm for sequence generation, reduces computational cost and enhances exploration diversity in reinforcement learning for large language models.",
      "ai_keywords": [
        "reinforcement learning",
        "sequence generation",
        "tree-structured searching",
        "dynamic tree sampling policy",
        "fixed-length segment decoding",
        "local uncertainty",
        "computation amortization",
        "low-value path pruning",
        "segment-wise sampling",
        "KV cache burden",
        "tree-based segment-level advantage estimation",
        "proximal policy optimization",
        "probability-driven dynamic divergence",
        "quality-driven fallback strategy",
        "trajectory-level sampling",
        "token-level sampling"
      ]
    },
    "publishedAt": "2025-08-24T12:52:37.000Z",
    "title": "TreePO: Bridging the Gap of Policy Optimization and Efficacy and\n  Inference Efficiency with Heuristic Tree-based Modeling",
    "summary": "Recent advancements in aligning large language models via reinforcement\nlearning have achieved remarkable gains in solving complex reasoning problems,\nbut at the cost of expensive on-policy rollouts and limited exploration of\ndiverse reasoning paths. In this work, we introduce TreePO, involving a\nself-guided rollout algorithm that views sequence generation as a\ntree-structured searching process. Composed of dynamic tree sampling policy and\nfixed-length segment decoding, TreePO leverages local uncertainty to warrant\nadditional branches. By amortizing computation across common prefixes and\npruning low-value paths early, TreePO essentially reduces the per-update\ncompute burden while preserving or enhancing exploration diversity. Key\ncontributions include: (1) a segment-wise sampling algorithm that alleviates\nthe KV cache burden through contiguous segments and spawns new branches along\nwith an early-stop mechanism; (2) a tree-based segment-level advantage\nestimation that considers both global and local proximal policy optimization.\nand (3) analysis on the effectiveness of probability and quality-driven dynamic\ndivergence and fallback strategy. We empirically validate the performance gain\nof TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours\nfrom 22\\% up to 43\\% of the sampling design for the trained models, meanwhile\nshowing up to 40\\% reduction at trajectory-level and 35\\% at token-level\nsampling compute for the existing models. While offering a free lunch of\ninference efficiency, TreePO reveals a practical path toward scaling RL-based\npost-training with fewer samples and less compute. Home page locates at\nhttps://m-a-p.ai/TreePO.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.17445.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "638efcf4c67af472d316d424",
      "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
      "fullname": "Ge Zhang",
      "name": "zhangysk",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 54
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.19247",
      "authors": [
        {
          "_id": "68ae6936364411bea07df706",
          "user": {
            "_id": "677c938c7d036e49073e2989",
            "avatarUrl": "/avatars/6f308ddbbd8c8eba8ecffbc28f4c941b.svg",
            "isPro": false,
            "fullname": "Li Lin",
            "user": "Nelipot",
            "type": "user"
          },
          "name": "Lin Li",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-27T07:10:57.843Z",
          "hidden": false
        },
        {
          "_id": "68ae6936364411bea07df707",
          "name": "Zehuan Huang",
          "hidden": false
        },
        {
          "_id": "68ae6936364411bea07df708",
          "user": {
            "_id": "65240d0ca801972b6eb12ed8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65240d0ca801972b6eb12ed8/hl2RAssBperb5JlgOIDvw.jpeg",
            "isPro": false,
            "fullname": "Haoran Feng",
            "user": "fenghora",
            "type": "user"
          },
          "name": "Haoran Feng",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-27T07:11:01.785Z",
          "hidden": false
        },
        {
          "_id": "68ae6936364411bea07df709",
          "name": "Gengxiong Zhuang",
          "hidden": false
        },
        {
          "_id": "68ae6936364411bea07df70a",
          "name": "Rui Chen",
          "hidden": false
        },
        {
          "_id": "68ae6936364411bea07df70b",
          "name": "Chunchao Guo",
          "hidden": false
        },
        {
          "_id": "68ae6936364411bea07df70c",
          "name": "Lu Sheng",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/6375d136dee28348a9c63cbf/xB1n4tiK3oy96ElCeYlbY.mp4"
      ],
      "publishedAt": "2025-08-26T17:59:47.000Z",
      "submittedOnDailyAt": "2025-08-27T00:43:10.461Z",
      "title": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D\n  Space",
      "submittedOnDailyBy": {
        "_id": "6375d136dee28348a9c63cbf",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6375d136dee28348a9c63cbf/gK465HBrQWIOZ-qtHb-Vh.jpeg",
        "isPro": false,
        "fullname": "zehuan-huang",
        "user": "huanngzh",
        "type": "user"
      },
      "summary": "3D local editing of specified regions is crucial for game industry and robot\ninteraction. Recent methods typically edit rendered multi-view images and then\nreconstruct 3D models, but they face challenges in precisely preserving\nunedited regions and overall coherence. Inspired by structured 3D generative\nmodels, we propose VoxHammer, a novel training-free approach that performs\nprecise and coherent editing in 3D latent space. Given a 3D model, VoxHammer\nfirst predicts its inversion trajectory and obtains its inverted latents and\nkey-value tokens at each timestep. Subsequently, in the denoising and editing\nphase, we replace the denoising features of preserved regions with the\ncorresponding inverted latents and cached key-value tokens. By retaining these\ncontextual features, this approach ensures consistent reconstruction of\npreserved areas and coherent integration of edited parts. To evaluate the\nconsistency of preserved regions, we constructed Edit3D-Bench, a\nhuman-annotated dataset comprising hundreds of samples, each with carefully\nlabeled 3D editing regions. Experiments demonstrate that VoxHammer\nsignificantly outperforms existing methods in terms of both 3D consistency of\npreserved regions and overall quality. Our method holds promise for\nsynthesizing high-quality edited paired data, thereby laying the data\nfoundation for in-context 3D generation. See our project page at\nhttps://huanngzh.github.io/VoxHammer-Page/.",
      "upvotes": 9,
      "discussionId": "68ae6936364411bea07df70d",
      "projectPage": "https://huanngzh.github.io/VoxHammer-Page/",
      "githubRepo": "https://github.com/Nelipot-Lee/VoxHammer",
      "ai_summary": "VoxHammer is a training-free method that performs precise and coherent 3D editing in latent space, ensuring consistency in preserved regions and high-quality overall results.",
      "ai_keywords": [
        "structured 3D generative models",
        "VoxHammer",
        "3D latent space",
        "inversion trajectory",
        "inverted latents",
        "key-value tokens",
        "denoising",
        "Edit3D-Bench",
        "3D consistency",
        "in-context 3D generation"
      ],
      "githubStars": 15
    },
    "publishedAt": "2025-08-26T13:59:47.000Z",
    "title": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D\n  Space",
    "summary": "3D local editing of specified regions is crucial for game industry and robot\ninteraction. Recent methods typically edit rendered multi-view images and then\nreconstruct 3D models, but they face challenges in precisely preserving\nunedited regions and overall coherence. Inspired by structured 3D generative\nmodels, we propose VoxHammer, a novel training-free approach that performs\nprecise and coherent editing in 3D latent space. Given a 3D model, VoxHammer\nfirst predicts its inversion trajectory and obtains its inverted latents and\nkey-value tokens at each timestep. Subsequently, in the denoising and editing\nphase, we replace the denoising features of preserved regions with the\ncorresponding inverted latents and cached key-value tokens. By retaining these\ncontextual features, this approach ensures consistent reconstruction of\npreserved areas and coherent integration of edited parts. To evaluate the\nconsistency of preserved regions, we constructed Edit3D-Bench, a\nhuman-annotated dataset comprising hundreds of samples, each with carefully\nlabeled 3D editing regions. Experiments demonstrate that VoxHammer\nsignificantly outperforms existing methods in terms of both 3D consistency of\npreserved regions and overall quality. Our method holds promise for\nsynthesizing high-quality edited paired data, thereby laying the data\nfoundation for in-context 3D generation. See our project page at\nhttps://huanngzh.github.io/VoxHammer-Page/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/6375d136dee28348a9c63cbf/xB1n4tiK3oy96ElCeYlbY.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.19247.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6375d136dee28348a9c63cbf",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6375d136dee28348a9c63cbf/gK465HBrQWIOZ-qtHb-Vh.jpeg",
      "fullname": "zehuan-huang",
      "name": "huanngzh",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 38
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.17437",
      "authors": [
        {
          "_id": "68ae7587364411bea07df741",
          "user": {
            "_id": "664e94ba1b242206a6aa1583",
            "avatarUrl": "/avatars/654b37699ba3f36f479274a2f372e7f8.svg",
            "isPro": false,
            "fullname": "Long Le",
            "user": "vlongle",
            "type": "user"
          },
          "name": "Long Le",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-27T07:10:46.864Z",
          "hidden": false
        },
        {
          "_id": "68ae7587364411bea07df742",
          "name": "Ryan Lucas",
          "hidden": false
        },
        {
          "_id": "68ae7587364411bea07df743",
          "name": "Chen Wang",
          "hidden": false
        },
        {
          "_id": "68ae7587364411bea07df744",
          "name": "Chuhao Chen",
          "hidden": false
        },
        {
          "_id": "68ae7587364411bea07df745",
          "name": "Dinesh Jayaraman",
          "hidden": false
        },
        {
          "_id": "68ae7587364411bea07df746",
          "name": "Eric Eaton",
          "hidden": false
        },
        {
          "_id": "68ae7587364411bea07df747",
          "name": "Lingjie Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/664e94ba1b242206a6aa1583/resJLo3iQcA9Ozg3A2xcA.mp4"
      ],
      "publishedAt": "2025-08-20T19:24:04.000Z",
      "submittedOnDailyAt": "2025-08-27T01:38:23.638Z",
      "title": "Pixie: Fast and Generalizable Supervised Learning of 3D Physics from\n  Pixels",
      "submittedOnDailyBy": {
        "_id": "664e94ba1b242206a6aa1583",
        "avatarUrl": "/avatars/654b37699ba3f36f479274a2f372e7f8.svg",
        "isPro": false,
        "fullname": "Long Le",
        "user": "vlongle",
        "type": "user"
      },
      "summary": "Inferring the physical properties of 3D scenes from visual information is a\ncritical yet challenging task for creating interactive and realistic virtual\nworlds. While humans intuitively grasp material characteristics such as\nelasticity or stiffness, existing methods often rely on slow, per-scene\noptimization, limiting their generalizability and application. To address this\nproblem, we introduce PIXIE, a novel method that trains a generalizable neural\nnetwork to predict physical properties across multiple scenes from 3D visual\nfeatures purely using supervised losses. Once trained, our feed-forward network\ncan perform fast inference of plausible material fields, which coupled with a\nlearned static scene representation like Gaussian Splatting enables realistic\nphysics simulation under external forces. To facilitate this research, we also\ncollected PIXIEVERSE, one of the largest known datasets of paired 3D assets and\nphysic material annotations. Extensive evaluations demonstrate that PIXIE is\nabout 1.46-4.39x better and orders of magnitude faster than test-time\noptimization methods. By leveraging pretrained visual features like CLIP, our\nmethod can also zero-shot generalize to real-world scenes despite only ever\nbeen trained on synthetic data. https://pixie-3d.github.io/",
      "upvotes": 9,
      "discussionId": "68ae7588364411bea07df748",
      "projectPage": "https://pixie-3d.github.io/",
      "githubRepo": "https://github.com/vlongle/pixie",
      "ai_summary": "PIXIE, a neural network method, predicts physical properties of 3D scenes from visual features, enabling fast and realistic physics simulation using supervised learning and pretrained visual features.",
      "ai_keywords": [
        "neural network",
        "material fields",
        "Gaussian Splatting",
        "physics simulation",
        "PIXIEVERSE",
        "CLIP",
        "zero-shot generalization"
      ],
      "githubStars": 17
    },
    "publishedAt": "2025-08-20T15:24:04.000Z",
    "title": "Pixie: Fast and Generalizable Supervised Learning of 3D Physics from\n  Pixels",
    "summary": "Inferring the physical properties of 3D scenes from visual information is a\ncritical yet challenging task for creating interactive and realistic virtual\nworlds. While humans intuitively grasp material characteristics such as\nelasticity or stiffness, existing methods often rely on slow, per-scene\noptimization, limiting their generalizability and application. To address this\nproblem, we introduce PIXIE, a novel method that trains a generalizable neural\nnetwork to predict physical properties across multiple scenes from 3D visual\nfeatures purely using supervised losses. Once trained, our feed-forward network\ncan perform fast inference of plausible material fields, which coupled with a\nlearned static scene representation like Gaussian Splatting enables realistic\nphysics simulation under external forces. To facilitate this research, we also\ncollected PIXIEVERSE, one of the largest known datasets of paired 3D assets and\nphysic material annotations. Extensive evaluations demonstrate that PIXIE is\nabout 1.46-4.39x better and orders of magnitude faster than test-time\noptimization methods. By leveraging pretrained visual features like CLIP, our\nmethod can also zero-shot generalize to real-world scenes despite only ever\nbeen trained on synthetic data. https://pixie-3d.github.io/",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/664e94ba1b242206a6aa1583/resJLo3iQcA9Ozg3A2xcA.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.17437.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "664e94ba1b242206a6aa1583",
      "avatarUrl": "/avatars/654b37699ba3f36f479274a2f372e7f8.svg",
      "fullname": "Long Le",
      "name": "vlongle",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.18621",
      "authors": [
        {
          "_id": "68ae77be364411bea07df771",
          "name": "Xin Gao",
          "hidden": false
        },
        {
          "_id": "68ae77be364411bea07df772",
          "name": "Li Hu",
          "hidden": false
        },
        {
          "_id": "68ae77be364411bea07df773",
          "name": "Siqi Hu",
          "hidden": false
        },
        {
          "_id": "68ae77be364411bea07df774",
          "name": "Mingyang Huang",
          "hidden": false
        },
        {
          "_id": "68ae77be364411bea07df775",
          "name": "Chaonan Ji",
          "hidden": false
        },
        {
          "_id": "68ae77be364411bea07df776",
          "name": "Dechao Meng",
          "hidden": false
        },
        {
          "_id": "68ae77be364411bea07df777",
          "name": "Jinwei Qi",
          "hidden": false
        },
        {
          "_id": "68ae77be364411bea07df778",
          "name": "Penchong Qiao",
          "hidden": false
        },
        {
          "_id": "68ae77be364411bea07df779",
          "name": "Zhen Shen",
          "hidden": false
        },
        {
          "_id": "68ae77be364411bea07df77a",
          "name": "Yafei Song",
          "hidden": false
        },
        {
          "_id": "68ae77be364411bea07df77b",
          "name": "Ke Sun",
          "hidden": false
        },
        {
          "_id": "68ae77be364411bea07df77c",
          "name": "Linrui Tian",
          "hidden": false
        },
        {
          "_id": "68ae77be364411bea07df77d",
          "name": "Guangyuan Wang",
          "hidden": false
        },
        {
          "_id": "68ae77be364411bea07df77e",
          "name": "Qi Wang",
          "hidden": false
        },
        {
          "_id": "68ae77be364411bea07df77f",
          "name": "Zhongjian Wang",
          "hidden": false
        },
        {
          "_id": "68ae77be364411bea07df780",
          "name": "Jiayu Xiao",
          "hidden": false
        },
        {
          "_id": "68ae77be364411bea07df781",
          "name": "Sheng Xu",
          "hidden": false
        },
        {
          "_id": "68ae77be364411bea07df782",
          "name": "Bang Zhang",
          "hidden": false
        },
        {
          "_id": "68ae77be364411bea07df783",
          "name": "Peng Zhang",
          "hidden": false
        },
        {
          "_id": "68ae77be364411bea07df784",
          "name": "Xindi Zhang",
          "hidden": false
        },
        {
          "_id": "68ae77be364411bea07df785",
          "name": "Zhe Zhang",
          "hidden": false
        },
        {
          "_id": "68ae77be364411bea07df786",
          "name": "Jingren Zhou",
          "hidden": false
        },
        {
          "_id": "68ae77be364411bea07df787",
          "name": "Lian Zhuo",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-26T02:51:31.000Z",
      "submittedOnDailyAt": "2025-08-27T01:43:25.737Z",
      "title": "Wan-S2V: Audio-Driven Cinematic Video Generation",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Current state-of-the-art (SOTA) methods for audio-driven character animation\ndemonstrate promising performance for scenarios primarily involving speech and\nsinging. However, they often fall short in more complex film and television\nproductions, which demand sophisticated elements such as nuanced character\ninteractions, realistic body movements, and dynamic camera work. To address\nthis long-standing challenge of achieving film-level character animation, we\npropose an audio-driven model, which we refere to as Wan-S2V, built upon Wan.\nOur model achieves significantly enhanced expressiveness and fidelity in\ncinematic contexts compared to existing approaches. We conducted extensive\nexperiments, benchmarking our method against cutting-edge models such as\nHunyuan-Avatar and Omnihuman. The experimental results consistently demonstrate\nthat our approach significantly outperforms these existing solutions.\nAdditionally, we explore the versatility of our method through its applications\nin long-form video generation and precise video lip-sync editing.",
      "upvotes": 6,
      "discussionId": "68ae77be364411bea07df788",
      "ai_summary": "Wan-S2V, an audio-driven model built on Wan, enhances expressiveness and fidelity in cinematic character animation compared to existing methods.",
      "ai_keywords": [
        "audio-driven model",
        "Wan-S2V",
        "Wan",
        "Hunyuan-Avatar",
        "Omnihuman",
        "long-form video generation",
        "video lip-sync editing"
      ]
    },
    "publishedAt": "2025-08-25T22:51:31.000Z",
    "title": "Wan-S2V: Audio-Driven Cinematic Video Generation",
    "summary": "Current state-of-the-art (SOTA) methods for audio-driven character animation\ndemonstrate promising performance for scenarios primarily involving speech and\nsinging. However, they often fall short in more complex film and television\nproductions, which demand sophisticated elements such as nuanced character\ninteractions, realistic body movements, and dynamic camera work. To address\nthis long-standing challenge of achieving film-level character animation, we\npropose an audio-driven model, which we refere to as Wan-S2V, built upon Wan.\nOur model achieves significantly enhanced expressiveness and fidelity in\ncinematic contexts compared to existing approaches. We conducted extensive\nexperiments, benchmarking our method against cutting-edge models such as\nHunyuan-Avatar and Omnihuman. The experimental results consistently demonstrate\nthat our approach significantly outperforms these existing solutions.\nAdditionally, we explore the versatility of our method through its applications\nin long-form video generation and precise video lip-sync editing.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18621.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 95
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.15774",
      "authors": [
        {
          "_id": "68ae79bd364411bea07df791",
          "name": "Haonan Qiu",
          "hidden": false
        },
        {
          "_id": "68ae79bd364411bea07df792",
          "name": "Ning Yu",
          "hidden": false
        },
        {
          "_id": "68ae79bd364411bea07df793",
          "name": "Ziqi Huang",
          "hidden": false
        },
        {
          "_id": "68ae79bd364411bea07df794",
          "name": "Paul Debevec",
          "hidden": false
        },
        {
          "_id": "68ae79bd364411bea07df795",
          "name": "Ziwei Liu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/63023b6ab002e9a4a2152890/ueiajDYs6w0OhOScln34R.mp4"
      ],
      "publishedAt": "2025-08-21T17:59:57.000Z",
      "submittedOnDailyAt": "2025-08-27T01:55:30.613Z",
      "title": "CineScale: Free Lunch in High-Resolution Cinematic Visual Generation",
      "submittedOnDailyBy": {
        "_id": "63023b6ab002e9a4a2152890",
        "avatarUrl": "/avatars/cae8ba0a8d61fb4e576934431f43991b.svg",
        "isPro": false,
        "fullname": "Haonan Qiu",
        "user": "MoonQiu",
        "type": "user"
      },
      "summary": "Visual diffusion models achieve remarkable progress, yet they are typically\ntrained at limited resolutions due to the lack of high-resolution data and\nconstrained computation resources, hampering their ability to generate\nhigh-fidelity images or videos at higher resolutions. Recent efforts have\nexplored tuning-free strategies to exhibit the untapped potential\nhigher-resolution visual generation of pre-trained models. However, these\nmethods are still prone to producing low-quality visual content with repetitive\npatterns. The key obstacle lies in the inevitable increase in high-frequency\ninformation when the model generates visual content exceeding its training\nresolution, leading to undesirable repetitive patterns deriving from the\naccumulated errors. In this work, we propose CineScale, a novel inference\nparadigm to enable higher-resolution visual generation. To tackle the various\nissues introduced by the two types of video generation architectures, we\npropose dedicated variants tailored to each. Unlike existing baseline methods\nthat are confined to high-resolution T2I and T2V generation, CineScale broadens\nthe scope by enabling high-resolution I2V and V2V synthesis, built atop\nstate-of-the-art open-source video generation frameworks. Extensive experiments\nvalidate the superiority of our paradigm in extending the capabilities of\nhigher-resolution visual generation for both image and video models.\nRemarkably, our approach enables 8k image generation without any fine-tuning,\nand achieves 4k video generation with only minimal LoRA fine-tuning. Generated\nvideo samples are available at our website:\nhttps://eyeline-labs.github.io/CineScale/.",
      "upvotes": 6,
      "discussionId": "68ae79bd364411bea07df796",
      "projectPage": "https://eyeline-labs.github.io/CineScale/",
      "githubRepo": "https://github.com/Eyeline-Labs/CineScale",
      "ai_summary": "CineScale is a novel inference paradigm that enables high-resolution visual generation for both images and videos without extensive fine-tuning, addressing issues of repetitive patterns and high-frequency information accumulation.",
      "ai_keywords": [
        "visual diffusion models",
        "high-resolution visual generation",
        "pre-trained models",
        "high-frequency information",
        "repetitive patterns",
        "inference paradigm",
        "high-resolution T2I",
        "high-resolution T2V",
        "high-resolution I2V",
        "high-resolution V2V synthesis",
        "LoRA fine-tuning"
      ],
      "githubStars": 83
    },
    "publishedAt": "2025-08-21T13:59:57.000Z",
    "title": "CineScale: Free Lunch in High-Resolution Cinematic Visual Generation",
    "summary": "Visual diffusion models achieve remarkable progress, yet they are typically\ntrained at limited resolutions due to the lack of high-resolution data and\nconstrained computation resources, hampering their ability to generate\nhigh-fidelity images or videos at higher resolutions. Recent efforts have\nexplored tuning-free strategies to exhibit the untapped potential\nhigher-resolution visual generation of pre-trained models. However, these\nmethods are still prone to producing low-quality visual content with repetitive\npatterns. The key obstacle lies in the inevitable increase in high-frequency\ninformation when the model generates visual content exceeding its training\nresolution, leading to undesirable repetitive patterns deriving from the\naccumulated errors. In this work, we propose CineScale, a novel inference\nparadigm to enable higher-resolution visual generation. To tackle the various\nissues introduced by the two types of video generation architectures, we\npropose dedicated variants tailored to each. Unlike existing baseline methods\nthat are confined to high-resolution T2I and T2V generation, CineScale broadens\nthe scope by enabling high-resolution I2V and V2V synthesis, built atop\nstate-of-the-art open-source video generation frameworks. Extensive experiments\nvalidate the superiority of our paradigm in extending the capabilities of\nhigher-resolution visual generation for both image and video models.\nRemarkably, our approach enables 8k image generation without any fine-tuning,\nand achieves 4k video generation with only minimal LoRA fine-tuning. Generated\nvideo samples are available at our website:\nhttps://eyeline-labs.github.io/CineScale/.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/63023b6ab002e9a4a2152890/ueiajDYs6w0OhOScln34R.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.15774.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63023b6ab002e9a4a2152890",
      "avatarUrl": "/avatars/cae8ba0a8d61fb4e576934431f43991b.svg",
      "fullname": "Haonan Qiu",
      "name": "MoonQiu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 3
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.19242",
      "authors": [
        {
          "_id": "68ae674d364411bea07df6f2",
          "name": "Miran Heo",
          "hidden": false
        },
        {
          "_id": "68ae674d364411bea07df6f3",
          "name": "Sukjun Hwang",
          "hidden": false
        },
        {
          "_id": "68ae674d364411bea07df6f4",
          "user": {
            "_id": "64ae22dd1aee69ece065cdcd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
            "isPro": false,
            "fullname": "Min-Hung Chen",
            "user": "cmhungsteve",
            "type": "user"
          },
          "name": "Min-Hung Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-27T07:11:04.091Z",
          "hidden": false
        },
        {
          "_id": "68ae674d364411bea07df6f5",
          "name": "Yu-Chiang Frank Wang",
          "hidden": false
        },
        {
          "_id": "68ae674d364411bea07df6f6",
          "name": "Albert Gu",
          "hidden": false
        },
        {
          "_id": "68ae674d364411bea07df6f7",
          "name": "Seon Joo Kim",
          "hidden": false
        },
        {
          "_id": "68ae674d364411bea07df6f8",
          "name": "Ryo Hachiuma",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-26T17:59:13.000Z",
      "submittedOnDailyAt": "2025-08-27T02:32:20.792Z",
      "title": "Autoregressive Universal Video Segmentation Model",
      "submittedOnDailyBy": {
        "_id": "64ae22dd1aee69ece065cdcd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
        "isPro": false,
        "fullname": "Min-Hung Chen",
        "user": "cmhungsteve",
        "type": "user"
      },
      "summary": "Recent video foundation models such as SAM2 excel at prompted video\nsegmentation by treating masks as a general-purpose primitive. However, many\nreal-world settings require unprompted segmentation that aims to detect and\ntrack all objects in a video without external cues, leaving today's landscape\nfragmented across task-specific models and pipelines. We recast streaming video\nsegmentation as sequential mask prediction, analogous to language modeling, and\nintroduce the Autoregressive Universal Segmentation Model (AUSM), a single\narchitecture that unifies both prompted and unprompted video segmentation.\nBuilt on recent state-space models, AUSM maintains a fixed-size spatial state\nand scales to video streams of arbitrary length. Furthermore, all components of\nAUSM are designed for parallel training across frames, yielding substantial\nspeedups over iterative training. On standard benchmarks (DAVIS17, YouTube-VOS\n2018 & 2019, MOSE, YouTube-VIS 2019 & 2021, and OVIS) AUSM outperforms prior\nuniversal streaming video segmentation methods and achieves up to 2.5x faster\ntraining on 16-frame sequences.",
      "upvotes": 5,
      "discussionId": "68ae674d364411bea07df6f9",
      "ai_summary": "AUSM, an autoregressive universal segmentation model, unifies prompted and unprompted video segmentation by treating it as sequential mask prediction, achieving superior performance and faster training on standard benchmarks.",
      "ai_keywords": [
        "sequential mask prediction",
        "language modeling",
        "state-space models",
        "parallel training",
        "DAVIS17",
        "YouTube-VOS 2018 & 2019",
        "MOSE",
        "YouTube-VIS 2019 & 2021",
        "OVIS"
      ]
    },
    "publishedAt": "2025-08-26T13:59:13.000Z",
    "title": "Autoregressive Universal Video Segmentation Model",
    "summary": "Recent video foundation models such as SAM2 excel at prompted video\nsegmentation by treating masks as a general-purpose primitive. However, many\nreal-world settings require unprompted segmentation that aims to detect and\ntrack all objects in a video without external cues, leaving today's landscape\nfragmented across task-specific models and pipelines. We recast streaming video\nsegmentation as sequential mask prediction, analogous to language modeling, and\nintroduce the Autoregressive Universal Segmentation Model (AUSM), a single\narchitecture that unifies both prompted and unprompted video segmentation.\nBuilt on recent state-space models, AUSM maintains a fixed-size spatial state\nand scales to video streams of arbitrary length. Furthermore, all components of\nAUSM are designed for parallel training across frames, yielding substantial\nspeedups over iterative training. On standard benchmarks (DAVIS17, YouTube-VOS\n2018 & 2019, MOSE, YouTube-VIS 2019 & 2021, and OVIS) AUSM outperforms prior\nuniversal streaming video segmentation methods and achieves up to 2.5x faster\ntraining on 16-frame sequences.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.19242.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ae22dd1aee69ece065cdcd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
      "fullname": "Min-Hung Chen",
      "name": "cmhungsteve",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.15804",
      "authors": [
        {
          "_id": "68ae8365364411bea07df821",
          "name": "Minghao Li",
          "hidden": false
        },
        {
          "_id": "68ae8365364411bea07df822",
          "name": "Ying Zeng",
          "hidden": false
        },
        {
          "_id": "68ae8365364411bea07df823",
          "name": "Zhihao Cheng",
          "hidden": false
        },
        {
          "_id": "68ae8365364411bea07df824",
          "name": "Cong Ma",
          "hidden": false
        },
        {
          "_id": "68ae8365364411bea07df825",
          "name": "Kai Jia",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-14T03:33:43.000Z",
      "submittedOnDailyAt": "2025-08-27T04:36:01.251Z",
      "title": "ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks",
      "submittedOnDailyBy": {
        "_id": "5f574f9e8bf55658acfed37e",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1637285694180-5f574f9e8bf55658acfed37e.jpeg",
        "isPro": false,
        "fullname": "Minghao Li",
        "user": "liminghao1630",
        "type": "user"
      },
      "summary": "The advent of Deep Research agents has substantially reduced the time\nrequired for conducting extensive research tasks. However, these tasks\ninherently demand rigorous standards of factual accuracy and comprehensiveness,\nnecessitating thorough evaluation before widespread adoption. In this paper, we\npropose ReportBench, a systematic benchmark designed to evaluate the content\nquality of research reports generated by large language models (LLMs). Our\nevaluation focuses on two critical dimensions: (1) the quality and relevance of\ncited literature, and (2) the faithfulness and veracity of the statements\nwithin the generated reports. ReportBench leverages high-quality published\nsurvey papers available on arXiv as gold-standard references, from which we\napply reverse prompt engineering to derive domain-specific prompts and\nestablish a comprehensive evaluation corpus. Furthermore, we develop an\nagent-based automated framework within ReportBench that systematically analyzes\ngenerated reports by extracting citations and statements, checking the\nfaithfulness of cited content against original sources, and validating\nnon-cited claims using web-based resources. Empirical evaluations demonstrate\nthat commercial Deep Research agents such as those developed by OpenAI and\nGoogle consistently generate more comprehensive and reliable reports than\nstandalone LLMs augmented with search or browsing tools. However, there remains\nsubstantial room for improvement in terms of the breadth and depth of research\ncoverage, as well as factual consistency. The complete code and data will be\nreleased at the following link: https://github.com/ByteDance-BandAI/ReportBench",
      "upvotes": 5,
      "discussionId": "68ae8365364411bea07df826",
      "ai_summary": "ReportBench evaluates the content quality of research reports generated by large language models, focusing on cited literature quality and statement faithfulness, demonstrating that commercial Deep Research agents produce more comprehensive and reliable reports than standalone LLMs.",
      "ai_keywords": [
        "Deep Research agents",
        "large language models",
        "ReportBench",
        "reverse prompt engineering",
        "evaluation corpus",
        "agent-based automated framework",
        "citations",
        "statements",
        "faithfulness",
        "factual consistency"
      ]
    },
    "publishedAt": "2025-08-13T23:33:43.000Z",
    "title": "ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks",
    "summary": "The advent of Deep Research agents has substantially reduced the time\nrequired for conducting extensive research tasks. However, these tasks\ninherently demand rigorous standards of factual accuracy and comprehensiveness,\nnecessitating thorough evaluation before widespread adoption. In this paper, we\npropose ReportBench, a systematic benchmark designed to evaluate the content\nquality of research reports generated by large language models (LLMs). Our\nevaluation focuses on two critical dimensions: (1) the quality and relevance of\ncited literature, and (2) the faithfulness and veracity of the statements\nwithin the generated reports. ReportBench leverages high-quality published\nsurvey papers available on arXiv as gold-standard references, from which we\napply reverse prompt engineering to derive domain-specific prompts and\nestablish a comprehensive evaluation corpus. Furthermore, we develop an\nagent-based automated framework within ReportBench that systematically analyzes\ngenerated reports by extracting citations and statements, checking the\nfaithfulness of cited content against original sources, and validating\nnon-cited claims using web-based resources. Empirical evaluations demonstrate\nthat commercial Deep Research agents such as those developed by OpenAI and\nGoogle consistently generate more comprehensive and reliable reports than\nstandalone LLMs augmented with search or browsing tools. However, there remains\nsubstantial room for improvement in terms of the breadth and depth of research\ncoverage, as well as factual consistency. The complete code and data will be\nreleased at the following link: https://github.com/ByteDance-BandAI/ReportBench",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.15804.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "5f574f9e8bf55658acfed37e",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1637285694180-5f574f9e8bf55658acfed37e.jpeg",
      "fullname": "Minghao Li",
      "name": "liminghao1630",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 14
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.18773",
      "authors": [
        {
          "_id": "68ae774a364411bea07df76a",
          "name": "Qianyu He",
          "hidden": false
        },
        {
          "_id": "68ae774a364411bea07df76b",
          "name": "Siyu Yuan",
          "hidden": false
        },
        {
          "_id": "68ae774a364411bea07df76c",
          "name": "Xuefeng Li",
          "hidden": false
        },
        {
          "_id": "68ae774a364411bea07df76d",
          "name": "Mingxuan Wang",
          "hidden": false
        },
        {
          "_id": "68ae774a364411bea07df76e",
          "name": "Jiangjie Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-26T07:57:28.000Z",
      "submittedOnDailyAt": "2025-08-27T01:41:12.128Z",
      "title": "ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large\n  Language Models",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Large language models (LLMs) with chain-of-thought reasoning have\ndemonstrated remarkable problem-solving capabilities, but controlling their\ncomputational effort remains a significant challenge for practical deployment.\nRecent proprietary systems like OpenAI's gpt-oss series have introduced\ndiscrete operational modes for intuitive reasoning control, but the open-source\ncommunity has largely failed to achieve such capabilities. In this paper, we\nintroduce ThinkDial, the first open-recipe end-to-end framework that\nsuccessfully implements gpt-oss-style controllable reasoning through discrete\noperational modes. Our system enables seamless switching between three distinct\nreasoning regimes: High mode (full reasoning capability), Medium mode (50\npercent token reduction with <10 percent performance degradation), and Low mode\n(75 percent token reduction with <15 percent performance degradation). We\nachieve this through an end-to-end training paradigm that integrates\nbudget-mode control throughout the entire pipeline: budget-mode supervised\nfine-tuning that embeds controllable reasoning capabilities directly into the\nlearning process, and two-phase budget-aware reinforcement learning with\nadaptive reward shaping. Extensive experiments demonstrate that ThinkDial\nachieves target compression-performance trade-offs with clear response length\nreductions while maintaining performance thresholds. The framework also\nexhibits strong generalization capabilities on out-of-distribution tasks.",
      "upvotes": 3,
      "discussionId": "68ae774a364411bea07df76f",
      "ai_summary": "ThinkDial is an open-source framework that implements controllable reasoning in large language models through discrete operational modes, achieving performance while reducing computational effort.",
      "ai_keywords": [
        "chain-of-thought reasoning",
        "large language models",
        "discrete operational modes",
        "ThinkDial",
        "budget-mode supervised fine-tuning",
        "two-phase budget-aware reinforcement learning",
        "adaptive reward shaping"
      ]
    },
    "publishedAt": "2025-08-26T03:57:28.000Z",
    "title": "ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large\n  Language Models",
    "summary": "Large language models (LLMs) with chain-of-thought reasoning have\ndemonstrated remarkable problem-solving capabilities, but controlling their\ncomputational effort remains a significant challenge for practical deployment.\nRecent proprietary systems like OpenAI's gpt-oss series have introduced\ndiscrete operational modes for intuitive reasoning control, but the open-source\ncommunity has largely failed to achieve such capabilities. In this paper, we\nintroduce ThinkDial, the first open-recipe end-to-end framework that\nsuccessfully implements gpt-oss-style controllable reasoning through discrete\noperational modes. Our system enables seamless switching between three distinct\nreasoning regimes: High mode (full reasoning capability), Medium mode (50\npercent token reduction with <10 percent performance degradation), and Low mode\n(75 percent token reduction with <15 percent performance degradation). We\nachieve this through an end-to-end training paradigm that integrates\nbudget-mode control throughout the entire pipeline: budget-mode supervised\nfine-tuning that embeds controllable reasoning capabilities directly into the\nlearning process, and two-phase budget-aware reinforcement learning with\nadaptive reward shaping. Extensive experiments demonstrate that ThinkDial\nachieves target compression-performance trade-offs with clear response length\nreductions while maintaining performance thresholds. The framework also\nexhibits strong generalization capabilities on out-of-distribution tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18773.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 95
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.18672",
      "authors": [
        {
          "_id": "68ae5797364411bea07df6b1",
          "user": {
            "_id": "6308c49c454dc257521bc7f9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6308c49c454dc257521bc7f9/UWUS6OPa6OpVu1T0gd-wJ.jpeg",
            "isPro": false,
            "fullname": "Taishi",
            "user": "Taishi-N324",
            "type": "user"
          },
          "name": "Taishi Nakamura",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-27T07:11:08.016Z",
          "hidden": false
        },
        {
          "_id": "68ae5797364411bea07df6b2",
          "name": "Satoki Ishikawa",
          "hidden": false
        },
        {
          "_id": "68ae5797364411bea07df6b3",
          "name": "Masaki Kawamura",
          "hidden": false
        },
        {
          "_id": "68ae5797364411bea07df6b4",
          "name": "Takumi Okamoto",
          "hidden": false
        },
        {
          "_id": "68ae5797364411bea07df6b5",
          "name": "Daisuke Nohara",
          "hidden": false
        },
        {
          "_id": "68ae5797364411bea07df6b6",
          "name": "Jun Suzuki",
          "hidden": false
        },
        {
          "_id": "68ae5797364411bea07df6b7",
          "name": "Rio Yokota",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-26T04:31:28.000Z",
      "submittedOnDailyAt": "2025-08-27T04:15:11.290Z",
      "title": "Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning\n  Tasks",
      "submittedOnDailyBy": {
        "_id": "6308c49c454dc257521bc7f9",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6308c49c454dc257521bc7f9/UWUS6OPa6OpVu1T0gd-wJ.jpeg",
        "isPro": false,
        "fullname": "Taishi",
        "user": "Taishi-N324",
        "type": "user"
      },
      "summary": "Empirical scaling laws have driven the evolution of large language models\n(LLMs), yet their coefficients shift whenever the model architecture or data\npipeline changes. Mixture-of-Experts (MoE) models, now standard in\nstate-of-the-art systems, introduce a new sparsity dimension that current\ndense-model frontiers overlook. We investigate how MoE sparsity influences two\ndistinct capability regimes: memorization and reasoning. We train families of\nMoE Transformers that systematically vary total parameters, active parameters,\nand top-k routing while holding the compute budget fixed. For every model we\nrecord pre-training loss, downstream task loss, and task accuracy, allowing us\nto separate the train-test generalization gap from the loss-accuracy gap.\nMemorization benchmarks improve monotonically with total parameters, mirroring\ntraining loss. By contrast, reasoning performance saturates and can even\nregress despite continued gains in both total parameters and training loss.\nAltering top-k alone has little effect when active parameters are constant,\nand classic hyperparameters such as learning rate and initialization modulate\nthe generalization gap in the same direction as sparsity. Neither post-training\nreinforcement learning (GRPO) nor extra test-time compute rescues the reasoning\ndeficit of overly sparse models. Our model checkpoints, code and logs are\nopen-source at https://github.com/rioyokotalab/optimal-sparsity.",
      "upvotes": 2,
      "discussionId": "68ae5797364411bea07df6b8",
      "ai_summary": "MoE models introduce sparsity that affects memorization and reasoning capabilities differently in large language models, with reasoning performance potentially regressing despite increased parameters.",
      "ai_keywords": [
        "Mixture-of-Experts",
        "MoE",
        "Transformers",
        "active parameters",
        "top-$k$ routing",
        "pre-training loss",
        "downstream task loss",
        "memorization",
        "reasoning",
        "sparsity",
        "generalization gap",
        "loss-accuracy gap",
        "learning rate",
        "initialization",
        "post-training reinforcement learning",
        "GRPO"
      ]
    },
    "publishedAt": "2025-08-26T00:31:28.000Z",
    "title": "Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning\n  Tasks",
    "summary": "Empirical scaling laws have driven the evolution of large language models\n(LLMs), yet their coefficients shift whenever the model architecture or data\npipeline changes. Mixture-of-Experts (MoE) models, now standard in\nstate-of-the-art systems, introduce a new sparsity dimension that current\ndense-model frontiers overlook. We investigate how MoE sparsity influences two\ndistinct capability regimes: memorization and reasoning. We train families of\nMoE Transformers that systematically vary total parameters, active parameters,\nand top-k routing while holding the compute budget fixed. For every model we\nrecord pre-training loss, downstream task loss, and task accuracy, allowing us\nto separate the train-test generalization gap from the loss-accuracy gap.\nMemorization benchmarks improve monotonically with total parameters, mirroring\ntraining loss. By contrast, reasoning performance saturates and can even\nregress despite continued gains in both total parameters and training loss.\nAltering top-k alone has little effect when active parameters are constant,\nand classic hyperparameters such as learning rate and initialization modulate\nthe generalization gap in the same direction as sparsity. Neither post-training\nreinforcement learning (GRPO) nor extra test-time compute rescues the reasoning\ndeficit of overly sparse models. Our model checkpoints, code and logs are\nopen-source at https://github.com/rioyokotalab/optimal-sparsity.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18672.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6308c49c454dc257521bc7f9",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6308c49c454dc257521bc7f9/UWUS6OPa6OpVu1T0gd-wJ.jpeg",
      "fullname": "Taishi",
      "name": "Taishi-N324",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.18370",
      "authors": [
        {
          "_id": "68ae7956364411bea07df78a",
          "user": {
            "_id": "62b7fb545233925f253531c8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b7fb545233925f253531c8/W50u2G1HK3EtUKHRU189V.jpeg",
            "isPro": false,
            "fullname": "Terry Yue Zhuo",
            "user": "terryyz",
            "type": "user"
          },
          "name": "Terry Yue Zhuo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-27T07:10:42.523Z",
          "hidden": false
        },
        {
          "_id": "68ae7956364411bea07df78b",
          "name": "Dingmin Wang",
          "hidden": false
        },
        {
          "_id": "68ae7956364411bea07df78c",
          "name": "Hantian Ding",
          "hidden": false
        },
        {
          "_id": "68ae7956364411bea07df78d",
          "name": "Varun Kumar",
          "hidden": false
        },
        {
          "_id": "68ae7956364411bea07df78e",
          "name": "Zijian Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-25T18:02:23.000Z",
      "submittedOnDailyAt": "2025-08-27T01:50:22.912Z",
      "title": "Training Language Model Agents to Find Vulnerabilities with CTF-Dojo",
      "submittedOnDailyBy": {
        "_id": "62b7fb545233925f253531c8",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b7fb545233925f253531c8/W50u2G1HK3EtUKHRU189V.jpeg",
        "isPro": false,
        "fullname": "Terry Yue Zhuo",
        "user": "terryyz",
        "type": "user"
      },
      "summary": "Large language models (LLMs) have demonstrated exceptional capabilities when\ntrained within executable runtime environments, notably excelling at software\nengineering tasks through verified feedback loops. Yet, scalable and\ngeneralizable execution-grounded environments remain scarce, limiting progress\nin training more capable ML agents. We introduce CTF-Dojo, the first\nlarge-scale executable runtime tailored for training LLMs with verifiable\nfeedback, featuring 658 fully functional Capture-The-Flag (CTF)-style\nchallenges containerized in Docker with guaranteed reproducibility. To enable\nrapid scaling without manual intervention, we develop CTF-Forge, an automated\npipeline that transforms publicly available artifacts into ready-to-use\nexecution environments in minutes, eliminating weeks of expert configuration\ntraditionally required. We trained LLM-based agents on just 486 high-quality,\nexecution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute\ngains over strong baselines across three competitive benchmarks: InterCode-CTF,\nNYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1,\nestablishing a new open-weight state-of-the-art that rivals frontier models\nlike DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as a\nbenchmark for executable-agent learning, CTF-Dojo demonstrates that\nexecution-grounded training signals are not only effective but pivotal in\nadvancing high-performance ML agents without dependence on costly proprietary\nsystems.",
      "upvotes": 2,
      "discussionId": "68ae7957364411bea07df78f",
      "ai_summary": "CTF-Dojo, a large-scale executable runtime with 658 CTF challenges, enables rapid training of LLM-based agents with verifiable feedback, achieving state-of-the-art performance in competitive benchmarks.",
      "ai_keywords": [
        "LLMs",
        "software engineering tasks",
        "verified feedback loops",
        "CTF-Dojo",
        "Capture-The-Flag",
        "Docker",
        "CTF-Forge",
        "automated pipeline",
        "execution-verified trajectories",
        "InterCode-CTF",
        "NYU CTF Bench",
        "Cybench",
        "Pass@1",
        "DeepSeek-V3-0324",
        "Gemini-2.5-Flash",
        "executable-agent learning"
      ]
    },
    "publishedAt": "2025-08-25T14:02:23.000Z",
    "title": "Training Language Model Agents to Find Vulnerabilities with CTF-Dojo",
    "summary": "Large language models (LLMs) have demonstrated exceptional capabilities when\ntrained within executable runtime environments, notably excelling at software\nengineering tasks through verified feedback loops. Yet, scalable and\ngeneralizable execution-grounded environments remain scarce, limiting progress\nin training more capable ML agents. We introduce CTF-Dojo, the first\nlarge-scale executable runtime tailored for training LLMs with verifiable\nfeedback, featuring 658 fully functional Capture-The-Flag (CTF)-style\nchallenges containerized in Docker with guaranteed reproducibility. To enable\nrapid scaling without manual intervention, we develop CTF-Forge, an automated\npipeline that transforms publicly available artifacts into ready-to-use\nexecution environments in minutes, eliminating weeks of expert configuration\ntraditionally required. We trained LLM-based agents on just 486 high-quality,\nexecution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute\ngains over strong baselines across three competitive benchmarks: InterCode-CTF,\nNYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1,\nestablishing a new open-weight state-of-the-art that rivals frontier models\nlike DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as a\nbenchmark for executable-agent learning, CTF-Dojo demonstrates that\nexecution-grounded training signals are not only effective but pivotal in\nadvancing high-performance ML agents without dependence on costly proprietary\nsystems.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18370.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62b7fb545233925f253531c8",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b7fb545233925f253531c8/W50u2G1HK3EtUKHRU189V.jpeg",
      "fullname": "Terry Yue Zhuo",
      "name": "terryyz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.16697",
      "authors": [
        {
          "_id": "68ae81c3364411bea07df81a",
          "name": "Nicole Cho",
          "hidden": false
        },
        {
          "_id": "68ae81c3364411bea07df81b",
          "name": "William Watson",
          "hidden": false
        },
        {
          "_id": "68ae81c3364411bea07df81c",
          "name": "Alec Koppel",
          "hidden": false
        },
        {
          "_id": "68ae81c3364411bea07df81d",
          "name": "Sumitra Ganesh",
          "hidden": false
        },
        {
          "_id": "68ae81c3364411bea07df81e",
          "name": "Manuela Veloso",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-22T01:41:49.000Z",
      "submittedOnDailyAt": "2025-08-27T02:29:18.606Z",
      "title": "QueryBandits for Hallucination Mitigation: Exploiting Semantic Features\n  for No-Regret Rewriting",
      "submittedOnDailyBy": {
        "_id": "6630a380b6d15534202ab83f",
        "avatarUrl": "/avatars/69af9465544b668c65ad4d9866fbfd06.svg",
        "isPro": false,
        "fullname": "Nicole Cho",
        "user": "NicoleCho",
        "type": "user"
      },
      "summary": "Advanced reasoning capabilities in Large Language Models (LLMs) have caused\nhigher hallucination prevalence; yet most mitigation work focuses on\nafter-the-fact filtering rather than shaping the queries that trigger them. We\nintroduce QueryBandits, a bandit framework that designs rewrite strategies to\nmaximize a reward model, that encapsulates hallucination propensity based upon\nthe sensitivities of 17 linguistic features of the input query-and therefore,\nproactively steer LLMs away from generating hallucinations. Across 13 diverse\nQA benchmarks and 1,050 lexically perturbed queries per dataset, our top\ncontextual QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a\nno-rewrite baseline and also outperforms zero-shot static prompting\n(\"paraphrase\" or \"expand\") by 42.6% and 60.3% respectively. Therefore, we\nempirically substantiate the effectiveness of QueryBandits in mitigating\nhallucination via the intervention that takes the form of a query rewrite.\nInterestingly, certain static prompting strategies, which constitute a\nconsiderable number of current query rewriting literature, have a higher\ncumulative regret than the no-rewrite baseline, signifying that static rewrites\ncan worsen hallucination. Moreover, we discover that the converged per-arm\nregression feature weight vectors substantiate that there is no single rewrite\nstrategy optimal for all queries. In this context, guided rewriting via\nexploiting semantic features with QueryBandits can induce significant shifts in\noutput behavior through forward-pass mechanisms, bypassing the need for\nretraining or gradient-based adaptation.",
      "upvotes": 2,
      "discussionId": "68ae81c4364411bea07df81f",
      "ai_summary": "QueryBandits, a bandit framework, effectively mitigates hallucinations in LLMs by proactively rewriting queries based on linguistic features, outperforming static prompting strategies.",
      "ai_keywords": [
        "QueryBandits",
        "bandit framework",
        "reward model",
        "hallucination propensity",
        "linguistic features",
        "query rewrite",
        "Thompson Sampling",
        "QA benchmarks",
        "cumulative regret",
        "per-arm regression",
        "semantic features",
        "forward-pass mechanisms"
      ]
    },
    "publishedAt": "2025-08-21T21:41:49.000Z",
    "title": "QueryBandits for Hallucination Mitigation: Exploiting Semantic Features\n  for No-Regret Rewriting",
    "summary": "Advanced reasoning capabilities in Large Language Models (LLMs) have caused\nhigher hallucination prevalence; yet most mitigation work focuses on\nafter-the-fact filtering rather than shaping the queries that trigger them. We\nintroduce QueryBandits, a bandit framework that designs rewrite strategies to\nmaximize a reward model, that encapsulates hallucination propensity based upon\nthe sensitivities of 17 linguistic features of the input query-and therefore,\nproactively steer LLMs away from generating hallucinations. Across 13 diverse\nQA benchmarks and 1,050 lexically perturbed queries per dataset, our top\ncontextual QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a\nno-rewrite baseline and also outperforms zero-shot static prompting\n(\"paraphrase\" or \"expand\") by 42.6% and 60.3% respectively. Therefore, we\nempirically substantiate the effectiveness of QueryBandits in mitigating\nhallucination via the intervention that takes the form of a query rewrite.\nInterestingly, certain static prompting strategies, which constitute a\nconsiderable number of current query rewriting literature, have a higher\ncumulative regret than the no-rewrite baseline, signifying that static rewrites\ncan worsen hallucination. Moreover, we discover that the converged per-arm\nregression feature weight vectors substantiate that there is no single rewrite\nstrategy optimal for all queries. In this context, guided rewriting via\nexploiting semantic features with QueryBandits can induce significant shifts in\noutput behavior through forward-pass mechanisms, bypassing the need for\nretraining or gradient-based adaptation.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.16697.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6630a380b6d15534202ab83f",
      "avatarUrl": "/avatars/69af9465544b668c65ad4d9866fbfd06.svg",
      "fullname": "Nicole Cho",
      "name": "NicoleCho",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.19188",
      "authors": [
        {
          "_id": "68ae75e7364411bea07df74e",
          "name": "Jeonghwan Kim",
          "hidden": false
        },
        {
          "_id": "68ae75e7364411bea07df74f",
          "name": "Yushi Lan",
          "hidden": false
        },
        {
          "_id": "68ae75e7364411bea07df750",
          "name": "Armando Fortes",
          "hidden": false
        },
        {
          "_id": "68ae75e7364411bea07df751",
          "name": "Yongwei Chen",
          "hidden": false
        },
        {
          "_id": "68ae75e7364411bea07df752",
          "name": "Xingang Pan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-26T16:51:02.000Z",
      "submittedOnDailyAt": "2025-08-27T01:35:22.417Z",
      "title": "FastMesh:Efficient Artistic Mesh Generation via Component Decoupling",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Recent mesh generation approaches typically tokenize triangle meshes into\nsequences of tokens and train autoregressive models to generate these tokens\nsequentially. Despite substantial progress, such token sequences inevitably\nreuse vertices multiple times to fully represent manifold meshes, as each\nvertex is shared by multiple faces. This redundancy leads to excessively long\ntoken sequences and inefficient generation processes. In this paper, we propose\nan efficient framework that generates artistic meshes by treating vertices and\nfaces separately, significantly reducing redundancy. We employ an\nautoregressive model solely for vertex generation, decreasing the token count\nto approximately 23\\% of that required by the most compact existing tokenizer.\nNext, we leverage a bidirectional transformer to complete the mesh in a single\nstep by capturing inter-vertex relationships and constructing the adjacency\nmatrix that defines the mesh faces. To further improve the generation quality,\nwe introduce a fidelity enhancer to refine vertex positioning into more natural\narrangements and propose a post-processing framework to remove undesirable edge\nconnections. Experimental results show that our method achieves more than\n8times faster speed on mesh generation compared to state-of-the-art\napproaches, while producing higher mesh quality.",
      "upvotes": 1,
      "discussionId": "68ae75e7364411bea07df753",
      "projectPage": "https://jhkim0759.github.io/projects/FastMesh/",
      "ai_summary": "A framework for efficient artistic mesh generation reduces redundancy by separating vertex and face generation, using an autoregressive model for vertices and a bidirectional transformer for faces, and includes a fidelity enhancer and post-processing to improve quality and speed.",
      "ai_keywords": [
        "autoregressive models",
        "bidirectional transformer",
        "vertex generation",
        "face generation",
        "fidelity enhancer",
        "post-processing",
        "mesh generation",
        "adjacency matrix",
        "mesh quality"
      ]
    },
    "publishedAt": "2025-08-26T12:51:02.000Z",
    "title": "FastMesh:Efficient Artistic Mesh Generation via Component Decoupling",
    "summary": "Recent mesh generation approaches typically tokenize triangle meshes into\nsequences of tokens and train autoregressive models to generate these tokens\nsequentially. Despite substantial progress, such token sequences inevitably\nreuse vertices multiple times to fully represent manifold meshes, as each\nvertex is shared by multiple faces. This redundancy leads to excessively long\ntoken sequences and inefficient generation processes. In this paper, we propose\nan efficient framework that generates artistic meshes by treating vertices and\nfaces separately, significantly reducing redundancy. We employ an\nautoregressive model solely for vertex generation, decreasing the token count\nto approximately 23\\% of that required by the most compact existing tokenizer.\nNext, we leverage a bidirectional transformer to complete the mesh in a single\nstep by capturing inter-vertex relationships and constructing the adjacency\nmatrix that defines the mesh faces. To further improve the generation quality,\nwe introduce a fidelity enhancer to refine vertex positioning into more natural\narrangements and propose a post-processing framework to remove undesirable edge\nconnections. Experimental results show that our method achieves more than\n8times faster speed on mesh generation compared to state-of-the-art\napproaches, while producing higher mesh quality.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.19188.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 95
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.19026",
      "authors": [
        {
          "_id": "68ae81ae364411bea07df810",
          "name": "Gueter Josmy Faure",
          "hidden": false
        },
        {
          "_id": "68ae81ae364411bea07df811",
          "user": {
            "_id": "64ae22dd1aee69ece065cdcd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
            "isPro": false,
            "fullname": "Min-Hung Chen",
            "user": "cmhungsteve",
            "type": "user"
          },
          "name": "Min-Hung Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-27T07:10:21.219Z",
          "hidden": false
        },
        {
          "_id": "68ae81ae364411bea07df812",
          "name": "Jia-Fong Yeh",
          "hidden": false
        },
        {
          "_id": "68ae81ae364411bea07df813",
          "name": "Ying Cheng",
          "hidden": false
        },
        {
          "_id": "68ae81ae364411bea07df814",
          "name": "Hung-Ting Su",
          "hidden": false
        },
        {
          "_id": "68ae81ae364411bea07df815",
          "name": "Yung-Hao Tang",
          "hidden": false
        },
        {
          "_id": "68ae81ae364411bea07df816",
          "name": "Shang-Hong Lai",
          "hidden": false
        },
        {
          "_id": "68ae81ae364411bea07df817",
          "name": "Winston H. Hsu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/64ae22dd1aee69ece065cdcd/d52riIhGDdAixd2A64em-.png"
      ],
      "publishedAt": "2025-08-26T13:43:45.000Z",
      "submittedOnDailyAt": "2025-08-27T02:34:21.127Z",
      "title": "MovieCORE: COgnitive REasoning in Movies",
      "submittedOnDailyBy": {
        "_id": "64ae22dd1aee69ece065cdcd",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
        "isPro": false,
        "fullname": "Min-Hung Chen",
        "user": "cmhungsteve",
        "type": "user"
      },
      "summary": "This paper introduces MovieCORE, a novel video question answering (VQA)\ndataset designed to probe deeper cognitive understanding of movie content.\nUnlike existing datasets that focus on surface-level comprehension, MovieCORE\nemphasizes questions that engage System-2 thinking while remaining specific to\nthe video material. We present an innovative agentic brainstorming approach,\nutilizing multiple large language models (LLMs) as thought agents to generate\nand refine high-quality question-answer pairs. To evaluate dataset quality, we\ndevelop a set of cognitive tests assessing depth, thought-provocation\npotential, and syntactic complexity. We also propose a comprehensive evaluation\nscheme for assessing VQA model performance on deeper cognitive tasks. To\naddress the limitations of existing video-language models (VLMs), we introduce\nan agentic enhancement module, Agentic Choice Enhancement (ACE), which improves\nmodel reasoning capabilities post-training by up to 25%. Our work contributes\nto advancing movie understanding in AI systems and provides valuable insights\ninto the capabilities and limitations of current VQA models when faced with\nmore challenging, nuanced questions about cinematic content. Our project page,\ndataset and code can be found at\nhttps://joslefaure.github.io/assets/html/moviecore.html.",
      "upvotes": 1,
      "discussionId": "68ae81af364411bea07df818",
      "ai_summary": "MovieCORE is a video question answering dataset that uses multiple large language models to generate deep cognitive questions, and introduces an agentic enhancement module to improve VQA model performance.",
      "ai_keywords": [
        "video question answering",
        "VQA",
        "dataset",
        "System-2 thinking",
        "large language models",
        "LLMs",
        "thought agents",
        "cognitive tests",
        "syntactic complexity",
        "video-language models",
        "VLMs",
        "agentic enhancement module",
        "Agentic Choice Enhancement",
        "ACE"
      ]
    },
    "publishedAt": "2025-08-26T09:43:45.000Z",
    "title": "MovieCORE: COgnitive REasoning in Movies",
    "summary": "This paper introduces MovieCORE, a novel video question answering (VQA)\ndataset designed to probe deeper cognitive understanding of movie content.\nUnlike existing datasets that focus on surface-level comprehension, MovieCORE\nemphasizes questions that engage System-2 thinking while remaining specific to\nthe video material. We present an innovative agentic brainstorming approach,\nutilizing multiple large language models (LLMs) as thought agents to generate\nand refine high-quality question-answer pairs. To evaluate dataset quality, we\ndevelop a set of cognitive tests assessing depth, thought-provocation\npotential, and syntactic complexity. We also propose a comprehensive evaluation\nscheme for assessing VQA model performance on deeper cognitive tasks. To\naddress the limitations of existing video-language models (VLMs), we introduce\nan agentic enhancement module, Agentic Choice Enhancement (ACE), which improves\nmodel reasoning capabilities post-training by up to 25%. Our work contributes\nto advancing movie understanding in AI systems and provides valuable insights\ninto the capabilities and limitations of current VQA models when faced with\nmore challenging, nuanced questions about cinematic content. Our project page,\ndataset and code can be found at\nhttps://joslefaure.github.io/assets/html/moviecore.html.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/64ae22dd1aee69ece065cdcd/d52riIhGDdAixd2A64em-.png"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.19026.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64ae22dd1aee69ece065cdcd",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
      "fullname": "Min-Hung Chen",
      "name": "cmhungsteve",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2508.18271",
      "authors": [
        {
          "_id": "68ae8b74364411bea07df828",
          "name": "Haitang Feng",
          "hidden": false
        },
        {
          "_id": "68ae8b74364411bea07df829",
          "name": "Jie Liu",
          "hidden": false
        },
        {
          "_id": "68ae8b74364411bea07df82a",
          "name": "Jie Tang",
          "hidden": false
        },
        {
          "_id": "68ae8b74364411bea07df82b",
          "name": "Gangshan Wu",
          "hidden": false
        },
        {
          "_id": "68ae8b74364411bea07df82c",
          "name": "Beiqi Chen",
          "hidden": false
        },
        {
          "_id": "68ae8b74364411bea07df82d",
          "name": "Jianhuang Lai",
          "hidden": false
        },
        {
          "_id": "68ae8b74364411bea07df82e",
          "name": "Guangcong Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-25T17:59:40.000Z",
      "submittedOnDailyAt": "2025-08-27T03:10:15.083Z",
      "title": "ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion\n  Models",
      "submittedOnDailyBy": {
        "_id": "62e893da40bd989bb71b8f89",
        "avatarUrl": "/avatars/34e755d1124303a498429a3c4d01367b.svg",
        "isPro": false,
        "fullname": "Guangcong Wang",
        "user": "GuangcongWang",
        "type": "user"
      },
      "summary": "3D inpainting often relies on multi-view 2D image inpainting, where the\ninherent inconsistencies across different inpainted views can result in blurred\ntextures, spatial discontinuities, and distracting visual artifacts. These\ninconsistencies pose significant challenges when striving for accurate and\nrealistic 3D object completion, particularly in applications that demand high\nfidelity and structural coherence. To overcome these limitations, we propose\nObjFiller-3D, a novel method designed for the completion and editing of\nhigh-quality and consistent 3D objects. Instead of employing a conventional 2D\nimage inpainting model, our approach leverages a curated selection of\nstate-of-the-art video editing model to fill in the masked regions of 3D\nobjects. We analyze the representation gap between 3D and videos, and propose\nan adaptation of a video inpainting model for 3D scene inpainting. In addition,\nwe introduce a reference-based 3D inpainting method to further enhance the\nquality of reconstruction. Experiments across diverse datasets show that\ncompared to previous methods, ObjFiller-3D produces more faithful and\nfine-grained reconstructions (PSNR of 26.6 vs. NeRFiller (15.9) and LPIPS of\n0.19 vs. Instant3dit (0.25)). Moreover, it demonstrates strong potential for\npractical deployment in real-world 3D editing applications. Project page:\nhttps://objfiller3d.github.io/ Code:\nhttps://github.com/objfiller3d/ObjFiller-3D .",
      "upvotes": 1,
      "discussionId": "68ae8b75364411bea07df82f",
      "projectPage": "https://objfiller3d.github.io/",
      "githubRepo": "https://github.com/objfiller3d/ObjFiller-3D",
      "ai_summary": "ObjFiller-3D uses video editing models to achieve high-quality and consistent 3D object completion, outperforming previous methods in terms of reconstruction fidelity and practical deployment.",
      "ai_keywords": [
        "3D inpainting",
        "multi-view 2D image inpainting",
        "video editing model",
        "3D scene inpainting",
        "reference-based 3D inpainting",
        "PSNR",
        "LPIPS"
      ],
      "githubStars": 14
    },
    "publishedAt": "2025-08-25T13:59:40.000Z",
    "title": "ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion\n  Models",
    "summary": "3D inpainting often relies on multi-view 2D image inpainting, where the\ninherent inconsistencies across different inpainted views can result in blurred\ntextures, spatial discontinuities, and distracting visual artifacts. These\ninconsistencies pose significant challenges when striving for accurate and\nrealistic 3D object completion, particularly in applications that demand high\nfidelity and structural coherence. To overcome these limitations, we propose\nObjFiller-3D, a novel method designed for the completion and editing of\nhigh-quality and consistent 3D objects. Instead of employing a conventional 2D\nimage inpainting model, our approach leverages a curated selection of\nstate-of-the-art video editing model to fill in the masked regions of 3D\nobjects. We analyze the representation gap between 3D and videos, and propose\nan adaptation of a video inpainting model for 3D scene inpainting. In addition,\nwe introduce a reference-based 3D inpainting method to further enhance the\nquality of reconstruction. Experiments across diverse datasets show that\ncompared to previous methods, ObjFiller-3D produces more faithful and\nfine-grained reconstructions (PSNR of 26.6 vs. NeRFiller (15.9) and LPIPS of\n0.19 vs. Instant3dit (0.25)). Moreover, it demonstrates strong potential for\npractical deployment in real-world 3D editing applications. Project page:\nhttps://objfiller3d.github.io/ Code:\nhttps://github.com/objfiller3d/ObjFiller-3D .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18271.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62e893da40bd989bb71b8f89",
      "avatarUrl": "/avatars/34e755d1124303a498429a3c4d01367b.svg",
      "fullname": "Guangcong Wang",
      "name": "GuangcongWang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.19202",
      "authors": [
        {
          "_id": "68aeac4f364411bea07df849",
          "name": "Alan Li",
          "hidden": false
        },
        {
          "_id": "68aeac4f364411bea07df84a",
          "name": "Yixin Liu",
          "hidden": false
        },
        {
          "_id": "68aeac4f364411bea07df84b",
          "name": "Arpan Sarkar",
          "hidden": false
        },
        {
          "_id": "68aeac4f364411bea07df84c",
          "name": "Doug Downey",
          "hidden": false
        },
        {
          "_id": "68aeac4f364411bea07df84d",
          "name": "Arman Cohan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-26T17:04:23.000Z",
      "submittedOnDailyAt": "2025-08-27T05:28:22.316Z",
      "title": "Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and\n  Reasoning",
      "submittedOnDailyBy": {
        "_id": "6209ab721c19916805e5b2d1",
        "avatarUrl": "/avatars/67bbb958f19cedb2ea09461127f8ac92.svg",
        "isPro": false,
        "fullname": "Alan Li",
        "user": "lihaoxin2020",
        "type": "user"
      },
      "summary": "Scientific problem solving poses unique challenges for LLMs, requiring both\ndeep domain knowledge and the ability to apply such knowledge through complex\nreasoning. While automated scientific reasoners hold great promise for\nassisting human scientists, there is currently no widely adopted holistic\nbenchmark for evaluating scientific reasoning, and few approaches\nsystematically disentangle the distinct roles of knowledge and reasoning in\nthese tasks. To address these gaps, we introduce SciReas, a diverse suite of\nexisting benchmarks for scientific reasoning tasks, and SciReas-Pro, a\nselective subset that requires more complex reasoning. Our holistic evaluation\nsurfaces insights about scientific reasoning performance that remain hidden\nwhen relying on individual benchmarks alone. We then propose KRUX, a probing\nframework for studying the distinct roles of reasoning and knowledge in\nscientific tasks. Combining the two, we conduct an in-depth analysis that\nyields several key findings: (1) Retrieving task-relevant knowledge from model\nparameters is a critical bottleneck for LLMs in scientific reasoning; (2)\nReasoning models consistently benefit from external knowledge added in-context\non top of the reasoning enhancement; (3) Enhancing verbalized reasoning\nimproves LLMs' ability to surface task-relevant knowledge. Finally, we conduct\na lightweight analysis, comparing our science-focused data composition with\nconcurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baseline\nfor scientific reasoning.",
      "upvotes": 0,
      "discussionId": "68aeac50364411bea07df84e",
      "ai_summary": "SciReas and SciReas-Pro benchmarks, along with KRUX framework, provide insights into the distinct roles of knowledge and reasoning in scientific tasks, highlighting critical bottlenecks and improvements for LLMs.",
      "ai_keywords": [
        "LLMs",
        "scientific reasoning",
        "SciReas",
        "SciReas-Pro",
        "KRUX",
        "task-relevant knowledge",
        "reasoning models",
        "external knowledge",
        "verbalized reasoning",
        "SciLit01"
      ]
    },
    "publishedAt": "2025-08-26T13:04:23.000Z",
    "title": "Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and\n  Reasoning",
    "summary": "Scientific problem solving poses unique challenges for LLMs, requiring both\ndeep domain knowledge and the ability to apply such knowledge through complex\nreasoning. While automated scientific reasoners hold great promise for\nassisting human scientists, there is currently no widely adopted holistic\nbenchmark for evaluating scientific reasoning, and few approaches\nsystematically disentangle the distinct roles of knowledge and reasoning in\nthese tasks. To address these gaps, we introduce SciReas, a diverse suite of\nexisting benchmarks for scientific reasoning tasks, and SciReas-Pro, a\nselective subset that requires more complex reasoning. Our holistic evaluation\nsurfaces insights about scientific reasoning performance that remain hidden\nwhen relying on individual benchmarks alone. We then propose KRUX, a probing\nframework for studying the distinct roles of reasoning and knowledge in\nscientific tasks. Combining the two, we conduct an in-depth analysis that\nyields several key findings: (1) Retrieving task-relevant knowledge from model\nparameters is a critical bottleneck for LLMs in scientific reasoning; (2)\nReasoning models consistently benefit from external knowledge added in-context\non top of the reasoning enhancement; (3) Enhancing verbalized reasoning\nimproves LLMs' ability to surface task-relevant knowledge. Finally, we conduct\na lightweight analysis, comparing our science-focused data composition with\nconcurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baseline\nfor scientific reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.19202.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6209ab721c19916805e5b2d1",
      "avatarUrl": "/avatars/67bbb958f19cedb2ea09461127f8ac92.svg",
      "fullname": "Alan Li",
      "name": "lihaoxin2020",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2508.18192",
      "authors": [
        {
          "_id": "68ae6de1364411bea07df71e",
          "user": {
            "_id": "64dfbcb18e2084e1d7b51b46",
            "avatarUrl": "/avatars/fafe30beea2d7e8eec3f3ba985c582f7.svg",
            "isPro": false,
            "fullname": "Kushal Raj Bhandari",
            "user": "KBhandari11",
            "type": "user"
          },
          "name": "Kushal Raj Bhandari",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-08-27T07:10:51.902Z",
          "hidden": false
        },
        {
          "_id": "68ae6de1364411bea07df71f",
          "name": "Pin-Yu Chen",
          "hidden": false
        },
        {
          "_id": "68ae6de1364411bea07df720",
          "name": "Jianxi Gao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-08-25T16:49:38.000Z",
      "submittedOnDailyAt": "2025-08-27T01:07:55.160Z",
      "title": "Unraveling the cognitive patterns of Large Language Models through\n  module communities",
      "submittedOnDailyBy": {
        "_id": "64dfbcb18e2084e1d7b51b46",
        "avatarUrl": "/avatars/fafe30beea2d7e8eec3f3ba985c582f7.svg",
        "isPro": false,
        "fullname": "Kushal Raj Bhandari",
        "user": "KBhandari11",
        "type": "user"
      },
      "summary": "Large Language Models (LLMs) have reshaped our world with significant\nadvancements in science, engineering, and society through applications ranging\nfrom scientific discoveries and medical diagnostics to Chatbots. Despite their\nubiquity and utility, the underlying mechanisms of LLM remain concealed within\nbillions of parameters and complex structures, making their inner architecture\nand cognitive processes challenging to comprehend. We address this gap by\nadopting approaches to understanding emerging cognition in biology and\ndeveloping a network-based framework that links cognitive skills, LLM\narchitectures, and datasets, ushering in a paradigm shift in foundation model\nanalysis. The skill distribution in the module communities demonstrates that\nwhile LLMs do not strictly parallel the focalized specialization observed in\nspecific biological systems, they exhibit unique communities of modules whose\nemergent skill patterns partially mirror the distributed yet interconnected\ncognitive organization seen in avian and small mammalian brains. Our numerical\nresults highlight a key divergence from biological systems to LLMs, where skill\nacquisition benefits substantially from dynamic, cross-regional interactions\nand neural plasticity. By integrating cognitive science principles with machine\nlearning, our framework provides new insights into LLM interpretability and\nsuggests that effective fine-tuning strategies should leverage distributed\nlearning dynamics rather than rigid modular interventions.",
      "upvotes": 0,
      "discussionId": "68ae6de1364411bea07df721",
      "ai_summary": "A network-based framework links cognitive skills, LLM architectures, and datasets, revealing unique emergent skill patterns in LLMs that benefit from dynamic, cross-regional interactions.",
      "ai_keywords": [
        "Large Language Models",
        "LLMs",
        "network-based framework",
        "cognitive skills",
        "module communities",
        "emergent skill patterns",
        "avian and small mammalian brains",
        "skill acquisition",
        "dynamic",
        "cross-regional interactions",
        "neural plasticity",
        "cognitive science principles",
        "machine learning",
        "LLM interpretability",
        "effective fine-tuning strategies",
        "distributed learning dynamics"
      ]
    },
    "publishedAt": "2025-08-25T12:49:38.000Z",
    "title": "Unraveling the cognitive patterns of Large Language Models through\n  module communities",
    "summary": "Large Language Models (LLMs) have reshaped our world with significant\nadvancements in science, engineering, and society through applications ranging\nfrom scientific discoveries and medical diagnostics to Chatbots. Despite their\nubiquity and utility, the underlying mechanisms of LLM remain concealed within\nbillions of parameters and complex structures, making their inner architecture\nand cognitive processes challenging to comprehend. We address this gap by\nadopting approaches to understanding emerging cognition in biology and\ndeveloping a network-based framework that links cognitive skills, LLM\narchitectures, and datasets, ushering in a paradigm shift in foundation model\nanalysis. The skill distribution in the module communities demonstrates that\nwhile LLMs do not strictly parallel the focalized specialization observed in\nspecific biological systems, they exhibit unique communities of modules whose\nemergent skill patterns partially mirror the distributed yet interconnected\ncognitive organization seen in avian and small mammalian brains. Our numerical\nresults highlight a key divergence from biological systems to LLMs, where skill\nacquisition benefits substantially from dynamic, cross-regional interactions\nand neural plasticity. By integrating cognitive science principles with machine\nlearning, our framework provides new insights into LLM interpretability and\nsuggests that effective fine-tuning strategies should leverage distributed\nlearning dynamics rather than rigid modular interventions.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18192.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64dfbcb18e2084e1d7b51b46",
      "avatarUrl": "/avatars/fafe30beea2d7e8eec3f3ba985c582f7.svg",
      "fullname": "Kushal Raj Bhandari",
      "name": "KBhandari11",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  }
]