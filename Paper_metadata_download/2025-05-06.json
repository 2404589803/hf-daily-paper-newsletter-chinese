[
  {
    "paper": {
      "id": "2505.02707",
      "authors": [
        {
          "_id": "6819982f17007d963b9d4166",
          "name": "Yemin Shi",
          "hidden": false
        },
        {
          "_id": "6819982f17007d963b9d4167",
          "name": "Yu Shu",
          "hidden": false
        },
        {
          "_id": "6819982f17007d963b9d4168",
          "name": "Siwei Dong",
          "hidden": false
        },
        {
          "_id": "6819982f17007d963b9d4169",
          "name": "Guangyi Liu",
          "hidden": false
        },
        {
          "_id": "6819982f17007d963b9d416a",
          "name": "Jaward Sesay",
          "hidden": false
        },
        {
          "_id": "6819982f17007d963b9d416b",
          "name": "Jingwen Li",
          "hidden": false
        },
        {
          "_id": "6819982f17007d963b9d416c",
          "name": "Zhiting Hu",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/665bfa1b0d71762b8613282d/zbWarqt8nFt0AwhF0gElE.mp4"
      ],
      "publishedAt": "2025-05-05T15:05:01.000Z",
      "submittedOnDailyAt": "2025-05-06T03:36:16.945Z",
      "title": "Voila: Voice-Language Foundation Models for Real-Time Autonomous\n  Interaction and Voice Role-Play",
      "submittedOnDailyBy": {
        "_id": "665bfa1b0d71762b8613282d",
        "avatarUrl": "/avatars/edbde7b1b47032339a1ecc59f8ea8f1a.svg",
        "isPro": false,
        "fullname": "Zhiting Hu",
        "user": "zhitinghu",
        "type": "user"
      },
      "summary": "A voice AI agent that blends seamlessly into daily life would interact with\nhumans in an autonomous, real-time, and emotionally expressive manner. Rather\nthan merely reacting to commands, it would continuously listen, reason, and\nrespond proactively, fostering fluid, dynamic, and emotionally resonant\ninteractions. We introduce Voila, a family of large voice-language foundation\nmodels that make a step towards this vision. Voila moves beyond traditional\npipeline systems by adopting a new end-to-end architecture that enables\nfull-duplex, low-latency conversations while preserving rich vocal nuances such\nas tone, rhythm, and emotion. It achieves a response latency of just 195\nmilliseconds, surpassing the average human response time. Its hierarchical\nmulti-scale Transformer integrates the reasoning capabilities of large language\nmodels (LLMs) with powerful acoustic modeling, enabling natural, persona-aware\nvoice generation -- where users can simply write text instructions to define\nthe speaker's identity, tone, and other characteristics. Moreover, Voila\nsupports over one million pre-built voices and efficient customization of new\nones from brief audio samples as short as 10 seconds. Beyond spoken dialogue,\nVoila is designed as a unified model for a wide range of voice-based\napplications, including automatic speech recognition (ASR), Text-to-Speech\n(TTS), and, with minimal adaptation, multilingual speech translation. Voila is\nfully open-sourced to support open research and accelerate progress toward\nnext-generation human-machine interactions.",
      "upvotes": 39,
      "discussionId": "6819983117007d963b9d4247",
      "projectPage": "https://voila.maitrix.org",
      "githubRepo": "https://github.com/maitrix-org/Voila",
      "ai_keywords": [
        "full-duplex",
        "low-latency conversations",
        "hierarchical multi-scale Transformer",
        "reasoning capabilities",
        "large language models (LLMs)",
        "acoustic modeling",
        "persona-aware voice generation",
        "automatic speech recognition (ASR)",
        "Text-to-Speech (TTS)",
        "multilingual speech translation",
        "pre-built voices",
        "efficient customization"
      ]
    },
    "publishedAt": "2025-05-05T11:05:01.000Z",
    "title": "Voila: Voice-Language Foundation Models for Real-Time Autonomous\n  Interaction and Voice Role-Play",
    "summary": "A voice AI agent that blends seamlessly into daily life would interact with\nhumans in an autonomous, real-time, and emotionally expressive manner. Rather\nthan merely reacting to commands, it would continuously listen, reason, and\nrespond proactively, fostering fluid, dynamic, and emotionally resonant\ninteractions. We introduce Voila, a family of large voice-language foundation\nmodels that make a step towards this vision. Voila moves beyond traditional\npipeline systems by adopting a new end-to-end architecture that enables\nfull-duplex, low-latency conversations while preserving rich vocal nuances such\nas tone, rhythm, and emotion. It achieves a response latency of just 195\nmilliseconds, surpassing the average human response time. Its hierarchical\nmulti-scale Transformer integrates the reasoning capabilities of large language\nmodels (LLMs) with powerful acoustic modeling, enabling natural, persona-aware\nvoice generation -- where users can simply write text instructions to define\nthe speaker's identity, tone, and other characteristics. Moreover, Voila\nsupports over one million pre-built voices and efficient customization of new\nones from brief audio samples as short as 10 seconds. Beyond spoken dialogue,\nVoila is designed as a unified model for a wide range of voice-based\napplications, including automatic speech recognition (ASR), Text-to-Speech\n(TTS), and, with minimal adaptation, multilingual speech translation. Voila is\nfully open-sourced to support open research and accelerate progress toward\nnext-generation human-machine interactions.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/665bfa1b0d71762b8613282d/zbWarqt8nFt0AwhF0gElE.mp4"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02707.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "665bfa1b0d71762b8613282d",
      "avatarUrl": "/avatars/edbde7b1b47032339a1ecc59f8ea8f1a.svg",
      "fullname": "Zhiting Hu",
      "name": "zhitinghu",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.02387",
      "authors": [
        {
          "_id": "681988d6d6a5fee26b52ac28",
          "name": "Xiusi Chen",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac29",
          "name": "Gaotang Li",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac2a",
          "name": "Ziqi Wang",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac2b",
          "name": "Bowen Jin",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac2c",
          "name": "Cheng Qian",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac2d",
          "name": "Yu Wang",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac2e",
          "name": "Hongru Wang",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac2f",
          "name": "Yu Zhang",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac30",
          "name": "Denghui Zhang",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac31",
          "name": "Tong Zhang",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac32",
          "name": "Hanghang Tong",
          "hidden": false
        },
        {
          "_id": "681988d6d6a5fee26b52ac33",
          "name": "Heng Ji",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T06:11:12.000Z",
      "submittedOnDailyAt": "2025-05-06T02:32:05.558Z",
      "title": "RM-R1: Reward Modeling as Reasoning",
      "submittedOnDailyBy": {
        "_id": "654d784d71a30c4bca09a319",
        "avatarUrl": "/avatars/ab9f93122903ccd662267232bab30ad8.svg",
        "isPro": false,
        "fullname": "Gaotang Li",
        "user": "gaotang",
        "type": "user"
      },
      "summary": "Reward modeling is essential for aligning large language models (LLMs) with\nhuman preferences, especially through reinforcement learning from human\nfeedback (RLHF). To provide accurate reward signals, a reward model (RM) should\nstimulate deep thinking and conduct interpretable reasoning before assigning a\nscore or a judgment. However, existing RMs either produce opaque scalar scores\nor directly generate the prediction of a preferred answer, making them struggle\nto integrate natural language critiques, thus lacking interpretability.\nInspired by recent advances of long chain-of-thought (CoT) on\nreasoning-intensive tasks, we hypothesize and validate that integrating\nreasoning capabilities into reward modeling significantly enhances RM's\ninterpretability and performance. In this work, we introduce a new class of\ngenerative reward models -- Reasoning Reward Models (ReasRMs) -- which\nformulate reward modeling as a reasoning task. We propose a reasoning-oriented\ntraining pipeline and train a family of ReasRMs, RM-R1. The training consists\nof two key stages: (1) distillation of high-quality reasoning chains and (2)\nreinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by\nself-generating reasoning traces or chat-specific rubrics and evaluating\ncandidate responses against them. Empirically, our models achieve\nstate-of-the-art or near state-of-the-art performance of generative RMs across\nmultiple comprehensive reward model benchmarks, outperforming much larger\nopen-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by\nup to 13.8%. Beyond final performance, we perform thorough empirical analysis\nto understand the key ingredients of successful ReasRM training. To facilitate\nfuture research, we release six ReasRM models along with code and data at\nhttps://github.com/RM-R1-UIUC/RM-R1.",
      "upvotes": 26,
      "discussionId": "681988d7d6a5fee26b52ac7e",
      "githubRepo": "https://github.com/RM-R1-UIUC/RM-R1",
      "ai_keywords": [
        "reward modeling",
        "reinforcement learning from human feedback (RLHF)",
        "reward model (RM)",
        "scalar scores",
        "preferred answer",
        "natural language critiques",
        "long chain-of-thought (CoT)",
        "reasoning capabilities",
        "Reasoning Reward Models (ReasRMs)",
        "reasoning-oriented training pipeline",
        "distillation",
        "high-quality reasoning chains",
        "reinforcement learning",
        "verifiable rewards",
        "LLM rollouts",
        "self-generating reasoning traces",
        "chat-specific rubrics",
        "candidate responses",
        "generative reward models",
        "state-of-the-art",
        "near state-of-the-art",
        "reward model benchmarks",
        "open-weight models",
        "proprietary models",
        "empirical analysis",
        "ReasRM models"
      ]
    },
    "publishedAt": "2025-05-05T02:11:12.000Z",
    "title": "RM-R1: Reward Modeling as Reasoning",
    "summary": "Reward modeling is essential for aligning large language models (LLMs) with\nhuman preferences, especially through reinforcement learning from human\nfeedback (RLHF). To provide accurate reward signals, a reward model (RM) should\nstimulate deep thinking and conduct interpretable reasoning before assigning a\nscore or a judgment. However, existing RMs either produce opaque scalar scores\nor directly generate the prediction of a preferred answer, making them struggle\nto integrate natural language critiques, thus lacking interpretability.\nInspired by recent advances of long chain-of-thought (CoT) on\nreasoning-intensive tasks, we hypothesize and validate that integrating\nreasoning capabilities into reward modeling significantly enhances RM's\ninterpretability and performance. In this work, we introduce a new class of\ngenerative reward models -- Reasoning Reward Models (ReasRMs) -- which\nformulate reward modeling as a reasoning task. We propose a reasoning-oriented\ntraining pipeline and train a family of ReasRMs, RM-R1. The training consists\nof two key stages: (1) distillation of high-quality reasoning chains and (2)\nreinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by\nself-generating reasoning traces or chat-specific rubrics and evaluating\ncandidate responses against them. Empirically, our models achieve\nstate-of-the-art or near state-of-the-art performance of generative RMs across\nmultiple comprehensive reward model benchmarks, outperforming much larger\nopen-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by\nup to 13.8%. Beyond final performance, we perform thorough empirical analysis\nto understand the key ingredients of successful ReasRM training. To facilitate\nfuture research, we release six ReasRM models along with code and data at\nhttps://github.com/RM-R1-UIUC/RM-R1.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02387.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "654d784d71a30c4bca09a319",
      "avatarUrl": "/avatars/ab9f93122903ccd662267232bab30ad8.svg",
      "fullname": "Gaotang Li",
      "name": "gaotang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.02835",
      "authors": [
        {
          "_id": "6819762e64ae18f1b6fde347",
          "name": "Yi-Fan Zhang",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde348",
          "name": "Xingyu Lu",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde349",
          "name": "Xiao Hu",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde34a",
          "name": "Chaoyou Fu",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde34b",
          "name": "Bin Wen",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde34c",
          "name": "Tianke Zhang",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde34d",
          "name": "Changyi Liu",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde34e",
          "name": "Kaiyu Jiang",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde34f",
          "name": "Kaibing Chen",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde350",
          "name": "Kaiyu Tang",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde351",
          "name": "Haojie Ding",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde352",
          "name": "Jiankang Chen",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde353",
          "name": "Fan Yang",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde354",
          "name": "Zhang Zhang",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde355",
          "name": "Tingting Gao",
          "hidden": false
        },
        {
          "_id": "6819762e64ae18f1b6fde356",
          "name": "Liang Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T17:59:50.000Z",
      "submittedOnDailyAt": "2025-05-06T01:09:45.446Z",
      "title": "R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement\n  Learning",
      "submittedOnDailyBy": {
        "_id": "623d8ca4c29adf5ef6175615",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
        "isPro": false,
        "fullname": "Yi-Fan Zhang",
        "user": "yifanzhang114",
        "type": "user"
      },
      "summary": "Multimodal Reward Models (MRMs) play a crucial role in enhancing the\nperformance of Multimodal Large Language Models (MLLMs). While recent\nadvancements have primarily focused on improving the model structure and\ntraining data of MRMs, there has been limited exploration into the\neffectiveness of long-term reasoning capabilities for reward modeling and how\nto activate these capabilities in MRMs. In this paper, we explore how\nReinforcement Learning (RL) can be used to improve reward modeling.\nSpecifically, we reformulate the reward modeling problem as a rule-based RL\ntask. However, we observe that directly applying existing RL algorithms, such\nas Reinforce++, to reward modeling often leads to training instability or even\ncollapse due to the inherent limitations of these algorithms. To address this\nissue, we propose the StableReinforce algorithm, which refines the training\nloss, advantage estimation strategy, and reward design of existing RL methods.\nThese refinements result in more stable training dynamics and superior\nperformance. To facilitate MRM training, we collect 200K preference data from\ndiverse datasets. Our reward model, R1-Reward, trained using the\nStableReinforce algorithm on this dataset, significantly improves performance\non multimodal reward modeling benchmarks. Compared to previous SOTA models,\nR1-Reward achieves a 8.4% improvement on the VL Reward-Bench and a 14.3%\nimprovement on the Multimodal Reward Bench. Moreover, with more inference\ncompute, R1-Reward's performance is further enhanced, highlighting the\npotential of RL algorithms in optimizing MRMs.",
      "upvotes": 13,
      "discussionId": "6819762f64ae18f1b6fde387",
      "projectPage": "https://github.com/yfzhang114/r1_reward",
      "githubRepo": "https://github.com/yfzhang114/r1_reward",
      "ai_keywords": [
        "Multimodal Reward Models (MRMs)",
        "Multimodal Large Language Models (MLLMs)",
        "Reinforcement Learning (RL)",
        "rule-based RL task",
        "Reinforce++",
        "StableReinforce",
        "training loss",
        "advantage estimation strategy",
        "reward design",
        "preference data",
        "VL Reward-Bench",
        "Multimodal Reward Bench"
      ]
    },
    "publishedAt": "2025-05-05T13:59:50.000Z",
    "title": "R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement\n  Learning",
    "summary": "Multimodal Reward Models (MRMs) play a crucial role in enhancing the\nperformance of Multimodal Large Language Models (MLLMs). While recent\nadvancements have primarily focused on improving the model structure and\ntraining data of MRMs, there has been limited exploration into the\neffectiveness of long-term reasoning capabilities for reward modeling and how\nto activate these capabilities in MRMs. In this paper, we explore how\nReinforcement Learning (RL) can be used to improve reward modeling.\nSpecifically, we reformulate the reward modeling problem as a rule-based RL\ntask. However, we observe that directly applying existing RL algorithms, such\nas Reinforce++, to reward modeling often leads to training instability or even\ncollapse due to the inherent limitations of these algorithms. To address this\nissue, we propose the StableReinforce algorithm, which refines the training\nloss, advantage estimation strategy, and reward design of existing RL methods.\nThese refinements result in more stable training dynamics and superior\nperformance. To facilitate MRM training, we collect 200K preference data from\ndiverse datasets. Our reward model, R1-Reward, trained using the\nStableReinforce algorithm on this dataset, significantly improves performance\non multimodal reward modeling benchmarks. Compared to previous SOTA models,\nR1-Reward achieves a 8.4% improvement on the VL Reward-Bench and a 14.3%\nimprovement on the Multimodal Reward Bench. Moreover, with more inference\ncompute, R1-Reward's performance is further enhanced, highlighting the\npotential of RL algorithms in optimizing MRMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02835.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "623d8ca4c29adf5ef6175615",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
      "fullname": "Yi-Fan Zhang",
      "name": "yifanzhang114",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 8
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.02735",
      "authors": [
        {
          "_id": "6819742e0d1c56fe9124fe3a",
          "name": "Zhouliang Yu",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe3b",
          "name": "Ruotian Peng",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe3c",
          "name": "Keyi Ding",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe3d",
          "name": "Yizhe Li",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe3e",
          "name": "Zhongyuan Peng",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe3f",
          "name": "Minghao Liu",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe40",
          "name": "Yifan Zhang",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe41",
          "name": "Zheng Yuan",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe42",
          "name": "Huajian Xin",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe43",
          "name": "Wenhao Huang",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe44",
          "name": "Yandong Wen",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe45",
          "name": "Ge Zhang",
          "hidden": false
        },
        {
          "_id": "6819742e0d1c56fe9124fe46",
          "name": "Weiyang Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T15:37:00.000Z",
      "submittedOnDailyAt": "2025-05-06T01:00:48.636Z",
      "title": "FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language\n  Models",
      "submittedOnDailyBy": {
        "_id": "62a80fe3ac97233f1625235a",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a80fe3ac97233f1625235a/_rGtpqdY7OEBz3pyqb6fE.jpeg",
        "isPro": false,
        "fullname": "Zhouliang Yu",
        "user": "zhouliang",
        "type": "user"
      },
      "summary": "Formal mathematical reasoning remains a critical challenge for artificial\nintelligence, hindered by limitations of existing benchmarks in scope and\nscale. To address this, we present FormalMATH, a large-scale Lean4 benchmark\ncomprising 5,560 formally verified problems spanning from high-school Olympiad\nchallenges to undergraduate-level theorems across diverse domains (e.g.,\nalgebra, applied mathematics, calculus, number theory, and discrete\nmathematics). To mitigate the inefficiency of manual formalization, we\nintroduce a novel human-in-the-loop autoformalization pipeline that integrates:\n(1) specialized large language models (LLMs) for statement autoformalization,\n(2) multi-LLM semantic verification, and (3) negation-based disproof filtering\nstrategies using off-the-shelf LLM-based provers. This approach reduces expert\nannotation costs by retaining 72.09% of statements before manual verification\nwhile ensuring fidelity to the original natural-language problems. Our\nevaluation of state-of-the-art LLM-based theorem provers reveals significant\nlimitations: even the strongest models achieve only 16.46% success rate under\npractical sampling budgets, exhibiting pronounced domain bias (e.g., excelling\nin algebra but failing in calculus) and over-reliance on simplified automation\ntactics. Notably, we identify a counterintuitive inverse relationship between\nnatural-language solution guidance and proof success in chain-of-thought\nreasoning scenarios, suggesting that human-written informal reasoning\nintroduces noise rather than clarity in the formal reasoning settings. We\nbelieve that FormalMATH provides a robust benchmark for benchmarking formal\nmathematical reasoning.",
      "upvotes": 13,
      "discussionId": "6819742f0d1c56fe9124fe8a",
      "projectPage": "https://spherelab.ai/FormalMATH/",
      "githubRepo": "https://github.com/Sphere-AI-Lab/FormalMATH-Bench"
    },
    "publishedAt": "2025-05-05T11:37:00.000Z",
    "title": "FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language\n  Models",
    "summary": "Formal mathematical reasoning remains a critical challenge for artificial\nintelligence, hindered by limitations of existing benchmarks in scope and\nscale. To address this, we present FormalMATH, a large-scale Lean4 benchmark\ncomprising 5,560 formally verified problems spanning from high-school Olympiad\nchallenges to undergraduate-level theorems across diverse domains (e.g.,\nalgebra, applied mathematics, calculus, number theory, and discrete\nmathematics). To mitigate the inefficiency of manual formalization, we\nintroduce a novel human-in-the-loop autoformalization pipeline that integrates:\n(1) specialized large language models (LLMs) for statement autoformalization,\n(2) multi-LLM semantic verification, and (3) negation-based disproof filtering\nstrategies using off-the-shelf LLM-based provers. This approach reduces expert\nannotation costs by retaining 72.09% of statements before manual verification\nwhile ensuring fidelity to the original natural-language problems. Our\nevaluation of state-of-the-art LLM-based theorem provers reveals significant\nlimitations: even the strongest models achieve only 16.46% success rate under\npractical sampling budgets, exhibiting pronounced domain bias (e.g., excelling\nin algebra but failing in calculus) and over-reliance on simplified automation\ntactics. Notably, we identify a counterintuitive inverse relationship between\nnatural-language solution guidance and proof success in chain-of-thought\nreasoning scenarios, suggesting that human-written informal reasoning\nintroduces noise rather than clarity in the formal reasoning settings. We\nbelieve that FormalMATH provides a robust benchmark for benchmarking formal\nmathematical reasoning.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02735.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "62a80fe3ac97233f1625235a",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a80fe3ac97233f1625235a/_rGtpqdY7OEBz3pyqb6fE.jpeg",
      "fullname": "Zhouliang Yu",
      "name": "zhouliang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 10
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.02391",
      "authors": [
        {
          "_id": "6819a63c64ae18f1b60a5c43",
          "name": "Jiarui Yao",
          "hidden": false
        },
        {
          "_id": "6819a63c64ae18f1b60a5c44",
          "name": "Yifan Hao",
          "hidden": false
        },
        {
          "_id": "6819a63c64ae18f1b60a5c45",
          "name": "Hanning Zhang",
          "hidden": false
        },
        {
          "_id": "6819a63c64ae18f1b60a5c46",
          "name": "Hanze Dong",
          "hidden": false
        },
        {
          "_id": "6819a63c64ae18f1b60a5c47",
          "name": "Wei Xiong",
          "hidden": false
        },
        {
          "_id": "6819a63c64ae18f1b60a5c48",
          "name": "Nan Jiang",
          "hidden": false
        },
        {
          "_id": "6819a63c64ae18f1b60a5c49",
          "name": "Tong Zhang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T06:26:00.000Z",
      "submittedOnDailyAt": "2025-05-06T04:34:14.120Z",
      "title": "Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization\n  in Rejection Sampling and RL",
      "submittedOnDailyBy": {
        "_id": "64d45451c34a346181b130dd",
        "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
        "isPro": false,
        "fullname": "Rui Yang",
        "user": "Ray2333",
        "type": "user"
      },
      "summary": "Chain-of-thought (CoT) reasoning in large language models (LLMs) can be\nformalized as a latent variable problem, where the model needs to generate\nintermediate reasoning steps. While prior approaches such as iterative\nreward-ranked fine-tuning (RAFT) have relied on such formulations, they\ntypically apply uniform inference budgets across prompts, which fails to\naccount for variability in difficulty and convergence behavior. This work\nidentifies the main bottleneck in CoT training as inefficient stochastic\ngradient estimation due to static sampling strategies. We propose GVM-RAFT, a\nprompt-specific Dynamic Sample Allocation Strategy designed to minimize\nstochastic gradient variance under a computational budget constraint. The\nmethod dynamically allocates computational resources by monitoring prompt\nacceptance rates and stochastic gradient norms, ensuring that the resulting\ngradient variance is minimized. Our theoretical analysis shows that the\nproposed dynamic sampling strategy leads to accelerated convergence guarantees\nunder suitable conditions. Experiments on mathematical reasoning show that\nGVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over\nvanilla RAFT. The proposed dynamic sampling strategy is general and can be\nincorporated into other reinforcement learning algorithms, such as GRPO,\nleading to similar improvements in convergence and test accuracy. Our code is\navailable at https://github.com/RLHFlow/GVM.",
      "upvotes": 13,
      "discussionId": "6819a63d64ae18f1b60a5c75",
      "ai_keywords": [
        "Chain-of-thought (CoT)",
        "latent variable problem",
        "iterative reward-ranked fine-tuning (RAFT)",
        "inference budget",
        "static sampling strategies",
        "GVM-RAFT",
        "Dynamic Sample Allocation Strategy",
        "prompt-specific",
        "computational budget constraint",
        "prompt acceptance rates",
        "stochastic gradient norms",
        "stochastic gradient variance",
        "accelerated convergence guarantees",
        "GRPO",
        "convergence",
        "test accuracy"
      ]
    },
    "publishedAt": "2025-05-05T02:26:00.000Z",
    "title": "Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization\n  in Rejection Sampling and RL",
    "summary": "Chain-of-thought (CoT) reasoning in large language models (LLMs) can be\nformalized as a latent variable problem, where the model needs to generate\nintermediate reasoning steps. While prior approaches such as iterative\nreward-ranked fine-tuning (RAFT) have relied on such formulations, they\ntypically apply uniform inference budgets across prompts, which fails to\naccount for variability in difficulty and convergence behavior. This work\nidentifies the main bottleneck in CoT training as inefficient stochastic\ngradient estimation due to static sampling strategies. We propose GVM-RAFT, a\nprompt-specific Dynamic Sample Allocation Strategy designed to minimize\nstochastic gradient variance under a computational budget constraint. The\nmethod dynamically allocates computational resources by monitoring prompt\nacceptance rates and stochastic gradient norms, ensuring that the resulting\ngradient variance is minimized. Our theoretical analysis shows that the\nproposed dynamic sampling strategy leads to accelerated convergence guarantees\nunder suitable conditions. Experiments on mathematical reasoning show that\nGVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over\nvanilla RAFT. The proposed dynamic sampling strategy is general and can be\nincorporated into other reinforcement learning algorithms, such as GRPO,\nleading to similar improvements in convergence and test accuracy. Our code is\navailable at https://github.com/RLHFlow/GVM.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02391.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64d45451c34a346181b130dd",
      "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
      "fullname": "Rui Yang",
      "name": "Ray2333",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 11
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.02094",
      "authors": [
        {
          "_id": "681992911e0fae3880173d43",
          "name": "Runyi Yu",
          "hidden": false
        },
        {
          "_id": "681992911e0fae3880173d44",
          "name": "Yinhuai Wang",
          "hidden": false
        },
        {
          "_id": "681992911e0fae3880173d45",
          "name": "Qihan Zhao",
          "hidden": false
        },
        {
          "_id": "681992911e0fae3880173d46",
          "name": "Hok Wai Tsui",
          "hidden": false
        },
        {
          "_id": "681992911e0fae3880173d47",
          "name": "Jingbo Wang",
          "hidden": false
        },
        {
          "_id": "681992911e0fae3880173d48",
          "name": "Ping Tan",
          "hidden": false
        },
        {
          "_id": "681992911e0fae3880173d49",
          "name": "Qifeng Chen",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-04T13:00:29.000Z",
      "submittedOnDailyAt": "2025-05-06T03:11:28.738Z",
      "title": "SkillMimic-V2: Learning Robust and Generalizable Interaction Skills from\n  Sparse and Noisy Demonstrations",
      "submittedOnDailyBy": {
        "_id": "66d59dc9b005ad82ca6fc61d",
        "avatarUrl": "/avatars/0ba424690afd1144a89665c5bacdfde7.svg",
        "isPro": false,
        "fullname": "Runyi YU",
        "user": "IngridYU",
        "type": "user"
      },
      "summary": "We address a fundamental challenge in Reinforcement Learning from Interaction\nDemonstration (RLID): demonstration noise and coverage limitations. While\nexisting data collection approaches provide valuable interaction\ndemonstrations, they often yield sparse, disconnected, and noisy trajectories\nthat fail to capture the full spectrum of possible skill variations and\ntransitions. Our key insight is that despite noisy and sparse demonstrations,\nthere exist infinite physically feasible trajectories that naturally bridge\nbetween demonstrated skills or emerge from their neighboring states, forming a\ncontinuous space of possible skill variations and transitions. Building upon\nthis insight, we present two data augmentation techniques: a Stitched\nTrajectory Graph (STG) that discovers potential transitions between\ndemonstration skills, and a State Transition Field (STF) that establishes\nunique connections for arbitrary states within the demonstration neighborhood.\nTo enable effective RLID with augmented data, we develop an Adaptive Trajectory\nSampling (ATS) strategy for dynamic curriculum generation and a historical\nencoding mechanism for memory-dependent skill learning. Our approach enables\nrobust skill acquisition that significantly generalizes beyond the reference\ndemonstrations. Extensive experiments across diverse interaction tasks\ndemonstrate substantial improvements over state-of-the-art methods in terms of\nconvergence stability, generalization capability, and recovery robustness.",
      "upvotes": 11,
      "discussionId": "681992931e0fae3880173dcf",
      "ai_keywords": [
        "Reinforcement Learning from Interaction Demonstration (RLID)",
        "demonstration noise",
        "coverage limitations",
        "interaction demonstrations",
        "sparse trajectories",
        "disconnected trajectories",
        "noise",
        "skill variations",
        "transitions",
        "physically feasible trajectories",
        "Stitched Trajectory Graph (STG)",
        "State Transition Field (STF)",
        "Adaptive Trajectory Sampling (ATS)",
        "dynamic curriculum generation",
        "historical encoding mechanism",
        "skill acquisition",
        "convergence stability",
        "generalization capability",
        "recovery robustness"
      ]
    },
    "publishedAt": "2025-05-04T09:00:29.000Z",
    "title": "SkillMimic-V2: Learning Robust and Generalizable Interaction Skills from\n  Sparse and Noisy Demonstrations",
    "summary": "We address a fundamental challenge in Reinforcement Learning from Interaction\nDemonstration (RLID): demonstration noise and coverage limitations. While\nexisting data collection approaches provide valuable interaction\ndemonstrations, they often yield sparse, disconnected, and noisy trajectories\nthat fail to capture the full spectrum of possible skill variations and\ntransitions. Our key insight is that despite noisy and sparse demonstrations,\nthere exist infinite physically feasible trajectories that naturally bridge\nbetween demonstrated skills or emerge from their neighboring states, forming a\ncontinuous space of possible skill variations and transitions. Building upon\nthis insight, we present two data augmentation techniques: a Stitched\nTrajectory Graph (STG) that discovers potential transitions between\ndemonstration skills, and a State Transition Field (STF) that establishes\nunique connections for arbitrary states within the demonstration neighborhood.\nTo enable effective RLID with augmented data, we develop an Adaptive Trajectory\nSampling (ATS) strategy for dynamic curriculum generation and a historical\nencoding mechanism for memory-dependent skill learning. Our approach enables\nrobust skill acquisition that significantly generalizes beyond the reference\ndemonstrations. Extensive experiments across diverse interaction tasks\ndemonstrate substantial improvements over state-of-the-art methods in terms of\nconvergence stability, generalization capability, and recovery robustness.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02094.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "66d59dc9b005ad82ca6fc61d",
      "avatarUrl": "/avatars/0ba424690afd1144a89665c5bacdfde7.svg",
      "fullname": "Runyi YU",
      "name": "IngridYU",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.02156",
      "authors": [
        {
          "_id": "681975a9fdcf582e6d0effdb",
          "name": "Minzheng Wang",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effdc",
          "name": "Yongbin Li",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effdd",
          "name": "Haobo Wang",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effde",
          "name": "Xinghua Zhang",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effdf",
          "name": "Nan Xu",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effe0",
          "name": "Bingli Wu",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effe1",
          "name": "Fei Huang",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effe2",
          "name": "Haiyang Yu",
          "hidden": false
        },
        {
          "_id": "681975a9fdcf582e6d0effe3",
          "name": "Wenji Mao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-04T15:39:58.000Z",
      "submittedOnDailyAt": "2025-05-06T01:07:27.275Z",
      "title": "Think on your Feet: Adaptive Thinking via Reinforcement Learning for\n  Social Agents",
      "submittedOnDailyBy": {
        "_id": "64bcc373ef8c0e42bf16acc5",
        "avatarUrl": "/avatars/873308203d28115ae1a9e4d0e26508f4.svg",
        "isPro": false,
        "fullname": "mz.w",
        "user": "iiiiwis",
        "type": "user"
      },
      "summary": "Effective social intelligence simulation requires language agents to\ndynamically adjust reasoning depth, a capability notably absent in current\napproaches. While existing methods either lack this kind of reasoning\ncapability or enforce uniform long chain-of-thought reasoning across all\nscenarios, resulting in excessive token usage and inappropriate social\nsimulation. In this paper, we propose Adaptive Mode\nLearning (AML) that strategically selects from four\nthinking modes (intuitive reaction rightarrow deep contemplation) based on\nreal-time context. Our framework's core innovation, the Adaptive\nMode Policy Optimization (AMPO)\nalgorithm, introduces three key advancements over existing methods: (1)\nMulti-granular thinking mode design, (2) Context-aware mode switching across\nsocial interaction, and (3) Token-efficient reasoning via depth-adaptive\nprocessing. Extensive experiments on social intelligence tasks confirm that AML\nachieves 15.6% higher task performance than state-of-the-art methods. Notably,\nour method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These\nresults demonstrate that context-sensitive thinking mode selection, as\nimplemented in AMPO, enables more human-like adaptive reasoning than GRPO's\nfixed-depth approach",
      "upvotes": 9,
      "discussionId": "681975a9fdcf582e6d0f0014",
      "githubRepo": "https://github.com/MozerWang/AMPO",
      "ai_keywords": [
        "Adaptive Mode Learning (AML)",
        "Adaptive Mode Policy Optimization (AMPO)",
        "multi-granular thinking mode design",
        "context-aware mode switching",
        "token-efficient reasoning",
        "depth-adaptive processing",
        "intuitive reaction",
        "deep contemplation",
        "social interaction",
        "reasoning chains",
        "fixed-depth approach"
      ]
    },
    "publishedAt": "2025-05-04T11:39:58.000Z",
    "title": "Think on your Feet: Adaptive Thinking via Reinforcement Learning for\n  Social Agents",
    "summary": "Effective social intelligence simulation requires language agents to\ndynamically adjust reasoning depth, a capability notably absent in current\napproaches. While existing methods either lack this kind of reasoning\ncapability or enforce uniform long chain-of-thought reasoning across all\nscenarios, resulting in excessive token usage and inappropriate social\nsimulation. In this paper, we propose Adaptive Mode\nLearning (AML) that strategically selects from four\nthinking modes (intuitive reaction rightarrow deep contemplation) based on\nreal-time context. Our framework's core innovation, the Adaptive\nMode Policy Optimization (AMPO)\nalgorithm, introduces three key advancements over existing methods: (1)\nMulti-granular thinking mode design, (2) Context-aware mode switching across\nsocial interaction, and (3) Token-efficient reasoning via depth-adaptive\nprocessing. Extensive experiments on social intelligence tasks confirm that AML\nachieves 15.6% higher task performance than state-of-the-art methods. Notably,\nour method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These\nresults demonstrate that context-sensitive thinking mode selection, as\nimplemented in AMPO, enables more human-like adaptive reasoning than GRPO's\nfixed-depth approach",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02156.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64bcc373ef8c0e42bf16acc5",
      "avatarUrl": "/avatars/873308203d28115ae1a9e4d0e26508f4.svg",
      "fullname": "mz.w",
      "name": "iiiiwis",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 2
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.01441",
      "authors": [
        {
          "_id": "68198aea57d4de18fb3e69d6",
          "name": "Joykirat Singh",
          "hidden": false
        },
        {
          "_id": "68198aea57d4de18fb3e69d7",
          "name": "Raghav Magazine",
          "hidden": false
        },
        {
          "_id": "68198aea57d4de18fb3e69d8",
          "user": {
            "_id": "64aba383fddf117e6e5ba818",
            "avatarUrl": "/avatars/ee7d25d865b34be5902872d060ad9153.svg",
            "isPro": false,
            "fullname": "Akshay  Nambi",
            "user": "akshaynambi",
            "type": "user"
          },
          "name": "Yash Pandya",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2025-05-06T04:08:10.843Z",
          "hidden": false
        },
        {
          "_id": "68198aea57d4de18fb3e69d9",
          "name": "Akshay Nambi",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-28T10:42:49.000Z",
      "submittedOnDailyAt": "2025-05-06T02:43:42.049Z",
      "title": "Agentic Reasoning and Tool Integration for LLMs via Reinforcement\n  Learning",
      "submittedOnDailyBy": {
        "_id": "64aba383fddf117e6e5ba818",
        "avatarUrl": "/avatars/ee7d25d865b34be5902872d060ad9153.svg",
        "isPro": false,
        "fullname": "Akshay  Nambi",
        "user": "akshaynambi",
        "type": "user"
      },
      "summary": "Large language models (LLMs) have achieved remarkable progress in complex\nreasoning tasks, yet they remain fundamentally limited by their reliance on\nstatic internal knowledge and text-only reasoning. Real-world problem solving\noften demands dynamic, multi-step reasoning, adaptive decision making, and the\nability to interact with external tools and environments. In this work, we\nintroduce ARTIST (Agentic Reasoning and Tool Integration in Self-improving\nTransformers), a unified framework that tightly couples agentic reasoning,\nreinforcement learning, and tool integration for LLMs. ARTIST enables models to\nautonomously decide when, how, and which tools to invoke within multi-turn\nreasoning chains, leveraging outcome-based RL to learn robust strategies for\ntool use and environment interaction without requiring step-level supervision.\nExtensive experiments on mathematical reasoning and multi-turn function calling\nbenchmarks show that ARTIST consistently outperforms state-of-the-art\nbaselines, with up to 22% absolute improvement over base models and strong\ngains on the most challenging tasks. Detailed studies and metric analyses\nreveal that agentic RL training leads to deeper reasoning, more effective tool\nuse, and higher-quality solutions. Our results establish agentic RL with tool\nintegration as a powerful new frontier for robust, interpretable, and\ngeneralizable problem-solving in LLMs.",
      "upvotes": 8,
      "discussionId": "68198aec57d4de18fb3e6a30",
      "projectPage": "https://www.microsoft.com/en-us/research/people/akshayn/unlocking-agentic-reasoning-in-llms/",
      "ai_keywords": [
        "agentic reasoning",
        "reinforcement learning",
        "tool integration",
        "ARTIST",
        "multi-turn reasoning chains",
        "outcome-based RL",
        "mathematical reasoning",
        "function calling",
        "agentic RL",
        "tool use",
        "environment interaction"
      ]
    },
    "publishedAt": "2025-04-28T06:42:49.000Z",
    "title": "Agentic Reasoning and Tool Integration for LLMs via Reinforcement\n  Learning",
    "summary": "Large language models (LLMs) have achieved remarkable progress in complex\nreasoning tasks, yet they remain fundamentally limited by their reliance on\nstatic internal knowledge and text-only reasoning. Real-world problem solving\noften demands dynamic, multi-step reasoning, adaptive decision making, and the\nability to interact with external tools and environments. In this work, we\nintroduce ARTIST (Agentic Reasoning and Tool Integration in Self-improving\nTransformers), a unified framework that tightly couples agentic reasoning,\nreinforcement learning, and tool integration for LLMs. ARTIST enables models to\nautonomously decide when, how, and which tools to invoke within multi-turn\nreasoning chains, leveraging outcome-based RL to learn robust strategies for\ntool use and environment interaction without requiring step-level supervision.\nExtensive experiments on mathematical reasoning and multi-turn function calling\nbenchmarks show that ARTIST consistently outperforms state-of-the-art\nbaselines, with up to 22% absolute improvement over base models and strong\ngains on the most challenging tasks. Detailed studies and metric analyses\nreveal that agentic RL training leads to deeper reasoning, more effective tool\nuse, and higher-quality solutions. Our results establish agentic RL with tool\nintegration as a powerful new frontier for robust, interpretable, and\ngeneralizable problem-solving in LLMs.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01441.png",
    "numComments": 2,
    "submittedBy": {
      "_id": "64aba383fddf117e6e5ba818",
      "avatarUrl": "/avatars/ee7d25d865b34be5902872d060ad9153.svg",
      "fullname": "Akshay  Nambi",
      "name": "akshaynambi",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.02370",
      "authors": [
        {
          "_id": "68197c200e4203d6bc84cdfb",
          "name": "Ming Li",
          "hidden": false
        },
        {
          "_id": "68197c200e4203d6bc84cdfc",
          "name": "Xin Gu",
          "hidden": false
        },
        {
          "_id": "68197c200e4203d6bc84cdfd",
          "name": "Fan Chen",
          "hidden": false
        },
        {
          "_id": "68197c200e4203d6bc84cdfe",
          "name": "Xiaoying Xing",
          "hidden": false
        },
        {
          "_id": "68197c200e4203d6bc84cdff",
          "name": "Longyin Wen",
          "hidden": false
        },
        {
          "_id": "68197c200e4203d6bc84ce00",
          "name": "Chen Chen",
          "hidden": false
        },
        {
          "_id": "68197c200e4203d6bc84ce01",
          "user": {
            "_id": "65cbdea6d6c974694f09249a",
            "avatarUrl": "/avatars/a317a1f545117e0699e1c56258980fd8.svg",
            "isPro": false,
            "fullname": "Sijie Zhu",
            "user": "Zilence006",
            "type": "user"
          },
          "name": "Sijie Zhu",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-06T03:04:04.536Z",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T05:19:40.000Z",
      "submittedOnDailyAt": "2025-05-06T01:34:41.608Z",
      "title": "SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based\n  Image Editing",
      "submittedOnDailyBy": {
        "_id": "637f0eb22438d7485b8ef5d7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f0eb22438d7485b8ef5d7/70h7dekqj7LuBobOXckmJ.jpeg",
        "isPro": false,
        "fullname": "Ming Li",
        "user": "limingcv",
        "type": "user"
      },
      "summary": "Due to the challenges of manually collecting accurate editing data, existing\ndatasets are typically constructed using various automated methods, leading to\nnoisy supervision signals caused by the mismatch between editing instructions\nand original-edited image pairs. Recent efforts attempt to improve editing\nmodels through generating higher-quality edited images, pre-training on\nrecognition tasks, or introducing vision-language models (VLMs) but fail to\nresolve this fundamental issue. In this paper, we offer a novel solution by\nconstructing more effective editing instructions for given image pairs. This\nincludes rectifying the editing instructions to better align with the\noriginal-edited image pairs and using contrastive editing instructions to\nfurther enhance their effectiveness. Specifically, we find that editing models\nexhibit specific generation attributes at different inference steps,\nindependent of the text. Based on these prior attributes, we define a unified\nguide for VLMs to rectify editing instructions. However, there are some\nchallenging editing scenarios that cannot be resolved solely with rectified\ninstructions. To this end, we further construct contrastive supervision signals\nwith positive and negative instructions and introduce them into the model\ntraining using triplet loss, thereby further facilitating supervision\neffectiveness. Our method does not require the VLM modules or pre-training\ntasks used in previous work, offering a more direct and efficient way to\nprovide better supervision signals, and providing a novel, simple, and\neffective solution for instruction-based image editing. Results on multiple\nbenchmarks demonstrate that our method significantly outperforms existing\napproaches. Compared with previous SOTA SmartEdit, we achieve 9.19%\nimprovements on the Real-Edit benchmark with 30x less training data and 13x\nsmaller model size.",
      "upvotes": 7,
      "discussionId": "68197c240e4203d6bc84cee9",
      "projectPage": "https://liming-ai.github.io/SuperEdit/",
      "githubRepo": "https://github.com/bytedance/SuperEdit",
      "ai_keywords": [
        "contrastive editing instructions",
        "triplet loss",
        "instruction-based image editing",
        "contrastive supervision signals",
        "generation attributes",
        "unified guide",
        "vision-language models (VLMs)",
        "real-edit benchmark",
        "smartedit"
      ]
    },
    "publishedAt": "2025-05-05T01:19:40.000Z",
    "title": "SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based\n  Image Editing",
    "summary": "Due to the challenges of manually collecting accurate editing data, existing\ndatasets are typically constructed using various automated methods, leading to\nnoisy supervision signals caused by the mismatch between editing instructions\nand original-edited image pairs. Recent efforts attempt to improve editing\nmodels through generating higher-quality edited images, pre-training on\nrecognition tasks, or introducing vision-language models (VLMs) but fail to\nresolve this fundamental issue. In this paper, we offer a novel solution by\nconstructing more effective editing instructions for given image pairs. This\nincludes rectifying the editing instructions to better align with the\noriginal-edited image pairs and using contrastive editing instructions to\nfurther enhance their effectiveness. Specifically, we find that editing models\nexhibit specific generation attributes at different inference steps,\nindependent of the text. Based on these prior attributes, we define a unified\nguide for VLMs to rectify editing instructions. However, there are some\nchallenging editing scenarios that cannot be resolved solely with rectified\ninstructions. To this end, we further construct contrastive supervision signals\nwith positive and negative instructions and introduce them into the model\ntraining using triplet loss, thereby further facilitating supervision\neffectiveness. Our method does not require the VLM modules or pre-training\ntasks used in previous work, offering a more direct and efficient way to\nprovide better supervision signals, and providing a novel, simple, and\neffective solution for instruction-based image editing. Results on multiple\nbenchmarks demonstrate that our method significantly outperforms existing\napproaches. Compared with previous SOTA SmartEdit, we achieve 9.19%\nimprovements on the Real-Edit benchmark with 30x less training data and 13x\nsmaller model size.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02370.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637f0eb22438d7485b8ef5d7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f0eb22438d7485b8ef5d7/70h7dekqj7LuBobOXckmJ.jpeg",
      "fullname": "Ming Li",
      "name": "limingcv",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 21
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2504.20752",
      "authors": [
        {
          "_id": "6818c145daa8955b2085667d",
          "name": "Roman Abramov",
          "hidden": false
        },
        {
          "_id": "6818c145daa8955b2085667e",
          "user": {
            "_id": "6679882913c63ebaa8ff62fe",
            "avatarUrl": "/avatars/ce462d80255f61bcb1f814dad58b888d.svg",
            "isPro": false,
            "fullname": "Felix Steinbauer",
            "user": "fsteinbauer",
            "type": "user"
          },
          "name": "Felix Steinbauer",
          "status": "extracted_pending",
          "statusLastChangedAt": "2025-05-05T13:46:46.742Z",
          "hidden": false
        },
        {
          "_id": "6818c145daa8955b2085667f",
          "name": "Gjergji Kasneci",
          "hidden": false
        }
      ],
      "publishedAt": "2025-04-29T13:33:29.000Z",
      "submittedOnDailyAt": "2025-05-06T03:38:21.809Z",
      "title": "Grokking in the Wild: Data Augmentation for Real-World Multi-Hop\n  Reasoning with Transformers",
      "submittedOnDailyBy": {
        "_id": "6679882913c63ebaa8ff62fe",
        "avatarUrl": "/avatars/ce462d80255f61bcb1f814dad58b888d.svg",
        "isPro": false,
        "fullname": "Felix Steinbauer",
        "user": "fsteinbauer",
        "type": "user"
      },
      "summary": "Transformers have achieved great success in numerous NLP tasks but continue\nto exhibit notable gaps in multi-step factual reasoning, especially when\nreal-world knowledge is sparse. Recent advances in grokking have demonstrated\nthat neural networks can transition from memorizing to perfectly generalizing\nonce they detect underlying logical patterns - yet these studies have primarily\nused small, synthetic tasks. In this paper, for the first time, we extend\ngrokking to real-world factual data and address the challenge of dataset\nsparsity by augmenting existing knowledge graphs with carefully designed\nsynthetic data to raise the ratio phi_r of inferred facts to atomic facts\nabove the threshold required for grokking. Surprisingly, we find that even\nfactually incorrect synthetic data can strengthen emergent reasoning circuits\nrather than degrade accuracy, as it forces the model to rely on relational\nstructure rather than memorization. When evaluated on multi-hop reasoning\nbenchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA -\nsubstantially improving over strong baselines and matching or exceeding current\nstate-of-the-art results. We further provide an in-depth analysis of how\nincreasing phi_r drives the formation of generalizing circuits inside\nTransformers. Our findings suggest that grokking-based data augmentation can\nunlock implicit multi-hop reasoning capabilities, opening the door to more\nrobust and interpretable factual reasoning in large-scale language models.",
      "upvotes": 7,
      "discussionId": "6818c146daa8955b208566f1",
      "ai_keywords": [
        "Transformers",
        "multi-step factual reasoning",
        "grokking",
        "neural networks",
        "perfect generalization",
        "logical patterns",
        "real-world factual data",
        "dataset sparsity",
        "knowledge graphs",
        "synthetic data",
        "inferred facts",
        "atomic facts",
        "factually incorrect synthetic data",
        "relational structure",
        "memorization",
        "multi-hop reasoning",
        "benchmarks",
        "2WikiMultiHopQA",
        "baselines",
        "state-of-the-art results",
        "generalizing circuits",
        "grokking-based data augmentation",
        "implicit multi-hop reasoning capabilities",
        "robust",
        "interpretable factual reasoning"
      ]
    },
    "publishedAt": "2025-04-29T09:33:29.000Z",
    "title": "Grokking in the Wild: Data Augmentation for Real-World Multi-Hop\n  Reasoning with Transformers",
    "summary": "Transformers have achieved great success in numerous NLP tasks but continue\nto exhibit notable gaps in multi-step factual reasoning, especially when\nreal-world knowledge is sparse. Recent advances in grokking have demonstrated\nthat neural networks can transition from memorizing to perfectly generalizing\nonce they detect underlying logical patterns - yet these studies have primarily\nused small, synthetic tasks. In this paper, for the first time, we extend\ngrokking to real-world factual data and address the challenge of dataset\nsparsity by augmenting existing knowledge graphs with carefully designed\nsynthetic data to raise the ratio phi_r of inferred facts to atomic facts\nabove the threshold required for grokking. Surprisingly, we find that even\nfactually incorrect synthetic data can strengthen emergent reasoning circuits\nrather than degrade accuracy, as it forces the model to rely on relational\nstructure rather than memorization. When evaluated on multi-hop reasoning\nbenchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA -\nsubstantially improving over strong baselines and matching or exceeding current\nstate-of-the-art results. We further provide an in-depth analysis of how\nincreasing phi_r drives the formation of generalizing circuits inside\nTransformers. Our findings suggest that grokking-based data augmentation can\nunlock implicit multi-hop reasoning capabilities, opening the door to more\nrobust and interpretable factual reasoning in large-scale language models.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2504.20752.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6679882913c63ebaa8ff62fe",
      "avatarUrl": "/avatars/ce462d80255f61bcb1f814dad58b888d.svg",
      "fullname": "Felix Steinbauer",
      "name": "fsteinbauer",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2505.01043",
      "authors": [
        {
          "_id": "68196e23d9cad0bb5c90dd9b",
          "name": "Zhiwei Hao",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dd9c",
          "name": "Jianyuan Guo",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dd9d",
          "name": "Li Shen",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dd9e",
          "name": "Yong Luo",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dd9f",
          "name": "Han Hu",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dda0",
          "name": "Guoxia Wang",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dda1",
          "name": "Dianhai Yu",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dda2",
          "name": "Yonggang Wen",
          "hidden": false
        },
        {
          "_id": "68196e23d9cad0bb5c90dda3",
          "name": "Dacheng Tao",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-02T06:33:25.000Z",
      "submittedOnDailyAt": "2025-05-06T00:36:54.063Z",
      "title": "Low-Precision Training of Large Language Models: Methods, Challenges,\n  and Opportunities",
      "submittedOnDailyBy": {
        "_id": "64a62e3302e46deb19a7937e",
        "avatarUrl": "/avatars/43553a80f2c5f6c91742c4ce2d23fe21.svg",
        "isPro": false,
        "fullname": "Zhiwei Hao",
        "user": "Zhiwei840",
        "type": "user"
      },
      "summary": "Large language models (LLMs) have achieved impressive performance across\nvarious domains. However, the substantial hardware resources required for their\ntraining present a significant barrier to efficiency and scalability. To\nmitigate this challenge, low-precision training techniques have been widely\nadopted, leading to notable advancements in training efficiency. Despite these\ngains, low-precision training involves several componentsx2013such\nas weights, activations, and gradientsx2013each of which can be\nrepresented in different numerical formats. The resulting diversity has created\na fragmented landscape in low-precision training research, making it difficult\nfor researchers to gain a unified overview of the field. This survey provides a\ncomprehensive review of existing low-precision training methods. To\nsystematically organize these approaches, we categorize them into three primary\ngroups based on their underlying numerical formats, which is a key factor\ninfluencing hardware compatibility, computational efficiency, and ease of\nreference for readers. The categories are: (1) fixed-point and integer-based\nmethods, (2) floating-point-based methods, and (3) customized format-based\nmethods. Additionally, we discuss quantization-aware training approaches, which\nshare key similarities with low-precision training during forward propagation.\nFinally, we highlight several promising research directions to advance this\nfield. A collection of papers discussed in this survey is provided in\nhttps://github.com/Hao840/Awesome-Low-Precision-Training.",
      "upvotes": 6,
      "discussionId": "68196e24d9cad0bb5c90de08",
      "githubRepo": "https://github.com/Hao840/Awesome-Low-Precision-Training",
      "ai_keywords": [
        "low-precision training",
        "weights",
        "activations",
        "gradients",
        "fixed-point",
        "integer-based methods",
        "floating-point-based methods",
        "customized format-based methods",
        "quantization-aware training"
      ]
    },
    "publishedAt": "2025-05-02T02:33:25.000Z",
    "title": "Low-Precision Training of Large Language Models: Methods, Challenges,\n  and Opportunities",
    "summary": "Large language models (LLMs) have achieved impressive performance across\nvarious domains. However, the substantial hardware resources required for their\ntraining present a significant barrier to efficiency and scalability. To\nmitigate this challenge, low-precision training techniques have been widely\nadopted, leading to notable advancements in training efficiency. Despite these\ngains, low-precision training involves several componentsx2013such\nas weights, activations, and gradientsx2013each of which can be\nrepresented in different numerical formats. The resulting diversity has created\na fragmented landscape in low-precision training research, making it difficult\nfor researchers to gain a unified overview of the field. This survey provides a\ncomprehensive review of existing low-precision training methods. To\nsystematically organize these approaches, we categorize them into three primary\ngroups based on their underlying numerical formats, which is a key factor\ninfluencing hardware compatibility, computational efficiency, and ease of\nreference for readers. The categories are: (1) fixed-point and integer-based\nmethods, (2) floating-point-based methods, and (3) customized format-based\nmethods. Additionally, we discuss quantization-aware training approaches, which\nshare key similarities with low-precision training during forward propagation.\nFinally, we highlight several promising research directions to advance this\nfield. A collection of papers discussed in this survey is provided in\nhttps://github.com/Hao840/Awesome-Low-Precision-Training.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01043.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64a62e3302e46deb19a7937e",
      "avatarUrl": "/avatars/43553a80f2c5f6c91742c4ce2d23fe21.svg",
      "fullname": "Zhiwei Hao",
      "name": "Zhiwei840",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.02471",
      "authors": [
        {
          "_id": "681973cfa70a4728958323aa",
          "name": "Biao Gong",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323ab",
          "name": "Cheng Zou",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323ac",
          "name": "Dandan Zheng",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323ad",
          "name": "Hu Yu",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323ae",
          "name": "Jingdong Chen",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323af",
          "name": "Jianxin Sun",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b0",
          "name": "Junbo Zhao",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b1",
          "name": "Jun Zhou",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b2",
          "name": "Kaixiang Ji",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b3",
          "name": "Lixiang Ru",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b4",
          "name": "Libin Wang",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b5",
          "name": "Qingpei Guo",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b6",
          "name": "Rui Liu",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b7",
          "name": "Weilong Chai",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b8",
          "name": "Xinyu Xiao",
          "hidden": false
        },
        {
          "_id": "681973cfa70a4728958323b9",
          "name": "Ziyuan Huang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T08:56:12.000Z",
      "submittedOnDailyAt": "2025-05-06T01:00:49.692Z",
      "title": "Ming-Lite-Uni: Advancements in Unified Architecture for Natural\n  Multimodal Interaction",
      "submittedOnDailyBy": {
        "_id": "644fcbea4f7316588267dc80",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg",
        "isPro": false,
        "fullname": "Biao Gong",
        "user": "BiaoGong",
        "type": "user"
      },
      "summary": "We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a\nnewly designed unified visual generator and a native multimodal autoregressive\nmodel tailored for unifying vision and language. Specifically, this project\nprovides an open-source implementation of the integrated MetaQueries and\nM2-omni framework, while introducing the novel multi-scale learnable tokens and\nmulti-scale representation alignment strategy. By leveraging a fixed MLLM and a\nlearnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to\nperform both text-to-image generation and instruction based image editing\ntasks, expanding their capabilities beyond pure visual understanding. Our\nexperimental results demonstrate the strong performance of Ming-Lite-Uni and\nillustrate the impressive fluid nature of its interactive process. All code and\nmodel weights are open-sourced to foster further exploration within the\ncommunity. Notably, this work aligns with concurrent multimodal AI milestones -\nsuch as ChatGPT-4o with native image generation updated in March 25, 2025 -\nunderscoring the broader significance of unified models like Ming-Lite-Uni on\nthe path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further\nrefined.",
      "upvotes": 4,
      "discussionId": "681973d2a70a47289583249d",
      "projectPage": "https://github.com/inclusionAI/Ming/tree/main/Ming-unify",
      "githubRepo": "https://github.com/inclusionAI/Ming/tree/main/Ming-unify",
      "ai_keywords": [
        "unified visual generator",
        "multimodal autoregressive model",
        "MetaQueries",
        "M2-omni framework",
        "multi-scale learnable tokens",
        "multi-scale representation alignment strategy",
        "MLLM",
        "learnable diffusion model",
        "text-to-image generation",
        "instruction based image editing"
      ]
    },
    "publishedAt": "2025-05-05T04:56:12.000Z",
    "title": "Ming-Lite-Uni: Advancements in Unified Architecture for Natural\n  Multimodal Interaction",
    "summary": "We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a\nnewly designed unified visual generator and a native multimodal autoregressive\nmodel tailored for unifying vision and language. Specifically, this project\nprovides an open-source implementation of the integrated MetaQueries and\nM2-omni framework, while introducing the novel multi-scale learnable tokens and\nmulti-scale representation alignment strategy. By leveraging a fixed MLLM and a\nlearnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to\nperform both text-to-image generation and instruction based image editing\ntasks, expanding their capabilities beyond pure visual understanding. Our\nexperimental results demonstrate the strong performance of Ming-Lite-Uni and\nillustrate the impressive fluid nature of its interactive process. All code and\nmodel weights are open-sourced to foster further exploration within the\ncommunity. Notably, this work aligns with concurrent multimodal AI milestones -\nsuch as ChatGPT-4o with native image generation updated in March 25, 2025 -\nunderscoring the broader significance of unified models like Ming-Lite-Uni on\nthe path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further\nrefined.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02471.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "644fcbea4f7316588267dc80",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg",
      "fullname": "Biao Gong",
      "name": "BiaoGong",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.01658",
      "authors": [
        {
          "_id": "6819950bd55db085708dd2e5",
          "name": "Sihyeong Park",
          "hidden": false
        },
        {
          "_id": "6819950bd55db085708dd2e6",
          "name": "Sungryeol Jeon",
          "hidden": false
        },
        {
          "_id": "6819950bd55db085708dd2e7",
          "name": "Chaelyn Lee",
          "hidden": false
        },
        {
          "_id": "6819950bd55db085708dd2e8",
          "name": "Seokhun Jeon",
          "hidden": false
        },
        {
          "_id": "6819950bd55db085708dd2e9",
          "name": "Byung-Soo Kim",
          "hidden": false
        },
        {
          "_id": "6819950bd55db085708dd2ea",
          "name": "Jemin Lee",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-03T02:47:43.000Z",
      "submittedOnDailyAt": "2025-05-06T03:21:53.083Z",
      "title": "A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency",
      "submittedOnDailyBy": {
        "_id": "65b9dee19c4955ae7aee4954",
        "avatarUrl": "/avatars/263f129605c7763185c49076174b891b.svg",
        "isPro": false,
        "fullname": "Jemin Lee",
        "user": "leejaymin",
        "type": "user"
      },
      "summary": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine",
      "upvotes": 4,
      "discussionId": "6819950cd55db085708dd32a",
      "ai_keywords": [
        "chain-of-thought",
        "complex reasoning",
        "agent services",
        "inference cost",
        "parallelism",
        "compression",
        "caching",
        "LLM inference engines",
        "ease-of-use",
        "ease-of-deployment",
        "general-purpose support",
        "scalability",
        "throughput-aware computation",
        "latency-aware computation",
        "optimization techniques",
        "ecosystem maturity",
        "performance",
        "cost policy",
        "LLM-based services",
        "enhanced security"
      ]
    },
    "publishedAt": "2025-05-02T22:47:43.000Z",
    "title": "A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency",
    "summary": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01658.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b9dee19c4955ae7aee4954",
      "avatarUrl": "/avatars/263f129605c7763185c49076174b891b.svg",
      "fullname": "Jemin Lee",
      "name": "leejaymin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.01583",
      "authors": [
        {
          "_id": "6819814653612b577df718e7",
          "name": "Jen-Hao Cheng",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718e8",
          "name": "Vivian Wang",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718e9",
          "name": "Huayu Wang",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718ea",
          "name": "Huapeng Zhou",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718eb",
          "name": "Yi-Hao Peng",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718ec",
          "name": "Hou-I Liu",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718ed",
          "name": "Hsiang-Wei Huang",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718ee",
          "name": "Kuang-Ming Chen",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718ef",
          "name": "Cheng-Yen Yang",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718f0",
          "name": "Wenhao Chai",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718f1",
          "name": "Yi-Ling Chen",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718f2",
          "name": "Vibhav Vineet",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718f3",
          "name": "Qin Cai",
          "hidden": false
        },
        {
          "_id": "6819814653612b577df718f4",
          "name": "Jenq-Neng Hwang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-02T21:00:17.000Z",
      "submittedOnDailyAt": "2025-05-06T01:56:09.960Z",
      "title": "TEMPURA: Temporal Event Masked Prediction and Understanding for\n  Reasoning in Action",
      "submittedOnDailyBy": {
        "_id": "637c7503fe115289cfecbe6b",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676361945047-637c7503fe115289cfecbe6b.jpeg",
        "isPro": false,
        "fullname": "Wenhao Chai",
        "user": "wchai",
        "type": "user"
      },
      "summary": "Understanding causal event relationships and achieving fine-grained temporal\ngrounding in videos remain challenging for vision-language models. Existing\nmethods either compress video tokens to reduce temporal resolution, or treat\nvideos as unsegmented streams, which obscures fine-grained event boundaries and\nlimits the modeling of causal dependencies. We propose TEMPURA (Temporal Event\nMasked Prediction and Understanding for Reasoning in Action), a two-stage\ntraining framework that enhances video temporal understanding. TEMPURA first\napplies masked event prediction reasoning to reconstruct missing events and\ngenerate step-by-step causal explanations from dense event annotations, drawing\ninspiration from effective infilling techniques. TEMPURA then learns to perform\nvideo segmentation and dense captioning to decompose videos into\nnon-overlapping events with detailed, timestamp-aligned descriptions. We train\nTEMPURA on VER, a large-scale dataset curated by us that comprises 1M training\ninstances and 500K videos with temporally aligned event descriptions and\nstructured reasoning steps. Experiments on temporal grounding and highlight\ndetection benchmarks demonstrate that TEMPURA outperforms strong baseline\nmodels, confirming that integrating causal reasoning with fine-grained temporal\nsegmentation leads to improved video understanding.",
      "upvotes": 3,
      "discussionId": "6819814853612b577df71943",
      "ai_keywords": [
        "TEMPURA",
        "masked event prediction",
        "causal explanations",
        "dense event annotations",
        "infilling techniques",
        "video segmentation",
        "dense captioning",
        "non-overlapping events",
        "timestamp-aligned descriptions",
        "VER",
        "temporal grounding",
        "highlight detection",
        "baseline models",
        "causal reasoning",
        "fine-grained temporal segmentation"
      ]
    },
    "publishedAt": "2025-05-02T17:00:17.000Z",
    "title": "TEMPURA: Temporal Event Masked Prediction and Understanding for\n  Reasoning in Action",
    "summary": "Understanding causal event relationships and achieving fine-grained temporal\ngrounding in videos remain challenging for vision-language models. Existing\nmethods either compress video tokens to reduce temporal resolution, or treat\nvideos as unsegmented streams, which obscures fine-grained event boundaries and\nlimits the modeling of causal dependencies. We propose TEMPURA (Temporal Event\nMasked Prediction and Understanding for Reasoning in Action), a two-stage\ntraining framework that enhances video temporal understanding. TEMPURA first\napplies masked event prediction reasoning to reconstruct missing events and\ngenerate step-by-step causal explanations from dense event annotations, drawing\ninspiration from effective infilling techniques. TEMPURA then learns to perform\nvideo segmentation and dense captioning to decompose videos into\nnon-overlapping events with detailed, timestamp-aligned descriptions. We train\nTEMPURA on VER, a large-scale dataset curated by us that comprises 1M training\ninstances and 500K videos with temporally aligned event descriptions and\nstructured reasoning steps. Experiments on temporal grounding and highlight\ndetection benchmarks demonstrate that TEMPURA outperforms strong baseline\nmodels, confirming that integrating causal reasoning with fine-grained temporal\nsegmentation leads to improved video understanding.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.01583.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "637c7503fe115289cfecbe6b",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676361945047-637c7503fe115289cfecbe6b.jpeg",
      "fullname": "Wenhao Chai",
      "name": "wchai",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 30
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.02823",
      "authors": [
        {
          "_id": "6819893117007d963b997a0b",
          "name": "Zinan Guo",
          "hidden": false
        },
        {
          "_id": "6819893117007d963b997a0c",
          "name": "Pengze Zhang",
          "hidden": false
        },
        {
          "_id": "6819893117007d963b997a0d",
          "name": "Yanze Wu",
          "hidden": false
        },
        {
          "_id": "6819893117007d963b997a0e",
          "name": "Chong Mou",
          "hidden": false
        },
        {
          "_id": "6819893117007d963b997a0f",
          "name": "Songtao Zhao",
          "hidden": false
        },
        {
          "_id": "6819893117007d963b997a10",
          "name": "Qian He",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T17:50:24.000Z",
      "submittedOnDailyAt": "2025-05-06T02:30:32.888Z",
      "title": "MUSAR: Exploring Multi-Subject Customization from Single-Subject Dataset\n  via Attention Routing",
      "submittedOnDailyBy": {
        "_id": "639709c2be8a14bb9eeea8f6",
        "avatarUrl": "/avatars/c142d71b541dccff91fcfd08a2cc0ce0.svg",
        "isPro": false,
        "fullname": "Yanze Wu",
        "user": "yanze",
        "type": "user"
      },
      "summary": "Current multi-subject customization approaches encounter two critical\nchallenges: the difficulty in acquiring diverse multi-subject training data,\nand attribute entanglement across different subjects. To bridge these gaps, we\npropose MUSAR - a simple yet effective framework to achieve robust\nmulti-subject customization while requiring only single-subject training data.\nFirstly, to break the data limitation, we introduce debiased diptych learning.\nIt constructs diptych training pairs from single-subject images to facilitate\nmulti-subject learning, while actively correcting the distribution bias\nintroduced by diptych construction via static attention routing and dual-branch\nLoRA. Secondly, to eliminate cross-subject entanglement, we introduce dynamic\nattention routing mechanism, which adaptively establishes bijective mappings\nbetween generated images and conditional subjects. This design not only\nachieves decoupling of multi-subject representations but also maintains\nscalable generalization performance with increasing reference subjects.\nComprehensive experiments demonstrate that our MUSAR outperforms existing\nmethods - even those trained on multi-subject dataset - in image quality,\nsubject consistency, and interaction naturalness, despite requiring only\nsingle-subject dataset.",
      "upvotes": 1,
      "discussionId": "6819893317007d963b997ab1",
      "githubRepo": "https://github.com/guozinan126/MUSAR",
      "ai_keywords": [
        "debiased diptych learning",
        "diptych training pairs",
        "static attention routing",
        "dual-branch LoRA",
        "dynamic attention routing mechanism",
        "bijective mappings",
        "multi-subject representations",
        "scalable generalization performance"
      ]
    },
    "publishedAt": "2025-05-05T13:50:24.000Z",
    "title": "MUSAR: Exploring Multi-Subject Customization from Single-Subject Dataset\n  via Attention Routing",
    "summary": "Current multi-subject customization approaches encounter two critical\nchallenges: the difficulty in acquiring diverse multi-subject training data,\nand attribute entanglement across different subjects. To bridge these gaps, we\npropose MUSAR - a simple yet effective framework to achieve robust\nmulti-subject customization while requiring only single-subject training data.\nFirstly, to break the data limitation, we introduce debiased diptych learning.\nIt constructs diptych training pairs from single-subject images to facilitate\nmulti-subject learning, while actively correcting the distribution bias\nintroduced by diptych construction via static attention routing and dual-branch\nLoRA. Secondly, to eliminate cross-subject entanglement, we introduce dynamic\nattention routing mechanism, which adaptively establishes bijective mappings\nbetween generated images and conditional subjects. This design not only\nachieves decoupling of multi-subject representations but also maintains\nscalable generalization performance with increasing reference subjects.\nComprehensive experiments demonstrate that our MUSAR outperforms existing\nmethods - even those trained on multi-subject dataset - in image quality,\nsubject consistency, and interaction naturalness, despite requiring only\nsingle-subject dataset.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02823.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "639709c2be8a14bb9eeea8f6",
      "avatarUrl": "/avatars/c142d71b541dccff91fcfd08a2cc0ce0.svg",
      "fullname": "Yanze Wu",
      "name": "yanze",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 140
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2505.02625",
      "authors": [
        {
          "_id": "681975abba26bf20601bb7ca",
          "name": "Qingkai Fang",
          "hidden": false
        },
        {
          "_id": "681975abba26bf20601bb7cb",
          "name": "Yan Zhou",
          "hidden": false
        },
        {
          "_id": "681975abba26bf20601bb7cc",
          "name": "Shoutao Guo",
          "hidden": false
        },
        {
          "_id": "681975abba26bf20601bb7cd",
          "name": "Shaolei Zhang",
          "hidden": false
        },
        {
          "_id": "681975abba26bf20601bb7ce",
          "name": "Yang Feng",
          "hidden": false
        }
      ],
      "publishedAt": "2025-05-05T12:53:09.000Z",
      "submittedOnDailyAt": "2025-05-06T01:07:20.259Z",
      "title": "LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive\n  Streaming Speech Synthesis",
      "submittedOnDailyBy": {
        "_id": "65b7573482d384513443875e",
        "avatarUrl": "/avatars/0f2175e4adf507f5ccb0636c1cb647de.svg",
        "isPro": false,
        "fullname": "Qingkai Fang",
        "user": "poeroz",
        "type": "user"
      },
      "summary": "Real-time, intelligent, and natural speech interaction is an essential part\nof the next-generation human-computer interaction. Recent advancements have\nshowcased the potential of building intelligent spoken chatbots based on large\nlanguage models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of\nspeech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable\nof achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built\nupon the Qwen2.5 series models, integrating a speech encoder and an\nautoregressive streaming speech decoder. Despite being trained on only 200K\nmulti-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong\nperformance on several spoken question answering and speech instruction\nfollowing benchmarks, surpassing previous state-of-the-art SpeechLMs like\nGLM-4-Voice, which was trained on millions of hours of speech data.",
      "upvotes": 1,
      "discussionId": "681975abba26bf20601bb7f2",
      "ai_keywords": [
        "speech language models (SpeechLMs)",
        "Qwen2.5",
        "speech encoder",
        "autoregressive streaming speech decoder",
        "spoken question answering",
        "speech instruction following",
        "GLM-4-Voice"
      ]
    },
    "publishedAt": "2025-05-05T08:53:09.000Z",
    "title": "LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive\n  Streaming Speech Synthesis",
    "summary": "Real-time, intelligent, and natural speech interaction is an essential part\nof the next-generation human-computer interaction. Recent advancements have\nshowcased the potential of building intelligent spoken chatbots based on large\nlanguage models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of\nspeech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable\nof achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built\nupon the Qwen2.5 series models, integrating a speech encoder and an\nautoregressive streaming speech decoder. Despite being trained on only 200K\nmulti-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong\nperformance on several spoken question answering and speech instruction\nfollowing benchmarks, surpassing previous state-of-the-art SpeechLMs like\nGLM-4-Voice, which was trained on millions of hours of speech data.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.02625.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65b7573482d384513443875e",
      "avatarUrl": "/avatars/0f2175e4adf507f5ccb0636c1cb647de.svg",
      "fullname": "Qingkai Fang",
      "name": "poeroz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 1
    },
    "isAuthorParticipating": false
  }
]