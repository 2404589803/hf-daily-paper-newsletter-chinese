[
  {
    "paper": {
      "id": "2511.02778",
      "authors": [
        {
          "_id": "690aba32d70e173c84528fa7",
          "name": "Kevin Qinghong Lin",
          "hidden": false
        },
        {
          "_id": "690aba32d70e173c84528fa8",
          "name": "Yuhao Zheng",
          "hidden": false
        },
        {
          "_id": "690aba32d70e173c84528fa9",
          "name": "Hangyu Ran",
          "hidden": false
        },
        {
          "_id": "690aba32d70e173c84528faa",
          "name": "Dantong Zhu",
          "hidden": false
        },
        {
          "_id": "690aba32d70e173c84528fab",
          "name": "Dongxing Mao",
          "hidden": false
        },
        {
          "_id": "690aba32d70e173c84528fac",
          "name": "Linjie Li",
          "hidden": false
        },
        {
          "_id": "690aba32d70e173c84528fad",
          "name": "Philip Torr",
          "hidden": false
        },
        {
          "_id": "690aba32d70e173c84528fae",
          "name": "Alex Jinpeng Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-04T18:00:18.000Z",
      "submittedOnDailyAt": "2025-11-05T00:18:37.196Z",
      "title": "VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual\n  Representation",
      "submittedOnDailyBy": {
        "_id": "64440be5af034cdfd69ca3a7",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
        "isPro": false,
        "fullname": "Qinghong (Kevin) Lin",
        "user": "KevinQHLin",
        "type": "user"
      },
      "summary": "Code has emerged as a precise and executable medium for reasoning and action\nin the agent era. Yet, progress has largely focused on language-centric tasks\nsuch as program synthesis and debugging, leaving visual-centric coding\nunderexplored. Inspired by how humans reason over sketches, we advocate SVG\ncode as a compact, interpretable, and executable visual representation. We\nintroduce VCode, a benchmark that reframes multimodal understanding as code\ngeneration: given an image, a model must produce SVG that preserves symbolic\nmeaning for downstream reasoning. VCode covers three domains - general\ncommonsense (MM-Vet), professional disciplines (MMMU), and visual-centric\nperception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel\nevaluation protocol in which a policy model answers questions over rendered\nSVGs; correct answers indicate faithful symbolic preservation. Empirically,\nfrontier VLMs struggle to generate faithful SVGs, revealing a persistent gap\nbetween language-centric and visual-centric coding. To close this gap, we\nintroduce VCoder, an agentic framework that augments VLMs along two axes: (i)\nThinking with Revision, which iteratively analyzes discrepancies and refines\nSVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply\nstructured cues such as objects, shapes, and text beyond the model's intrinsic\ncapacity. Across benchmarks, frontier VLMs with strong reasoning capabilities\nscore well overall yet remain limited in professional knowledge and 3D\nreasoning. VCoder delivers a 12.3-point overall gain over the top-performing\nClaude-4-Opus. Human studies show that both humans and VLMs perform worse on\nrendered SVGs, their consistency reveals the promise of symbolic visual\nrepresentation. The benchmark and code are available at\nhttps://github.com/CSU-JPG/VCode.",
      "upvotes": 44,
      "discussionId": "690aba33d70e173c84528faf",
      "projectPage": "https://csu-jpg.github.io/VCode/",
      "githubRepo": "https://github.com/CSU-JPG/VCode/tree/main",
      "ai_summary": "VCode introduces a benchmark for generating SVG code from images to preserve symbolic meaning, highlighting gaps in visual-centric coding and proposing VCoder to improve performance.",
      "ai_keywords": [
        "SVG",
        "VCode",
        "multimodal understanding",
        "code generation",
        "CodeVQA",
        "VLMs",
        "Thinking with Revision",
        "Acting with Visual Tools",
        "professional knowledge",
        "3D reasoning",
        "VCoder"
      ],
      "githubStars": 4,
      "organization": {
        "_id": "67ab7720792eebb05080c926",
        "name": "CSU-JPG",
        "fullname": "Jinpeng Group",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62333a88fd7bb4a39b92d387/MHfLrhVz0KqH6ydx1UrOc.jpeg"
      }
    },
    "publishedAt": "2025-11-04T13:00:18.000Z",
    "title": "VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual\n  Representation",
    "summary": "Code has emerged as a precise and executable medium for reasoning and action\nin the agent era. Yet, progress has largely focused on language-centric tasks\nsuch as program synthesis and debugging, leaving visual-centric coding\nunderexplored. Inspired by how humans reason over sketches, we advocate SVG\ncode as a compact, interpretable, and executable visual representation. We\nintroduce VCode, a benchmark that reframes multimodal understanding as code\ngeneration: given an image, a model must produce SVG that preserves symbolic\nmeaning for downstream reasoning. VCode covers three domains - general\ncommonsense (MM-Vet), professional disciplines (MMMU), and visual-centric\nperception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel\nevaluation protocol in which a policy model answers questions over rendered\nSVGs; correct answers indicate faithful symbolic preservation. Empirically,\nfrontier VLMs struggle to generate faithful SVGs, revealing a persistent gap\nbetween language-centric and visual-centric coding. To close this gap, we\nintroduce VCoder, an agentic framework that augments VLMs along two axes: (i)\nThinking with Revision, which iteratively analyzes discrepancies and refines\nSVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply\nstructured cues such as objects, shapes, and text beyond the model's intrinsic\ncapacity. Across benchmarks, frontier VLMs with strong reasoning capabilities\nscore well overall yet remain limited in professional knowledge and 3D\nreasoning. VCoder delivers a 12.3-point overall gain over the top-performing\nClaude-4-Opus. Human studies show that both humans and VLMs perform worse on\nrendered SVGs, their consistency reveals the promise of symbolic visual\nrepresentation. The benchmark and code are available at\nhttps://github.com/CSU-JPG/VCode.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.02778.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "64440be5af034cdfd69ca3a7",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg",
      "fullname": "Qinghong (Kevin) Lin",
      "name": "KevinQHLin",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 37
    },
    "organization": {
      "_id": "67ab7720792eebb05080c926",
      "name": "CSU-JPG",
      "fullname": "Jinpeng Group",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62333a88fd7bb4a39b92d387/MHfLrhVz0KqH6ydx1UrOc.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.02779",
      "authors": [
        {
          "_id": "690adcf8d70e173c84529049",
          "name": "Yiyang Zhou",
          "hidden": false
        },
        {
          "_id": "690adcf8d70e173c8452904a",
          "name": "Haoqin Tu",
          "hidden": false
        },
        {
          "_id": "690adcf8d70e173c8452904b",
          "name": "Zijun Wang",
          "hidden": false
        },
        {
          "_id": "690adcf8d70e173c8452904c",
          "name": "Zeyu Wang",
          "hidden": false
        },
        {
          "_id": "690adcf8d70e173c8452904d",
          "name": "Niklas Muennighoff",
          "hidden": false
        },
        {
          "_id": "690adcf8d70e173c8452904e",
          "name": "Fan Nie",
          "hidden": false
        },
        {
          "_id": "690adcf8d70e173c8452904f",
          "name": "Yejin Choi",
          "hidden": false
        },
        {
          "_id": "690adcf8d70e173c84529050",
          "name": "James Zou",
          "hidden": false
        },
        {
          "_id": "690adcf8d70e173c84529051",
          "name": "Chaorui Deng",
          "hidden": false
        },
        {
          "_id": "690adcf8d70e173c84529052",
          "name": "Shen Yan",
          "hidden": false
        },
        {
          "_id": "690adcf8d70e173c84529053",
          "name": "Haoqi Fan",
          "hidden": false
        },
        {
          "_id": "690adcf8d70e173c84529054",
          "name": "Cihang Xie",
          "hidden": false
        },
        {
          "_id": "690adcf8d70e173c84529055",
          "name": "Huaxiu Yao",
          "hidden": false
        },
        {
          "_id": "690adcf8d70e173c84529056",
          "name": "Qinghao Ye",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-04T18:00:51.000Z",
      "submittedOnDailyAt": "2025-11-05T02:45:55.873Z",
      "title": "When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for\n  Visual Chain-of-Thought",
      "submittedOnDailyBy": {
        "_id": "6433722c5277e3b24ef49055",
        "avatarUrl": "/avatars/f3c5560d500c699e452986a6a45ba3ee.svg",
        "isPro": false,
        "fullname": "Yiyang Zhou",
        "user": "YiyangAiLab",
        "type": "user"
      },
      "summary": "We propose MIRA, a new benchmark designed to evaluate models in scenarios\nwhere generating intermediate visual images is essential for successful\nreasoning. Unlike traditional CoT methods that rely solely on text, tasks in\nMIRA require models to generate and utilize intermediate images - such as\nsketches, structural diagrams, or path drawings - to guide their reasoning\nprocess. This setup closely mirrors how humans solve complex problems through\n\"drawing to think\". To solve this, MIRA focuses on tasks that are intrinsically\nchallenging and involve complex structures, spatial relationships, or reasoning\nsteps that are difficult to express through language alone. To ensure that our\nevaluation data is of high-quality, we include 546 multimodal problems,\nannotated with intermediate visual images and final answers. We also propose a\nunified evaluation protocol for MIRA that spans three levels of evaluation\ninput: direct input with image and question only, text-only CoT input with\nimage and thinking prompts, and Visual-CoT input with both annotated image\nclues and textual thinking prompts. To probe the upper bound of model capacity\non our benchmark, we also report pass@k and majority voting accuracies under\ndifferent k settings. Experimental results show that existing multimodal large\nlanguage models, including strongest private models as well as strong\nopen-weight models, perform poorly when relying solely on textual prompts.\nHowever, when intermediate visual cues are provided, model performance improves\nconsistently, yielding an average relative gain of 33.7% across all models and\ntasks. We also probe the upper bound by expanding the search space and\ndesigning textual prompts aligned with Visual-CoT, but both yield only limited\nimprovements compared to our Visual-CoT setting. These results underscore the\ncritical role of imagined visual information in enabling successful reasoning\non MIRA.",
      "upvotes": 28,
      "discussionId": "690adcf8d70e173c84529057",
      "projectPage": "https://mira-benchmark.github.io/",
      "ai_summary": "MIRA is a benchmark that evaluates models using intermediate visual images to enhance reasoning, showing significant performance improvements over text-only methods.",
      "ai_keywords": [
        "CoT methods",
        "intermediate visual images",
        "sketches",
        "structural diagrams",
        "path drawings",
        "multimodal problems",
        "evaluation protocol",
        "pass@k",
        "majority voting accuracies",
        "multimodal large language models",
        "Visual-CoT input"
      ],
      "organization": {
        "_id": "67d1140985ea0644e2f14b99",
        "name": "ByteDance-Seed",
        "fullname": "ByteDance Seed",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
      }
    },
    "publishedAt": "2025-11-04T13:00:51.000Z",
    "title": "When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for\n  Visual Chain-of-Thought",
    "summary": "We propose MIRA, a new benchmark designed to evaluate models in scenarios\nwhere generating intermediate visual images is essential for successful\nreasoning. Unlike traditional CoT methods that rely solely on text, tasks in\nMIRA require models to generate and utilize intermediate images - such as\nsketches, structural diagrams, or path drawings - to guide their reasoning\nprocess. This setup closely mirrors how humans solve complex problems through\n\"drawing to think\". To solve this, MIRA focuses on tasks that are intrinsically\nchallenging and involve complex structures, spatial relationships, or reasoning\nsteps that are difficult to express through language alone. To ensure that our\nevaluation data is of high-quality, we include 546 multimodal problems,\nannotated with intermediate visual images and final answers. We also propose a\nunified evaluation protocol for MIRA that spans three levels of evaluation\ninput: direct input with image and question only, text-only CoT input with\nimage and thinking prompts, and Visual-CoT input with both annotated image\nclues and textual thinking prompts. To probe the upper bound of model capacity\non our benchmark, we also report pass@k and majority voting accuracies under\ndifferent k settings. Experimental results show that existing multimodal large\nlanguage models, including strongest private models as well as strong\nopen-weight models, perform poorly when relying solely on textual prompts.\nHowever, when intermediate visual cues are provided, model performance improves\nconsistently, yielding an average relative gain of 33.7% across all models and\ntasks. We also probe the upper bound by expanding the search space and\ndesigning textual prompts aligned with Visual-CoT, but both yield only limited\nimprovements compared to our Visual-CoT setting. These results underscore the\ncritical role of imagined visual information in enabling successful reasoning\non MIRA.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.02779.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6433722c5277e3b24ef49055",
      "avatarUrl": "/avatars/f3c5560d500c699e452986a6a45ba3ee.svg",
      "fullname": "Yiyang Zhou",
      "name": "YiyangAiLab",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 5
    },
    "organization": {
      "_id": "67d1140985ea0644e2f14b99",
      "name": "ByteDance-Seed",
      "fullname": "ByteDance Seed",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.02243",
      "authors": [
        {
          "_id": "690ae419d70e173c8452906b",
          "name": "Zhuoran Zhang",
          "hidden": false
        },
        {
          "_id": "690ae419d70e173c8452906c",
          "name": "Tengyue Wang",
          "hidden": false
        },
        {
          "_id": "690ae419d70e173c8452906d",
          "name": "Xilin Gong",
          "hidden": false
        },
        {
          "_id": "690ae419d70e173c8452906e",
          "name": "Yang Shi",
          "hidden": false
        },
        {
          "_id": "690ae419d70e173c8452906f",
          "name": "Haotian Wang",
          "hidden": false
        },
        {
          "_id": "690ae419d70e173c84529070",
          "name": "Di Wang",
          "hidden": false
        },
        {
          "_id": "690ae419d70e173c84529071",
          "name": "Lijie Hu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-04T04:11:31.000Z",
      "submittedOnDailyAt": "2025-11-05T03:14:18.767Z",
      "title": "When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs\n  Preference Dynamics in MLLMs",
      "submittedOnDailyBy": {
        "_id": "673c7319d11b1c2e246ead9c",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
        "isPro": false,
        "fullname": "Yang Shi",
        "user": "DogNeverSleep",
        "type": "user"
      },
      "summary": "Multimodal large language models (MLLMs) must resolve conflicts when\ndifferent modalities provide contradictory information, a process we term\nmodality following. Prior work measured this behavior only with coarse\ndataset-level statistics, overlooking the influence of model's confidence in\nunimodal reasoning. In this paper, we introduce a new framework that decomposes\nmodality following into two fundamental factors: relative reasoning uncertainty\n(the case-specific confidence gap between unimodal predictions) and inherent\nmodality preference( a model's stable bias when uncertainties are balanced). To\nvalidate this framework, we construct a controllable dataset that\nsystematically varies the reasoning difficulty of visual and textual inputs.\nUsing entropy as a fine-grained uncertainty metric, we uncover a universal law:\nthe probability of following a modality decreases monotonically as its relative\nuncertainty increases. At the relative difficulty level where the model tends\nto follow both modalities with comparable probability what we call the balance\npoint, a practical indicator of the model's inherent preference. Unlike\ntraditional macro-level ratios, this measure offers a more principled and less\nconfounded way to characterize modality bias, disentangling it from unimodal\ncapabilities and dataset artifacts. Further, by probing layer-wise predictions,\nwe reveal the internal mechanism of oscillation: in ambiguous regions near the\nbalance point, models vacillate between modalities across layers, explaining\nexternally observed indecision. Together, these findings establish relative\nuncertainty and inherent preference as the two governing principles of modality\nfollowing, offering both a quantitative framework and mechanistic insight into\nhow MLLMs resolve conflicting information.",
      "upvotes": 14,
      "discussionId": "690ae41ad70e173c84529072",
      "ai_summary": "A framework decomposes modality following in multimodal large language models into relative reasoning uncertainty and inherent modality preference, providing insights into how models resolve conflicting information.",
      "ai_keywords": [
        "multimodal large language models",
        "modality following",
        "relative reasoning uncertainty",
        "inherent modality preference",
        "entropy",
        "balance point",
        "layer-wise predictions"
      ]
    },
    "publishedAt": "2025-11-03T23:11:31.000Z",
    "title": "When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs\n  Preference Dynamics in MLLMs",
    "summary": "Multimodal large language models (MLLMs) must resolve conflicts when\ndifferent modalities provide contradictory information, a process we term\nmodality following. Prior work measured this behavior only with coarse\ndataset-level statistics, overlooking the influence of model's confidence in\nunimodal reasoning. In this paper, we introduce a new framework that decomposes\nmodality following into two fundamental factors: relative reasoning uncertainty\n(the case-specific confidence gap between unimodal predictions) and inherent\nmodality preference( a model's stable bias when uncertainties are balanced). To\nvalidate this framework, we construct a controllable dataset that\nsystematically varies the reasoning difficulty of visual and textual inputs.\nUsing entropy as a fine-grained uncertainty metric, we uncover a universal law:\nthe probability of following a modality decreases monotonically as its relative\nuncertainty increases. At the relative difficulty level where the model tends\nto follow both modalities with comparable probability what we call the balance\npoint, a practical indicator of the model's inherent preference. Unlike\ntraditional macro-level ratios, this measure offers a more principled and less\nconfounded way to characterize modality bias, disentangling it from unimodal\ncapabilities and dataset artifacts. Further, by probing layer-wise predictions,\nwe reveal the internal mechanism of oscillation: in ambiguous regions near the\nbalance point, models vacillate between modalities across layers, explaining\nexternally observed indecision. Together, these findings establish relative\nuncertainty and inherent preference as the two governing principles of modality\nfollowing, offering both a quantitative framework and mechanistic insight into\nhow MLLMs resolve conflicting information.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.02243.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "673c7319d11b1c2e246ead9c",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg",
      "fullname": "Yang Shi",
      "name": "DogNeverSleep",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 6
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.25976",
      "authors": [
        {
          "_id": "6905a52db26ae8b17699fd22",
          "name": "Roman Beliy",
          "hidden": false
        },
        {
          "_id": "6905a52db26ae8b17699fd23",
          "name": "Amit Zalcher",
          "hidden": false
        },
        {
          "_id": "6905a52db26ae8b17699fd24",
          "name": "Jonathan Kogman",
          "hidden": false
        },
        {
          "_id": "6905a52db26ae8b17699fd25",
          "name": "Navve Wasserman",
          "hidden": false
        },
        {
          "_id": "6905a52db26ae8b17699fd26",
          "name": "Michal Irani",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-29T21:21:54.000Z",
      "submittedOnDailyAt": "2025-11-05T03:33:58.960Z",
      "title": "Brain-IT: Image Reconstruction from fMRI via Brain-Interaction\n  Transformer",
      "submittedOnDailyBy": {
        "_id": "67d844b5e31360c75fa7410d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QgryioCNwCGDx_tdlHeym.png",
        "isPro": false,
        "fullname": "Zalcher",
        "user": "Amitz244",
        "type": "user"
      },
      "summary": "Reconstructing images seen by people from their fMRI brain recordings\nprovides a non-invasive window into the human brain. Despite recent progress\nenabled by diffusion models, current methods often lack faithfulness to the\nactual seen images. We present \"Brain-IT\", a brain-inspired approach that\naddresses this challenge through a Brain Interaction Transformer (BIT),\nallowing effective interactions between clusters of functionally-similar\nbrain-voxels. These functional-clusters are shared by all subjects, serving as\nbuilding blocks for integrating information both within and across brains. All\nmodel components are shared by all clusters & subjects, allowing efficient\ntraining with a limited amount of data. To guide the image reconstruction, BIT\npredicts two complementary localized patch-level image features: (i)high-level\nsemantic features which steer the diffusion model toward the correct semantic\ncontent of the image; and (ii)low-level structural features which help to\ninitialize the diffusion process with the correct coarse layout of the image.\nBIT's design enables direct flow of information from brain-voxel clusters to\nlocalized image features. Through these principles, our method achieves image\nreconstructions from fMRI that faithfully reconstruct the seen images, and\nsurpass current SotA approaches both visually and by standard objective\nmetrics. Moreover, with only 1-hour of fMRI data from a new subject, we achieve\nresults comparable to current methods trained on full 40-hour recordings.",
      "upvotes": 3,
      "discussionId": "6905a52db26ae8b17699fd27",
      "projectPage": "https://amitzalcher.github.io/Brain-IT/",
      "ai_summary": "Brain-IT uses a Brain Interaction Transformer to reconstruct images from fMRI data with high fidelity, surpassing current methods and requiring less training data.",
      "ai_keywords": [
        "diffusion models",
        "Brain Interaction Transformer",
        "BIT",
        "brain-voxels",
        "functional-clusters",
        "high-level semantic features",
        "low-level structural features",
        "image reconstruction",
        "fMRI",
        "standard objective metrics"
      ],
      "organization": {
        "_id": "62e28e39555a866437a78225",
        "name": "weizmannscience",
        "fullname": "Weizmann Institute of Science",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1659014711791-624bebf604abc7ebb01789af.jpeg"
      }
    },
    "publishedAt": "2025-10-29T17:21:54.000Z",
    "title": "Brain-IT: Image Reconstruction from fMRI via Brain-Interaction\n  Transformer",
    "summary": "Reconstructing images seen by people from their fMRI brain recordings\nprovides a non-invasive window into the human brain. Despite recent progress\nenabled by diffusion models, current methods often lack faithfulness to the\nactual seen images. We present \"Brain-IT\", a brain-inspired approach that\naddresses this challenge through a Brain Interaction Transformer (BIT),\nallowing effective interactions between clusters of functionally-similar\nbrain-voxels. These functional-clusters are shared by all subjects, serving as\nbuilding blocks for integrating information both within and across brains. All\nmodel components are shared by all clusters & subjects, allowing efficient\ntraining with a limited amount of data. To guide the image reconstruction, BIT\npredicts two complementary localized patch-level image features: (i)high-level\nsemantic features which steer the diffusion model toward the correct semantic\ncontent of the image; and (ii)low-level structural features which help to\ninitialize the diffusion process with the correct coarse layout of the image.\nBIT's design enables direct flow of information from brain-voxel clusters to\nlocalized image features. Through these principles, our method achieves image\nreconstructions from fMRI that faithfully reconstruct the seen images, and\nsurpass current SotA approaches both visually and by standard objective\nmetrics. Moreover, with only 1-hour of fMRI data from a new subject, we achieve\nresults comparable to current methods trained on full 40-hour recordings.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.25976.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67d844b5e31360c75fa7410d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QgryioCNwCGDx_tdlHeym.png",
      "fullname": "Zalcher",
      "name": "Amitz244",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "62e28e39555a866437a78225",
      "name": "weizmannscience",
      "fullname": "Weizmann Institute of Science",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1659014711791-624bebf604abc7ebb01789af.jpeg"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.02832",
      "authors": [
        {
          "_id": "690abfa3d70e173c84528fe6",
          "name": "Yanjie Ze",
          "hidden": false
        },
        {
          "_id": "690abfa3d70e173c84528fe7",
          "name": "Siheng Zhao",
          "hidden": false
        },
        {
          "_id": "690abfa3d70e173c84528fe8",
          "name": "Weizhuo Wang",
          "hidden": false
        },
        {
          "_id": "690abfa3d70e173c84528fe9",
          "name": "Angjoo Kanazawa",
          "hidden": false
        },
        {
          "_id": "690abfa3d70e173c84528fea",
          "name": "Rocky Duan",
          "hidden": false
        },
        {
          "_id": "690abfa3d70e173c84528feb",
          "name": "Pieter Abbeel",
          "hidden": false
        },
        {
          "_id": "690abfa3d70e173c84528fec",
          "name": "Guanya Shi",
          "hidden": false
        },
        {
          "_id": "690abfa3d70e173c84528fed",
          "name": "Jiajun Wu",
          "hidden": false
        },
        {
          "_id": "690abfa3d70e173c84528fee",
          "name": "C. Karen Liu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-04T18:58:35.000Z",
      "submittedOnDailyAt": "2025-11-05T00:38:36.561Z",
      "title": "TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Large-scale data has driven breakthroughs in robotics, from language models\nto vision-language-action models in bimanual manipulation. However, humanoid\nrobotics lacks equally effective data collection frameworks. Existing humanoid\nteleoperation systems either use decoupled control or depend on expensive\nmotion capture setups. We introduce TWIST2, a portable, mocap-free humanoid\nteleoperation and data collection system that preserves full whole-body control\nwhile advancing scalability. Our system leverages PICO4U VR for obtaining\nreal-time whole-body human motions, with a custom 2-DoF robot neck (cost around\n$250) for egocentric vision, enabling holistic human-to-humanoid control. We\ndemonstrate long-horizon dexterous and mobile humanoid skills and we can\ncollect 100 demonstrations in 15 minutes with an almost 100% success rate.\nBuilding on this pipeline, we propose a hierarchical visuomotor policy\nframework that autonomously controls the full humanoid body based on egocentric\nvision. Our visuomotor policy successfully demonstrates whole-body dexterous\nmanipulation and dynamic kicking tasks. The entire system is fully reproducible\nand open-sourced at https://yanjieze.com/TWIST2 . Our collected dataset is also\nopen-sourced at https://twist-data.github.io .",
      "upvotes": 2,
      "discussionId": "690abfa3d70e173c84528fef",
      "projectPage": "https://yanjieze.com/TWIST2/",
      "ai_summary": "TWIST2, a portable mocap-free system, enables efficient data collection and hierarchical visuomotor policy control for humanoid robots.",
      "ai_keywords": [
        "humanoid teleoperation",
        "PICO4U VR",
        "egocentric vision",
        "hierarchical visuomotor policy",
        "whole-body control",
        "dexterous manipulation",
        "dynamic kicking tasks"
      ]
    },
    "publishedAt": "2025-11-04T13:58:35.000Z",
    "title": "TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System",
    "summary": "Large-scale data has driven breakthroughs in robotics, from language models\nto vision-language-action models in bimanual manipulation. However, humanoid\nrobotics lacks equally effective data collection frameworks. Existing humanoid\nteleoperation systems either use decoupled control or depend on expensive\nmotion capture setups. We introduce TWIST2, a portable, mocap-free humanoid\nteleoperation and data collection system that preserves full whole-body control\nwhile advancing scalability. Our system leverages PICO4U VR for obtaining\nreal-time whole-body human motions, with a custom 2-DoF robot neck (cost around\n$250) for egocentric vision, enabling holistic human-to-humanoid control. We\ndemonstrate long-horizon dexterous and mobile humanoid skills and we can\ncollect 100 demonstrations in 15 minutes with an almost 100% success rate.\nBuilding on this pipeline, we propose a hierarchical visuomotor policy\nframework that autonomously controls the full humanoid body based on egocentric\nvision. Our visuomotor policy successfully demonstrates whole-body dexterous\nmanipulation and dynamic kicking tasks. The entire system is fully reproducible\nand open-sourced at https://yanjieze.com/TWIST2 . Our collected dataset is also\nopen-sourced at https://twist-data.github.io .",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.02832.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 154
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.02650",
      "authors": [
        {
          "_id": "690aeddcd70e173c84529080",
          "name": "Tianfan Peng",
          "hidden": false
        },
        {
          "_id": "690aeddcd70e173c84529081",
          "name": "Yuntao Du",
          "hidden": false
        },
        {
          "_id": "690aeddcd70e173c84529082",
          "name": "Pengzhou Ji",
          "hidden": false
        },
        {
          "_id": "690aeddcd70e173c84529083",
          "name": "Shijie Dong",
          "hidden": false
        },
        {
          "_id": "690aeddcd70e173c84529084",
          "name": "Kailin Jiang",
          "hidden": false
        },
        {
          "_id": "690aeddcd70e173c84529085",
          "name": "Mingchuan Ma",
          "hidden": false
        },
        {
          "_id": "690aeddcd70e173c84529086",
          "name": "Yijun Tian",
          "hidden": false
        },
        {
          "_id": "690aeddcd70e173c84529087",
          "name": "Jinhe Bi",
          "hidden": false
        },
        {
          "_id": "690aeddcd70e173c84529088",
          "name": "Qian Li",
          "hidden": false
        },
        {
          "_id": "690aeddcd70e173c84529089",
          "name": "Wei Du",
          "hidden": false
        },
        {
          "_id": "690aeddcd70e173c8452908a",
          "name": "Feng Xiao",
          "hidden": false
        },
        {
          "_id": "690aeddcd70e173c8452908b",
          "name": "Lizhen Cui",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-04T15:17:06.000Z",
      "submittedOnDailyAt": "2025-11-05T04:06:26.868Z",
      "title": "Can Visual Input Be Compressed? A Visual Token Compression Benchmark for\n  Large Multimodal Models",
      "submittedOnDailyBy": {
        "_id": "65745569839aa08899ea5d27",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/4X8waDwiphbfKZySrYlFy.jpeg",
        "isPro": false,
        "fullname": "Kailin Jiang, 蒋凯林",
        "user": "kailinjiang",
        "type": "user"
      },
      "summary": "Large multimodal models (LMMs) often suffer from severe inference\ninefficiency due to the large number of visual tokens introduced by image\nencoders. While recent token compression methods, such as pruning and merging,\nhave shown promise in reducing redundancy, their evaluation remains fragmented\nand inconsistent. In this work, we present UniPruneBench, a unified and\nextensible benchmark for visual token pruning in multimodal LLMs. UniPruneBench\nprovides standardized protocols across six ability dimensions and ten datasets,\ncovering ten representative compression algorithms and three families of LMMs\n(LLaVA-v1.5, Intern-VL3, and Qwen2.5-VL). Beyond task accuracy, it incorporates\nsystem-level metrics such as runtime and prefilling latency to provide a\nholistic view. Our experiments uncover several key findings: (1) random pruning\nis a surprisingly strong baseline, (2) no single method consistently\noutperforms others across scenarios, (3) pruning sensitivity varies\nsignificantly across tasks, with OCR being most vulnerable, and (4) pruning\nratio is the dominant factor governing performance degradation. We believe\nUniPruneBench will serve as a reliable foundation for future research on\nefficient multimodal modeling.",
      "upvotes": 2,
      "discussionId": "690aeddcd70e173c8452908c",
      "projectPage": "https://uniprunebench-lmm.github.io/",
      "githubRepo": "https://github.com/TianfanPeng/VLMUniPruneBench",
      "ai_summary": "UniPruneBench is a unified benchmark for evaluating visual token pruning in multimodal LLMs, providing standardized protocols and system-level metrics to assess performance across various tasks and models.",
      "ai_keywords": [
        "multimodal models",
        "visual tokens",
        "image encoders",
        "token compression",
        "pruning",
        "merging",
        "UniPruneBench",
        "ability dimensions",
        "datasets",
        "compression algorithms",
        "LLaVA-v1.5",
        "Intern-VL3",
        "Qwen2.5-VL",
        "runtime",
        "prefilling latency",
        "OCR",
        "pruning sensitivity",
        "pruning ratio"
      ],
      "githubStars": 2
    },
    "publishedAt": "2025-11-04T10:17:06.000Z",
    "title": "Can Visual Input Be Compressed? A Visual Token Compression Benchmark for\n  Large Multimodal Models",
    "summary": "Large multimodal models (LMMs) often suffer from severe inference\ninefficiency due to the large number of visual tokens introduced by image\nencoders. While recent token compression methods, such as pruning and merging,\nhave shown promise in reducing redundancy, their evaluation remains fragmented\nand inconsistent. In this work, we present UniPruneBench, a unified and\nextensible benchmark for visual token pruning in multimodal LLMs. UniPruneBench\nprovides standardized protocols across six ability dimensions and ten datasets,\ncovering ten representative compression algorithms and three families of LMMs\n(LLaVA-v1.5, Intern-VL3, and Qwen2.5-VL). Beyond task accuracy, it incorporates\nsystem-level metrics such as runtime and prefilling latency to provide a\nholistic view. Our experiments uncover several key findings: (1) random pruning\nis a surprisingly strong baseline, (2) no single method consistently\noutperforms others across scenarios, (3) pruning sensitivity varies\nsignificantly across tasks, with OCR being most vulnerable, and (4) pruning\nratio is the dominant factor governing performance degradation. We believe\nUniPruneBench will serve as a reliable foundation for future research on\nefficient multimodal modeling.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.02650.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "65745569839aa08899ea5d27",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/4X8waDwiphbfKZySrYlFy.jpeg",
      "fullname": "Kailin Jiang, 蒋凯林",
      "name": "kailinjiang",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 4
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.02490",
      "authors": [
        {
          "_id": "690ae401d70e173c84529063",
          "name": "Rajan Das Gupta",
          "hidden": false
        },
        {
          "_id": "690ae401d70e173c84529064",
          "name": "Md Kishor Morol",
          "hidden": false
        },
        {
          "_id": "690ae401d70e173c84529065",
          "name": "Nafiz Fahad",
          "hidden": false
        },
        {
          "_id": "690ae401d70e173c84529066",
          "name": "Md Tanzib Hosain",
          "hidden": false
        },
        {
          "_id": "690ae401d70e173c84529067",
          "name": "Sumaya Binte Zilani Choya",
          "hidden": false
        },
        {
          "_id": "690ae401d70e173c84529068",
          "name": "Md Jakir Hossen",
          "hidden": false
        }
      ],
      "mediaUrls": [
        "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/GOjumJN8KFk2GcHqyer5G.jpeg"
      ],
      "publishedAt": "2025-11-04T11:27:03.000Z",
      "submittedOnDailyAt": "2025-11-05T03:19:16.438Z",
      "title": "BRAINS: A Retrieval-Augmented System for Alzheimer's Detection and\n  Monitoring",
      "submittedOnDailyBy": {
        "_id": "67a3002c637d195f3c4bf371",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a3002c637d195f3c4bf371/bOUHNfZHrvVOjJpycZtHU.jpeg",
        "isPro": false,
        "fullname": "Rajan Das Gupta",
        "user": "rajandasgupta",
        "type": "user"
      },
      "summary": "As the global burden of Alzheimer's disease (AD) continues to grow, early and\naccurate detection has become increasingly critical, especially in regions with\nlimited access to advanced diagnostic tools. We propose BRAINS (Biomedical\nRetrieval-Augmented Intelligence for Neurodegeneration Screening) to address\nthis challenge. This novel system harnesses the powerful reasoning capabilities\nof Large Language Models (LLMs) for Alzheimer's detection and monitoring.\nBRAINS features a dual-module architecture: a cognitive diagnostic module and a\ncase-retrieval module. The Diagnostic Module utilizes LLMs fine-tuned on\ncognitive and neuroimaging datasets -- including MMSE, CDR scores, and brain\nvolume metrics -- to perform structured assessments of Alzheimer's risk.\nMeanwhile, the Case Retrieval Module encodes patient profiles into latent\nrepresentations and retrieves similar cases from a curated knowledge base.\nThese auxiliary cases are fused with the input profile via a Case Fusion Layer\nto enhance contextual understanding. The combined representation is then\nprocessed with clinical prompts for inference. Evaluations on real-world\ndatasets demonstrate BRAINS effectiveness in classifying disease severity and\nidentifying early signs of cognitive decline. This system not only shows strong\npotential as an assistive tool for scalable, explainable, and early-stage\nAlzheimer's disease detection, but also offers hope for future applications in\nthe field.",
      "upvotes": 1,
      "discussionId": "690ae402d70e173c84529069",
      "ai_summary": "BRAINS, a system using Large Language Models, effectively detects and monitors Alzheimer's disease by integrating cognitive assessments and case retrieval.",
      "ai_keywords": [
        "Large Language Models",
        "cognitive diagnostic module",
        "case-retrieval module",
        "latent representations",
        "Case Fusion Layer",
        "clinical prompts",
        "Alzheimer's risk",
        "disease severity",
        "cognitive decline"
      ],
      "organization": {
        "_id": "68b97f8992f0e22420e79325",
        "name": "eliteresearch",
        "fullname": "ELITE Research Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68b97676948b70f005d923fe/2QCfcp6rhnh8GI6AVJ_yQ.png"
      }
    },
    "publishedAt": "2025-11-04T06:27:03.000Z",
    "title": "BRAINS: A Retrieval-Augmented System for Alzheimer's Detection and\n  Monitoring",
    "summary": "As the global burden of Alzheimer's disease (AD) continues to grow, early and\naccurate detection has become increasingly critical, especially in regions with\nlimited access to advanced diagnostic tools. We propose BRAINS (Biomedical\nRetrieval-Augmented Intelligence for Neurodegeneration Screening) to address\nthis challenge. This novel system harnesses the powerful reasoning capabilities\nof Large Language Models (LLMs) for Alzheimer's detection and monitoring.\nBRAINS features a dual-module architecture: a cognitive diagnostic module and a\ncase-retrieval module. The Diagnostic Module utilizes LLMs fine-tuned on\ncognitive and neuroimaging datasets -- including MMSE, CDR scores, and brain\nvolume metrics -- to perform structured assessments of Alzheimer's risk.\nMeanwhile, the Case Retrieval Module encodes patient profiles into latent\nrepresentations and retrieves similar cases from a curated knowledge base.\nThese auxiliary cases are fused with the input profile via a Case Fusion Layer\nto enhance contextual understanding. The combined representation is then\nprocessed with clinical prompts for inference. Evaluations on real-world\ndatasets demonstrate BRAINS effectiveness in classifying disease severity and\nidentifying early signs of cognitive decline. This system not only shows strong\npotential as an assistive tool for scalable, explainable, and early-stage\nAlzheimer's disease detection, but also offers hope for future applications in\nthe field.",
    "mediaUrls": [
      "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/GOjumJN8KFk2GcHqyer5G.jpeg"
    ],
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.02490.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "67a3002c637d195f3c4bf371",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a3002c637d195f3c4bf371/bOUHNfZHrvVOjJpycZtHU.jpeg",
      "fullname": "Rajan Das Gupta",
      "name": "rajandasgupta",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "68b97f8992f0e22420e79325",
      "name": "eliteresearch",
      "fullname": "ELITE Research Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68b97676948b70f005d923fe/2QCfcp6rhnh8GI6AVJ_yQ.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.02415",
      "authors": [
        {
          "_id": "690ac0bbd70e173c84528ff1",
          "name": "Duo Xu",
          "hidden": false
        },
        {
          "_id": "690ac0bbd70e173c84528ff2",
          "name": "Hao Cheng",
          "hidden": false
        },
        {
          "_id": "690ac0bbd70e173c84528ff3",
          "name": "Xin Lin",
          "hidden": false
        },
        {
          "_id": "690ac0bbd70e173c84528ff4",
          "name": "Zhen Xie",
          "hidden": false
        },
        {
          "_id": "690ac0bbd70e173c84528ff5",
          "name": "Hao Wang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-04T09:45:34.000Z",
      "submittedOnDailyAt": "2025-11-05T00:43:20.949Z",
      "title": "ChartM^3: A Multi-Stage Code-Driven Pipeline for Constructing\n  Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Complex chart understanding tasks demand advanced visual recognition and\nreasoning capabilities from multimodal large language models (MLLMs). However,\ncurrent research provides limited coverage of complex chart scenarios and\ncomputation-intensive reasoning tasks prevalent in real-world applications.\nThis study proposes an automated multi-stage code-driven pipeline for\nsystematically generating visual reasoning datasets to address these\nlimitations. The pipeline integrates retrieval-augmented generation (RAG) to\nretrieve professional chart templates and employs chain-of-thought (CoT)\nstrategies to generate reasoning codes that simulate real data distributions,\nthereby driving chart rendering and question-related statistical computations.\nThrough model-based evaluation, the pipeline enhances chart diversity and data\nquality. Using this framework, we construct ChartM^3, a multi-dimensional and\nmulti-step dataset containing 38K charts and 142K Q&A pairs for training, along\nwith 2,871 high-quality evaluation samples for enabling practical performance\nassessment. Supervised fine-tuning (SFT) and reinforcement learning (RL)\nexperiments demonstrate that our dataset significantly improves reasoning\ncapabilities and cross-domain generalization performance, enabling smaller\nmodels to achieve performance comparable to larger-scale models in complex\nchart comprehension.",
      "upvotes": 1,
      "discussionId": "690ac0bbd70e173c84528ff6",
      "ai_summary": "An automated pipeline using retrieval-augmented generation and chain-of-thought strategies creates a diverse dataset to enhance reasoning capabilities in complex chart understanding tasks.",
      "ai_keywords": [
        "retrieval-augmented generation",
        "chain-of-thought",
        "visual reasoning datasets",
        "chart rendering",
        "statistical computations",
        "model-based evaluation",
        "supervised fine-tuning",
        "reinforcement learning",
        "cross-domain generalization"
      ]
    },
    "publishedAt": "2025-11-04T04:45:34.000Z",
    "title": "ChartM^3: A Multi-Stage Code-Driven Pipeline for Constructing\n  Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension",
    "summary": "Complex chart understanding tasks demand advanced visual recognition and\nreasoning capabilities from multimodal large language models (MLLMs). However,\ncurrent research provides limited coverage of complex chart scenarios and\ncomputation-intensive reasoning tasks prevalent in real-world applications.\nThis study proposes an automated multi-stage code-driven pipeline for\nsystematically generating visual reasoning datasets to address these\nlimitations. The pipeline integrates retrieval-augmented generation (RAG) to\nretrieve professional chart templates and employs chain-of-thought (CoT)\nstrategies to generate reasoning codes that simulate real data distributions,\nthereby driving chart rendering and question-related statistical computations.\nThrough model-based evaluation, the pipeline enhances chart diversity and data\nquality. Using this framework, we construct ChartM^3, a multi-dimensional and\nmulti-step dataset containing 38K charts and 142K Q&A pairs for training, along\nwith 2,871 high-quality evaluation samples for enabling practical performance\nassessment. Supervised fine-tuning (SFT) and reinforcement learning (RL)\nexperiments demonstrate that our dataset significantly improves reasoning\ncapabilities and cross-domain generalization performance, enabling smaller\nmodels to achieve performance comparable to larger-scale models in complex\nchart comprehension.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.02415.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 154
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.02347",
      "authors": [
        {
          "_id": "690ad8c4d70e173c8452902c",
          "name": "Liuhao Lin",
          "hidden": false
        },
        {
          "_id": "690ad8c4d70e173c8452902d",
          "name": "Ke Li",
          "hidden": false
        },
        {
          "_id": "690ad8c4d70e173c8452902e",
          "name": "Zihan Xu",
          "hidden": false
        },
        {
          "_id": "690ad8c4d70e173c8452902f",
          "name": "Yuchen Shi",
          "hidden": false
        },
        {
          "_id": "690ad8c4d70e173c84529030",
          "name": "Yulei Qin",
          "hidden": false
        },
        {
          "_id": "690ad8c4d70e173c84529031",
          "name": "Yan Zhang",
          "hidden": false
        },
        {
          "_id": "690ad8c4d70e173c84529032",
          "name": "Xing Sun",
          "hidden": false
        },
        {
          "_id": "690ad8c4d70e173c84529033",
          "name": "Rongrong Ji",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-04T08:11:23.000Z",
      "submittedOnDailyAt": "2025-11-05T02:35:20.480Z",
      "title": "LTD-Bench: Evaluating Large Language Models by Letting Them Draw",
      "submittedOnDailyBy": {
        "_id": "63280915eeee4dd858083092",
        "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg",
        "isPro": false,
        "fullname": "Ke Li",
        "user": "tristanli",
        "type": "user"
      },
      "summary": "Current evaluation paradigms for large language models (LLMs) represent a\ncritical blind spot in AI research--relying on opaque numerical metrics that\nconceal fundamental limitations in spatial reasoning while providing no\nintuitive understanding of model capabilities. This deficiency creates a\ndangerous disconnect between reported performance and practical abilities,\nparticularly for applications requiring physical world understanding. We\nintroduce LTD-Bench, a breakthrough benchmark that transforms LLM evaluation\nfrom abstract scores to directly observable visual outputs by requiring models\nto generate drawings through dot matrices or executable code. This approach\nmakes spatial reasoning limitations immediately apparent even to non-experts,\nbridging the fundamental gap between statistical performance and intuitive\nassessment. LTD-Bench implements a comprehensive methodology with complementary\ngeneration tasks (testing spatial imagination) and recognition tasks (assessing\nspatial perception) across three progressively challenging difficulty levels,\nmethodically evaluating both directions of the critical language-spatial\nmapping. Our extensive experiments with state-of-the-art models expose an\nalarming capability gap: even LLMs achieving impressive results on traditional\nbenchmarks demonstrate profound deficiencies in establishing bidirectional\nmappings between language and spatial concept--a fundamental limitation that\nundermines their potential as genuine world models. Furthermore, LTD-Bench's\nvisual outputs enable powerful diagnostic analysis, offering a potential\napproach to investigate model similarity.",
      "upvotes": 1,
      "discussionId": "690ad8c4d70e173c84529034",
      "ai_summary": "LTD-Bench evaluates large language models' spatial reasoning by requiring them to generate visual outputs, revealing significant limitations in their ability to map language to spatial concepts.",
      "ai_keywords": [
        "LTD-Bench",
        "large language models",
        "spatial reasoning",
        "dot matrices",
        "executable code",
        "spatial imagination",
        "spatial perception",
        "bidirectional mappings",
        "language-spatial mapping",
        "world models",
        "diagnostic analysis"
      ],
      "organization": {
        "_id": "66543b6e420092799d2f625c",
        "name": "tencent",
        "fullname": "Tencent",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
      }
    },
    "publishedAt": "2025-11-04T03:11:23.000Z",
    "title": "LTD-Bench: Evaluating Large Language Models by Letting Them Draw",
    "summary": "Current evaluation paradigms for large language models (LLMs) represent a\ncritical blind spot in AI research--relying on opaque numerical metrics that\nconceal fundamental limitations in spatial reasoning while providing no\nintuitive understanding of model capabilities. This deficiency creates a\ndangerous disconnect between reported performance and practical abilities,\nparticularly for applications requiring physical world understanding. We\nintroduce LTD-Bench, a breakthrough benchmark that transforms LLM evaluation\nfrom abstract scores to directly observable visual outputs by requiring models\nto generate drawings through dot matrices or executable code. This approach\nmakes spatial reasoning limitations immediately apparent even to non-experts,\nbridging the fundamental gap between statistical performance and intuitive\nassessment. LTD-Bench implements a comprehensive methodology with complementary\ngeneration tasks (testing spatial imagination) and recognition tasks (assessing\nspatial perception) across three progressively challenging difficulty levels,\nmethodically evaluating both directions of the critical language-spatial\nmapping. Our extensive experiments with state-of-the-art models expose an\nalarming capability gap: even LLMs achieving impressive results on traditional\nbenchmarks demonstrate profound deficiencies in establishing bidirectional\nmappings between language and spatial concept--a fundamental limitation that\nundermines their potential as genuine world models. Furthermore, LTD-Bench's\nvisual outputs enable powerful diagnostic analysis, offering a potential\napproach to investigate model similarity.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.02347.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "63280915eeee4dd858083092",
      "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg",
      "fullname": "Ke Li",
      "name": "tristanli",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "organization": {
      "_id": "66543b6e420092799d2f625c",
      "name": "tencent",
      "fullname": "Tencent",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.01937",
      "authors": [
        {
          "_id": "690ae1c8d70e173c84529059",
          "name": "Abdelaziz Bounhar",
          "hidden": false
        },
        {
          "_id": "690ae1c8d70e173c8452905a",
          "name": "Hadi Abdine",
          "hidden": false
        },
        {
          "_id": "690ae1c8d70e173c8452905b",
          "name": "Evan Dufraisse",
          "hidden": false
        },
        {
          "_id": "690ae1c8d70e173c8452905c",
          "name": "Ahmad Chamma",
          "hidden": false
        },
        {
          "_id": "690ae1c8d70e173c8452905d",
          "name": "Amr Mohamed",
          "hidden": false
        },
        {
          "_id": "690ae1c8d70e173c8452905e",
          "name": "Dani Bouch",
          "hidden": false
        },
        {
          "_id": "690ae1c8d70e173c8452905f",
          "name": "Michalis Vazirgiannis",
          "hidden": false
        },
        {
          "_id": "690ae1c8d70e173c84529060",
          "name": "Guokan Shang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-02T17:29:16.000Z",
      "submittedOnDailyAt": "2025-11-05T03:15:19.121Z",
      "title": "Shorter but not Worse: Frugal Reasoning via Easy Samples as Length\n  Regularizers in Math RLVR",
      "submittedOnDailyBy": {
        "_id": "6380e53efb49cd1c12052c17",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6380e53efb49cd1c12052c17/b5CweexfrVn-W_xto2agR.jpeg",
        "isPro": false,
        "fullname": "Abdelaziz Bounhar",
        "user": "BounharAbdelaziz",
        "type": "user"
      },
      "summary": "Large language models (LLMs) trained for step-by-step reasoning often become\nexcessively verbose, raising inference cost. Standard Reinforcement Learning\nwith Verifiable Rewards (RLVR) pipelines filter out ``easy'' problems for\ntraining efficiency, leaving the model to train primarily on harder problems\nthat require longer reasoning chains. This skews the output length distribution\nupward, resulting in a model that conflates ``thinking longer'' with\n``thinking better''. In this work, we show that retaining and modestly\nup-weighting moderately easy problems acts as an implicit length regularizer.\nExposing the model to solvable short-chain tasks constrains its output\ndistribution and prevents runaway verbosity. The result is\n\\emph{emergent brevity for free}: the model learns to solve harder\nproblems without inflating the output length,  despite the absence of\nany explicit length penalization. RLVR experiments using this approach on\nQwen3-4B-Thinking-2507 (with a 16k token limit) achieve baseline\npass@1 AIME25 accuracy while generating solutions that are, on average, nearly\ntwice as short. The code is available at\nhttps://github.com/MBZUAI-Paris/Frugal-AI{GitHub}, with datasets and\nmodels on\nhttps://huggingface.co/collections/MBZUAI-Paris/k2-think-mini-68dcfa8b114686a4bd3dc2bc{Hugging\nFace}.",
      "upvotes": 1,
      "discussionId": "690ae1c8d70e173c84529061",
      "githubRepo": "https://github.com/MBZUAI-Paris/Frugal-AI-Math",
      "ai_summary": "Retaining and up-weighting moderately easy problems in RLVR pipelines for LLMs reduces output verbosity without explicit length penalization.",
      "ai_keywords": [
        "Reinforcement Learning with Verifiable Rewards (RLVR)",
        "Large language models (LLMs)",
        "step-by-step reasoning",
        "inference cost",
        "output length distribution",
        "thinking longer",
        "thinking better",
        "emergent brevity",
        "Qwen3-4B-Thinking-2507",
        "pass@1 AIME25 accuracy"
      ],
      "githubStars": 6,
      "organization": {
        "_id": "6656df18bfefce0a724e65d6",
        "name": "MBZUAI-Paris",
        "fullname": "MBZUAI-IFM Paris Lab",
        "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6087e598e2b7cc3a117b0dc5/JUiDkClou70ZdFQaiZ3hK.png"
      }
    },
    "publishedAt": "2025-11-02T12:29:16.000Z",
    "title": "Shorter but not Worse: Frugal Reasoning via Easy Samples as Length\n  Regularizers in Math RLVR",
    "summary": "Large language models (LLMs) trained for step-by-step reasoning often become\nexcessively verbose, raising inference cost. Standard Reinforcement Learning\nwith Verifiable Rewards (RLVR) pipelines filter out ``easy'' problems for\ntraining efficiency, leaving the model to train primarily on harder problems\nthat require longer reasoning chains. This skews the output length distribution\nupward, resulting in a model that conflates ``thinking longer'' with\n``thinking better''. In this work, we show that retaining and modestly\nup-weighting moderately easy problems acts as an implicit length regularizer.\nExposing the model to solvable short-chain tasks constrains its output\ndistribution and prevents runaway verbosity. The result is\n\\emph{emergent brevity for free}: the model learns to solve harder\nproblems without inflating the output length,  despite the absence of\nany explicit length penalization. RLVR experiments using this approach on\nQwen3-4B-Thinking-2507 (with a 16k token limit) achieve baseline\npass@1 AIME25 accuracy while generating solutions that are, on average, nearly\ntwice as short. The code is available at\nhttps://github.com/MBZUAI-Paris/Frugal-AI{GitHub}, with datasets and\nmodels on\nhttps://huggingface.co/collections/MBZUAI-Paris/k2-think-mini-68dcfa8b114686a4bd3dc2bc{Hugging\nFace}.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01937.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6380e53efb49cd1c12052c17",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6380e53efb49cd1c12052c17/b5CweexfrVn-W_xto2agR.jpeg",
      "fullname": "Abdelaziz Bounhar",
      "name": "BounharAbdelaziz",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 59
    },
    "organization": {
      "_id": "6656df18bfefce0a724e65d6",
      "name": "MBZUAI-Paris",
      "fullname": "MBZUAI-IFM Paris Lab",
      "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6087e598e2b7cc3a117b0dc5/JUiDkClou70ZdFQaiZ3hK.png"
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.01914",
      "authors": [
        {
          "_id": "690abf69d70e173c84528fde",
          "name": "Yuan Zhang",
          "hidden": false
        },
        {
          "_id": "690abf69d70e173c84528fdf",
          "name": "Chenyu Xue",
          "hidden": false
        },
        {
          "_id": "690abf69d70e173c84528fe0",
          "name": "Wenjie Xu",
          "hidden": false
        },
        {
          "_id": "690abf69d70e173c84528fe1",
          "name": "Chao Ji",
          "hidden": false
        },
        {
          "_id": "690abf69d70e173c84528fe2",
          "name": "Jiajia wu",
          "hidden": false
        },
        {
          "_id": "690abf69d70e173c84528fe3",
          "name": "Jia Pan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-01T06:24:56.000Z",
      "submittedOnDailyAt": "2025-11-05T00:37:39.223Z",
      "title": "iFlyBot-VLA Technical Report",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "We introduce iFlyBot-VLA, a large-scale Vision-Language-Action (VLA) model\ntrained under a novel framework. The main contributions are listed as follows:\n(1) a latent action model thoroughly trained on large-scale human and robotic\nmanipulation videos; (2) a dual-level action representation framework that\njointly supervises both the Vision-Language Model (VLM) and the action expert\nduring training; (3) a mixed training strategy that combines robot trajectory\ndata with general QA and spatial QA datasets, effectively enhancing the 3D\nperceptual and reasoning capabilities of the VLM backbone. Specifically, the\nVLM is trained to predict two complementary forms of actions: latent actions,\nderived from our latent action model pretrained on cross-embodiment\nmanipulation data, which capture implicit high-level intentions; and structured\ndiscrete action tokens, obtained through frequency-domain transformations of\ncontinuous control signals, which encode explicit low-level dynamics. This dual\nsupervision aligns the representation spaces of language, vision, and action,\nenabling the VLM to directly contribute to action generation. Experimental\nresults on the LIBERO Franka benchmark demonstrate the superiority of our\nframe-work, while real-world evaluations further show that iFlyBot-VLA achieves\ncompetitive success rates across diverse and challenging manipulation tasks.\nFurthermore, we plan to open-source a portion of our self-constructed dataset\nto support future research in the community",
      "upvotes": 1,
      "discussionId": "690abf69d70e173c84528fe4",
      "projectPage": "https://xuwenjie401.github.io/iFlyBot-VLA.github.io/",
      "ai_summary": "iFlyBot-VLA, a large-scale VLA model, uses a latent action model and dual-level action representation to enhance 3D perceptual and reasoning capabilities, achieving superior performance in manipulation tasks.",
      "ai_keywords": [
        "latent action model",
        "dual-level action representation",
        "Vision-Language Model (VLM)",
        "action expert",
        "mixed training strategy",
        "robot trajectory data",
        "general QA",
        "spatial QA",
        "latent actions",
        "structured discrete action tokens",
        "frequency-domain transformations",
        "continuous control signals",
        "LIBERO Franka benchmark"
      ]
    },
    "publishedAt": "2025-11-01T02:24:56.000Z",
    "title": "iFlyBot-VLA Technical Report",
    "summary": "We introduce iFlyBot-VLA, a large-scale Vision-Language-Action (VLA) model\ntrained under a novel framework. The main contributions are listed as follows:\n(1) a latent action model thoroughly trained on large-scale human and robotic\nmanipulation videos; (2) a dual-level action representation framework that\njointly supervises both the Vision-Language Model (VLM) and the action expert\nduring training; (3) a mixed training strategy that combines robot trajectory\ndata with general QA and spatial QA datasets, effectively enhancing the 3D\nperceptual and reasoning capabilities of the VLM backbone. Specifically, the\nVLM is trained to predict two complementary forms of actions: latent actions,\nderived from our latent action model pretrained on cross-embodiment\nmanipulation data, which capture implicit high-level intentions; and structured\ndiscrete action tokens, obtained through frequency-domain transformations of\ncontinuous control signals, which encode explicit low-level dynamics. This dual\nsupervision aligns the representation spaces of language, vision, and action,\nenabling the VLM to directly contribute to action generation. Experimental\nresults on the LIBERO Franka benchmark demonstrate the superiority of our\nframe-work, while real-world evaluations further show that iFlyBot-VLA achieves\ncompetitive success rates across diverse and challenging manipulation tasks.\nFurthermore, we plan to open-source a portion of our self-constructed dataset\nto support future research in the community",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01914.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 154
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2510.19278",
      "authors": [
        {
          "_id": "69095f1c812eca10f9cc6163",
          "user": {
            "_id": "69095e06b8c21a27dbc0d2b6",
            "avatarUrl": "/avatars/3c5f5c1140024c084bf8c8d935dd3d90.svg",
            "isPro": false,
            "fullname": "Nobline Yoo",
            "user": "n-yoo",
            "type": "user"
          },
          "name": "Nobline Yoo",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-11-04T20:26:53.098Z",
          "hidden": false
        },
        {
          "_id": "69095f1c812eca10f9cc6164",
          "name": "Olga Russakovsky",
          "hidden": false
        },
        {
          "_id": "69095f1c812eca10f9cc6165",
          "name": "Ye Zhu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-10-22T06:27:05.000Z",
      "submittedOnDailyAt": "2025-11-05T00:11:33.890Z",
      "title": "D2D: Detector-to-Differentiable Critic for Improved Numeracy in\n  Text-to-Image Generation",
      "submittedOnDailyBy": {
        "_id": "69095e06b8c21a27dbc0d2b6",
        "avatarUrl": "/avatars/3c5f5c1140024c084bf8c8d935dd3d90.svg",
        "isPro": false,
        "fullname": "Nobline Yoo",
        "user": "n-yoo",
        "type": "user"
      },
      "summary": "Text-to-image (T2I) diffusion models have achieved strong performance in\nsemantic alignment, yet they still struggle with generating the correct number\nof objects specified in prompts. Existing approaches typically incorporate\nauxiliary counting networks as external critics to enhance numeracy. However,\nsince these critics must provide gradient guidance during generation, they are\nrestricted to regression-based models that are inherently differentiable, thus\nexcluding detector-based models with superior counting ability, whose\ncount-via-enumeration nature is non-differentiable. To overcome this\nlimitation, we propose Detector-to-Differentiable (D2D), a novel framework that\ntransforms non-differentiable detection models into differentiable critics,\nthereby leveraging their superior counting ability to guide numeracy\ngeneration. Specifically, we design custom activation functions to convert\ndetector logits into soft binary indicators, which are then used to optimize\nthe noise prior at inference time with pre-trained T2I models. Our extensive\nexperiments on SDXL-Turbo, SD-Turbo, and Pixart-DMD across four benchmarks of\nvarying complexity (low-density, high-density, and multi-object scenarios)\ndemonstrate consistent and substantial improvements in object counting accuracy\n(e.g., boosting up to 13.7% on D2D-Small, a 400-prompt, low-density benchmark),\nwith minimal degradation in overall image quality and computational overhead.",
      "upvotes": 1,
      "discussionId": "69095f1d812eca10f9cc6166",
      "ai_summary": "A novel framework, Detector-to-Differentiable (D2D), transforms non-differentiable detection models into differentiable critics to improve object counting accuracy in text-to-image diffusion models with minimal impact on image quality.",
      "ai_keywords": [
        "text-to-image (T2I) diffusion models",
        "semantic alignment",
        "auxiliary counting networks",
        "gradient guidance",
        "detector-based models",
        "non-differentiable",
        "Detector-to-Differentiable (D2D)",
        "custom activation functions",
        "soft binary indicators",
        "noise prior",
        "SDXL-Turbo",
        "SD-Turbo",
        "Pixart-DMD",
        "object counting accuracy",
        "image quality",
        "computational overhead"
      ]
    },
    "publishedAt": "2025-10-22T02:27:05.000Z",
    "title": "D2D: Detector-to-Differentiable Critic for Improved Numeracy in\n  Text-to-Image Generation",
    "summary": "Text-to-image (T2I) diffusion models have achieved strong performance in\nsemantic alignment, yet they still struggle with generating the correct number\nof objects specified in prompts. Existing approaches typically incorporate\nauxiliary counting networks as external critics to enhance numeracy. However,\nsince these critics must provide gradient guidance during generation, they are\nrestricted to regression-based models that are inherently differentiable, thus\nexcluding detector-based models with superior counting ability, whose\ncount-via-enumeration nature is non-differentiable. To overcome this\nlimitation, we propose Detector-to-Differentiable (D2D), a novel framework that\ntransforms non-differentiable detection models into differentiable critics,\nthereby leveraging their superior counting ability to guide numeracy\ngeneration. Specifically, we design custom activation functions to convert\ndetector logits into soft binary indicators, which are then used to optimize\nthe noise prior at inference time with pre-trained T2I models. Our extensive\nexperiments on SDXL-Turbo, SD-Turbo, and Pixart-DMD across four benchmarks of\nvarying complexity (low-density, high-density, and multi-object scenarios)\ndemonstrate consistent and substantial improvements in object counting accuracy\n(e.g., boosting up to 13.7% on D2D-Small, a 400-prompt, low-density benchmark),\nwith minimal degradation in overall image quality and computational overhead.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2510.19278.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "69095e06b8c21a27dbc0d2b6",
      "avatarUrl": "/avatars/3c5f5c1140024c084bf8c8d935dd3d90.svg",
      "fullname": "Nobline Yoo",
      "name": "n-yoo",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": true
  },
  {
    "paper": {
      "id": "2511.02712",
      "authors": [
        {
          "_id": "690abebbd70e173c84528fd5",
          "name": "Zhicheng Zhang",
          "hidden": false
        },
        {
          "_id": "690abebbd70e173c84528fd6",
          "name": "Weicheng Wang",
          "hidden": false
        },
        {
          "_id": "690abebbd70e173c84528fd7",
          "name": "Yongjie Zhu",
          "hidden": false
        },
        {
          "_id": "690abebbd70e173c84528fd8",
          "name": "Wenyu Qin",
          "hidden": false
        },
        {
          "_id": "690abebbd70e173c84528fd9",
          "name": "Pengfei Wan",
          "hidden": false
        },
        {
          "_id": "690abebbd70e173c84528fda",
          "name": "Di Zhang",
          "hidden": false
        },
        {
          "_id": "690abebbd70e173c84528fdb",
          "name": "Jufeng Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-04T16:31:09.000Z",
      "submittedOnDailyAt": "2025-11-05T00:34:58.030Z",
      "title": "VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation\n  Models",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "Understanding and predicting emotion from videos has gathered significant\nattention in recent studies, driven by advancements in video large language\nmodels (VideoLLMs). While advanced methods have made progress in video emotion\nanalysis, the intrinsic nature of emotions poses significant challenges.\nEmotions are characterized by dynamic and cues-dependent properties, making it\ndifficult to understand complex and evolving emotional states with reasonable\nrationale. To tackle these challenges, we propose a novel affective cues-guided\nreasoning framework that unifies fundamental attribute perception, expression\nanalysis, and high-level emotional understanding in a stage-wise manner. At the\ncore of our approach is a family of video emotion foundation models (VidEmo),\nspecifically designed for emotion reasoning and instruction-following. These\nmodels undergo a two-stage tuning process: first, curriculum emotion learning\nfor injecting emotion knowledge, followed by affective-tree reinforcement\nlearning for emotion reasoning. Moreover, we establish a foundational data\ninfrastructure and introduce a emotion-centric fine-grained dataset (Emo-CFG)\nconsisting of 2.1M diverse instruction-based samples. Emo-CFG includes\nexplainable emotional question-answering, fine-grained captions, and associated\nrationales, providing essential resources for advancing emotion understanding\ntasks. Experimental results demonstrate that our approach achieves competitive\nperformance, setting a new milestone across 15 face perception tasks.",
      "upvotes": 0,
      "discussionId": "690abebbd70e173c84528fdc",
      "projectPage": "https://zzcheng.top/VidEmo",
      "ai_summary": "A novel affective cues-guided reasoning framework using video emotion foundation models and a fine-grained dataset achieves competitive performance in emotion understanding tasks.",
      "ai_keywords": [
        "video large language models",
        "VideoLLMs",
        "video emotion analysis",
        "affective cues-guided reasoning",
        "video emotion foundation models",
        "VidEmo",
        "curriculum emotion learning",
        "affective-tree reinforcement learning",
        "emotion-centric fine-grained dataset",
        "Emo-CFG",
        "explainable emotional question-answering",
        "fine-grained captions",
        "face perception tasks"
      ]
    },
    "publishedAt": "2025-11-04T11:31:09.000Z",
    "title": "VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation\n  Models",
    "summary": "Understanding and predicting emotion from videos has gathered significant\nattention in recent studies, driven by advancements in video large language\nmodels (VideoLLMs). While advanced methods have made progress in video emotion\nanalysis, the intrinsic nature of emotions poses significant challenges.\nEmotions are characterized by dynamic and cues-dependent properties, making it\ndifficult to understand complex and evolving emotional states with reasonable\nrationale. To tackle these challenges, we propose a novel affective cues-guided\nreasoning framework that unifies fundamental attribute perception, expression\nanalysis, and high-level emotional understanding in a stage-wise manner. At the\ncore of our approach is a family of video emotion foundation models (VidEmo),\nspecifically designed for emotion reasoning and instruction-following. These\nmodels undergo a two-stage tuning process: first, curriculum emotion learning\nfor injecting emotion knowledge, followed by affective-tree reinforcement\nlearning for emotion reasoning. Moreover, we establish a foundational data\ninfrastructure and introduce a emotion-centric fine-grained dataset (Emo-CFG)\nconsisting of 2.1M diverse instruction-based samples. Emo-CFG includes\nexplainable emotional question-answering, fine-grained captions, and associated\nrationales, providing essential resources for advancing emotion understanding\ntasks. Experimental results demonstrate that our approach achieves competitive\nperformance, setting a new milestone across 15 face perception tasks.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.02712.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 154
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.02366",
      "authors": [
        {
          "_id": "690abdafd70e173c84528fc1",
          "name": "Yudong Li",
          "hidden": false
        },
        {
          "_id": "690abdafd70e173c84528fc2",
          "name": "Zhongliang Yang",
          "hidden": false
        },
        {
          "_id": "690abdafd70e173c84528fc3",
          "name": "Kejiang Chen",
          "hidden": false
        },
        {
          "_id": "690abdafd70e173c84528fc4",
          "name": "Wenxuan Wang",
          "hidden": false
        },
        {
          "_id": "690abdafd70e173c84528fc5",
          "name": "Tianxin Zhang",
          "hidden": false
        },
        {
          "_id": "690abdafd70e173c84528fc6",
          "name": "Sifang Wan",
          "hidden": false
        },
        {
          "_id": "690abdafd70e173c84528fc7",
          "name": "Kecheng Wang",
          "hidden": false
        },
        {
          "_id": "690abdafd70e173c84528fc8",
          "name": "Haitian Li",
          "hidden": false
        },
        {
          "_id": "690abdafd70e173c84528fc9",
          "name": "Xu Wang",
          "hidden": false
        },
        {
          "_id": "690abdafd70e173c84528fca",
          "name": "Lefan Cheng",
          "hidden": false
        },
        {
          "_id": "690abdafd70e173c84528fcb",
          "name": "Youdan Yang",
          "hidden": false
        },
        {
          "_id": "690abdafd70e173c84528fcc",
          "name": "Baocheng Chen",
          "hidden": false
        },
        {
          "_id": "690abdafd70e173c84528fcd",
          "name": "Ziyu Liu",
          "hidden": false
        },
        {
          "_id": "690abdafd70e173c84528fce",
          "name": "Yufei Sun",
          "hidden": false
        },
        {
          "_id": "690abdafd70e173c84528fcf",
          "name": "Liyan Wu",
          "hidden": false
        },
        {
          "_id": "690abdafd70e173c84528fd0",
          "name": "Wenya Wen",
          "hidden": false
        },
        {
          "_id": "690abdafd70e173c84528fd1",
          "name": "Xingchi Gu",
          "hidden": false
        },
        {
          "_id": "690abdafd70e173c84528fd2",
          "name": "Peiru Yang",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-04T08:44:09.000Z",
      "submittedOnDailyAt": "2025-11-05T00:30:20.539Z",
      "title": "LiveSecBench: A Dynamic and Culturally-Relevant AI Safety Benchmark for\n  LLMs in Chinese Context",
      "submittedOnDailyBy": {
        "_id": "6039478ab3ecf716b1a5fd4d",
        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
        "isPro": true,
        "fullname": "taesiri",
        "user": "taesiri",
        "type": "user"
      },
      "summary": "In this work, we propose LiveSecBench, a dynamic and continuously updated\nsafety benchmark specifically for Chinese-language LLM application scenarios.\nLiveSecBench evaluates models across six critical dimensions (Legality, Ethics,\nFactuality, Privacy, Adversarial Robustness, and Reasoning Safety) rooted in\nthe Chinese legal and social frameworks. This benchmark maintains relevance\nthrough a dynamic update schedule that incorporates new threat vectors, such as\nthe planned inclusion of Text-to-Image Generation Safety and Agentic Safety in\nthe next update. For now, LiveSecBench (v251030) has evaluated 18 LLMs,\nproviding a landscape of AI safety in the context of Chinese language. The\nleaderboard is publicly accessible at https://livesecbench.intokentech.cn/.",
      "upvotes": 0,
      "discussionId": "690abdb0d70e173c84528fd3",
      "projectPage": "https://livesecbench.intokentech.cn/",
      "ai_summary": "LiveSecBench is a continuously updated safety benchmark for Chinese-language LLMs, evaluating them across six critical dimensions including legality, ethics, factuality, privacy, adversarial robustness, and reasoning safety.",
      "ai_keywords": [
        "LLM",
        "LiveSecBench",
        "Text-to-Image Generation Safety",
        "Agentic Safety"
      ]
    },
    "publishedAt": "2025-11-04T03:44:09.000Z",
    "title": "LiveSecBench: A Dynamic and Culturally-Relevant AI Safety Benchmark for\n  LLMs in Chinese Context",
    "summary": "In this work, we propose LiveSecBench, a dynamic and continuously updated\nsafety benchmark specifically for Chinese-language LLM application scenarios.\nLiveSecBench evaluates models across six critical dimensions (Legality, Ethics,\nFactuality, Privacy, Adversarial Robustness, and Reasoning Safety) rooted in\nthe Chinese legal and social frameworks. This benchmark maintains relevance\nthrough a dynamic update schedule that incorporates new threat vectors, such as\nthe planned inclusion of Text-to-Image Generation Safety and Agentic Safety in\nthe next update. For now, LiveSecBench (v251030) has evaluated 18 LLMs,\nproviding a landscape of AI safety in the context of Chinese language. The\nleaderboard is publicly accessible at https://livesecbench.intokentech.cn/.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.02366.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "6039478ab3ecf716b1a5fd4d",
      "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
      "fullname": "taesiri",
      "name": "taesiri",
      "type": "user",
      "isPro": true,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false,
      "followerCount": 154
    },
    "isAuthorParticipating": false
  },
  {
    "paper": {
      "id": "2511.01502",
      "authors": [
        {
          "_id": "690ab99cd70e173c84528f9c",
          "name": "Mengtan Zhang",
          "hidden": false
        },
        {
          "_id": "690ab99cd70e173c84528f9d",
          "name": "Zizhan Guo",
          "hidden": false
        },
        {
          "_id": "690ab99cd70e173c84528f9e",
          "name": "Hongbo Zhao",
          "hidden": false
        },
        {
          "_id": "690ab99cd70e173c84528f9f",
          "name": "Yi Feng",
          "hidden": false
        },
        {
          "_id": "690ab99cd70e173c84528fa0",
          "name": "Zuyi Xiong",
          "hidden": false
        },
        {
          "_id": "690ab99cd70e173c84528fa1",
          "name": "Yue Wang",
          "hidden": false
        },
        {
          "_id": "690ab99cd70e173c84528fa2",
          "name": "Shaoyi Du",
          "hidden": false
        },
        {
          "_id": "690ab99cd70e173c84528fa3",
          "name": "Hanli Wang",
          "hidden": false
        },
        {
          "_id": "690ab99cd70e173c84528fa4",
          "name": "Rui Fan",
          "hidden": false
        }
      ],
      "publishedAt": "2025-11-03T12:14:52.000Z",
      "submittedOnDailyAt": "2025-11-05T01:00:57.758Z",
      "title": "Discriminately Treating Motion Components Evolves Joint Depth and\n  Ego-Motion Learning",
      "submittedOnDailyBy": {
        "_id": "69098f192b98bb91436f24f0",
        "avatarUrl": "/avatars/103c790078d5e8c6f7e141de42c8e261.svg",
        "isPro": false,
        "fullname": "Mengtan Zhang",
        "user": "mengtanZ",
        "type": "user"
      },
      "summary": "Unsupervised learning of depth and ego-motion, two fundamental 3D perception\ntasks, has made significant strides in recent years. However, most methods\ntreat ego-motion as an auxiliary task, either mixing all motion types or\nexcluding depth-independent rotational motions in supervision. Such designs\nlimit the incorporation of strong geometric constraints, reducing reliability\nand robustness under diverse conditions. This study introduces a discriminative\ntreatment of motion components, leveraging the geometric regularities of their\nrespective rigid flows to benefit both depth and ego-motion estimation. Given\nconsecutive video frames, network outputs first align the optical axes and\nimaging planes of the source and target cameras. Optical flows between frames\nare transformed through these alignments, and deviations are quantified to\nimpose geometric constraints individually on each ego-motion component,\nenabling more targeted refinement. These alignments further reformulate the\njoint learning process into coaxial and coplanar forms, where depth and each\ntranslation component can be mutually derived through closed-form geometric\nrelationships, introducing complementary constraints that improve depth\nrobustness. DiMoDE, a general depth and ego-motion joint learning framework\nincorporating these designs, achieves state-of-the-art performance on multiple\npublic datasets and a newly collected diverse real-world dataset, particularly\nunder challenging conditions. Our source code will be publicly available at\nmias.group/DiMoDE upon publication.",
      "upvotes": 0,
      "discussionId": "690ab99dd70e173c84528fa5",
      "ai_summary": "A discriminative approach to depth and ego-motion estimation leverages geometric constraints to improve performance and robustness in 3D perception tasks.",
      "ai_keywords": [
        "unsupervised learning",
        "depth",
        "ego-motion",
        "3D perception",
        "optical axes",
        "imaging planes",
        "optical flows",
        "geometric constraints",
        "coaxial",
        "coplanar",
        "closed-form geometric relationships",
        "DiMoDE"
      ]
    },
    "publishedAt": "2025-11-03T07:14:52.000Z",
    "title": "Discriminately Treating Motion Components Evolves Joint Depth and\n  Ego-Motion Learning",
    "summary": "Unsupervised learning of depth and ego-motion, two fundamental 3D perception\ntasks, has made significant strides in recent years. However, most methods\ntreat ego-motion as an auxiliary task, either mixing all motion types or\nexcluding depth-independent rotational motions in supervision. Such designs\nlimit the incorporation of strong geometric constraints, reducing reliability\nand robustness under diverse conditions. This study introduces a discriminative\ntreatment of motion components, leveraging the geometric regularities of their\nrespective rigid flows to benefit both depth and ego-motion estimation. Given\nconsecutive video frames, network outputs first align the optical axes and\nimaging planes of the source and target cameras. Optical flows between frames\nare transformed through these alignments, and deviations are quantified to\nimpose geometric constraints individually on each ego-motion component,\nenabling more targeted refinement. These alignments further reformulate the\njoint learning process into coaxial and coplanar forms, where depth and each\ntranslation component can be mutually derived through closed-form geometric\nrelationships, introducing complementary constraints that improve depth\nrobustness. DiMoDE, a general depth and ego-motion joint learning framework\nincorporating these designs, achieves state-of-the-art performance on multiple\npublic datasets and a newly collected diverse real-world dataset, particularly\nunder challenging conditions. Our source code will be publicly available at\nmias.group/DiMoDE upon publication.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.01502.png",
    "numComments": 1,
    "submittedBy": {
      "_id": "69098f192b98bb91436f24f0",
      "avatarUrl": "/avatars/103c790078d5e8c6f7e141de42c8e261.svg",
      "fullname": "Mengtan Zhang",
      "name": "mengtanZ",
      "type": "user",
      "isPro": false,
      "isHf": false,
      "isHfAdmin": false,
      "isMod": false
    },
    "isAuthorParticipating": false
  }
]