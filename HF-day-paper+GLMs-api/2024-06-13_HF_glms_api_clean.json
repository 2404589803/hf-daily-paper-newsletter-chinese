[
    {
        "url": "https://arxiv.org/abs/2406.06523",
        "content": "这篇文章标题为“NaRCan: Natural Refined Canonical Image with Integration of Diffusion Prior for Video Editing”，发表在arXiv上，编号为2406.06523。文章提出了一种名为NaRCan的视频编辑框架，该框架集成了混合变形场和扩散先验，用于生成高质量的自然规范图像来表示输入视频。该方法使用单应性来模拟全局运动，并采用多层感知器（MLPs）来捕捉局部残余变形，增强了模型处理复杂视频动态的能力。通过在训练的早期阶段引入扩散先验，该模型确保生成的图像保持高质量的自然外观，使得产生的规范图像适用于视频编辑中的各种下游任务，这是当前基于规范的方法所无法实现的。此外，文章还结合了低秩适应（LoRA）微调，并引入了噪声和扩散先验更新调度技术，将训练过程加速了14倍。广泛的实验结果表明，该方法在多种视频编辑任务中优于现有方法，生成了连贯且高质量编辑的视频序列。\n\n搜索结果来自：\n[2406.06523] NaRCan: Natural Refined Canonical Image with Integration of Diffusion Prior for Video Editing - https://arxiv.org/abs/2406.06523"
    },
    {
        "url": "https://arxiv.org/abs/2406.05338",
        "content": "这篇文章标题为“MotionClone: Training-Free Motion Cloning for Controllable Video Generation”，发表在计算机视觉和模式识别领域。文章的主要贡献是提出了一种名为MotionClone的训练无关的运动克隆框架，用于控制文本到视频的生成。这种方法通过从参考视频中克隆运动来控制视频生成，无需对模型进行训练以编码运动线索或微调视频扩散模型。\n\n在技术细节上，作者们采用了视频反演中的时间注意力来表示参考视频中的运动，并引入了主要的时间注意力引导来减轻注意力权重中噪声或非常细微运动的影响。此外，为了帮助生成模型合成合理的空间关系并增强其遵循提示的能力，他们提出了一个位置感知的语义引导机制，该机制利用参考视频中前景的粗略位置和原始的无分类器引导特征来指导视频生成。\n\n文章通过广泛的实验证明了MotionClone在全局相机运动和局部对象运动方面都表现出色，特别是在运动保真度、文本对齐和时序一致性方面具有显著优势。这篇文章对于希望了解如何在不进行训练的情况下控制视频生成的研究人员和技术人员来说，是一个重要的参考资料。\n\n搜索结果来自：\n[2406.05338] MotionClone: Training-Free Motion Cloning for Controllable Video Generation - https://arxiv.org/abs/2406.05338\n[2406.05338] MotionClone: Training-Free Motion Cloning for Controllable Video Generation - http://export.arxiv.org/abs/2406.05338"
    },
    {
        "url": "https://arxiv.org/abs/2406.06282",
        "content": "这篇文章标题为“PowerInfer-2: Fast Large Language Model Inference on a Smartphone”，是一篇关于机器学习领域的学术论文。文章主要介绍了一个名为PowerInfer-2的框架，该框架旨在提高智能手机上大型语言模型（LLMs）的推理速度，尤其是对于那些大小超过设备内存容量的模型。\n\nPowerInfer-2的关键创新点在于，它通过将传统的矩阵计算分解为更细粒度的神经元集群计算，有效地利用了智能手机中的异构计算、内存和I/O资源。该框架特别引入了一个多态神经元引擎，能够为LLM推理的不同阶段调整计算策略。此外，它还采用了分段神经元缓存和细粒度神经元集群级别的流水线技术，有效减少并隐藏了I/O操作带来的开销。\n\nPowerInfer-2的实现和评估表明，它能够支持多种LLM模型在两款智能手机上的运行，与最先进的框架相比，速度提高了高达29.2倍。值得注意的是，PowerInfer-2是首个在智能手机上以每秒11.68个令牌的速度运行TurboSparse-Mixtral-47B模型的系统。对于完全适合内存的模型，PowerInfer-2可以在保持与llama.cpp和MLC-LLM相当推理速度的同时，减少约40%的内存使用。\n\n更多详情，包括演示视频，可以在项目网站www.powerinfer.ai/v2上找到。\n\n搜索结果来自：\n[2406.06282] PowerInfer-2: Fast Large Language Model Inference on a Smartphone - https://arxiv.org/abs/2406.06282\n[2406.06282] PowerInfer-2: Fast Large Language Model Inference on a Smartphone - http://export.arxiv.org/abs/2406.06282"
    },
    {
        "url": "https://arxiv.org/abs/2406.08478",
        "content": "这篇文章的标题是《What If We Recaption Billions of Web Images with LLaMA-3?》，主要探讨了使用LLaMA-3模型重新标记数十亿网络图像的可能性。研究指出，网络爬取的图像-文本对本质上是带有噪声的。以前的研究表明，对这些对的文本描述进行语义对齐和丰富可以显著提升模型在多种视觉-语言任务上的训练效果，特别是在文本到图像生成方面。然而，这一领域的大规模研究仍然主要是闭源的。该论文旨在通过利用强大且开源的LLaMA-3模型（一种GPT-4级别的语言模型）来桥接这一社区努力。他们的重新标记管道很简单：首先，他们对一个由LLaMA-3-8B驱动的LLaVA-1.5进行微调，然后使用它来重新标记DataComp-1B数据集中的13亿图像。他们的实证结果显示，这个增强的数据集Recap-DataComp-1B在训练先进的视觉-语言模型方面提供了实质性的好处。例如，对于像CLIP这样的判别模型，他们在跨模态检索任务中观察到了零样本性能的提升。对于像文本到图像的扩散变换器这样的生成模型，生成的图像在符合用户的文本指令方面有了显著的改进，特别是在遵循复杂查询方面。\n\n搜索结果来自：\n[2406.08478] What If We Recaption Billions of Web Images with LLaMA-3? - https://arxiv.org/abs/2406.08478"
    },
    {
        "url": "https://arxiv.org/abs/2406.04338",
        "content": "文章《Physics3D: Learning Physical Properties of 3D Gaussians via Video Diffusion》提出了一种新的方法，用于通过视频扩散模型学习三维物体的各种物理属性。这个方法涉及设计一个基于粘弹性材料模型的高度通用物理模拟系统，能够模拟广泛材料的高保真能力。此外，文章还从包含更多现实对象材料理解的视频扩散模型中提取物理先验。通过大量实验，证明了这种方法在弹性和塑性材料上的有效性。Physics3D显示了在物理世界和虚拟神经空间之间架起桥梁的巨大潜力，为虚拟环境中真实物理原理的更好整合和应用提供了可能。\n\n搜索结果来自：\n[2406.04338] Physics3D: Learning Physical Properties of 3D Gaussians via Video Diffusion - https://arxiv.org/abs/2406.04338\n[2406.04338] Physics3D: Learning Physical Properties of 3D Gaussians via Video Diffusion - http://export.arxiv.org/abs/2406.04338"
    },
    {
        "url": "https://arxiv.org/abs/2406.04127",
        "content": "文章《Are We Done with MMLU?》主要分析了广泛使用的Massive Multitask Language Understanding (MMLU)基准测试中的错误。研究表明，尽管MMLU被广泛采用，但其中存在大量的事实错误，这些错误掩盖了LLMs（大型语言模型）的真实能力。例如，研究发现，在病毒学部分的分析中，有57%的问题包含错误。为了解决这个问题，研究者引入了一个全面的框架，使用一种新颖的错误分类法来识别数据集中的错误。然后，他们创建了MMLU-Redux，这是一个包含30个MMLU主题的3,000个手动重新注释的问题的子集。通过使用MMLU-Redux，研究者展示了与原始报告的模型性能指标存在显著差异。这些结果强烈建议修订MMLU中充满错误的问题，以增强其作为基准的未来实用性和可靠性。因此，研究者将MMLU-Redux开放供进一步注释。\n\n搜索结果来自：\n[2406.04127] Are We Done with MMLU? - https://arxiv.org/abs/2406.04127\n[2406.04127] Are We Done with MMLU? - http://export.arxiv.org/abs/2406.04127"
    },
    {
        "url": "https://arxiv.org/abs/2406.07476",
        "content": "无法访问您提供的文章链接 \"https://arxiv.org/abs/2406.07476\"。由于我无法直接浏览网页内容，我无法提供这篇文章的具体内容。建议您直接访问该链接以获取详细信息。如果您有关于这篇文章的特定问题或需要其他帮助，请告诉我，我会尽力协助您。"
    },
    {
        "url": "https://arxiv.org/abs/2406.08464",
        "content": "这篇文章标题为“Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing”，主要探讨了如何通过提示已经对齐的大型语言模型（LLMs）来合成高质量的指令数据。作者提出了一种名为Magpie的自我合成方法，用于生成大规模的对齐数据。此方法的关键观察是，像Llama-3-Instruct这样的对齐LLMs可以在仅输入左侧模板的情况下生成用户查询，利用其自回归特性。作者使用此方法提示Llama-3-Instruct生成了400万条指令及其对应响应，并从中选择了30万条高质量实例。文章还比较了Magpie数据与其他公共指令数据集，展示了在某些任务中，使用Magpie微调的模型性能与官方Llama-3-8B-Instruct相当。这一发现表明，Magpie单独用于监督式微调（SFT）可以超越先前用于SFT和偏好优化的公共数据集的性能。\n\n搜索结果来自：\n[2406.08464] Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing - https://arxiv.org/abs/2406.08464"
    },
    {
        "url": "https://arxiv.org/abs/2406.05955",
        "content": "这篇文章的标题是《Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters》，由Yixin Song和其他六位作者共同撰写。文章发表于2024年6月10日，并在6月11日进行了修订。\n\n文章的主要内容是关于如何通过利用激活稀疏性来显著加速大型语言模型（LLMs）的推理过程，同时不牺牲性能。激活稀疏性由激活函数决定，但常用的激活函数如SwiGLU和GeGLU展现出的稀疏性有限。简单地用ReLU替换这些函数无法实现足够的稀疏性。此外，训练数据不足可能会进一步增加性能下降的风险。\n\n为了解决这些挑战，作者们提出了一个新的dReLU函数，旨在改善LLM的激活稀疏性，并配合高质量的训练数据混合比率以实现有效的稀疏化。此外，他们还利用了混合专家（MoE）模型中前馈网络（FFN）专家内的稀疏激活模式来进一步提高效率。通过将他们的神经元稀疏化方法应用于Mistral和Mixtral模型，每次推理迭代仅激活2.5亿和4.3亿个参数，同时实现了更强大的模型性能。评估结果表明，这种稀疏性实现了2-5倍的解码速度提升。值得注意的是，在手机上，他们的TurboSparse-Mixtral-47B实现了每秒11个令牌的推理速度。\n\n搜索结果来自：\n[2406.05955] Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters - https://arxiv.org/abs/2406.05955\n[2406.05955] Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters - http://export.arxiv.org/abs/2406.05955"
    },
    {
        "url": "https://arxiv.org/abs/2406.05132",
        "content": "这篇文章标题为“3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination”，发表于2024年6月，属于计算机视觉和模式识别、人工智能、计算和语言、机器学习以及机器人学领域。主要研究内容是关于3D大语言模型（3D-LLMs）的发展。\n\n文章的核心贡献是介绍了3D-GRAND，这是一个开创性的大规模数据集，包含40,087个家庭场景，与6.2百万个密集地与场景语言指令相关联。该研究显示，使用3D-GRAND进行指令调整，可以显著提高3D-LLMs的定位能力和减少幻觉现象。此外，文章还提出了一个全面的基准测试3D-POPE，用于系统地评估3D-LLMs中的幻觉现象，使得未来模型之间可以进行公平比较。\n\n研究强调了数据集大小与3D-LLM性能之间的比例效应，突显了大规模3D文本数据集在推动具身人工智能研究中的关键作用。值得注意的是，研究结果表明，在大型合成数据上训练的模型可以在真实世界的3D扫描上表现良好，显示出有效的模拟到真实世界转移的早期信号。通过3D-GRAND和3D-POPE，作者旨在为具身AI社区提供必要的资源和洞察力，为更可靠、更扎实的3D-LLMs奠定基础。\n\n更多详细信息，您可以访问文章的摘要页面。\n\n搜索结果来自：\n[2406.05132] 3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination - https://arxiv.org/abs/2406.05132\n[2406.05132] 3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination - http://export.arxiv.org/abs/2406.05132"
    },
    {
        "url": "https://arxiv.org/abs/2406.08407",
        "content": "无法访问您提供的文章链接 \"https://arxiv.org/abs/2406.08407\"。因此，我无法提供关于这篇文章的具体内容。您可以尝试直接访问该链接以获取更多信息。如果您有其他问题或需要帮助，请随时告诉我。"
    },
    {
        "url": "https://arxiv.org/abs/2406.08392",
        "content": "这篇文章的标题是《FontStudio: Shape-Adaptive Diffusion Model for Coherent and Consistent Font Effect Generation》，发表于2024年6月12日，属于计算机视觉和模式识别领域。文章的主要研究内容是关于一种新型的形状自适应扩散模型，用于生成多语言字体的文本效果。\n\n这项研究不同于以往主要集中在艺术字体生成的研究，而是致力于解决一个更具挑战性的问题：在字体形状的画布内生成连贯且一致的视觉内容，而不是传统的矩形画布。为了实现这一目标，研究人员引入了一种能够解释给定形状并策略性地规划不规则画布内像素分布的形状自适应扩散模型。为了达到这个效果，他们策划了一个高质量的形状自适应图像-文本数据集，并引入了分割掩模作为视觉条件，指导图像生成过程在不规则画布内进行。\n\n此外，为了保持多个字母之间的一致性，研究人员还提出了一种无需训练的形状自适应效果转移方法，用于将生成的参考字母的纹理转移到其他字母上。这种方法的关键在于构建字体效果噪声先验，并在连接的潜在空间中传播字体效果信息。\n\n该FontStudio系统的有效性通过用户偏好研究得到了证实，这些研究表明，与最新的无与伦比商业产品Adobe Firefly相比，用户对FontStudio系统的偏好更为明显，尤其是在美学方面，胜率为78%。\n\n总的来说，这篇文章介绍了一种创新的形状自适应扩散模型，用于生成多语言字体的文本效果，其方法在保持字体形状和整体美学方面表现出色。\n\n搜索结果来自：\n[2406.08392] FontStudio: Shape-Adaptive Diffusion Model for Coherent and Consistent Font Effect Generation - https://arxiv.org/abs/2406.08392\nBytez: FontStudio: Shape-Adaptive Diffusion Model for Coherent and Consistent Font Effect Generation - https://bytez.com/docs/arxiv/2406.08392/paper\n[2406.08392] FontStudio: Shape-Adaptive Diffusion Model for Coherent and Consistent Font Effect Generation - http://export.arxiv.org/abs/2406.08392"
    },
    {
        "url": "https://arxiv.org/abs/2406.07792",
        "content": "这篇文章的标题是《Hierarchical Patch Diffusion Models for High-Resolution Video Generation》，发表于2024年6月，属于计算机视觉和模式识别领域。文章主要探讨了扩散模型在图像和视频合成中的应用，特别关注于将这些模型应用于高分辨率输入的挑战。\n\n研究指出，将扩散模型扩展到高分辨率输入是一个难题，因为这需要将扩散流程重构为多个独立组件，这不仅限制了可扩展性，还复杂化了后续应用。为了解决这些问题，作者提出了两种改进方法：\n\n1. **深度上下文融合**：这是一种架构技术，它以分层方式将上下文信息从低尺度传递到高尺度补丁，以此来强制补丁之间的一致性。\n\n2. **自适应计算**：这种方法通过将更多的网络容量和计算资源分配给粗糙的图像细节，以加速训练和推理过程。\n\n这些改进使得该模型在UCF-101 $256^2$上的类条件视频生成中，实现了新的最先进的FVD分数66.32和Inception分数87.68，比最近的方法提高了100%以上。此外，该模型能够从基础$36\\times 64$低分辨率生成器快速微调，以进行高分辨率$64 \\times 288 \\times 512$的文本到视频合成。据作者所知，这是第一个在如此高分辨率上完全端到端训练的基于扩散的架构。\n\n更多详细信息，您可以访问文章的网页：[Hierarchical Patch Diffusion Models for High-Resolution Video Generation](https://snap-research.github.io/hpdm)。\n\n搜索结果来自：\n[2406.07792] Hierarchical Patch Diffusion Models for High-Resolution Video Generation - https://arxiv.org/abs/2406.07792\n[2406.07792] Hierarchical Patch Diffusion Models for High-Resolution Video Generation - http://export.arxiv.org/abs/2406.07792\nHierarchical Patch Diffusion Models for High-Resolution Video Generation - NASA/ADS - https://ui.adsabs.harvard.edu/abs/2024arXiv240607792S/abstract"
    },
    {
        "url": "https://arxiv.org/abs/2406.07686",
        "content": "这篇文章标题为《AV-DiT: Efficient Audio-Visual Diffusion Transformer for Joint Audio and Video Generation》，主要探讨了近期发展起来的扩散变换器（Diffusion Transformers, DiTs）在生成高质量单模态内容（包括图像、视频和音频）方面的能力。然而，这种基于变换器的扩散器是否能够有效去噪高斯噪声，以实现卓越的多模态内容创作，尚未得到充分探索。\n\n为了填补这一空白，文章引入了一种名为AV-DiT的新型高效音视频扩散变换器，旨在生成具有视觉和音频轨道的高质量、真实视频。AV-DiT通过使用预训练在仅图像数据上的共享DiT主干，并仅对轻量级、新插入的适配器进行训练，来最小化模型复杂性和计算成本。这种共享主干促进了音频和视频的生成。具体来说，视频分支在预训练的DiT块中引入了可训练的时间注意力层，以实现时间一致性。此外，通过少量可训练参数调整基于图像的DiT块以生成音频。额外的共享DiT块，配备了轻量级参数，促进了音频和视觉模态之间的特征交互，确保了对齐。\n\n在AIST++和Landscape数据集上的广泛实验表明，AV-DiT在联合音视频生成方面实现了最先进的性能，且可调参数数量显著减少。此外，研究结果表明，单个共享图像生成主干，配合模态特定的适配，足以构建联合音视频生成器。该研究的源代码和预训练模型将会发布。\n\n搜索结果来自：\n[2406.07686] AV-DiT: Efficient Audio-Visual Diffusion Transformer for Joint Audio and Video Generation - https://arxiv.org/abs/2406.07686"
    },
    {
        "url": "https://arxiv.org/abs/2406.08414",
        "content": "这篇文章的标题是《Discovering Preference Optimization Algorithms with and for Large Language Models》，由Chris Lu和其他六位作者撰写。文章的主要内容是关于大型语言模型（LLM）的偏好优化算法的发现。文章提出了一种新的方法，用于自动发现和优化LLM输出的质量，而不需要人工干预。这种方法通过迭代提示LLM，基于之前评估的性能指标，提出和实施新的偏好优化损失函数。文章中介绍的最佳算法被称为Discovered Preference Optimization（DiscoPOP），它是一种自适应结合逻辑和指数损失的新算法。实验表明，DiscoPOP在性能上达到了最先进的水平，并成功应用于新的任务。\n\n搜索结果来自：\n[2406.08414] Discovering Preference Optimization Algorithms with and for Large Language Models - https://arxiv.org/abs/2406.08414"
    },
    {
        "url": "https://arxiv.org/abs/2406.06462",
        "content": "这篇文章标题为“VCR: Visual Caption Restoration”，提交于2024年6月10日。文章介绍了一个名为“视觉字幕恢复”(Visual Caption Restoration, VCR)的新颖的视觉-语言任务。这个任务要求模型利用图像中的像素级提示来准确恢复部分遮挡的文本。这一任务源于一个观察：嵌入在图像中的文本与常见的视觉元素和自然语言本质上不同，因为需要将视觉、文本以及嵌入在图像中的文本这三种模态对齐。尽管已有许多工作将图像中的文本整合到视觉问答任务中，但这些方法通常依赖于光学字符识别或遮蔽语言模型，从而将任务主要简化为基于文本的处理。然而，在VCR任务中，基于文本的处理变得无效，因为准确的文本恢复依赖于提供的图像、上下文以及被遮蔽文本微小暴露区域中的微妙提示的综合信息。文章还开发了一个管道，用于生成VCR任务的合成图像，使用图像-字幕对，并具有可调节的字幕可见性来控制任务难度。通过这个管道，作者构建了一个名为VCR-Wiki的数据集，该数据集由维基百科中的图像和字幕组成，包含211万英语和34.6万中文实体，分为简单和困难两种变体。研究结果表明，当前的视觉语言模型在VCR任务中的表现显著落后于人类，仅仅在他们的数据集上微调模型并不能带来显著的改进。作者公开了VCR-Wiki数据集和数据构建代码，以促进未来的研究。\n\n更多详情可以在arXiv网站查看。\n\n搜索结果来自：\n[2406.06462] VCR: Visual Caption Restoration - http://export.arxiv.org/abs/2406.06462"
    },
    {
        "url": "https://arxiv.org/abs/2406.08487",
        "content": "这篇文章的标题是《超越LLaVA-HD：深入高分辨率大型多模态模型》。文章主要探讨了在大型多模态模型（LMMs）中实现高分辨率视觉感知和推理的重要性。现有的研究通常采用直接提高分辨率的办法，但这会导致计算成本高昂，并可能削弱全局上下文的重要性。作者提出了一种新的框架和优化策略，通过从全局视角提取上下文信息和使用混合适配器来提高性能。此外，文章还介绍了一个对图像细节要求较高的挑战性数据集，用于训练局部压缩层。这种方法在各种基准测试中取得了领先性能。\n\n搜索结果来自：\n[2406.08487] Beyond LLaVA-HD: Diving into High-Resolution Large Multimodal Models - https://arxiv.org/abs/2406.08487"
    },
    {
        "url": "https://arxiv.org/abs/2406.07933",
        "content": "这篇文章的标题是《大型语言模型的嵌入破坏提示下的知识遗忘》，主要研究了如何从大型语言模型（LLMs）中准确高效地“遗忘”特定知识。由于大型模型中知识之间的纠缠和遗忘与保留之间的模糊界限，以及针对数百亿参数的最先进模型进行优化的巨大计算需求，这一任务颇具挑战性。文章提出了一种名为“嵌入破坏（ECO）提示”的轻量级遗忘框架，通过使用提示分类器来识别和保护需要遗忘的提示，并在线下学习对提示嵌入的破坏，以达到遗忘目标。实验表明，这种方法不仅能产生满足遗忘目标的理想输出，而且能近似从未在目标数据上训练过的模型的输出。该方法在多种领域上均表现出优异的遗忘效果，且几乎无副作用，可扩展至参数数量从0.5B到236B的100个LLMs，且参数数量增加时不会产生额外成本。\n\n搜索结果来自：\n[2406.07933] Large Language Model Unlearning via Embedding-Corrupted Prompts - https://arxiv.org/abs/2406.07933"
    },
    {
        "url": "https://arxiv.org/abs/2406.04320",
        "content": "文章《Chimera: Effectively Modeling Multivariate Time Series with 2-Dimensional State Space Models》讨论了如何有效地使用二维状态空间模型（SSMs）对多变量时间序列进行建模。传统的一维SSMs在处理单变量时间序列时因其简单性和表现力而受到青睐，但它们在捕捉非线性依赖、处理速度以及模拟变量间信息流动方面存在根本性的限制。尽管最近有研究尝试通过使用深度结构化SSMs来增强其表现力，但这些方法要么仅限于单变量时间序列，要么无法模拟复杂模式（如季节性模式），要么不能动态地模拟变量和时间维度的依赖性，且通常是输入独立的。\n\nChimera提出了一种新方法，使用两个输入依赖的二维SSM头来学习长期进展和季节性模式。为了提高复杂二维递归的效率，文章还提出了一种新的二维并行选择性扫描的快速训练方法。此外，文章还介绍了Mamba和Mamba-2作为其二维SSM的空间案例。实验评估显示，Chimera在广泛的多样性基准测试中表现优异，包括心电图和语音时间序列分类、长期和短期时间序列预测以及时间序列异常检测。\n\n搜索结果来自：\n[2406.04320] Chimera: Effectively Modeling Multivariate Time Series with 2-Dimensional State Space Models - http://export.arxiv.org/abs/2406.04320"
    },
    {
        "url": "https://arxiv.org/abs/2406.05074",
        "content": "这篇文章标题为《Hibou: A Family of Foundational Vision Transformers for Pathology》，发表在电气工程和系统科学领域，专注于图像和视频处理。文章的主要内容是关于病理学领域的视觉转换器的发展。病理学是通过对患病组织进行显微镜检查来诊断各种医疗状况，特别是癌症。传统方法劳动强度大，易受人为错误影响。数字病理学通过将玻璃片转换为高分辨率数字图像供计算机算法分析，通过自动图像分析和大规模数据处理，提高了诊断的准确性、一致性和效率。\n\n文章介绍了一个名为Hibou的视觉转换器家族，该家族利用DINOv2框架在超过一百万张全玻片图像（WSIs）的专有数据集上预训练了两种模型变体，Hibou-B和Hibou-L。这些图像代表了多种组织类型和染色技术。预训练后的模型在片级和切片级基准测试中表现出色，超过了现有的最先进方法。特别是，Hibou-L在多个基准数据集上实现了最高的平均准确率。为了支持该领域进一步的研究和应用，作者开源了Hibou-B模型，该模型可以在GitHub上访问。\n\n搜索结果来自：\n[2406.05074] Hibou: A Family of Foundational Vision Transformers for Pathology - https://arxiv.org/abs/2406.05074"
    },
    {
        "url": "https://arxiv.org/abs/2406.04329",
        "content": "文章《Simplified and Generalized Masked Diffusion for Discrete Data》探讨了一种称为“masked (or absorbing) diffusion”的模型，这是一种用于离散数据生成建模的替代方法，可以替代自回归模型。该研究旨在提供一个简单且通用的框架，以充分实现masked diffusion模型的可能性。文章指出，masked diffusion模型的连续时间变分目标是交叉熵损失的简单加权积分。此外，该框架还允许训练具有状态依赖性掩蔽计划的一般化masked diffusion模型。\n\n在OpenWebText上训练的模型在复杂度方面超越了先前的扩散语言模型，并在5个零样本语言建模任务中的4个表现出卓越的性能。此外，这些模型在像素级图像建模方面也大大超过了之前的离散扩散模型，实现了2.78 (CIFAR-10) 和 3.42 (ImageNet 64×64) 比特每维度的性能，与类似大小的自回归模型相比具有竞争力或更优。\n\n搜索结果来自：\n[2406.04329] Simplified and Generalized Masked Diffusion for Discrete Data - https://arxiv.org/abs/2406.04329\n[2406.04329] Simplified and Generalized Masked Diffusion for Discrete Data - http://export.arxiv.org/abs/2406.04329"
    }
]