[
    {
        "url": "https://arxiv.org/abs/2407.16741",
        "content": "这篇文章介绍了一个名为OpenDevin的平台，这是一个面向AI软件开发者的开放平台。OpenDevin旨在开发强大且灵活的AI代理，这些代理能够以类似于人类开发者方式与世界互动，包括编写代码、与命令行交互和浏览网页。文章描述了该平台如何实现新代理的开发、与沙盒环境的安全交互、多个代理之间的协调以及评估基准的整合。基于当前纳入的基准，作者对代理在15个挑战性任务上的表现进行了评估，包括软件工程（如SWE-Bench）和网页浏览（如WebArena）等。OpenDevin是一个社区项目，涵盖了学术界和工业界，已有超过1.3千次的贡献，来自超过160名贡献者，并将继续改进。\n\n您可以点击[这里](https://arxiv.org/abs/2407.16741)查看完整的文章。"
    },
    {
        "url": "https://arxiv.org/abs/2407.17453",
        "content": "这篇文章标题为 \"$VILA^2$: VILA Augmented VILA\"，主要探讨了视觉语言模型（VLMs）的发展。文章提出了一种新的方法，包括自我增强和专家增强两个步骤，以迭代提高数据质量和模型性能。在自我增强步骤中，VLM重新描述其自身的预训练数据以提升数据质量，然后使用这个改进的数据集从头开始重新训练以提升模型性能。这个过程可以迭代多次。当自我增强达到饱和后，文章使用几个专门领域的VLM，通过任务导向的重新描述和训练，将专业知识进一步融入通用VLM中。通过这种自我增强和专家增强相结合的训练，文章介绍了一个新的VLM家族——$VILA^2$，它在各种任务上持续提高准确性，并在公开来源模型中达到了新的最先进水平。这篇文章发表在计算机视觉和模式识别领域。\n\n如需了解更多详细信息，请访问文章的链接：[arXiv:2407.17453](https://arxiv.org/abs/2407.17453)。"
    },
    {
        "url": "https://arxiv.org/abs/2407.17438",
        "content": "这篇文章标题为“HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation”，主要研究了用于可控制摄像机的人像动画的训练数据。文章提出了一种新的大型高质量数据集HumanVid，专门用于人像动画，结合了实际采集和合成数据。实际数据部分，研究团队从互联网上收集了大量的无版权真实世界视频，并使用基于规则的方法筛选出高质量视频。对于合成数据，他们收集了2300个无版权的3D头像资源以增强现有数据。此外，文章还介绍了一个名为CamAnimate的基准模型，该模型考虑了人和摄像机的运动，展示了在HumanVid上的训练效果。该研究为视频和电影制作中人像动画的控制提供了新的可能性。\n\n更多细节和深入研究，您可以访问文章的链接：[HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation](https://arxiv.org/abs/2407.17438)。"
    },
    {
        "url": "https://arxiv.org/abs/2407.17387",
        "content": "这篇文章标题为“PERSONA: A Reproducible Testbed for Pluralistic Alignment”，主要讨论了语言模型（LMs）与多样化用户价值观的稳健对齐问题。文章指出，当前的首选优化方法往往无法捕捉用户观点的多样性，反而强化了多数观点，边缘化了少数观点。为此，作者们引入了PERSONA，这是一个可复现的测试平台，旨在评估和改进LMs的多元对齐。他们从美国人口普查数据中程序化生成多样化的用户档案，创建了1586个具有不同人口统计和心理特征的合成人物。接着，他们生成了一个大规模的评估数据集，包含3868个提示和317200个反馈对，这些数据来自他们的合成人物。利用这个数据集，他们系统地评估了LMs在扮演多样化用户方面的能力，并通过人类评委进行验证。此外，他们还建立了一个基准，PERSONA Bench，用于评估多元对齐方法，以及一个广泛的 数据集，用于创建新的和未来的基准。完整的 数据集和基准可以在www.synthlabs.ai上找到。"
    },
    {
        "url": "https://arxiv.org/abs/2407.17470",
        "content": "这篇文章标题为“SV4D: Dynamic 3D Content Generation with Multi-Frame and Multi-View Consistency”，主要介绍了一个名为Stable Video 4D（SV4D）的模型。SV4D是一个用于生成动态3D内容的潜在视频扩散模型，它能够保持多帧和多视角的一致性。与以往依赖单独训练的视频生成和新视角合成的生成模型不同，SV4D设计了一个统一的扩散模型来生成动态3D对象的全新视角视频。具体来说，给定一个单目参考视频，SV4D能够为每个视频帧生成时间上连贯的新视角。然后，使用这些生成的新视角视频来高效优化一个隐式的4D表示（动态NeRF），而无需使用大多数先前工作中使用的繁琐的SDS优化。为了训练这个统一的新视角视频生成模型，研究人员从现有的Objaverse数据集中策划了一个动态3D对象数据集。在多个数据集上的广泛实验结果和用户研究证明了SV4D在全新视角视频合成以及4D生成方面相对于先前工作的先进性能。"
    },
    {
        "url": "https://arxiv.org/abs/2407.17353",
        "content": "这篇文章标题为“Scalify: scale propagation for efficient low-precision LLM training”，主要讨论了在大型语言模型（LLM）训练中，使用低精度格式（如float8）来提高计算效率的方法。文章提出了一种名为“Scalify”的端到端尺度传播范式，用于计算图，这概括并形式化了现有的张量缩放方法。实验结果表明，Scalify支持开箱即用的float8矩阵乘法和梯度表示，以及float16优化器状态存储。作者还提供了JAX实现的Scalify，并在GitHub上开源。这篇文章是ICML 2024 WANT工作坊的投稿。\n\n更多详情，请查看原文：[Scalify: scale propagation for efficient low-precision LLM training](https://arxiv.org/abs/2407.17353)。"
    },
    {
        "url": "https://arxiv.org/abs/2407.16154",
        "content": "这篇文章标题为《DDK: Distilling Domain Knowledge for Efficient Large Language Models》，由Jiaheng Liu等作者撰写。文章主要讨论了大型语言模型（LLMs）在应用中的智能能力虽然先进，但仍然面临计算和存储方面的重大需求。文章提出了一种名为DDK的新LLM蒸馏框架，该框架根据教师和学生模型之间的领域性能差异，动态调整蒸馏数据集的组成，使得蒸馏过程更加稳定和有效。通过大量评估，DDK显著提高了学生模型的性能，优于持续预训练的基线和现有的知识蒸馏方法。这篇文章属于计算与语言（cs.CL）领域。\n\n您可以点击此链接查看完整文章：[DDK: Distilling Domain Knowledge for Efficient Large Language Models](https://arxiv.org/abs/2407.16154)。"
    },
    {
        "url": "https://arxiv.org/abs/2407.17365",
        "content": "这篇文章标题为《ViPer: Visual Personalization of Generative Models via Individual Preference Learning》，发表在计算机视觉和模式识别领域。文章主要探讨了如何根据个人偏好来个性化生成模型。\n\n文章的摘要提到，不同的用户对于同一提示生成的图像有不同的偏好。因此，文章提出了一个个性化的图像生成方法，通过一次性的过程捕获用户的通用偏好。具体来说，邀请用户评论一组图像，解释他们喜欢或不喜欢的原因。然后，利用大型语言模型根据这些评论推断出用户喜欢的视觉属性，即他们的视觉偏好。这些属性随后用于指导文本到图像模型，使其生成符合个人用户视觉偏好的图像。\n\n文章通过一系列用户研究和大型语言模型引导的评估来展示这种方法能够生成与个人用户视觉偏好高度一致的图像。"
    },
    {
        "url": "https://arxiv.org/abs/2407.16312",
        "content": "很抱歉，我无法直接打开并查看您提供的链接内容。不过，我可以尝试通过其他方式来查找关于这篇文章的信息。您能提供这篇文章的标题或主要作者吗？这样我可以更准确地搜索相关信息。"
    },
    {
        "url": "https://arxiv.org/abs/2407.14207",
        "content": "这篇文章标题为《Longhorn: State Space Models are Amortized Online Learners》，主要探讨了现代人工智能方法，尤其是大型语言模型（LLMs）在序列建模方面的能力。文章指出，虽然变换器模型（Transformers）目前在序列建模中占据主导地位，但其与序列长度成二次方的计算成本是一个重大缺点。状态空间模型（SSMs）因其线性的解码效率和训练期间的高度并行化，提供了一个有前景的替代方案。文章通过在线学习的视角探索SSM设计，将SSMs概念化为特定在线学习问题的元模块，并基于此引入了一种新的深度SSM架构，用于优化在线回归目标。实验结果表明，该模型在标准序列建模基准和语言建模任务上优于最新的SSMs，包括Mamba模型。这篇文章的主题涉及机器学习，由Bo Liu、Rui Wang、Lemeng Wu、Yihao Feng、Peter Stone和Qiang Liu共同撰写。\n\n您可以通过以下链接查看完整文章：[Longhorn: State Space Models are Amortized Online Learners](https://arxiv.org/abs/2407.14207)。"
    },
    {
        "url": "https://arxiv.org/abs/2407.15815",
        "content": "这篇文章标题为《Learning to Manipulate Anywhere: A Visual Generalizable Framework For Reinforcement Learning》，由Zhecheng Yuan等人撰写。文章主要探讨了如何赋予视觉运动机器人以泛化能力，以在多样化的开放世界场景中操作。作者提出了一个名为Maniwhere的通用框架，专门用于视觉强化学习，使训练出的机器人策略能够泛化到多种视觉干扰类型组合中。文章还介绍了一种结合空间变换网络（STN）模块的多视图表示学习方法，以捕捉不同视角之间的共享语义信息和对应关系。此外，文章还采用了基于课程的随机化和增强方法来稳定强化学习训练过程，并增强视觉泛化能力。文章通过精心设计的8个任务，包括关节物体、双手和灵巧手操作任务，展示了Maniwhere在3个硬件平台上的强大视觉泛化能力和模拟到真实世界的转移能力。实验结果表明，Maniwhere显著优于现有的最先进方法。\n\n更多详情请查看原文：[arXiv:2407.15815](https://arxiv.org/abs/2407.15815)。"
    },
    {
        "url": "https://arxiv.org/abs/2407.16988",
        "content": "这篇文章标题为《DreamCar: Leveraging Car-specific Prior for in-the-wild 3D Car Reconstruction》，主要研究的是自动驾驶行业中3D汽车模型的重建问题。作者提出了一种新方法，名为DreamCar，能够从少量甚至单张汽车图片中重建高质量的3D汽车模型。为了改进生成模型，作者收集了一个包含5600多种车辆的汽车数据集Car360。此外，文章还提出了一种姿势优化方法，以解决在处理野外图像时由于相机姿态估计误差大导致的纹理错位问题。该方法在重建高质量3D汽车方面显著优于现有技术。更多细节和深入内容，请查看原文：[DreamCar: Leveraging Car-specific Prior for in-the-wild 3D Car Reconstruction](https://arxiv.org/abs/2407.16988)。"
    }
]