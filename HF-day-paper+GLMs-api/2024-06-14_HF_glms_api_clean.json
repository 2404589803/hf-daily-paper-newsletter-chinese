[
    {
        "url": "https://arxiv.org/abs/2406.09414",
        "content": "无法访问您提供的文章链接 \"https://arxiv.org/abs/2406.09414\"。因此，我无法提供关于这篇文章的具体内容。如果您能提供文章的标题或摘要，我可以尝试通过其他方式帮助您。"
    },
    {
        "url": "https://arxiv.org/abs/2406.09415",
        "content": "这篇文章的标题是《An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels》，主要探讨了一个有趣的现象，即标准的Transformer模型可以直接将每个像素视为一个标记（token）来操作，并在计算机视觉的三个任务中表现出色。这三个任务包括：监督学习中的对象分类、通过掩蔽自编码进行自监督学习，以及使用扩散模型进行图像生成。这一发现挑战了现代计算机视觉架构中局部性的归纳偏置（inductive bias）的必要性。尽管直接对每个像素进行操作在计算上不太实用，但作者认为，在开发下一代计算机视觉神经架构时，社区应当意识到这一惊人的知识。\n\n搜索结果来自：\n[2406.09415] An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels - https://arxiv.org/abs/2406.09415"
    },
    {
        "url": "https://arxiv.org/abs/2406.09416",
        "content": "无法访问您提供的文章链接 \"https://arxiv.org/abs/2406.09416\"。因此，我无法提供关于这篇文章的具体内容。如果您能提供文章的标题、作者或摘要等信息，我可以尝试通过这些信息来帮助您。"
    },
    {
        "url": "https://arxiv.org/abs/2406.09413",
        "content": "这篇文章的标题是《Interpreting the Weight Space of Customized Diffusion Models》，由Amil Dravid和其他六位作者撰写。文章的主要内容是对定制的扩散模型的权重空间进行了研究。研究团队创建了一个包含超过60,000个模型的数据库，每个模型都是基于基础模型微调的，以插入不同人的视觉身份。文章将这些权重的底层流形建模为一个子空间，称为weights2weights。文章展示了这个空间的三个直接应用：采样、编辑和反转。例如，空间中的每个点都对应一个身份，从中采样一组权重可以生成一个编码新身份的模型。此外，文章还展示了在这个空间中找到的线性方向，可以对身份进行语义编辑（比如添加胡须），这些编辑在生成的样本中保持外观一致。最后，文章还表明，将单张图像反转到这个空间中可以重建一个真实的身份，即使输入图像是分布之外的（例如一幅画）。这些结果表明，微调后的扩散模型的权重空间表现为一个可解释的身份潜在空间。\n\n搜索结果来自：\n[2406.09413] Interpreting the Weight Space of Customized Diffusion Models - https://arxiv.org/abs/2406.09413"
    },
    {
        "url": "https://arxiv.org/abs/2406.09246",
        "content": "无法访问您提供的文章链接 \"https://arxiv.org/abs/2406.09246\"。因此，我无法提供关于这篇文章的具体内容。如果您能提供文章的标题或摘要，我可以尝试通过其他方式帮助您获取相关信息。"
    },
    {
        "url": "https://arxiv.org/abs/2406.09170",
        "content": "文章《Test of Time: A Benchmark for Evaluating LLMs on Temporal Reasoning》主要探讨了大型语言模型（LLMs）在时间推理任务中的表现。尽管LLMs展现了出色的推理能力，但在涉及复杂时间逻辑的推理任务中，它们仍然容易出错。现有研究已经探索了LLMs在时间推理上的性能，但这些研究往往依赖于LLMs可能在预训练期间遇到的真实世界数据，或者使用可能无意中引入事实不一致的匿名化技术。\n\n为了解决这些局限性，该研究引入了新的合成数据集，专门设计用于在各种场景中评估LLMs的时间推理能力。这些数据集的问题类型多样性使得可以系统地研究问题结构、大小、问题类型、事实顺序等因素对LLMs性能的影响。该研究的结果为当前LLMs在时间推理任务中的优势和弱点提供了宝贵的见解。为了促进这一领域的进一步研究，研究团队正在开源他们在实验中使用的数据集和评估框架。\n\n搜索结果来自：\n[2406.09170] Test of Time: A Benchmark for Evaluating LLMs on Temporal Reasoning - http://export.arxiv.org/abs/2406.09170"
    },
    {
        "url": "https://arxiv.org/abs/2406.08552",
        "content": "这篇文章的标题是《DiTFastAttn: Attention Compression for Diffusion Transformer Models》，作者是 Zhihang Yuan 等人。文章主要讨论了扩散变换器模型（Diffusion Transformers, DiT）在图像和视频生成方面的优势，以及由于自注意力的二次复杂度所面临的计算挑战。为了缓解这一计算瓶颈，作者提出了 DiTFastAttn，这是一种新颖的后期训练压缩方法。文章识别了在 DiT 推理过程中注意力计算的三个关键冗余：空间冗余、时间冗余和条件冗余，并针对这些冗余提出了相应的技术。通过在图像和视频生成任务上应用 DiTFastAttn，结果显示，对于图像生成，该方法最多减少了 88% 的浮点运算次数，并在高分辨率生成上实现了高达 1.6 倍的速度提升。\n\n搜索结果来自：\n[2406.08552] DiTFastAttn: Attention Compression for Diffusion Transformer Models - https://arxiv.org/abs/2406.08552"
    },
    {
        "url": "https://arxiv.org/abs/2406.09411",
        "content": "这篇文章介绍了一个名为MuirBench的综合基准测试，旨在评估多模态大型语言模型（LLMs）在稳健的多图像理解能力方面的表现。MuirBench包含12个多样化的多图像任务，涉及10类多图像关系，如多视角和时序关系。该测试包含11,264张图片和2,600个多项选择题，每个标准实例都与一个语义差异最小的不可回答变体配对，以进行可靠的评估。研究结果显示，即使是表现最好的模型，如GPT-4o和Gemini Pro，在MuirBench上的准确率也分别只有68.0%和49.3%。这表明，MuirBench在鼓励社区开发能够超越单一图像的多模态LLMs方面具有重要意义，并为未来的改进指出了可能的途径。\n\n搜索结果来自：\n[2406.09411] MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding - https://arxiv.org/abs/2406.09411"
    },
    {
        "url": "https://arxiv.org/abs/2406.07522",
        "content": "这篇文章的标题是“Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling”，提交于2024年6月11日。文章主要探讨了如何高效地建模具有无限上下文长度的序列，这是一个长期存在的问题。以往的工作要么受到二次计算复杂度的困扰，要么在长度泛化上的外推能力有限。\n\n在这项工作中，作者们提出了一个名为Samba的简单混合架构，该架构逐层结合了Mamba（一种选择性状态空间模型）和滑动窗口注意力（SWA）。Samba能够选择性地将给定序列压缩为递归隐藏状态，同时仍然保持使用注意力机制精确回忆记忆的能力。作者们将Samba扩展到38亿参数，训练了32万亿个标记，并展示了Samba在广泛的基准测试中显著优于基于纯注意力或SSM的最先进模型。当在4K长度的序列上训练时，Samba可以有效地外推到256K的上下文长度，并实现完美的记忆回忆，在1M的上下文长度上显示出改进的标记预测。作为一个线性时间序列模型，Samba在处理128K长度的用户提示时，比具有分组查询注意力的Transformers具有3.73倍的高吞吐量，在生成64K标记的无限制流媒体时，速度提高了3.64倍。Samba的一个示例实现已在GitHub上公开提供。\n\n搜索结果来自：\n[2406.07522] Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling - https://arxiv.org/abs/2406.07522\n[2406.07522] Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling - http://export.arxiv.org/abs/2406.07522"
    },
    {
        "url": "https://arxiv.org/abs/2406.08587",
        "content": "这篇文章标题为“CS-Bench: A Comprehensive Benchmark for Large Language Models towards Computer Science Mastery”，由Xiaoshuai Song等作者撰写。文章主要介绍了CS-Bench，这是一个针对大型语言模型（LLMs）在计算机科学领域性能评估的综合性基准。CS-Bench包括大约5,000个精心挑选的测试样本，涵盖计算机科学的26个子领域，包括四个关键领域。通过使用CS-Bench，作者对超过30个主流LLMs进行了全面评估，揭示了模型规模与计算机科学性能之间的关系。此外，文章还定量分析了现有LLMs失败的原因，并指出了改进的方向，包括知识补充和特定于计算机科学的推理。文章还进行了跨能力实验，显示LLMs在计算机科学方面的能力与它们在数学和编码方面的能力高度相关。作者期望CS-Bench成为LLMs在计算机科学领域应用的基石，并为评估LLMs的多样化推理能力开辟新的途径。\n\n搜索结果来自：\n[2406.08587] CS-Bench: A Comprehensive Benchmark for Large Language Models towards Computer Science Mastery - https://arxiv.org/abs/2406.08587"
    },
    {
        "url": "https://arxiv.org/abs/2406.09308",
        "content": "这篇文章的标题是《Transformers meet Neural Algorithmic Reasoners》，由Wilfried Bounsi和其他七位作者共同撰写。文章发表于2024年6月13日，将在CVPR 2024 Multimodal Algorithmic Reasoning (MAR) Workshop上展示。文章的主要内容包括：\n\n1. **背景与问题**：Transformers在机器学习领域因其简单而有效的架构而引发了一场革命。通过在互联网上的大规模文本数据集上进行预训练，Transformers在自然语言理解（NLU）任务中展现出了无与伦比的泛化能力。然而，当面对算法形式的推理任务时，这些语言模型仍然显得脆弱，因为在这种情况下，计算需要精确和健壮。\n\n2. **方法与贡献**：为了解决这个局限性，文章提出了一种新颖的方法，将Transformer的语言理解能力与基于图神经网络（GNN）的神经算法推理器（NARs）的健壮性结合起来。NARs作为算法任务的通用求解器，在以图形式指定时证明是有效的。为了使NAR的嵌入可以被Transformer访问，文章提出了一种混合架构，并采用两阶段训练程序，允许语言模型中的令牌跨关注NAR的节点嵌入。\n\n3. **实验与结果**：作者在CLRS-Text上评估了他们的TransNAR模型，这是CLRS-30基准的文本版本。实验结果表明，与仅使用Transformer的模型相比，TransNAR在算法推理任务中取得了显著的增益，无论是在分布内还是分布外。\n\n4. **领域与应用**：这项工作对计算与语言（cs.CL）和机器学习（cs.LG）领域做出了贡献，展示了如何通过结合不同类型的神经网络来提升模型的推理能力。\n\n这篇文章提供了一种新的思路，通过结合Transformers和神经算法推理器，来提升机器学习模型在复杂推理任务上的表现。\n\n搜索结果来自：\n[2406.09308] Transformers meet Neural Algorithmic Reasoners - https://arxiv.org/abs/2406.09308"
    },
    {
        "url": "https://arxiv.org/abs/2406.08862",
        "content": "这篇文章标题为《Cognitively Inspired Energy-Based World Models》，由Alexi Gladstone等作者撰写。文章主要讨论了一种新型的世界模型训练方法，即基于能量的世界模型（Energy-Based World Models, EBWM）。这种方法受到人类认知的启发，与传统的自回归预测模型不同。EBWM通过训练一个能量模型（Energy-Based Model, EBM）来预测给定上下文和预测的未来状态之间的兼容性。这种方法使模型能够实现人类认知的三个特点：主动影响内部认知过程、自然评估未来状态的合理性以及动态分配预测时间。文章还介绍了一种专为能量模型设计的自回归变换器变体，称为能量变换器（Energy-Based Transformer, EBT）。实验结果表明，与传统的自回归变换器相比，EBWM在计算机视觉和自然语言处理领域具有更好的数据扩展性和GPU小时利用率。这一方法为训练具有系统2思维和智能搜索状态空间能力的未来模型提供了新的路径。\n\n搜索结果来自：\n[2406.08862] Cognitively Inspired Energy-Based World Models - https://arxiv.org/abs/2406.08862"
    },
    {
        "url": "https://arxiv.org/abs/2406.09162",
        "content": "这篇文章标题为“EMMA: Your Text-to-Image Diffusion Model Can Secretly Accept Multi-Modal Prompts”，提交于2024年6月13日。文章主要讨论了在图像生成领域的最新进展，特别是在处理多模态条件下的图像生成问题。现有的方法在平衡多种条件时往往表现出对某一模态的偏好。为了解决这个挑战，作者们引入了EMMA，这是一个新型的图像生成模型，它基于最先进的文本到图像（T2I）扩散模型ELLA构建，能够接受多模态提示。\n\nEMMA通过一种创新的多模态特征连接器设计，有效地整合文本和补充模态信息，使用一种特殊的注意力机制。这种方法冻结了原始T2I扩散模型中的所有参数，只调整一些额外的层。研究揭示了一个有趣的发现，即预训练的T2I扩散模型可以秘密地接受多模态提示。这一特性使得EMMA能够轻松适应不同的现有框架，成为一个灵活有效的工具，用于生成个性化和上下文感知的图像甚至视频。\n\n此外，文章还介绍了一种策略，用于组装学到的EMMA模块，以同时条件于多个模态生成图像，无需使用混合多模态提示进行额外的训练。广泛的实验证明了EMMA在保持生成图像的高保真度和细节方面的有效性，展示了其作为高级多模态条件图像生成任务的强大解决方案的潜力。\n\n搜索结果来自：\n[2406.09162] EMMA: Your Text-to-Image Diffusion Model Can Secretly Accept Multi-Modal Prompts - https://arxiv.org/abs/2406.09162\n[2406.09162] EMMA: Your Text-to-Image Diffusion Model Can Secretly Accept Multi-Modal Prompts - http://export.arxiv.org/abs/2406.09162"
    },
    {
        "url": "https://arxiv.org/abs/2406.08656",
        "content": "无法访问您提供的文章链接 \"https://arxiv.org/abs/2406.08656\"。由于我无法直接浏览网页内容，我无法提供这篇文章的具体内容。您可以尝试访问该链接以获取更多信息，或者提供文章的标题、作者、摘要等详细信息，这样我可能能提供更多帮助。"
    },
    {
        "url": "https://arxiv.org/abs/2406.08707",
        "content": "这篇文章标题为“mOSCAR: A Large-scale Multilingual and Multimodal Document-level Corpus”，主要介绍了一个名为mOSCAR的大型多语言和多模态文档级语料库。这个语料库是从网络上爬取的，包含了163种语言、3.15亿个文档、2140亿个令牌和12亿张图像。文章的作者们通过一系列的过滤和评估步骤，确保mOSCAR语料库的安全性、多样性和高质量。此外，他们还训练了两种类型的多语言模型来证明mOSCAR的好处：一种是在mOSCAR的一个子集和字幕数据上训练的模型，另一种是仅在字幕数据上训练的模型。结果显示，在mOSCAR上额外训练的模型在多种多语言图像-文本任务和基准测试中展现出了显著的少样本学习性能提升，这证实了之前仅针对英语的多模态大型语言模型（mLLMs）的发现。\n\n搜索结果来自：\n[2406.08707] mOSCAR: A Large-scale Multilingual and Multimodal Document-level Corpus - https://arxiv.org/abs/2406.08707"
    },
    {
        "url": "https://arxiv.org/abs/2406.09412",
        "content": "这篇文章标题为《探索大规模跨模态预训练的极限》，主要提出了构建能够理解任何模态并学习通用表示的跨模态智能。文章中提出了一种可扩展的预训练范式，称为多模态上下文（MiCo），它可以在预训练过程中增加模态数量、数据量和模型参数。通过MiCo预训练的模型在多模态学习方面表现出了显著的涌现能力，这些能力在10种不同模态的单模态感知基准、25个跨模态理解任务（包括检索、问答、字幕）以及18个多模态大型语言模型基准上进行了评估。该研究希望对跨模态智能的发展做出贡献。\n\n搜索结果来自：\n[2406.09412] Explore the Limits of Omni-modal Pretraining at Scale - https://arxiv.org/abs/2406.09412"
    },
    {
        "url": "https://arxiv.org/abs/2406.08657",
        "content": "无法访问您提供的文章链接 \"https://arxiv.org/abs/2406.08657\"。因此，我无法提供关于这篇文章的具体内容。如果您能提供文章的标题或摘要，我可以尝试通过这些信息来帮助您。"
    },
    {
        "url": "https://arxiv.org/abs/2406.08598",
        "content": "很抱歉，我无法直接访问或检索文章的具体内容。您可以直接访问提供的URL链接，以获取该文章的详细信息。如果您有关于文章主题或概念的具体问题，我会尽力根据我的现有知识库为您提供帮助。"
    },
    {
        "url": "https://arxiv.org/abs/2406.08479",
        "content": "这篇文章标题为《Real3D: Scaling Up Large Reconstruction Models with Real-World Images》，由Hanwen Jiang等作者撰写，提交于2024年6月12日。文章主要探讨了如何利用真实世界图像训练单视角大型重建模型（LRMs）。传统训练策略依赖于使用合成3D资产或多视角捕捉的大型数据集进行全监督学习。然而，这些资源难以扩展且不一定代表真实物体形状的分布。为此，文章提出了Real3D，这是首个可以使用单视角真实世界图像训练的LRM系统。Real3D引入了一种新颖的自训练框架，能够利用现有合成数据和多样的单视角真实图像。文章还提出了两种无监督损失函数，即使在缺乏真实3D或新视角的训练样本中，也能在像素和语义层面上监督LRMs。此外，为了提高性能和扩展图像数据，研究团队开发了一种自动数据筛选方法，从野生图像中收集高质量示例。实验表明，Real3D在包括真实和合成数据以及域内和域外形状的四种不同评估设置中，均优于先前的工作。\n\n搜索结果来自：\n[2406.08479] Real3D: Scaling Up Large Reconstruction Models with Real-World Images - https://arxiv.org/abs/2406.08479"
    },
    {
        "url": "https://arxiv.org/abs/2406.09305",
        "content": "很抱歉，我无法直接访问您提供的链接内容。然而，我可以根据文章的标题和摘要提供一些信息。\n\n文章标题为 \"On the Use of Curriculum Learning in Few-Shot Classification\"，这表明文章讨论的是在少样本分类中应用课程学习（Curriculum Learning）的方法。课程学习是一种机器学习策略，它模仿人类学习的顺序性，通过先学习简单的任务再逐步过渡到更复杂的任务，以提高学习效率和效果。\n\n在少样本分类的背景下，这意味着文章可能探讨了如何通过课程学习来提高模型在只有少量样本可用的条件下的分类性能。这可能包括如何选择和排序训练样本，以及如何设计学习过程，以便模型能够从这些少量的样本中更有效地学习。\n\n如果您能提供更多关于文章的具体信息，或者有关于文章内容的具体问题，我会尽力帮助您进一步理解文章的内容。"
    },
    {
        "url": "https://arxiv.org/abs/2406.07546",
        "content": "这篇文章的标题是《Commonsense-T2I Challenge: Can Text-to-Image Generation Models Understand Commonsense?》，提交于2024年6月11日。文章的主要内容是提出了一个新颖的任务和基准，用于评估文本到图像（T2I）生成模型在现实生活中生成符合常理的图像的能力，这个基准被称为Commonsense-T2I。\n\n文章的实验部分包括给模型提供两个对抗性的文本提示，这些提示包含相同的行为词但存在细微差异，例如“没有电的灯泡”和“有电的灯泡”。然后评估T2I模型是否能进行视觉-常识推理，例如生成与“灯泡未亮”和“灯泡亮了”相对应的图像。Commonsense-T2I提出了一种对抗性挑战，提供了成对的文本提示以及预期的输出。数据集由专家精心手工整理，并标注了详细的标签，如常识类型和预期输出的可能性，以帮助分析模型行为。\n\n文章对多种最先进的T2I模型进行了基准测试，发现图像合成与真实生活照片之间仍存在较大差距。即使是DALL-E 3模型在Commonsense-T2I上的准确率也只能达到48.92%，而稳定扩散XL模型的准确率仅为24.92%。研究还表明，GPT增强的提示无法解决这一挑战，并详细分析了这种不足的可能原因。\n\n总的来说，这项研究旨在为T2I常识检查提供一个高质量的评估基准，促进真实生活图像生成的进步。\n\n搜索结果来自：\n[2406.07546] Commonsense-T2I Challenge: Can Text-to-Image Generation Models Understand Commonsense? - https://arxiv.org/abs/2406.07546\n[2406.07546] Commonsense-T2I Challenge: Can Text-to-Image Generation Models Understand Commonsense? - http://export.arxiv.org/abs/2406.07546"
    },
    {
        "url": "https://arxiv.org/abs/2406.09371",
        "content": "文章《LRM-Zero: Training Large Reconstruction Models with Synthesized Data》介绍了一种名为LRM-Zero的大型重建模型，这个模型完全使用合成的3D数据进行训练，实现了高质量的稀疏视角3D重建。LRM-Zero的核心是一个名为Zeroverse的程序化3D数据集，它自动从简单的原始形状、随机纹理和增强（例如高度场、布尔差异和线框）中合成。与以往由人类捕获或制作以近似真实3D数据的3D数据集（如Objaverse）不同，Zeroverse完全忽略了真实的全局语义，但在复杂的几何和纹理细节上与真实物体局部相似甚至更为复杂。文章展示了LRM-Zero使用完全合成的Zeroverse进行训练，可以在重建真实世界物体时达到高视觉质量，与在Objaverse上训练的模型相媲美。此外，文章还分析了Zeroverse的几个关键设计选择，这些选择有助于LRM-Zero的能力和训练稳定性。这项工作表明，3D重建作为3D视觉中的核心任务之一，可能不需要真实世界物体的语义就可以解决。Zeroverse的程序化合成代码和交互式可视化可在提供的网址上找到。\n\n搜索结果来自：\n[2406.09371] LRM-Zero: Training Large Reconstruction Models with Synthesized Data - http://export.arxiv.org/abs/2406.09371"
    },
    {
        "url": "https://arxiv.org/abs/2406.09356",
        "content": "这篇文章标题为“CMC-Bench: Towards a New Paradigm of Visual Signal Compression”，主要探讨了超低比特率图像压缩的新方法。文章提出了一种跨模态压缩（CMC）的范式，即图像-文本-图像的转换。与传统的编解码器相比，这种基于语义层面的压缩可以将图像数据大小减少到0.1%甚至更低，具有广泛的应用潜力。然而，CMC在保持与原始图像的一致性和感知质量方面存在一些缺陷。为了解决这个问题，文章引入了CMC-Bench，这是一个用于图像压缩的图像到文本（I2T）和文本到图像（T2I）模型的合作性能基准。该基准涵盖了18,000和40,000张图像，分别验证了6种主流的I2T和12种T2I模型，包括由人类专家标注的160,000个主观偏好分数。在超低比特率下，文章证明了一些I2T和T2I模型的组合已经超过了最先进的视觉信号编解码器；同时，它还强调了大型多模态模型（LMMs）在压缩任务上可以进一步优化的地方。文章鼓励LMM开发者参与这个测试，以促进视觉信号编解码器协议的演变。\n\n搜索结果来自：\n[2406.09356] CMC-Bench: Towards a New Paradigm of Visual Signal Compression - https://arxiv.org/abs/2406.09356"
    },
    {
        "url": "https://arxiv.org/abs/2406.09297",
        "content": "文章《MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding》是一篇关于机器学习的学术论文，提交于2024年6月13日。这篇论文介绍了一种名为“多层键值（MLKV）共享”的新方法，用于扩展Transformer模型中的键值（KV）缓存。这种方法旨在减少随着模型大小、批量大小和序列长度增加而导致的内存瓶颈问题。通过在Transformer层间共享KV缓存，MLKV显著降低了内存使用，同时性能损失最小。与Multi-Query Attention (MQA)和Grouped-Query Attention (GQA)相比，MLKV能将KV缓存大小减少到原来的1/6。这项研究对于高效部署大规模Transformer模型具有潜在的重要意义。论文作者还提供了相关代码，可通过提供的链接访问。\n\n搜索结果来自：\n[2406.09297] MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding - http://export.arxiv.org/abs/2406.09297"
    },
    {
        "url": "https://arxiv.org/abs/2406.05967",
        "content": "这篇文章的标题是《CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark》，由David Romero和其他74位作者共同撰写，提交于2024年6月10日。文章的主要内容集中在多模态人工智能中的一个重要任务——视觉问题回答（Visual Question Answering, VQA）。VQA任务用于测试视觉-语言模型理解和推理视觉和文本数据中知识的能力。\n\n文章指出，当前的VQA模型大多使用以英语和少数主要世界语言为主的数据库，且图像通常以西方为中心。虽然最近有努力增加VQA数据库覆盖的语言数量，但在低资源语言方面仍缺乏多样性。更重要的是，尽管这些数据库通常通过翻译或其他方法扩展其语言范围，但它们通常保持图像不变，导致文化代表性狭窄。\n\n为了解决这些限制，作者构建了CVQA，这是一个新的文化多样性和多语言的视觉问题回答基准。CVQA旨在覆盖丰富的语言和文化集合，其中作者在数据收集过程中邀请了母语者和文化专家参与。结果，CVQA包括了来自四大洲28个国家的文化驱动图像和问题，涵盖26种语言和11种文字，总共提供了9千个问题。然后，作者在CVQA上对几种多模态大型语言模型（Multimodal Large Language Models, MLLMs）进行了基准测试，并显示该数据集对当前最先进的模型具有挑战性。这个基准可以作为评估多模态模型的文化能力和偏见的探测性评估套件，并希望鼓励更多研究努力，以增加该领域的文化意识和语言多样性。\n\n搜索结果来自：\n[2406.05967] CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark - https://arxiv.org/abs/2406.05967\n[2406.05967] CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark - http://export.arxiv.org/abs/2406.05967"
    },
    {
        "url": "https://arxiv.org/abs/2406.07457",
        "content": "这篇文章的标题是《估计生成式人工智能的幻觉率》（Estimating the Hallucination Rate of Generative AI），作者是Andrew Jesson、Nicolas Beltran-Velez、Quentin Chu、Sweta Karlekar、Jannik Kossen、Yarin Gal、John P. Cunningham和David Blei。文章的主要内容是关于估计在上下文学习（In-Context Learning, ICL）中使用生成式人工智能时的幻觉率。在ICL中，条件生成模型（Conditional Generative Model, CGM）被提供数据集，并基于该数据集进行预测。文章从贝叶斯的角度解释ICL，认为CGM是在计算关于未知贝叶斯模型的潜在参数和数据的后验预测分布。在此视角下，文章定义了“幻觉”为在真实潜在参数下概率较低的生成预测。文章提出了一种新方法，该方法接受一个ICL问题（即CGM、数据集和预测问题），并估计CGM生成幻觉的概率。该方法仅需要从模型生成查询和响应，并评估其响应的对数概率。文章在合成回归和自然语言ICL任务上使用大型语言模型对方法进行了实证评估。\n\n搜索结果来自：\n[2406.07457] Estimating the Hallucination Rate of Generative AI - https://arxiv.org/abs/2406.07457"
    }
]