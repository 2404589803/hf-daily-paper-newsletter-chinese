[
    {
        "url": "https://arxiv.org/abs/2406.09760",
        "content": "这篇文章的标题是《Bootstrapping Language Models with DPO Implicit Rewards》，提交于2024年6月14日。文章主要探讨了在大型语言模型（LLMs）中实现人类对齐（human alignment）的方法。文章介绍了一种名为直接偏好优化（DPO）的突破性工作，它简化了从人类反馈中学习强化学习（RLHF）的过程，通过绕过RLHF中的奖励学习阶段。经过训练后，DPO提供了一个隐式的奖励模型。\n\n文章中的一个新颖观察是，这个隐式奖励模型本身可以以自举（bootstrapping）的方式使用，以进一步对齐LLM。作者的方法是使用当前LLM模型的奖励来构建一个偏好数据集，然后用于后续的DPO轮次。文章还包含了一些改进措施，如消除响应长度的偏差，提高偏好数据集的质量，以进一步提升方法的效果。这种方法被称为自我对齐与DPO隐式奖励（DICE），在AlpacaEval 2上的表现优于Gemini Pro，达到了27.55%的长度控制胜率，仅使用8B参数且没有外部反馈。\n\n研究的主题涉及计算与语言（cs.CL）以及机器学习（cs.LG）领域。文章的代码可以在GitHub上找到。\n\n搜索结果来自：\n[2406.09760] Bootstrapping Language Models with DPO Implicit Rewards - https://arxiv.org/abs/2406.09760\n[2406.09760] Bootstrapping Language Models with DPO Implicit Rewards - http://export.arxiv.org/abs/2406.09760"
    },
    {
        "url": "https://arxiv.org/abs/2406.11931",
        "content": "这篇文章是关于DeepSeek-Coder-V2，一个开源的Mixture-of-Experts (MoE) 代码语言模型。该模型在代码特定任务上实现了与GPT4-Turbo相媲美的性能。DeepSeek-Coder-V2是从DeepSeek-V2的中间检查点进一步预训练得到的，增加了额外的6000亿个标记。通过这种持续的预训练，DeepSeek-Coder-V2显著增强了DeepSeek-V2的编程和数学推理能力，同时在一般语言任务上保持了相当的性能。与DeepSeek-Coder-33B相比，DeepSeek-Coder-V2在代码相关任务的各个方面以及推理和一般能力方面都取得了显著进步。此外，DeepSeek-Coder-V2将支持的编程语言从86种扩展到338种，同时将上下文长度从16K扩展到128K。在标准基准评估中，DeepSeek-Coder-V2在编程和数学基准测试中优于闭源模型，如GPT4-Turbo、Claude 3 Opus和Gemini 1.5 Pro。\n\n搜索结果来自：\n[2406.11931] DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence - https://arxiv.org/abs/2406.11931"
    },
    {
        "url": "https://arxiv.org/abs/2406.12246",
        "content": "这篇文章标题为“DrVideo: Document Retrieval Based Long Video Understanding”，由Ziyu Ma等作者撰写。文章主要探讨了长视频理解的方法。现有的长视频理解技术主要集中于仅持续数十秒的视频，对于更长时间的视频处理技术探索有限。长视频中的帧数增加带来了两大挑战：关键信息的定位困难和长距离推理的执行难度。因此，作者提出了DrVideo，这是一个基于文档检索的长视频理解系统。该系统的核心思想是将长视频理解问题转化为长文档理解任务，以有效利用大型语言模型的强大能力。具体来说，DrVideo将长视频转换为基于文本的长文档，以初步检索关键帧并增强这些帧的信息，作为系统的起点。然后，系统使用基于代理的迭代循环，不断搜索缺失信息，增强相关数据，并在收集到足够的问题相关信息后，以链式思考的方式提供最终预测。在长视频基准测试上的广泛实验证实了该方法的有效性。DrVideo在EgoSchema基准测试（3分钟）上的准确率提高了+3.8，在MovieChat-1K中断模式上提高了+17.9，在MovieChat-1K全局模式（10分钟）上提高了+38.0，在LLama-Vid QA数据集（超过60分钟）上提高了+30.2，均优于现有的最先进方法。\n\n搜索结果来自：\n[2406.12846] DrVideo: Document Retrieval Based Long Video Understanding - https://arxiv.org/abs/2406.12846"
    },
    {
        "url": "https://arxiv.org/abs/2406.12849",
        "content": "无法访问您提供的文章链接 \"https://arxiv.org/abs/2406.12849\"。因此，我无法提供关于这篇文章的具体内容。如果您能提供文章的标题或摘要，我可以尝试通过其他方式来查找相关信息。"
    },
    {
        "url": "https://arxiv.org/abs/2406.12275",
        "content": "这篇文章标题为《VoCo-LLaMA: Towards Vision Compression with Large Language Models》，由Xubing Ye和其他五位作者撰写。文章发表于2024年6月18日，属于计算机视觉和模式识别领域。文章的主要研究内容是关于视觉-语言模型（VLMs）在处理高分辨率图像输入和视频时遇到的限制，如有限的上下文窗口和高计算成本。为了解决这个问题，作者提出了VoCo-LLaMA方法，这是首个利用大型语言模型（LLMs）压缩视觉标记的方法。通过在视觉指令调整阶段引入视觉压缩标记，并利用注意力蒸馏，该方法将LLMs对视觉标记的理解蒸馏到对VoCo标记的处理中。VoCo-LLaMA有效实现了视觉压缩，并提高了推理阶段的计算效率。该方法在压缩比为576倍时实现了最小的性能损失，减少了高达94.8%的FLOPs和69.6%的推理时间。此外，通过使用视频帧的时间序列压缩标记进行持续训练，VoCo-LLaMA展现了理解时间相关性的能力，并在流行的视频问答基准测试中超越了先前的方法。这一方法为解锁VLMs的上下文窗口的完整潜力提供了新的途径，使更多的可扩展多模态应用成为可能。\n\n搜索结果来自：\n[2406.12275] VoCo-LLaMA: Towards Vision Compression with Large Language Models - https://arxiv.org/abs/2406.12275"
    },
    {
        "url": "https://arxiv.org/abs/2406.12793",
        "content": "这篇文章是关于智谱AI发布的大语言模型家族ChatGLM的介绍。文章详细描述了ChatGLM的发展历程，从GLM-130B到GLM-4系列，包括GLM-4、GLM-4-Air和GLM-4-9B等模型。这些模型代表了该家族中最强大的版本，它们结合了之前几代ChatGLM的经验和技术。\n\nChatGLM是一个多功能、高性能的大型语言模型，它通过不断学习和适应，能够在多种语言环境下理解和生成语言，完成复杂的任务。模型的主要功能包括多语言支持、高质量的对齐、指令跟随和长文本处理。其主要特点包括大规模预训练、性能优越和自主工具使用。\n\nChatGLM模型基于Transformer架构，通过自回归填空目标进行预训练。在预训练后，模型会经历一系列后训练过程，包括监督式微调（SFT）和强化学习（RLHF），以更好地符合人类偏好和指令。模型通过分析用户输入的意图，规划解决过程，并调用外部工具来完成任务。\n\n具体应用场景包括信息检索、数学问题解决、图像生成、编程任务和多轮对话等。这些模型在预训练过程中使用了超过十万亿个token，主要针对中文和英文，但也支持其他24种语言。\n\n总的来说，ChatGLM模型在多个通用评估指标上与GPT-4等其他先进模型相媲美或更优，尤其在中文对齐方面表现突出。\n\n搜索结果来自：\n[2406.12793] ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools - https://arxiv.org/abs/2406.12793\n智谱AI发布论文介绍了大语言模型家族ChatGLM：由GLM-130B发展到GLM-4系列，包括GLM-4、GLM-4-Air和GLM-4-9B_大语言模型-猎游人 - https://www.lieyouren.cn/79767.html"
    },
    {
        "url": "https://arxiv.org/abs/2406.11912",
        "content": "文章《AgileCoder: Dynamic Collaborative Agents for Software Development based on Agile Methodology》提出了一种名为AgileCoder的多智能体系统，旨在通过集成敏捷方法论（Agile Methodology，AM）来改进软件开发流程。这个系统将特定的敏捷角色，如产品经理、开发人员和测试人员分配给不同的智能体，它们根据用户输入协作开发软件。AgileCoder通过将工作组织成冲刺（sprints）来提高开发效率，专注于通过冲刺增量式地开发软件。\n\n此外，文章还介绍了一个名为Dynamic Code Graph Generator的模块，该模块能够在代码库更新时动态创建代码依赖关系图。这使智能体能够更好地理解代码库，从而在软件开发生命周期中进行更精确的代码生成和修改。AgileCoder在性能上超越了现有的基准，如ChatDev和MetaGPT，为高级软件工程环境中的多智能体系统设定了新的标准。该研究的源代码可以在GitHub上找到。\n\n搜索结果来自：\n[2406.11912] AgileCoder: Dynamic Collaborative Agents for Software Development based on Agile Methodology - https://arxiv.org/abs/2406.11912\n[2406.11912] AgileCoder: Dynamic Collaborative Agents for Software Development based on Agile Methodology - http://export.arxiv.org/abs/2406.11912"
    },
    {
        "url": "https://arxiv.org/abs/2406.11811",
        "content": "这篇文章标题为“RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen Reference Content”，主要介绍了一个名为RepLiQA的新测试数据集。该数据集用于评估大型语言模型（LLMs）在未见过的参考内容上的性能，适用于问答和主题检索任务。RepLiQA包含五个测试集分裂，其中四个在此次发表前未在互联网上发布或暴露给LLM API。每个样本包括一个由人类注释者编写的参考文档、一个关于文档主题的问题、直接从文档信息中得出的真实答案，以及包含答案的文档段落。文章的目的是通过这个数据集来促进对语言模型的健全评估。\n\n搜索结果来自：\n[2406.11811] RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen Reference Content - https://arxiv.org/abs/2406.11811"
    },
    {
        "url": "https://arxiv.org/abs/2406.12459",
        "content": "很抱歉，我无法访问您提供的文章链接（https://arxiv.org/abs/2406.12459）。因此，我无法提供关于这篇文章的具体内容。您可以简要描述文章的主题或主要内容，我会尽力根据这些信息来帮助您。"
    },
    {
        "url": "https://arxiv.org/abs/2406.12742",
        "content": "这篇文章的标题是《Benchmarking Multi-Image Understanding in Vision and Language Models: Perception, Knowledge, Reasoning, and Multi-Hop Reasoning》。文章的主要内容是关于视觉和语言模型在处理多图像理解方面的研究。作者提出了一种新的基准测试方法，称为多图像关系基准（MIRB），用于评估视觉语言模型（VLMs）在比较、分析和推理多个图像方面的能力。研究涵盖了四个类别：感知、视觉世界知识、推理和多重推理。通过对多种开源和闭源模型的全面评估，文章发现，尽管开源VLMs在单图像任务上接近GPT-4V的性能，但在多图像推理任务上仍存在显著性能差距。此外，即使是最先进的GPT-4V模型也难以应对这一基准测试，这突显了该领域需要进一步的研究和发展。\n\n搜索结果来自：\n[2406.12742] Benchmarking Multi-Image Understanding in Vision and Language Models: Perception, Knowledge, Reasoning, and Multi-Hop Reasoning - https://arxiv.org/abs/2406.12742"
    },
    {
        "url": "https://arxiv.org/abs/2406.12824",
        "content": "无法访问您提供的文章链接 \"https://arxiv.org/abs/2406.12824\"。因此，我无法提供有关该文章的具体内容。如果您能提供文章的标题或摘要，我可以尝试通过其他方式帮助您。"
    },
    {
        "url": "https://arxiv.org/abs/2406.12168",
        "content": "这篇文章标题为“BPO: Supercharging Online Preference Learning by Adhering to the Proximity of Behavior LLM”，主要探讨了如何通过遵循行为语言模型（LLM）的邻近性来增强在线偏好学习。文章提出了一种称为“在线行为LLM邻近偏好优化”（BPO）的方法，强调为LLM对齐构建适当信任区域的重要性。研究通过将这种方法与各种直接对齐偏好（DAP）方法结合，验证了其在多种任务上的有效性和适用性，甚至在仅引入一个额外的数据收集阶段时，也显著提升了性能。\n\n搜索结果来自：\n[2406.12168] BPO: Supercharging Online Preference Learning by Adhering to the Proximity of Behavior LLM - https://arxiv.org/abs/2406.12168"
    },
    {
        "url": "https://arxiv.org/abs/2406.11687",
        "content": "这篇文章的标题是《Tokenization Falling Short: The Curse of Tokenization》，由Yekun Chai和其他三位作者撰写。文章主要探讨了语言模型在文本标记化（tokenization）过程中遇到的问题。标记化是将原始文本转换为预定义词汇中的子词标识符序列的过程，这一过程对打字错误、长度变化非常敏感，并且很大程度上忽略了标记的内部结构。文章通过三个关键的研究问题系统地研究了这些挑战及其对大型语言模型（LLMs）的影响：（1）复杂问题解决，（2）标记结构探测，（3）对排版变化的适应性。研究发现，扩大模型参数可以缓解标记化问题，但LLMs仍然受到由打字错误和其他文本格式变化引起的偏见影响。实验表明，子词正则化方法（如BPE-dropout）可以减轻这一问题。作者们将发布他们的代码和数据以促进进一步的研究。\n\n搜索结果来自：\n[2406.11687] Tokenization Falling Short: The Curse of Tokenization - https://arxiv.org/abs/2406.11687"
    },
    {
        "url": "https://arxiv.org/abs/2406.12753",
        "content": "这篇文章标题为《OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI》，由Zhen Huang和其他27位作者共同撰写。文章主要讨论了人工智能（AI）在大型语言模型（LLMs）和大型多模态模型（LMMs）的推动下的发展，特别是在多学科认知推理方面的进展。文章介绍了一个名为OlympicArena的新基准测试平台，该平台包含了11,163个涉及文本和图像的双语问题，涵盖七个学科领域的62个国际奥林匹克竞赛。这些问题的复杂性和跨学科性质使其成为评估AI认知推理能力的理想选择。研究显示，即使是像GPT-4o这样的先进模型，在整体准确率上也仅达到39.97%，这突显了当前AI在复杂推理和多模态整合方面的局限性。通过OlympicArena，作者们旨在推动AI向超级智能发展，以解决科学及更广泛领域中的更复杂挑战。文章还提供了一系列资源，包括基准数据集、开源注释平台、详细评估工具和自动提交功能的排行榜，以支持AI研究。\n\n搜索结果来自：\n[2406.12753] OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI - https://arxiv.org/abs/2406.12753"
    },
    {
        "url": "https://arxiv.org/abs/2406.12311",
        "content": "无法访问您提供的文章链接 \"https://arxiv.org/abs/2406.12311\"。因此，我无法提供关于这篇文章的具体内容。如果您能提供文章的标题或摘要，我可以尝试通过这些信息来帮助您。"
    },
    {
        "url": "https://arxiv.org/abs/2406.12066",
        "content": "这篇文章的标题是《Language Models are Surprisingly Fragile to Drug Names in Biomedical Benchmarks》，提交于2024年6月17日。文章的主要内容是探讨语言模型在生物医学基准测试中对药物名称的脆弱性。医学知识是依赖于上下文的，需要在各种自然语言表达中保持一致的推理，这对于药物名称尤其重要，因为患者经常使用品牌名称（如Advil或Tylenol）而不是其通用等效名称。\n\n为了研究这个问题，作者创建了一个新的鲁棒性数据集RABBITS，用于评估在医学基准测试中替换品牌和通用药物名称后的性能差异。这个数据集使用了医师专家的注释。文章评估了开源和基于API的大型语言模型（LLMs）在MedQA和MedMCQA上的表现，发现性能下降了1-10%。此外，文章还识别了这种脆弱性的一个可能来源，即广泛使用的预训练数据集中的测试数据被污染。\n\n这篇文章的代码可以在GitHub上找到，地址为 [https://github.com](https://github.com)，还有一个HuggingFace排行榜，地址为 [https://huggingface.co](https://huggingface.co)。\n\n搜索结果来自：\n[2406.12066] Language Models are Surprisingly Fragile to Drug Names in Biomedical Benchmarks - https://arxiv.org/abs/2406.12066\n[2406.12066] Language Models are Surprisingly Fragile to Drug Names in Biomedical Benchmarks - http://export.arxiv.org/abs/2406.12066"
    },
    {
        "url": "https://arxiv.org/abs/2406.12031",
        "content": "这篇文章标题为《大型表格数据的语言模型迁移学习》(Large Scale Transfer Learning for Tabular Data via Language Modeling)，由Josh Gardner等作者撰写。文章主要探讨了如何利用语言模型来改进表格数据的预测，特别是在大型数据集上的应用。\n\n文章的摘要指出，表格数据——一种结构化、异质性的数据，常见于行和列的形式，在许多领域都有广泛应用。尽管最近的基础模型已经在语言建模和计算机视觉等领域减少了对特定任务数据集和预测器开发的需求，但这一迁移学习范式在表格数据领域并未产生类似的影响。因此，作者们旨在缩小这一差距，并提出了TabuLa-8B，这是一种用于表格预测的语言模型。\n\n为了构建这个模型，作者们从TabLib语料库中提取了一个大型、高质量的训练数据集，并提出了用于表格数据过滤和质量控制的方法。他们使用了超过16亿行、来自310万张独特表格的数据集来微调一个Llama 3-8B的大型语言模型（LLM），用于表格数据的预测（分类和分箱回归）。通过在329个数据集的测试套件上进行评估，作者们发现TabuLa-8B在未见过的表格上的零样本准确率比随机猜测高出15个百分点以上，这是现有最先进的表格预测模型无法实现的。在少量样本设置（1-32个样本）下，TabuLa-8B在未针对目标数据集进行任何微调的情况下，比XGBoost和TabPFN模型准确率高5-15个百分点，而这些模型是在相等甚至更多（多达16倍）的数据上专门训练的。\n\n总的来说，这项工作展示了如何利用大型语言模型来提高表格数据的预测性能，特别是在少量样本的情况下，这些模型可以超越传统的、针对特定任务训练的模型。\n\n搜索结果来自：\n[2406.12031] Large Scale Transfer Learning for Tabular Data via Language Modeling - https://arxiv.org/abs/2406.12031"
    },
    {
        "url": "https://arxiv.org/abs/2406.12644",
        "content": "这篇文章标题为《Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models》，主要讨论了如何评估大型语言模型（LLMs）在处理不同任务时的有效性和能力。文章提出了一种名为“分层提示分类法”（Hierarchical Prompting Taxonomy，HPT）的评估框架，该框架包含五种独特的提示策略，从最简单到最复杂排列，用于更精确地评估LLMs。此外，文章还介绍了一种自适应分层提示框架，可自动为每个任务选择合适的提示策略。研究使用了四种指令调整的LLMs，并在四个数据集上进行了比较，证明了HPT的有效性，为比较不同任务和LLM能力提供了一种可靠方法。\n\n搜索结果来自：\n[2406.12644] Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models - https://arxiv.org/abs/2406.12644"
    },
    {
        "url": "https://arxiv.org/abs/2406.12292",
        "content": "这篇文章标题为“JEN-1 DreamStyler: Customized Musical Concept Learning via Pivotal Parameters Tuning”，主要探讨了文本到音乐生成领域的新方法。文章提出了一种定制化的文本到音乐生成方法，能够从两分钟的参考音乐中捕捉概念，并生成符合该概念的新音乐作品。为实现这一目标，作者对预训练的文本到音乐模型进行了微调。为避免直接微调所有参数导致的过拟合问题，文章还提出了一种关键参数微调方法，使模型能够在吸收新概念的同时保留其原始的生成能力。此外，文章还介绍了一种概念增强策略，以区分多个概念，并使微调后的模型能够同时生成包含一个或多个概念的音乐。由于这是首次在定制音乐生成任务上的工作，文章还引入了一个新的数据集和评估协议。该方法在定性和定量评估中均优于几个基线模型。更多详情可访问 [arXiv网站](https://arxiv.org/abs/2406.12292)。\n\n搜索结果来自：\n[2406.12292] JEN-1 DreamStyler: Customized Musical Concept Learning via Pivotal Parameters Tuning - https://arxiv.org/abs/2406.12292"
    },
    {
        "url": "https://arxiv.org/abs/2406.11939",
        "content": "这篇文章标题为《从众包数据到高质量基准：Arena-Hard和BenchBuilder流程》，由Tianle Li等作者撰写。文章主要讨论了随着语言模型的快速发展，对更具有挑战性的基准测试的需求日益增加。当前的静态基准测试往往难以持续区分不同模型的能力，并且与真实世界的用户偏好不一致。文章提出了一种名为BenchBuilder的动态基准测试系统，该系统可以从实时数据源中筛选出高质量的提示，以实现离线评估新模型。BenchBuilder使用七个高质量提示的指标，如需求数据知识等，并利用语言模型注释器从各种主题集群中选择高质量的提示子集。该系统在Chatbot Arena的提示上应用，创建了Arena-Hard-Auto v0.1，包含500个来自广泛任务的挑战性用户提示。这个新基准测试提供了比MT-Bench高三倍的置信区间，并与人类偏好排名达到了89.1%的一致性，且成本仅为25美元，无需人类标签员。\n\n搜索结果来自：\n[2406.11939] From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline - https://arxiv.org/abs/2406.11939"
    }
]