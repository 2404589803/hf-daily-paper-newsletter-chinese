[
    {
        "url": "https://arxiv.org/abs/2406.08973",
        "content": "这篇文章介绍了一个名为“XLand-100B”的大型多任务数据集，用于情境强化学习。这个数据集基于XLand-MiniGrid环境，包含近30,000个不同任务的完整学习历史，覆盖了1000亿个过渡和25亿个情节。收集这个数据集花费了50,000个GPU小时，这对大多数学术实验室来说是难以达到的。作者们还提供了用于复制或进一步扩展数据集的工具。这个数据集的目的是民主化情境强化学习领域的研究，并为进一步扩展提供坚实的基础。该代码是开源的，可在Apache 2.0许可下获得。\n\n搜索结果来自：\n[2406.08973] XLand-100B: A Large-Scale Multi-Task Dataset for In-Context Reinforcement Learning - https://arxiv.org/abs/2406.08973"
    },
    {
        "url": "https://arxiv.org/abs/2406.10210",
        "content": "无法访问您提供的文章链接 \"https://arxiv.org/abs/2406.10210\"。由于无法直接获取文章内容，我无法提供关于这篇文章的具体信息。如果您能提供文章的标题、作者或摘要等任何详细信息，我或许能够帮助您了解更多。"
    },
    {
        "url": "https://arxiv.org/abs/2406.07230",
        "content": "很抱歉，我无法直接访问或检索文章的具体内容。您可以直接访问提供的URL链接，以获取文章的详细信息和内容。如果您有关于文章主题或相关领域的问题，我会尽力根据我的现有知识库为您提供帮助。"
    },
    {
        "url": "https://arxiv.org/abs/2406.09961",
        "content": "这篇文章的标题是《ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via Chart-to-Code Generation》，发表于2024年6月14日，属于计算机科学领域，专注于软件工程、计算与语言、计算机视觉与模式识别等方面。文章的作者是Chufan Shi等共14人。\n\n文章的主要内容是介绍了一个名为ChartMimic的新基准，用于评估大型多模态模型（LMMs）的视觉基础代码生成能力。ChartMimic利用信息密集型的视觉图表和文本指令作为输入，要求LMMs生成相应的代码以渲染图表。该基准包括1,000个人工策划的（图表、指令、代码）三元组，这些三元组代表了科学论文中各种领域（如物理学、计算机科学、经济学等）中真实的图表使用案例。这些图表涵盖了18种常规类型和4种高级类型，细分为191个子类别。\n\n文章还提出了多级评估指标，以自动且全面地评估输出代码和渲染的图表。与现有的代码生成基准不同，ChartMimic强调评估LMMs在视觉理解、代码生成和跨模态推理等方面的综合能力。通过对3个专有模型和11个开源模型的评估，文章突显了ChartMimic提出的挑战。即使是先进的GPT-4V和Claude-3-opus模型，平均得分也只有73.2和53.7，表明还有很大的改进空间。作者预计ChartMimic将激励LMMs的发展，推动人工通用智能的追求。\n\n这篇文章的数据和代码都可在GitHub上找到。\n\n搜索结果来自：\n[2406.09961] ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via Chart-to-Code Generation - https://arxiv.org/abs/2406.09961\n[2406.09961] ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via Chart-to-Code Generation - http://export.arxiv.org/abs/2406.09961\nBytez: ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via Chart-to-Code Generation - https://bytez.com/docs/arxiv/2406.09961/paper"
    },
    {
        "url": "https://arxiv.org/abs/2406.08418",
        "content": "这篇文章介绍了一个名为OmniCorpus的大型多模态数据集。这个数据集包含了大约86亿张图片和1696亿个文本标记，这些数据以自然文档格式交错排列，类似于互联网数据的展示方式，也接近人类的阅读习惯。研究显示，这种图像与文本交错的数据有助于多模态的上下文学习，并在多模态微调过程中保持大型语言模型的能力。OmniCorpus数据集相比于其他类似数据集（如MMC4、OBELICS），具有更大的规模（大15倍）和更高质量的数据，同时来源更加多样化，包括英语和非英语网站以及以视频为中心的网站。此外，该数据集还具有灵活性，可以轻松地从图像文本交错格式降解为纯文本语料库或图像文本对。文章通过全面的分析和实验验证了所提出数据集的质量、可用性和有效性，希望为未来的多模态模型研究提供坚实的数据基础。相关的代码和数据已发布在GitHub上。"
    },
    {
        "url": "https://arxiv.org/abs/2406.10149",
        "content": "这篇文章标题为“BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack”，主要探讨了大型语言模型（LLMs）在处理长文本上下文方面的能力。文章指出，尽管LLMs的输入上下文大小近年来显著增加，但现有的评估方法未能全面衡量模型处理长上下文的有效性。为了填补这一空白，文章引入了BABILong基准，用于测试语言模型在极其长的文档中跨事实进行推理的能力。BABILong包含20个不同的推理任务，如事实链、简单归纳、演绎、计数以及处理列表/集合等。这些任务本身具有挑战性，当所需事实分散在长自然文本中时，挑战性更大。研究表明，流行的LLMs实际上只有效利用了10-20%的上下文，并且随着推理复杂度的增加，其性能急剧下降。在替代的上下文推理方法中，检索增强生成方法在单事实问答上实现了60%的准确率，且与上下文长度无关。在上下文扩展方法中，循环记忆转换器表现最佳，能够处理长达1100万令牌的长度。BABILong基准可扩展到任何长度，以支持评估新出现的、能力更强的模型，并且提供了长达100万令牌长度的分割。\n\n搜索结果来自：\n[2406.10149] BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack - https://arxiv.org/abs/2406.10149"
    },
    {
        "url": "https://arxiv.org/abs/2406.10118",
        "content": "文章《SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages》主要探讨了东南亚地区在人工智能模型中的代表性不足问题。东南亚拥有丰富的语言多样性和文化多样性，超过1300种土著语言和6.71亿人口。然而，现有人工智能模型在文本、图像和音频数据集方面严重缺乏东南亚语言的代表性，这影响了东南亚语言的人工智能模型的质量。由于高质量数据集的稀缺，评估东南亚语言模型变得具有挑战性，再加上英语训练数据的主导地位，引发了关于潜在文化误表征的担忧。\n\n为了解决这些挑战，文章介绍了SEACrowd，这是一个协作倡议，旨在通过提供近1000种东南亚语言的标准化语料库来填补资源缺口，涵盖三种模态。通过SEACrowd基准测试，文章对13个任务中的36种土著语言的人工智能模型质量进行了评估，为东南亚当前的人工智能景观提供了宝贵见解。此外，文章还提出了促进人工智能更大进步的策略，旨在最大化人工智能在东南亚未来的潜在效用和资源公平性。\n\n搜索结果来自：\n[2406.10118] SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages - http://export.arxiv.org/abs/2406.10118"
    },
    {
        "url": "https://arxiv.org/abs/2406.08451",
        "content": "文章《GUI Odyssey: A Comprehensive Dataset for Cross-App GUI Navigation on Mobile Devices》是一篇关于移动设备上跨应用程序（app）图形用户界面（GUI）导航的综合数据集的研究。智能手机用户经常需要在多个应用程序之间导航，以完成诸如在社交媒体平台之间共享内容等任务。自主GUI导航代理可以通过简化工作流程和减少手动干预，从而在通信、娱乐和生产力方面增强用户体验。然而，以前的GUI代理通常使用包含可以在单个应用程序内完成的简单任务的数据库进行训练，导致在跨应用程序导航方面表现不佳。\n\n为了解决这个问题，研究人员引入了GUI Odyssey，这是一个用于训练和评估跨应用程序导航代理的综合数据集。GUI Odyssey包含来自6个移动设备的7,735个剧集，涵盖了6种跨应用程序任务、201个应用程序和1.4K个应用程序组合。利用GUI Odyssey，研究人员开发了OdysseyAgent，这是一个通过使用历史重采样模块对Qwen-VL模型进行微调的多模态跨应用程序导航代理。广泛的实验表明，与现有模型相比，OdysseyAgent的准确性更高。例如，OdysseyAgent在域内准确率上超过了微调后的Qwen-VL和零样本GPT-4V，平均分别提高了1.44%和55.49%，在域外准确率上分别提高了2.29%和48.14%。该数据集和代码将在GitHub上发布。\n\n搜索结果来自：\n[2406.08451] GUI Odyssey: A Comprehensive Dataset for Cross-App GUI Navigation on Mobile Devices - https://arxiv.org/abs/2406.08451\n[2406.08451] GUI Odyssey: A Comprehensive Dataset for Cross-App GUI Navigation on Mobile Devices - http://export.arxiv.org/abs/2406.08451\nGUI Odyssey: A Comprehensive Dataset for Cross-App GUI Navigation on Mobile Devices - NASA/ADS - https://ui.adsabs.harvard.edu/abs/2024arXiv240608451L/abstract"
    },
    {
        "url": "https://arxiv.org/abs/2406.10208",
        "content": "这篇文章标题为“Glyph-ByT5-v2: A Strong Aesthetic Baseline for Accurate Multilingual Visual Text Rendering”，主要讨论了Glyph-ByT5-v2和Glyph-SDXL-v2两个模型的开发。这些模型旨在解决多语言视觉文本渲染中的准确性和美学质量问题。研究团队创建了一个包含超过100万种字形文本对和1000万种图形设计图像文本对的多语言字形文本和图形设计数据集，涵盖了九种其他语言。此外，他们还建立了一个包含1000个提示的多语言视觉段落基准，每个语言100个，以评估多语言视觉拼写准确性。通过利用最新的步进感知偏好学习方法，这些模型在视觉美学质量方面也有所提升。这项工作被视为在多语言视觉文本渲染任务方面的重要进展。\n\n搜索结果来自：\n[2406.10208] Glyph-ByT5-v2: A Strong Aesthetic Baseline for Accurate Multilingual Visual Text Rendering - https://arxiv.org/abs/2406.10208"
    },
    {
        "url": "https://arxiv.org/abs/2406.09900",
        "content": "这篇文章介绍了一种名为GEB-1.3B的开源轻量级大型语言模型。该模型在中文和英文语言上接受了5500亿个标记的训练，并采用了包括ROPE、Group-Query-Attention和FlashAttention-2在内的创新训练技术。GEB-1.3B在通用基准测试中表现出色，如MMLU、C-Eval和CMMLU，超过了MindLLM-1.3B和TinyLLaMA-1.1B等比较模型。此外，该模型的FP32版本在CPU上的推理时间表现优异，目前正通过先进的量化技术进一步优化速度。GEB-1.3B的发布为轻量级大型语言模型的发展做出了重要贡献，有望推动该领域的进一步研究和创新。\n\n搜索结果来自：\n[2406.09900] GEB-1.3B: Open Lightweight Large Language Model - https://arxiv.org/abs/2406.09900"
    },
    {
        "url": "https://arxiv.org/abs/2406.07882",
        "content": "这篇文章的标题是《Designing a Dashboard for Transparency and Control of Conversational AI》，提交于2024年6月12日，并在6月15日进行了修订。文章的主要研究内容是关于对话式大型语言模型（LLM）的透明度和控制性问题。当前的对话式LLM通常被视为“黑箱”系统，用户难以理解其输出结果的原因。这种缺乏透明度的问题在考虑到偏见和真实性问题时尤为突出。\n\n为了解决这个问题，文章提出了一种端到端的原型设计，该设计将可解释性技术与用户体验设计相结合，旨在使聊天机器人更加透明。研究首先展示了证据，证明一个著名开源LLM具有“用户模型”：通过检查系统的内部状态，可以提取与用户年龄、性别、教育水平和社会经济状况相关的数据。接着，文章描述了一个伴随聊天机器人界面的仪表板的设计，该仪表板可以实时显示这个用户模型。此外，仪表板还可以用来控制用户模型和系统的行为。\n\n最后，文章讨论了一项研究，其中用户与这个经过仪器测试的系统进行了对话。研究结果表明，用户欣赏能够看到内部状态，这帮助他们揭露了偏见行为并增加了他们的控制感。参与者还提出了宝贵的建议，为设计和机器学习研究指明了未来的方向。该项目的页面和视频演示可在提供的链接中找到。\n\n搜索结果来自：\n[2406.07882] Designing a Dashboard for Transparency and Control of Conversational AI - https://arxiv.org/abs/2406.07882\n[2406.07882] Designing a Dashboard for Transparency and Control of Conversational AI - http://export.arxiv.org/abs/2406.07882"
    },
    {
        "url": "https://arxiv.org/abs/2406.08845",
        "content": "这篇文章的标题是《Rethinking Human Evaluation Protocol for Text-to-Video Models: Enhancing Reliability, Reproducibility, and Practicality》。文章的主要内容是关于文本到视频（T2V）技术的最新进展，例如Gen2、Pika和Sora等模型，这些进展显著扩大了T2V技术的应用范围和普及度。然而，尽管取得了这些进步，评估这些模型仍然面临重大挑战。主要原因是自动度量标准的局限性，因此手动评估通常被认为是评估T2V生成的更好方法。但是，现有的手动评估协议存在可重复性、可靠性和实用性问题。为了解决这些挑战，这篇文章引入了文本到视频人类评估（T2VHE）协议，这是一个全面且标准化的T2V模型评估协议。T2VHE协议包括明确定义的度量标准、彻底的注释者培训和有效的动态评估模块。实验结果表明，这个协议不仅确保了高质量的注释，而且几乎可以将评估成本降低50%。作者计划开源整个T2VHE协议的设置，包括完整的协议工作流程、动态评估组件的细节和注释界面代码。这将帮助社区建立更复杂的人类评估协议。\n\n搜索结果来自：\n[2406.08845] Rethinking Human Evaluation Protocol for Text-to-Video Models: Enhancing Reliability,Reproducibility, and Practicality - https://arxiv.org/abs/2406.08845"
    },
    {
        "url": "https://arxiv.org/abs/2406.06263",
        "content": "无法访问您提供的文章链接，因此我无法提供有关该文章的具体内容。如果您能提供文章的标题或摘要，我可以尝试通过其他方式帮助您获取相关信息。"
    },
    {
        "url": "https://arxiv.org/abs/2406.09559",
        "content": "无法访问您提供的文章链接 \"https://arxiv.org/abs/2406.09559\"。因此，我无法提供关于这篇文章的具体内容。如果您能提供文章的标题或摘要，我可以尝试通过其他方式帮助您。"
    },
    {
        "url": "https://arxiv.org/abs/2406.08920",
        "content": "这篇文章标题为“AV-GS: Learning Material and Geometry Aware Priors for Novel View Acoustic Synthesis”，主要研究了一种名为新颖视角声学合成（Novel View Acoustic Synthesis, NVAS）的技术。NVAS的目标是在任意目标视角渲染双耳音频，给定一个由3D场景中的声源发出的单声道音频。文章提出了一种新的音频-视觉高斯散射（Audio-Visual Gaussian Splatting, AV-GS）模型，通过学习基于点的场景表示和音频引导参数，来获得用于音频合成的材料和几何感知条件。此外，文章还提出了一种点密度化和修剪策略，以优化高斯点的分布，从而实现视觉场景模型的音频适应性。该方法在真实世界的RWAS和基于模拟的SoundSpaces数据集上进行了广泛实验，验证了其优于现有方法的性能。\n\n搜索结果来自：\n[2406.08920] AV-GS: Learning Material and Geometry Aware Priors for Novel View Acoustic Synthesis - https://arxiv.org/abs/2406.08920"
    },
    {
        "url": "https://arxiv.org/abs/2406.10227",
        "content": "这篇文章标题为“VideoGUI: A Benchmark for GUI Automation from Instructional Videos”，由Kevin Qinghong Lin和其他七位作者共同撰写。文章主要探讨了图形用户界面（GUI）自动化在提升人类工作效率方面的潜力，尤其是在处理计算机任务方面。现有的任务制定主要集中于可以通过单一语言指令指定的简单任务，例如“插入新幻灯片”。\n\n在这项工作中，作者们介绍了VideoGUI，这是一个新颖的多模态基准，旨在评估GUI助手在以视觉为中心的GUI任务上的表现。这个基准的数据来源于高质量的网络教学视频，专注于涉及专业和新软件（如Adobe Photoshop或Stable Diffusion WebUI）以及复杂活动（如视频编辑）的任务。\n\nVideoGUI通过一个分层过程评估GUI助手，允许识别它们可能在哪些具体层面上失败：（i）高级规划：从视觉条件中重建程序子任务，无需语言描述；（ii）中级规划：基于视觉状态（即屏幕截图）和目标生成精确动作叙述序列；（iii）原子动作执行：执行特定动作，如准确点击指定元素。作者为每个层面设计了评估指标，以提供清晰的信号，例如在原子动作执行中点击、拖动、键入和滚动的个人表现。\n\nVideoGUI的评估显示，即使是目前最先进的大型多模态模型GPT4o，在以视觉为中心的GUI任务上也表现不佳，特别是在高级规划方面。\n\n搜索结果来自：\n[2406.10227] VideoGUI: A Benchmark for GUI Automation from Instructional Videos - https://arxiv.org/abs/2406.10227"
    },
    {
        "url": "https://arxiv.org/abs/2406.10126",
        "content": "这篇文章的标题是《Training-free Camera Control for Video Generation》，发表在计算机视觉和模式识别领域。文章提出了一种无需训练的解决方案，用于控制现成的视频扩散模型的摄像机运动。这种方法不需要在带有摄像机注释的数据集上进行监督微调，也不需要通过数据增强进行自监督训练。相反，它可以与大多数预训练的视频扩散模型即插即用，并仅凭单张图像或文本提示作为输入生成可控制摄像机的视频。\n\n该工作的灵感来源于中间潜在结果对生成内容的布局先验。通过重新排列其中的噪声像素，可以使输出内容重新分配。由于摄像机移动也可以被视为由视角变化引起的像素重新排列，因此如果相应的噪声潜在结果发生变化，视频可以根据特定的摄像机运动重新组织。基于这一点，作者提出了他们的方法CamTrol，它通过两阶段过程实现了对视频扩散模型的稳健摄像机控制。首先，他们在3D点云空间中通过显式的摄像机移动来模拟图像布局的重新排列。其次，他们使用由一系列重新排列的图像形成的噪声潜力的布局先验来生成具有摄像机运动的视频。广泛的实验证明了该方法在控制生成视频的摄像机运动方面的稳健性。此外，文章还展示了该方法在生成具有动态内容的3D旋转视频方面的显著成果。\n\n更多详情请查看原文链接: [Training-free Camera Control for Video Generation](https://arxiv.org/abs/2406.10126)。\n\n搜索结果来自：\n[2406.10126] Training-free Camera Control for Video Generation - http://export.arxiv.org/abs/2406.10126"
    },
    {
        "url": "https://arxiv.org/abs/2406.10111",
        "content": "无法访问您提供的文章链接 \"https://arxiv.org/abs/2406.10111\"。因此，我无法提供关于这篇文章的具体内容。如果您能提供文章的标题或摘要，我可以尝试通过其他方式来查找相关信息。"
    }
]