[
    {
        "url": "https://arxiv.org/abs/2406.07550",
        "content": "这篇文章标题为《An Image is Worth 32 Tokens for Reconstruction and Generation》，由Qihang Yu和其他五位作者撰写，提交于2024年6月11日。文章主要讨论了在生成模型中，图像标记化（image tokenization）在高效合成高分辨率图像中的关键作用。标记化将图像转换为潜在表示，与直接处理像素相比，减少了计算需求，并提高了生成过程的有效性和效率。\n\n文章介绍了一种创新的标记化方法——基于Transformer的一维标记器（TiTok），它将图像标记化为1D潜在序列。这种方法提供了更紧凑的潜在表示，比传统技术更高效、有效。例如，一个256 x 256 x 3的图像可以缩减至仅32个离散标记，相比之前的方法（256或1024个标记）大幅减少。尽管TiTok的表示更为紧凑，但它仍实现了与最先进方法相媲美的性能。在ImageNet 256 x 256基准测试中，TiTok使用相同的生成器框架，达到了1.97 gFID，显著优于MaskGIT基线（4.21）。在更高分辨率的ImageNet 512 x 512基准测试中，TiTok不仅超过了最先进的扩散模型DiT-XL/2（gFID 2.74 vs. 3.04），还将图像标记减少了64倍，使得生成过程加快了410倍。TiTok的最佳变体在显著超过DiT-XL/2（gFID 2.13 vs. 3.04）的同时，仍能以74倍的速度生成高质量样本。\n\n搜索结果来自：\n[2406.07550] An Image is Worth 32 Tokens for Reconstruction and Generation - https://arxiv.org/abs/2406.07550"
    },
    {
        "url": "https://arxiv.org/abs/2406.07547",
        "content": "这篇文章标题为“Zero-shot Image Editing with Reference Imitation”，主要探讨了一种新型的图像编辑方法。在这种方法中，用户可以通过直接从野外参考（例如在线遇到的 相关图片）中汲取灵感来编辑图像的兴趣区域，而无需处理参考与源图像之间的匹配问题。为了实现这一点，作者们提出了一个名为MimicBrush的生成性训练框架，该框架从视频片段中随机选择两帧，对其中一帧的某些区域进行掩蔽，并学习使用另一帧的信息来恢复掩蔽区域。通过这种方式，基于扩散先验开发的模型能够在自我监督的方式下捕获分离图像之间的语义对应关系。文章还通过实验展示了该方法在各种测试案例下的有效性，并证明了其优于现有方法的优越性。此外，作者们还构建了一个基准以促进进一步的研究。"
    },
    {
        "url": "https://arxiv.org/abs/2406.07436",
        "content": "文章《McEval: Massively Multilingual Code Evaluation》讨论了大规模多语言代码评估的研究。这项研究主要关注代码大型语言模型（LLMs）在代码理解、补全和生成任务方面的显著进步。现有的编程基准测试主要集中于Python，并且仍然局限于有限的几种语言，其中其他语言通常是从Python样本翻译过来的，这降低了数据多样性。为了进一步促进代码LLMs的研究，作者提出了一个覆盖40种编程语言的大规模多语言代码基准测试（McEval），包含16,000个测试样本，这在多语言场景中显著推动了代码LLMs的极限。这个基准测试包含了具有精心策划的大规模多语言指令语料库McEval-Instruct的挑战性代码补全、理解和生成评估任务。此外，作者还介绍了一个在McEval-Instruct上训练的有效多语言编码器mCoder，以支持多语言编程语言生成。McEval上的广泛实验结果表明，在众多语言中，开源模型和闭源LLMs（例如GPT系列模型）之间仍然存在一条艰难的道路。该研究的指令语料库、评估基准和排行榜可在 [McEval官网](https://mceval.github.io/) 查看。\n\n搜索结果来自：\n[2406.07436] McEval: Massively Multilingual Code Evaluation - https://arxiv.org/abs/2406.07436\nMcEval: Massively Multilingual Code Evaluation - NASA/ADS - https://ui.adsabs.harvard.edu/abs/2024arXiv240607436C/abstract"
    },
    {
        "url": "https://arxiv.org/abs/2406.07496",
        "content": "这篇文章标题为“TextGrad: Automatic 'Differentiation' via Text”，由Mert Yuksekgonul等作者撰写，提交于2024年6月11日。文章主要探讨了人工智能领域的一个新挑战，即开发出用于复合AI系统的原则性和自动化优化方法。受神经网络早期发展中回传和自动微分技术的启发，作者们引入了TextGrad框架，这是一种通过文本进行“微分”的强大工具。TextGrad利用大型语言模型（LLMs）提供的文本反馈，来改进复合AI系统的各个组件。此框架允许LLMs提供丰富的自然语言建议，以优化计算图中的变量，应用范围广泛，从代码片段到分子结构设计。TextGrad遵循PyTorch的语法和抽象，灵活且易于使用。文章还展示了TextGrad在多种应用中的有效性和通用性，例如提高GPT-4o在Google-Proof问题回答上的零样本准确性，优化LeetCode难题的编程解决方案，改进推理提示，设计具有理想模拟结合的新药样小分子，以及设计高特异性的放射肿瘤学治疗计划。\n\n搜索结果来自：\n[2406.07496] TextGrad: Automatic \"Differentiation\" via Text - https://arxiv.org/abs/2406.07496"
    },
    {
        "url": "https://arxiv.org/abs/2406.06608",
        "content": "这篇文章的标题是《The Prompt Report: A Systematic Survey of Prompting Techniques》，由Sander Schulhoff和其他30位作者共同撰写。文章主要关注的是生成式人工智能（GenAI）系统中的提示技术。这些系统在工业和研究领域越来越普及，开发者和最终用户通过提示技术或提示工程与这些系统互动。尽管提示技术是一个广泛研究和使用的概念，但由于该领域的新兴性质，关于提示的定义存在术语上的冲突和本体理解上的不足。\n\n文章旨在建立对提示的系统性理解，通过汇编提示技术的分类法并分析其使用。研究提出了33个词汇术语的全面词汇表，58种仅文本的提示技术分类，以及40种其他模态的技术。此外，文章还对关于自然语言前缀提示的整个文献进行了元分析。\n\n这篇文章涵盖了计算和语言（cs.CL）以及人工智能（cs.AI）领域，对于理解和使用生成式AI系统中的提示技术提供了深入的见解。\n\n搜索结果来自：\n[2406.06608] The Prompt Report: A Systematic Survey of Prompting Techniques - https://arxiv.org/abs/2406.06608"
    },
    {
        "url": "https://arxiv.org/abs/2406.06563",
        "content": "这篇文章标题为《Skywork-MoE: A Deep Dive into Training Techniques for Mixture-of-Experts Language Models》，主要介绍了Skywork-MoE这个高性能的混合专家（MoE）大型语言模型（LLM）的开发和训练方法。Skywork-MoE拥有1460亿参数和16个专家，是从作者们之前开发的Skywork-13B模型的密集检查点初始化而来的。文章探讨了从现有密集检查点循环利用与从零开始初始化这两种方法的相对有效性，并强调了两种创新技术：门控日志归一化，提高专家多样化；自适应辅助损失系数，允许对辅助损失系数进行层特异性调整。实验结果表明，这些方法的有效性。利用这些技术和见解，作者们在SkyPile语料库的一个精简子集上训练了他们的循环利用Skywork-MoE模型，评估结果显示该模型在广泛的基准测试中表现强劲。\n\n搜索结果来自：\n[2406.06563] Skywork-MoE: A Deep Dive into Training Techniques for Mixture-of-Experts Language Models - https://arxiv.org/abs/2406.06563"
    },
    {
        "url": "https://arxiv.org/abs/2406.06592",
        "content": "这篇文章的标题是《通过自动化过程监督提高语言模型中的数学推理能力》。文章主要讨论了如何提高大型语言模型（LLMs）在复杂多步骤推理任务中的性能，例如解决数学问题或生成代码。作者提出了一种名为OmegaPRM的蒙特卡洛树搜索算法，用于高效收集高质量的过程监督数据。通过这种方法，作者能够收集超过150万的过程监督注释，用于训练过程奖励模型（PRM）。利用这种完全自动化的过程监督和加权的自我一致性算法，作者提高了Gemini Pro模型的数学推理性能，在MATH基准测试中达到了69.4%的成功率，相比基础模型的51%性能有了36%的相对提升。整个过程无需人工干预，使得这种方法在财务和计算成本上相比现有方法更为高效。\n\n搜索结果来自：\n[2406.06592] Improve Mathematical Reasoning in Language Models by Automated Process Supervision - https://arxiv.org/abs/2406.06592"
    },
    {
        "url": "https://arxiv.org/abs/2406.06911",
        "content": "这篇文章标题为 \"AsyncDiff: Parallelizing Diffusion Models by Asynchronous Denoising\"，主要探讨了如何通过异步去噪的方式加速扩散模型的并行计算。文章提出了一种名为AsyncDiff的通用且即插即用的加速方案，该方案能够在多个设备上实现模型并行。通过将庞大的噪声预测模型分解为多个组件，并分配给不同的设备，以及利用连续扩散步骤中隐藏状态之间的高度相似性，将传统的顺序去噪过程转换为异步过程，从而实现每个组件在单独的设备上并行计算。对于Stable Diffusion v2.1，AsyncDiff在四块NVIDIA A5000 GPUs上实现了2.7倍的速度提升，且质量下降微乎其微，在CLIP Score上仅下降了0.38。该研究还展示了AsyncDiff可以轻松应用于视频扩散模型，并展现出鼓舞人心的性能。更多细节和代码可以在提供的链接中找到。"
    },
    {
        "url": "https://arxiv.org/abs/2406.06612",
        "content": "这篇文章的标题是 \"SEE-2-SOUND: Zero-Shot Spatial Environment-to-Spatial Sound\"，发表于2024年6月，是一篇关于计算机视觉和模式识别、机器学习、声音以及音频和语音处理的研究论文。文章的主要研究内容是关于生成结合视觉和听觉感官体验的技术，这对于沉浸式内容的生产至关重要。近期，神经生成模型的发展已经使得在多种模态（如图像、文本、语音和视频）中创建高分辨率内容成为可能。然而，尽管取得了这些成功，目前在生成与视觉内容相辅相成的高质量空间音频方面仍存在重大差距。此外，当前的音频生成模型在生成自然音频、语音或音乐方面表现出色，但在整合沉浸式体验所需的空间音频提示方面则显得不足。\n\n在这项工作中，作者们介绍了一种名为SEE-2-SOUND的零样本方法，该方法将任务分解为以下几个步骤：\n1. 识别视觉感兴趣的区域；\n2. 在3D空间中定位这些元素；\n3. 为每个元素生成单声道音频；\n4. 将它们整合成空间音频。\n\n通过使用他们的框架，作者们展示了为来自互联网的高质量视频、图像和动态图像以及通过学习生成的方法生成的媒体生成空间音频的引人注目的结果。\n\n搜索结果来自：\n[2406.06612] SEE-2-SOUND: Zero-Shot Spatial Environment-to-Spatial Sound - https://arxiv.org/abs/2406.06612\n[2406.06612] SEE-2-SOUND: Zero-Shot Spatial Environment-to-Spatial Sound - http://export.arxiv.org/abs/2406.06612\nSEE-2-SOUND: Zero-Shot Spatial Environment-to-Spatial Sound - NASA/ADS - https://ui.adsabs.harvard.edu/abs/2024arXiv240606612D/abstract"
    },
    {
        "url": "https://arxiv.org/abs/2406.07472",
        "content": "这篇文章标题为“4Real: Towards Photorealistic 4D Scene Generation via Video Diffusion Models”，由Heng Yu、Chaoyang Wang、Peiye Zhuang、Willi Menapace、Aliaksandr Siarohin、Junli Cao、Laszlo A Jeni、Sergey Tulyakov和Hsin-Ying Lee共同撰写。文章发表于2024年6月11日，属于计算机视觉和模式识别领域。\n\n文章的主要内容是提出了一种新的用于生成逼真4D场景的管道。现有的动态场景生成方法大多依赖于从预训练的3D生成模型中提取知识，这些模型通常在合成对象数据集上进行微调。因此，生成的场景通常以对象为中心，缺乏逼真性。为了解决这些限制，文章引入了一种新的管道，用于生成逼真的文本到4D场景，摒弃了对多视图生成模型的依赖，而是充分利用了在多样化的真实世界数据集上训练的视频生成模型。该方法首先使用视频生成模型生成参考视频，然后使用从参考视频中精心生成的冻结时间视频学习视频的规范3D表示。为了处理冻结时间视频中的不一致性，文章共同学习每帧的变形来模拟这些不完美之处。然后，文章基于规范表示学习时间变形，以捕捉参考视频中的动态交互。该管道促进了具有增强逼真性和结构完整性的动态场景的生成，可从多个角度观看，从而在4D场景生成方面设定了新的标准。\n\n搜索结果来自：\n[2406.07472] 4Real: Towards Photorealistic 4D Scene Generation via Video Diffusion Models - https://arxiv.org/abs/2406.07472"
    },
    {
        "url": "https://arxiv.org/abs/2406.07394",
        "content": "这篇文章标题为《Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B》，由Di Zhang和其他五位作者共同撰写。文章介绍了一种名为MCT Self-Refine (MCTSr)的算法，这是一种将大型语言模型（LLMs）与蒙特卡洛树搜索（MCTS）创新结合的方法，旨在提高在复杂数学推理任务中的性能。MCTSr算法通过迭代的选择、自我精炼、自我评估和反向传播过程构建蒙特卡洛搜索树，并使用改进的上置信界（UCB）公式来优化探索-利用平衡。该研究在解决奥林匹克级别的数学问题方面取得了显著成效，提高了包括GSM8K、GSM Hard、MATH以及Math Odyssey、AIME和OlympiadBench等多个数据集的成功率。这项研究推进了LLMs在复杂推理任务中的应用，并为未来AI的集成奠定了基础，提高了LLM驱动应用中的决策准确性和可靠性。\n\n搜索结果来自：\n[2406.07394] Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B - https://arxiv.org/abs/2406.07394"
    },
    {
        "url": "https://arxiv.org/abs/2406.06573",
        "content": "这篇文章的标题是《MedFuzz: 探索大型语言模型在医疗问答中的稳健性》。文章主要研究了大型语言模型（LLM）在医疗问答基准测试中的表现。虽然这些模型在这些测试中取得了很高的准确率，但这并不意味着它们在实际临床环境中的表现也能如此。文章指出，医疗问答基准测试依赖于一些假设，这些假设在现实的临床环境中可能不成立。因此，作者提出了一种名为MedFuzz的对抗性方法，旨在通过修改基准测试中的问题来混淆LLM。文章通过针对MedQA基准测试中关于患者特征的强假设进行攻击，展示了这种方法的效果。这些成功的“攻击”能够使LLM从正确答案变为错误答案，尽管这些修改对于医学专家来说不太可能造成误导。此外，文章还提出了一种排列测试技术，可以确保成功的攻击在统计上是显著的。这些方法有望提供关于LLM在更现实环境中稳健运作能力的洞见。\n\n搜索结果来自：\n[2406.06573] MedFuzz: Exploring the Robustness of Large Language Models in Medical Question Answering - https://arxiv.org/abs/2406.06573"
    },
    {
        "url": "https://arxiv.org/abs/2406.05629",
        "content": "这篇文章标题为“Separating the 'Chirp' from the 'Chat': Self-supervised Visual Grounding of Sound and Language”，发表于2024年6月9日。文章主要介绍了一种名为DenseAV的新型双编码器接地架构。这个架构通过观看视频，自主学习高分辨率、语义上有意义且视听对齐的特征。DenseAV能够在没有显式定位监督的情况下发现单词的“意义”和声音的“位置”，并且能够自动发现和区分这两种类型的关联。文章还提出了一种新的多头特征聚合运算符，用于对比学习，直接比较密集的图像和音频表示。此外，作者还贡献了两个新的数据集，用于通过语音和声音提示的语义分割来改进视听表示的评价。在多个数据集上，DenseAV在语音和声音提示的语义分割方面显著优于先前技术。相比于先前的最先进技术ImageBind，DenseAV在跨模态检索方面表现更佳，参数数量却少于后者的一半。\n\n搜索结果来自：\n[2406.05629] Separating the \"Chirp\" from the \"Chat\": Self-supervised Visual Grounding of Sound and Language - https://arxiv.org/abs/2406.05629"
    },
    {
        "url": "https://arxiv.org/abs/2406.07524",
        "content": "这篇文章的标题是《简单有效的掩蔽扩散语言模型》（Simple and Effective Masked Diffusion Language Models），由Subham Sekhar Sahoo和其他作者共同撰写。文章主要探讨了在语言建模领域中，扩散模型与自回归（AR）方法之间的性能差异。研究显示，简单的掩蔽离散扩散模型比之前认为的更加高效。文章提出了一种有效的训练方法，改进了掩蔽扩散模型的性能，并推导出一个简化的、经过Rao-Blackwell化的目标函数，带来了额外的改进。\n\n该目标函数形式简单，是经典掩蔽语言建模损失的一种混合，可用于训练仅编码器的语言模型，这些模型支持高效的采样器，包括能像传统语言模型一样半自回归地生成任意长度文本的采样器。在语言建模基准测试中，一系列使用现代工程实践训练的掩蔽扩散模型在扩散模型中达到了新的最先进水平，并且接近AR模型的困惑度。研究团队已在其GitHub"
    },
    {
        "url": "https://arxiv.org/abs/2406.07188",
        "content": "文章《Merging Improves Self-Critique Against Jailbreak Attacks》讨论了大型语言模型（LLMs）对抗敌意操纵，如越狱攻击的鲁棒性问题。在这项工作中，作者提出了一种方法，通过增强LLM的自我批评能力并在消毒的合成数据上进一步微调，以应对敌意提示。这种方法通过添加一个可以与原始模型合并的外部批评模型来实现，从而加强自我批评能力并提高LLMs对敌意提示的响应鲁棒性。研究结果表明，合并和自我批评的结合可以显著降低攻击者的成功率，因此提供了一种对抗越狱攻击的有前途的防御机制。\n\n搜索结果来自：\n[2406.07188] Merging Improves Self-Critique Against Jailbreak Attacks - http://export.arxiv.org/abs/2406.07188"
    },
    {
        "url": "https://arxiv.org/abs/2406.07520",
        "content": "这篇文章标题为《Neural Gaffer: Relighting Any Object via Diffusion》，由Haian Jin和其他八位作者共同撰写。文章主要提出了一种名为“Neural Gaffer”的端到端2D重光照扩散模型。这个模型能够处理任何物体的单张图片，并在任何新的环境光照条件下合成准确、高质量的重光照图片。它通过在目标环境地图上调节图像生成器来实现这一点，无需对场景进行显式分解。该方法基于预训练的扩散模型，并在合成重光照数据集上进行了微调，揭示了扩散模型对光照的内在理解。该模型在合成数据集和野外网络图像上进行了评估，并展示了其在泛化和准确性方面的优势。此外，通过与其他生成方法结合，该模型还能启用许多下游2D任务，如基于文本的重光照和对象插入。它也可以作为3D任务（如重光照辐射场）的强大重光照先验。\n\n搜索结果来自：\n[2406.07520] Neural Gaffer: Relighting Any Object via Diffusion - https://arxiv.org/abs/2406.07520"
    }
]