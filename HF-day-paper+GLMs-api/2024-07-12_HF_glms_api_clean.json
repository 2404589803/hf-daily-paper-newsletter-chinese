[
    {
        "url": "https://arxiv.org/abs/2407.08348",
        "content": "这篇文章的标题是《Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large Language Models -- The Story Goes On》。文章主要探讨了增强大型语言模型（LLMs）数学推理能力的潜在因素。作者们提出，现代LLMs在数学推理能力方面的数据扩展定律远未达到饱和，强调了模型质量如何随着数据量的增加而提高。为了支持这一观点，作者们引入了Skywork-Math模型系列，这些模型是在常见的7B LLMs上使用他们提出的2.5M实例Skywork-MathQA数据集进行监督微调（SFT）的。Skywork-Math 7B在MATH基准测试上取得了51.2%的准确率，在GSM8K基准测试上取得了83.9%的准确率，仅使用SFT数据就超过了早期版本的GPT-4。Skywork-Math模型的卓越性能归功于他们新颖的两阶段数据合成和模型SFT流程，包括三种不同的增强方法和多样化的种子问题集，确保了Skywork-MathQA数据集在各个难度级别上的数量和质量。最重要的是，作者们提供了几个实用的启示，以增强LLMs在研究和工业应用中的数学推理能力。\n\n搜索结果来自：\n[2407.08348] Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large Language Models -- The Story Goes On - https://arxiv.org/abs/2407.08348"
    },
    {
        "url": "https://arxiv.org/abs/2407.07053",
        "content": "这篇文章的标题是《Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model》，由Wenqi Zhang等作者撰写。文章主要探讨了当前大型多模态模型（LMMs）在理解抽象图像（如图表、地图或布局）和视觉推理能力方面的局限性。尽管这些模型已经能够理解自然场景和肖像照片，但它们在处理抽象图像和视觉推理任务时仍表现出初级水平。例如，它们常常难以完成读取时钟时间、理解流程图或使用路线图规划路线等简单日常任务。\n\n为了解决这一问题，作者设计了一个多模态自我指导方法，利用大型语言模型及其编码能力来合成大量的抽象图像和视觉推理指令。这种方法轻松创建了一个包含11,193个指令的多模态基准，涵盖八个视觉场景：图表、表格、模拟地图、仪表板、流程图、关系图、平面图和视觉谜题。这些基准由简单的线条和几何元素构成，揭示了大多数先进LMMs在抽象图像理解、空间关系推理和视觉元素归纳方面的不足。\n\n此外，为了验证合成数据的质量，作者使用62,476个合成的图表、表格和路线图指令对LMM进行了微调。结果显示，图表理解能力和地图导航性能得到了提升，同时也展示了这种方法对其他视觉推理任务可能带来的潜在益处。文章的代码和数据集已公开提供。\n\n搜索结果来自：\n[2407.07053] Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model - https://arxiv.org/abs/2407.07053\n[2407.07053] Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model - http://export.arxiv.org/abs/2407.07053"
    },
    {
        "url": "https://arxiv.org/abs/2407.08737",
        "content": "这篇文章的标题是《Video Diffusion Alignment via Reward Gradients》，由Mihir Prabhudesai、Russell Mendonca、Zheyang Qin、Katerina Fragkiadaki和Deepak Pathak共同撰写。文章主要探讨了视频扩散模型的基础构建方面的重大进展。这些模型通过大规模无监督数据进行训练，因此将其适应到特定的下游任务变得至关重要。文章提出了一种方法，即利用基于强大视觉判别模型学习到的偏好上的预训练奖励模型来适应视频扩散模型。这些模型包含了对生成RGB像素的密集梯度信息，这对于在复杂的搜索空间（如视频）中高效学习至关重要。研究显示，从这些奖励模型向视频扩散模型反向传播梯度，可以允许计算和样本高效地对齐视频扩散模型。文章通过多种奖励模型和视频扩散模型展示了结果，证明了这种方法在奖励查询和计算方面比之前的无梯度方法学习效率更高。\n\n搜索结果来自：\n[2407.08737] Video Diffusion Alignment via Reward Gradients - https://arxiv.org/abs/2407.08737"
    },
    {
        "url": "https://arxiv.org/abs/2407.08739",
        "content": "无法访问您提供的文章链接。因此，我无法提供关于这篇文章的具体内容。如果您能提供文章的标题、作者或摘要等更多信息，我可能能够提供一些帮助。"
    },
    {
        "url": "https://arxiv.org/abs/2407.08733",
        "content": "这篇文章的标题是《Is Your Model Really A Good Math Reasoner? Evaluating Mathematical Reasoning with Checklist》，由Zihao Zhou和其他八位作者共同撰写。文章主要探讨了如何全面定义和评估大型语言模型（LLMs）的数学能力，并反映现实世界场景中的用户体验。目前的标准测试主要集中在问题解决能力上，但这可能会导致模型过度拟合，无法准确代表真正的数学推理能力。文章提出，如果一个模型真正理解问题，它应该能够跨多种任务稳健且容易地应用。\n\n为此，作者们介绍了MATHCHECK，这是一个设计精良的清单，用于测试任务泛化和推理稳健性，以及一个自动工具，用于高效生成清单。MATHCHECK包括多种数学推理任务和稳健性测试类型，以全面评估数学推理能力和行为测试。利用MATHCHECK，作者们开发了MATHCHECK-GSM和MATHCHECK-GEO，分别评估数学文本推理和多模态推理能力，作为GSM8k、GeoQA、UniGeo和Geometry3K等基准的升级版本。\n\n文章通过MATHCHECK-GSM和MATHCHECK-GEO评估了超过20个LLMs和11个MLLMs，全面评估了它们的数学推理能力。结果显示，尽管像GPT-4o这样的前沿LLMs在清单上的各种能力继续表现出色，但许多其他模型家族显示出显著的下降。进一步实验表明，与传统的数学基准相比，MATHCHECK更能反映真正的数学能力，并且更线性地代表数学智能，从而支持作者的设计。在MATHCHECK上，可以轻松进行详细的行为分析，深入调查模型。\n\n搜索结果来自：\n[2407.08733] Is Your Model Really A Good Math Reasoner? Evaluating Mathematical Reasoning with Checklist - https://arxiv.org/abs/2407.08733"
    },
    {
        "url": "https://arxiv.org/abs/2407.08083",
        "content": "这篇文章标题为“MambaVision: A Hybrid Mamba-Transformer Vision Backbone”，提交于2024年7月10日。文章提出了一种新型的混合Mamba-Transformer视觉架构，专门针对视觉应用进行设计。研究的主要贡献包括重新设计Mamba公式，以增强其高效建模视觉特征的能力。此外，作者还对将Vision Transformers（ViT）与Mamba结合的可行性进行了全面的研究。研究结果表明，在Mamba架构的最终层配备几个自注意力块，可以显著提高模型捕捉长距离空间依赖的能力。基于这些发现，作者引入了一系列具有分层架构的MambaVision模型，以满足不同的设计标准。在ImageNet-1K数据集的图像分类任务中，MambaVision模型变体在Top-1准确性和图像吞吐量方面实现了新的最先进（SOTA）性能。在MS COCO和ADE20K数据集上的下游任务，如目标检测、实例分割和语义分割中，MambaVision也优于相似大小的架构，并展现出更优越的性能。\n\n搜索结果来自：\n[2407.08083] MambaVision: A Hybrid Mamba-Transformer Vision Backbone - https://arxiv.org/abs/2407.08083"
    },
    {
        "url": "https://arxiv.org/abs/2407.06946",
        "content": "这篇文章的标题是《Self-Recognition in Language Models》，发表在arXiv上，文章编号为2407.06946。文章的主要内容是探讨当前越来越多的应用程序依赖于少数封闭源语言模型（LMs），这种依赖可能引入新的安全风险，如果LMs发展出自识别能力的话。受人类身份验证方法的启发，作者提出了一种新颖的方法，用于评估LMs中的自识别能力，该方法使用模型生成的“安全问题”。这种测试可以外部管理，以跟踪前沿模型，因为它不需要访问内部模型参数或输出概率。\n\n作者使用这种测试来检查当前公开可用的十个最强大的开源和封闭源LMs中的自识别能力。他们广泛的实验发现，在任何检查的LM中都没有普遍或一致的自识别能力的实证证据。相反，他们的结果表明，给定一组替代品，LMs寻求选择“最佳”答案，而不考虑其来源。此外，他们发现关于哪个模型产生最佳答案的偏好在不同LMs之间是一致的。他们还揭示了在多选设置中对LMs位置偏差考虑的新见解。\n\n文章的代码可用于复现实验和复制发现，可以在GitHub上找到。\n\n搜索结果来自：\n[2407.06946] Self-Recognition in Language Models - http://export.arxiv.org/abs/2407.06946\nSelf-Recognition in Language Models - NASA/ADS - https://ui.adsabs.harvard.edu/abs/2024arXiv240706946D/abstract"
    },
    {
        "url": "https://arxiv.org/abs/2407.08296",
        "content": "无法访问您提供的文章链接 \"https://arxiv.org/abs/2407.08296\"。因此，我无法提供关于这篇文章的具体内容。如果您能提供文章的标题或摘要，我可以尝试通过其他方式帮助您获取相关信息。"
    },
    {
        "url": "https://arxiv.org/abs/2407.08683",
        "content": "这篇文章标题为“SEED-Story: Multimodal Long Story Generation with Large Language Model”，提交于2024年7月11日，属于计算机视觉和模式识别领域。文章的作者是Shuai Yang和其他六位作者。\n\n文章的主要内容是关于一种新的多模态长故事生成方法，名为SEED-Story。这种方法利用了多模态大型语言模型（MLLM）来生成扩展的多模态故事。SEED-Story模型基于MLLM的强大理解能力，能够预测文本标记和视觉标记，随后通过适配的视觉去标记器处理这些标记，以生成具有一致角色和风格的图像。此外，文章还提出了一个多模态注意力汇聚机制，使得模型能够以高度高效的方式生成多达25个序列（训练时仅用10个）的故事。\n\n文章强调，随着图像生成和开放式文本生成的显著进步，创建交织的图像-文本内容已成为一个越来越有趣的领域。多模态故事生成，其特点是以交织的方式产生叙述文本和生动的图像，已经成为一个有价值且实用的任务，具有广泛的应用。然而，这个任务带来了重大挑战，因为它需要理解文本和图像之间的复杂相互作用，并能够生成长期连贯、上下文相关的文本和视觉序列。\n\n为了训练和定量评估多模态故事生成任务，文章还介绍了一个名为StoryStream的大规模和高分辨率数据集。文章的模型、代码和数据集均已在GitHub上发布。\n\n搜索结果来自：\n[2407.08683] SEED-Story: Multimodal Long Story Generation with Large Language Model - https://arxiv.org/abs/2407.08683\n[2407.08683] SEED-Story: Multimodal Long Story Generation with Large Language Model - http://export.arxiv.org/abs/2407.08683"
    },
    {
        "url": "https://arxiv.org/abs/2407.08303",
        "content": "这篇文章的标题是《DenseFusion-1M: Merging Vision Experts for Comprehensive Multimodal Perception》，由Xiaotong Li等作者于2024年7月11日提交至arXiv。文章主要聚焦于多模态大型语言模型（MLLMs）的发展，这些模型越来越强调对各种视觉元素，包括多个对象、文本信息和空间关系的复杂理解。为了实现全面视觉感知，需要高质量的形象-文本数据集，提供多样的视觉元素和详尽的图像描述。然而，目前这种超详细数据集的稀缺性阻碍了MLLM社区的发展。\n\n文章提出了一种名为“Perceptual Fusion”的方法，使用低成本但高效的标题引擎来提供完整和准确的图像描述。具体来说，Perceptual Fusion整合了多样的感知专家作为图像先验，以提供关于视觉元素的明确信息，并采用高效的MLLM作为中心支点，模仿先进MLLMs的感知能力。研究团队从未经整理的LAION数据集中精心选择了100万张具有高度代表性的图像，并使用他们的引擎生成了密集描述，命名为DenseFusion-1M。广泛的实验验证了他们的引擎优于同类产品，生成的数据集显著提高了现有MLLMs在多种视觉-语言基准上的感知和认知能力，特别是在使用高分辨率图像作为输入时。该数据集和代码已公开提供。\n\n搜索结果来自：\n[2407.08303] DenseFusion-1M: Merging Vision Experts for Comprehensive Multimodal Perception - https://arxiv.org/abs/2407.08303\n[2407.08303] DenseFusion-1M: Merging Vision Experts for Comprehensive Multimodal Perception - http://export.arxiv.org/abs/2407.08303"
    },
    {
        "url": "https://arxiv.org/abs/2407.08713",
        "content": "这篇文章标题为《GTA: A Benchmark for General Tool Agents》，发表在arXiv上，编号为2407.08713。文章主要关注的是将大型语言模型（LLMs）与各种工具整合，以开发通用型代理（general-purpose agents）。这一整合对LLMs的工具使用能力提出了挑战。目前，现有的工具使用评估与真实世界场景之间存在明显差距，因为当前的评估通常使用AI生成的查询、单步任务、虚拟工具和仅文本的交互，这些方法无法有效地揭示代理的真实世界问题解决能力。\n\n为了解决这一问题，文章提出了GTA，一个用于通用工具代理的基准，具有三个主要方面：\n1. 真实用户查询：由人类编写的查询，具有简单的真实世界目标，但隐含工具使用，要求LLM推理合适的工具并计划解决方案步骤。\n2. 真实部署的工具：一个评估平台，配备了感知、操作、逻辑和创造力类别的工具，以评估代理的实际任务执行性能。\n3. 真实多模态输入：真实的图像文件，如空间场景、网页截图、表格、代码片段和打印/手写材料，作为查询上下文，以与真实世界场景紧密对齐。\n\n作者设计了229个真实世界的任务和可执行的工具链来评估主流LLMs。研究发现，真实世界的用户查询对现有LLMs来说是一个挑战，其中GPT-4完成了不到50%的任务，而大多数LLMs的完成率低于25%。这一评估揭示了当前LLMs在真实世界场景中工具使用能力的瓶颈，为未来通用工具代理的进步提供了方向。文章的代码和数据集可在GitHub上找到。\n\n搜索结果来自：\n[2407.08713] GTA: A Benchmark for General Tool Agents - https://arxiv.org/abs/2407.08713\n[2407.08713] GTA: A Benchmark for General Tool Agents - http://export.arxiv.org/abs/2407.08713"
    },
    {
        "url": "https://arxiv.org/abs/2407.08583",
        "content": "这篇文章的标题是《The Synergy between Data and Multi-Modal Large Language Models: A Survey from Co-Development Perspective》，由Zhen Qin和其他七位作者撰写。文章主要探讨了近年来大型语言模型（LLMs）的快速发展，尤其是多模态大型语言模型（MLLMs）。MLLMs将模态从文本扩展到更广泛的领域，因此吸引了广泛关注。文章强调，由于LLMs和MLLMs依赖于大量的模型参数和数据来实现其能力，数据的重要性日益受到关注。文章通过追踪和分析最近针对MLLMs的数据相关工作，发现模型和数据的发展不是两条独立的路径，而是相互联系的。一方面，更广泛和高品质的数据有助于提高MLLMs的性能；另一方面，MLLMs可以促进数据的发展。为了推动MLLM社区的数据模型共同发展，文章从数据模型共同发展的角度系统地回顾了与MLLMs相关的现有工作。\n\n搜索结果来自：\n[2407.08583] The Synergy between Data and Multi-Modal Large Language Models: A Survey from Co-Development Perspective - https://arxiv.org/abs/2407.08583"
    },
    {
        "url": "https://arxiv.org/abs/2407.08701",
        "content": "这篇文章的标题是《Live2Diff: Live Stream Translation via Uni-directional Attention in Video Diffusion Models》，提交于2024年7月11日。文章的主要内容是关于一种新型的视频扩散模型，名为Live2Diff。这个模型特别适用于直播视频的翻译处理。它采用了单向时间注意力机制，与现有的双向时间注意力机制不同，这使得它可以处理直播视频流，而不需要考虑未来的帧。Live2Diff通过关联当前帧与前导帧以及一些初始预热帧来确保时间的一致性和平滑性。此外，该模型还采用了一种高效的去噪方案，包括KV缓存机制和流水线处理，以支持交互式帧率的视频流翻译。实验结果表明，这种方法在时间平滑性和效率方面优于先前的方法。\n\n搜索结果来自：\n[2407.08701] Live2Diff: Live Stream Translation via Uni-directional Attention in Video Diffusion Models - http://export.arxiv.org/abs/2407.08701"
    },
    {
        "url": "https://arxiv.org/abs/2407.08551",
        "content": "这篇文章标题为《Autoregressive Speech Synthesis without Vector Quantization》，由Lingwei Meng等作者撰写。文章介绍了一种名为MELLE的新型连续值令牌基础的语言建模方法，用于文本到语音合成（TTS）。MELLE从文本条件直接自动回归地生成连续的梅尔频谱帧，绕过了原本用于音频压缩的矢量量化过程，从而避免了与梅尔频谱相比的保真度损失。文章还提出了一种新的频谱通量损失函数，并引入了变分推理来增强模型的输出多样性和鲁棒性。实验结果表明，与两阶段的编解码器语言模型相比，单阶段的MELLE在多个指标上实现了更优的性能，并提供了一种更简洁的范式。\n\n搜索结果来自：\n[2407.08551] Autoregressive Speech Synthesis without Vector Quantization - https://arxiv.org/abs/2407.08551"
    },
    {
        "url": "https://arxiv.org/abs/2407.08250",
        "content": "这篇文章的标题是《Gradient Boosting Reinforcement Learning》（梯度增强强化学习），由Benjamin Fuhrer等作者于2024年7月11日提交。文章主要探讨了神经网络（NN）在各种任务中取得了显著成果，但缺乏一些关键特征，如可解释性、对分类特征的支持以及适用于边缘设备的轻量级实现。而梯度提升树（GBT）本质上满足了这些要求。因此，GBT已成为许多实际应用和竞赛中监督学习任务的首选方法。然而，它们在在线学习场景中的应用，特别是在强化学习（RL）中，一直受到限制。\n\n为了弥补这一差距，作者们引入了梯度增强RL（GBRL），这是一个将GBT的优势扩展到RL领域的框架。通过GBRL框架，作者们实现了各种演员-评论家算法，并将其性能与NN对应算法进行了比较。受到NN中共享骨干的启发，作者们引入了一种树共享方法，用于策略和价值函数，并具有不同的学习率，从而在数百万次交互中提高了学习效率。GBRL在一系列多样化的任务中实现了具有竞争力的性能，特别是在结构化或分类特征领域表现突出。此外，作者们还展示了一个高性能的、GPU加速的实现，该实现与广泛使用的RL库无缝集成。GBRL为RL实践者扩展了工具包，展示了GBT在RL范式中的可行性和前景，特别是在由结构化或分类特征表征的领域中。\n\n这篇文章的主题涉及机器学习（cs.LG）和人工智能（cs.AI）领域。\n\n搜索结果来自：\n[2407.08250] Gradient Boosting Reinforcement Learning - https://arxiv.org/abs/2407.08250\n[2407.08250] Gradient Boosting Reinforcement Learning - http://export.arxiv.org/abs/2407.08250"
    },
    {
        "url": "https://arxiv.org/abs/2407.08642",
        "content": "这篇文章标题为《Towards Building Specialized Generalist AI with System 1 and System 2 Fusion》，由Kaiyan Zhang等人撰写。文章主要介绍了专门通用人工智能（Specialized Generalist Artificial Intelligence，简称SGAI或SGI）的概念，作为一种朝着人工通用智能（Artificial General Intelligence，简称AGI）发展的关键里程碑。与直接扩展通用能力不同，SGI被定义为在至少一个任务上超越人类专家的专门化AI，同时保留通用能力。这种融合路径使SGI能够快速实现高价值领域。文章还根据专业技能掌握程度和通用性能水平，将SGI分为三个阶段。此外，文章讨论了SGI在解决大型语言模型相关问题方面的必要性，如它们的通用性不足、专门化能力、创新不确定性以及实际应用。文章还提出了一个概念框架，用于开发集成了系统1和系统2认知处理优势的SGI。这个框架包括三个层次和四个关键组成部分，专注于提升个人能力和促进协作进化。最后，文章总结了潜在挑战并提出了未来发展方向。\n\n搜索结果来自：\n[2407.08642] Towards Building Specialized Generalist AI with System 1 and System 2 Fusion - https://arxiv.org/abs/2407.08642"
    },
    {
        "url": "https://arxiv.org/abs/2407.07176",
        "content": "这篇文章的标题是《通过任务向量定制扩展个性化美学评估》（Scaling Up Personalized Aesthetic Assessment via Task Vector Customization），由Jooyeol Yun等人撰写，提交于2024年7月9日。文章主要探讨了个性化图像美学评估的任务，旨在通过仅用户提供的一些输入来定制美学评分预测模型，以匹配个人偏好。然而，当前方法的可扩展性和泛化能力受到对昂贵策划数据库的依赖的限制。为了克服这一长期存在的可扩展性挑战，文章提出了一种独特的方法，利用现成的数据库进行一般图像美学评估和图像质量评估。具体来说，文章将每个数据库视为一个独特的图像评分回归任务，表现出不同程度的个性化潜力。通过确定代表每个数据库特定特征的任务向量的最佳组合，文章成功地创建了个性化模型。这种集成多个模型的方法允许我们利用大量的数据。文章的广泛实验证明了这种方法在泛化到以前未见的领域方面的有效性——这是以前的方法难以实现的挑战，使其非常适合实际应用场景。这种新颖的方法通过为个性化美学评估提供可扩展的解决方案，并确立未来研究的高标准，显著推进了该领域。\n\n搜索结果来自：\n[2407.07176] Scaling Up Personalized Aesthetic Assessment via Task Vector Customization - https://arxiv.org/abs/2407.07176\n[2407.07176] Scaling Up Personalized Aesthetic Assessment via Task Vector Customization - http://export.arxiv.org/abs/2407.07176"
    },
    {
        "url": "https://arxiv.org/abs/2407.08726",
        "content": "无法访问您提供的文章链接，因此我无法提供有关该文章的具体内容。如果您能提供文章的标题或摘要，我可以尝试通过其他方式帮助您获取相关信息。"
    },
    {
        "url": "https://arxiv.org/abs/2407.08680",
        "content": "这篇文章标题为《Generalizable Implicit Motion Modeling for Video Frame Interpolation》，由Zujin Guo等人撰写，提交于2024年7月11日。文章主要讨论了视频帧插值（VFI）中的运动建模问题。现有的方法要么考虑双向流的线性组合，要么直接预测给定时间戳的双边流，而没有探索有利的运动先验，因此无法有效地模拟现实世界视频中的时空动态。为了解决这个限制，研究引入了一种名为“Generalizable Implicit Motion Modeling”（GIMM）的新方法，这是一种新颖有效的VFI运动建模方法。具体来说，为了使GIMM成为一个有效的运动建模范式，研究者设计了一个运动编码流程，从预训练的流估计器提取的双向流中模拟时空运动潜在变量，有效地表示输入特定的运动先验。然后，通过自适应坐标基础的神经网络隐式预测两个相邻输入帧内的任意时间步的光流，输入包括时空坐标和运动潜在变量。GIMM可以与现有的基于流的VFI工作无缝集成，无需进一步修改。研究表明，GIMM在VFI基准测试上的表现优于当前最先进的技术。\n\n搜索结果来自：\n[2407.08680] Generalizable Implicit Motion Modeling for Video Frame Interpolation - https://arxiv.org/abs/2407.08680"
    },
    {
        "url": "https://arxiv.org/abs/2407.08447",
        "content": "这篇文章的标题是《WildGaussians: 3D Gaussian Splatting in the Wild》，由Jonáš Kulhánek和其他四位作者共同撰写，提交于2024年7月11日。文章主要探讨了3D场景重建领域的一个新方法，特别关注在复杂、不可控环境下的应用。\n\n文章的核心内容是关于3D高斯散射（3D Gaussian Splatting，简称3DGS）技术。这项技术最近出现，提供与NeRF（Neural Radiance Fields）相似的光照质量和实时渲染速度。NeRF因其在3D场景重建中的照片级真实感效果而占据主导地位。然而，这两种方法在处理控制良好的3D场景时表现出色，但在面对野外数据——如遮挡、动态物体和变化的光照条件——时仍面临挑战。\n\n为了解决这一问题，作者们提出了WildGaussians方法，这是一种处理3DGS中遮挡和外观变化的新方法。通过利用健壮的DINO特征和在3DGS内集成外观建模模块，WildGaussians在处理野外数据方面实现了最先进的结果。文章还展示了WildGaussians在保持3DGS的实时渲染速度的同时，超越了3DGS和NeRF基线，所有这些都包含在一个简单的架构框架内。\n\n总的来说，这篇文章提出了一个新颖的方法，用于改进3D高斯散射技术在复杂环境下的性能，特别是在处理遮挡和外观变化方面。这对于3D场景重建和计算机视觉领域是一个重要的贡献。\n\n更多详情可以在原文链接中查看：[WildGaussians: 3D Gaussian Splatting in the Wild](https://arxiv.org/abs/2407.08447)。\n\n搜索结果来自：\n[2407.08447] WildGaussians: 3D Gaussian Splatting in the Wild - https://arxiv.org/abs/2407.08447\n[2407.08447] WildGaussians: 3D Gaussian Splatting in the Wild - http://export.arxiv.org/abs/2407.08447\nWildGaussians: 3D Gaussian Splatting in the Wild - https://www.niftycent.com/external-link/781272/d/wildgaussians-3d-gaussian-splatting-in-the-wild"
    },
    {
        "url": "https://arxiv.org/abs/2407.08711",
        "content": "这篇文章标题为《OmniNOCS: A unified NOCS dataset and model for 3D lifting of 2D objects》，由Akshay Krishnan和其他四位作者共同撰写，提交于2024年7月11日。文章主要提出了一个名为OmniNOCS的大规模单目数据集，该数据集包含3D标准化对象坐标空间（NOCS）图、对象掩码和室内外场景的3D边界框注释。OmniNOCS拥有比现有NOCS数据集（如NOCS-Real275、Wild6D）多20倍的对象类别和200倍的对象实例。\n\n文章还介绍了一个基于变换器的新型单目NOCS预测模型（NOCSformer），该模型能够从2D对象检测中预测准确的NOCS、实例掩码和姿态。这是第一个能够在提示2D框时泛化到广泛类别范围的NOCS模型。作者在3D定向边界框预测任务上评估了他们的模型，结果显示其性能与最先进的3D检测方法（如Cube R-CNN）相当。与其他3D检测方法不同，该模型还提供了详细且准确的3D对象形状和分割。\n\n此外，文章还提出了一个基于OmniNOCS的NOCS预测新基准，希望这将成为该领域未来工作的有用基线。该数据集和代码将发布在项目网站上。\n\n搜索结果来自：\n[2407.08711] OmniNOCS: A unified NOCS dataset and model for 3D lifting of 2D objects - https://arxiv.org/abs/2407.08711\n[2407.08711] OmniNOCS: A unified NOCS dataset and model for 3D lifting of 2D objects - http://export.arxiv.org/abs/2407.08711"
    }
]