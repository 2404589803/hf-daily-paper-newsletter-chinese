[
    {
        "url": "https://arxiv.org/abs/2406.16793",
        "content": "文章《Adam-mini: Use Fewer Learning Rates To Gain More》提出了一种名为Adam-mini的优化器。这个优化器在性能上与AdamW相当或更优，同时内存占用减少了45%到50%。Adam-mini通过减少Adam中的学习率资源来降低内存使用。研究发现，通过仔细地按照所提出的Hessian结构原则将参数划分为块，并为每个参数块分配单个但合适的学习率，可以无害地移除超过90%的学习率。此外，对于每个参数块，存在一个高质量的学习率，可以超越Adam的性能。文章还提供了一种成本效益高的方法来寻找好的学习率，并提出了Adam-mini。在多种语言模型上的实验验证了Adam-mini在预训练、监督微调和RLHF方面与AdamW相当或更优的性能。Adam-mini的减少内存占用也减轻了GPU和CPU之间的通信开销，从而提高了吞吐量。例如，在预训练Llama2-7B时，Adam-mini比AdamW的吞吐量提高了49.6%，在预训练上节省了33%的挂钟时间。\n\n搜索结果来自：\n[2406.16793] Adam-mini: Use Fewer Learning Rates To Gain More - https://web3.arxiv.org/abs/2406.16793"
    },
    {
        "url": "https://arxiv.org/abs/2406.18521",
        "content": "这篇文章标题为“CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs”，提交于2024年6月26日。文章的主要研究内容是关于多模态大型语言模型（Multimodal Large Language Models, MLLMs）在现实世界任务中的应用，特别是它们在理解图表方面的能力。文章指出，现有的数据集往往关注于过于简化和同质化的图表，以及基于模板的问题，这导致了对进展的过度乐观评估。\n\n在这项工作中，作者们提出了CharXiv，这是一个全面的评估套件，包含了来自arXiv论文的2,323个自然、具有挑战性和多样化的图表。CharXiv包括两种类型的问题：1）关于检查基本图表元素的描述性问题；2）需要综合图表中复杂视觉元素信息的推理问题。为了确保质量，所有的图表和问题都是由人类专家手工挑选、策划和验证的。\n\n研究结果显示，最强的专有模型（即GPT-4o）和最强的开源模型（即InternVL Chat V1.5）之间在推理技能上存在显著的、之前被低估的差距，前者达到47.1%的准确率，而后者只有29.2%。所有模型在图表理解能力方面都远远落后于人类的80.5%表现。文章强调，现有的MLLMs在图表理解能力上存在不足。作者们希望CharXiv能够通过提供一个更现实和可靠的进展衡量标准，促进未来对MLLM图表理解的研究。\n\n这篇文章的发表对于推动多模态大型语言模型在图表理解方面的研究具有重要意义，特别是在提高这些模型在处理复杂和多样化图表数据时的性能方面。\n\n搜索结果来自：\n[2406.18521] CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs - https://arxiv.org/abs/2406.18521\n[2406.18521] CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs - http://export.arxiv.org/abs/2406.18521"
    },
    {
        "url": "https://arxiv.org/abs/2406.18082",
        "content": "这篇文章标题为“Octo-planner: On-device Language Model for Planner-Action Agents”，主要探讨了AI代理在各个领域的重要性，特别是在自主决策和问题解决方面。文章提出了一种高效的设备上Planner-Action框架，该框架将规划和行动执行分为两个独立的部分：一个基于Phi-3 Mini的规划代理，这是一个针对边缘设备优化的38亿参数LLM，以及一个使用章鱼模型进行功能执行的行动代理。规划代理首先响应用户查询，将任务分解为一系列子步骤，然后由行动代理执行。为了在资源受限的设备上优化性能，文章采用了模型微调而不是上下文学习，从而减少了计算成本和能源消耗，同时提高了响应时间。该方法涉及使用GPT-4生成基于可用功能的多样规划查询和响应，并进行后续验证以确保数据质量。文章在领域内测试环境中实现了97%的成功率。为了应对多领域规划挑战，文章还开发了一种多LoRA训练方法，该方法合并了针对不同功能子集训练的LoRAs的权重。这种处理复杂、多领域查询的方法在保持资源受限设备上计算效率的同时，也具有灵活性。文章还开源了模型权重，以支持进一步的研究。"
    },
    {
        "url": "https://arxiv.org/abs/2406.18219",
        "content": "无法访问您提供的文章链接 \"https://arxiv.org/abs/2406.18219\"。因此，我无法提供这篇文章的具体内容。您可以尝试直接访问这个链接，或者提供文章的标题和作者，这样我可能能找到更多相关信息。"
    },
    {
        "url": "https://arxiv.org/abs/2406.16341",
        "content": "文章《EHRCon: Dataset for Checking Consistency between Unstructured Notes and Structured Tables in Electronic Health Records》的编号是2406.16341。这篇文章讨论了电子健康记录（EHRs）中结构化表格和非结构化笔记之间的一致性检查。EHRs是存储患者全面医疗记录的重要工具，它们结合了结构化数据（如药物）和详细的临床笔记（如医生笔记）。这些元素对于直接数据检索和提供深入、上下文相关的患者护理洞察至关重要。然而，由于不直观的EHR系统设计和人为错误，它们经常存在差异，给患者安全带来严重风险。为了解决这个问题，作者开发了EHRCon，这是一个新的数据集和任务，专门用于确保EHRs中结构化表格和非结构化笔记之间的数据一致性。EHRCon是使用MIMIC-III EHR数据集与医疗专业人员合作制作的，包括对3,943个实体在105份临床笔记中进行手动注释，以检查与数据库条目的一致性。EHRCon有两个版本，一个是使用原始的MIMIC-III模式，另一个是使用OMOP CDM模式，以提高其适用性和泛化性。此外，利用大型语言模型的能力，作者介绍了CheckEHR，这是一个新的框架，用于验证临床笔记和数据库表格之间的一致性。CheckEHR使用一个八阶段过程，并在少量样本和无样本设置中显示出有希望的结果。代码可以在提供的GitHub URL上找到。\n\n搜索结果来自：\n[2406.16341] EHRCon: Dataset for Checking Consistency between Unstructured Notes and Structured Tables in Electronic Health Records - http://export.arxiv.org/abs/2406.16341"
    },
    {
        "url": "https://arxiv.org/abs/2406.17294",
        "content": "这篇文章标题为“Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models”，提交于2024年6月25日，并在6月26日进行了修订。文章的主要研究内容是关于提升大型语言模型（LLMs）在多模态数学推理方面的能力。\n\n在这项研究中，作者们指出，尽管大型语言模型在文本数学问题解决方面表现出色，但现有的开源图像指令微调数据集，由于每张图像包含的问答对数量有限，并没有充分利用视觉信息来增强多模态LLMs（MLLMs）的数学推理能力。为了弥补这一不足，研究团队收集了来自24个现有数据集的40,000张高质量图像及其问答对，并合成了320,000个新的问答对，创建了MathV360K数据集。这个数据集不仅扩大了多模态数学问题的广度，还增加了其深度。\n\n文章接着介绍了Math-LLaVA，这是一个基于LLaVA-1.5模型，并使用MathV360K数据集进行微调的新型模型。这种方法显著提升了LLaVA-1.5的多模态数学推理能力，实现了19个百分点的提升，并在MathVista的minitest分割上与GPT-4V的表现相当。此外，Math-LLaVA还展示了更强的泛化能力，在MMMU基准测试上取得了显著改进。研究强调了数据集多样性和合成在推进MLLMs数学推理能力方面的重要性。相关代码和数据可在提供的GitHub链接中找到。\n\n搜索结果来自：\n[2406.17294] Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models - https://arxiv.org/abs/2406.17294\n[2406.17294] Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models - http://export.arxiv.org/abs/2406.17294"
    },
    {
        "url": "https://arxiv.org/abs/2406.18495",
        "content": "无法访问您提供的文章链接 \"https://arxiv.org/abs/2406.18495\"。因此，我无法提供关于这篇文章的具体内容。如果您能提供文章的标题或摘要，我可以尝试通过其他方式来查找相关信息。"
    },
    {
        "url": "https://arxiv.org/abs/2406.18530",
        "content": "这篇文章的标题是《MatchTime: Towards Automatic Soccer Game Commentary Generation》，提交于2024年6月26日。文章的主要内容是关于构建一个自动化的足球比赛解说模型，以改善观众的观看体验。文章的主要贡献包括：\n\n1. 观察现有数据集中的视频-文本不对齐问题，作者手动为49场比赛标注时间戳，建立了一个更稳健的足球比赛解说生成基准，称为SN-Caption-test-align。\n2. 提出了一个多模态时间对齐管道，自动纠正和过滤现有数据集，创建了一个高质量的足球比赛解说训练数据集，称为MatchTime。\n3. 基于精心策划的数据集，训练了一个自动解说生成模型，名为MatchVoice。广泛的实验和消融研究证明了作者对齐管道的有效性，以及在该数据集上训练的模型在解说生成方面的最先进性能，展示了更好的对齐可以显著提高下游任务的性能。\n\n这篇文章属于计算机科学领域，具体是计算机视觉和模式识别（cs.CV）方向。文章的作者是Jiayuan Rao和其他四位作者。\n\n搜索结果来自：\n[2406.18530] MatchTime: Towards Automatic Soccer Game Commentary Generation - https://arxiv.org/abs/2406.18530\n[2406.18530] MatchTime: Towards Automatic Soccer Game Commentary Generation - http://export.arxiv.org/abs/2406.18530"
    },
    {
        "url": "https://arxiv.org/abs/2406.18532",
        "content": "文章《Symbolic Learning Enables Self-Evolving Agents》探讨了人工智能社区如何通过开发“语言代理”来寻求实现人工通用智能（AGI）的途径。这些语言代理是涉及提示技术和工具使用方法的大型语言模型（LLMs）的复杂管道。虽然语言代理在许多现实世界任务中展示了令人印象深刻的能力，但当前语言代理研究的一个基本限制是它们以模型为中心或以工程为中心。这意味着语言代理在提示、工具和管道方面的进展需要人类专家的大量手动工程努力，而不是自动从数据中学习。\n\n在这项工作中，作者们引入了代理符号学习，这是一个系统框架，使语言代理能够使用符号优化器以数据为中心的方式自行优化。具体来说，作者们将代理视为符号网络，其中的可学习权重由提示、工具以及它们堆叠的方式定义。代理符号学习旨在通过模仿连接主义学习中的两个基本算法：反向传播和梯度下降，来优化语言代理内的符号网络。与处理数字权重不同，代理符号学习使用权重、损失和梯度的自然语言模拟。作者们在标准基准和复杂现实世界任务上进行了概念验证实验，并展示了代理符号学习使语言代理能够在创建和部署后自行更新，从而实现“自我进化代理”。\n\n搜索结果来自：\n[2406.18532] Symbolic Learning Enables Self-Evolving Agents - https://arxiv.org/abs/2406.18532\n[2406.18532] Symbolic Learning Enables Self-Evolving Agents - http://export.arxiv.org/abs/2406.18532"
    },
    {
        "url": "https://arxiv.org/abs/2406.16979",
        "content": "这篇文章的标题是《Understanding and Diagnosing Deep Reinforcement Learning》，作者是Ezgi Korkmaz。文章主要探讨了深度神经网络策略在多种设置中的应用，从生物技术到自动化金融系统。然而，由于深度神经网络的高度非凸性和复杂性，使用深度神经网络来近似价值函数会导致对决策边界稳定性的担忧，尤其是对于政策决策制定对不可分辨的、非鲁棒特征的敏感性。文章介绍了一种理论基础上方法，系统地分析深度神经策略决策边界在时间和空间上的不稳定方向。通过在Arcade学习环境（ALE）中的实验，文章展示了这种方法在识别相关不稳定方向以及测量样本变化如何重塑神经策略景观中的敏感方向方面的有效性。此外，文章还展示了与标准训练相比，最先进的鲁棒训练技术会导致学习到的不稳定方向明显更大，振荡幅度也更大。作者认为，这些结果揭示了强化学习策略决策过程的基本特性，并有助于构建可靠和鲁棒的深度神经网络策略。\n\n搜索结果来自：\n[2406.16979] Understanding and Diagnosing Deep Reinforcement Learning - https://arxiv.org/abs/2406.16979"
    },
    {
        "url": "https://arxiv.org/abs/2406.18522",
        "content": "这篇文章标题为《ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of Text-to-Time-lapse Video Generation》，主要提出了一个新的文本到视频（T2V）生成基准，名为ChronoMagic-Bench。这个基准旨在评估T2V模型（例如Sora和Lumiere）在生成延时视频方面的时序和变化能力。与现有专注于生成视频的视觉质量和文本相关性的基准不同，ChronoMagic-Bench侧重于模型生成具有显著变化幅度和时序连贯性的延时视频的能力。该基准通过自由形式的文本查询，探测T2V模型在物理、生物和化学方面的能力。为此，ChronoMagic-Bench引入了1,649个提示词和真实世界的视频作为参考，分为四大类延时视频：生物、人为创造、气象和物理现象，进一步细分为75个子类别。这种分类全面评估了模型处理多样化和复杂变化的能力。为了准确地将人类偏好与基准对齐，文章还引入了两个新的自动度量标准，MTScore和CHScore，以评估视频的变化属性和时序连贯性。MTScore衡量变化幅度，反映随时间的变化程度，而CHScore评估时序连贯性，确保生成的视频保持逻辑进展和连续性。基于ChronoMagic-Bench，文章对十个代表性的T2V模型进行了全面的手动评估，揭示了它们在不同类别提示词中的优点和缺点，并提供了一个全面的评估框架，解决了视频生成研究中的当前差距。此外，文章还创建了一个大规模的ChronoMagic-Pro数据集，包含460k对720p延时视频和详细描述，确保高的物理相关性和大的变化幅度。\n\n搜索结果来自：\n[2406.18522] ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of Text-to-Time-lapse Video Generation - https://arxiv.org/abs/2406.18522"
    },
    {
        "url": "https://arxiv.org/abs/2406.17565",
        "content": "这篇文章标题为“MemServe: Context Caching for Disaggregated LLM Serving with Elastic Memory Pool”，主要讨论了大型语言模型（LLM）服务从无状态系统转变为有状态系统的趋势。文章提出了一种名为MemServe的统一系统，该系统整合了请求间和请求内的优化。MemServe引入了MemPool，一个弹性内存池，用于管理分布式内存和KV缓存。通过MemPool API，MemServe首次将上下文缓存与分散推理结合，由一个全局调度器支持，该调度器通过基于全局提示树的局部感知策略增强缓存重用。实验表明，MemServe显著提高了作业完成时间和首次响应时间。\n\n搜索结果来自：\n[2406.17565] MemServe: Context Caching for Disaggregated LLM Serving with Elastic Memory Pool - https://arxiv.org/abs/2406.17565"
    },
    {
        "url": "https://arxiv.org/abs/2406.18510",
        "content": "这篇文章标题为“WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models”，由Liwei Jiang等作者撰写。文章介绍了一种名为WildTeaming的自动LLM（大型语言模型）安全性红队测试框架。该框架通过挖掘野生环境中的用户与聊天机器人的交互，发现了5.7K种独特的越狱策略新集群，并组合多种策略以系统探索新的越狱方法。与之前通过招募人类工作者、基于梯度优化或与LLM迭代修订进行红队测试的工作不同，WildTeaming调查了那些没有特别指示破坏系统的聊天机器人用户的越狱行为。WildTeaming揭示了前沿LLM之前未识别的安全漏洞，导致比最新越狱方法更多样化、成功的对抗性攻击，成功率高达4.6倍。文章还创建了一个名为WildJailbreak的大规模开源合成安全数据集，包含262K个直接请求和对抗性越狱的提示-响应对，以改善LLM的安全行为，同时保持其通用能力。\n\n搜索结果来自：\n[2406.18510] WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models - https://arxiv.org/abs/2406.18510"
    }
]