{
  "title": "Enabling Scalable Oversight via Self-Evolving Critic",
  "summary": "Despite their remarkable performance, the development of Large Language\nModels (LLMs) faces a critical challenge in scalable oversight: providing\neffective feedback for tasks where human evaluation is difficult or where LLMs\noutperform humans. While there is growing interest in using LLMs for critique,\ncurrent approaches still rely on human annotations or more powerful models,\nleaving the issue of enhancing critique capabilities without external\nsupervision unresolved. We introduce SCRIT (Self-evolving CRITic), a framework\nthat enables genuine self-evolution of critique abilities. Technically, SCRIT\nself-improves by training on synthetic data, generated by a contrastive-based\nself-critic that uses reference solutions for step-by-step critique, and a\nself-validation mechanism that ensures critique quality through correction\noutcomes. Implemented with Qwen2.5-72B-Instruct, one of the most powerful LLMs,\nSCRIT achieves up to a 10.3\\% improvement on critique-correction and error\nidentification benchmarks. Our analysis reveals that SCRIT's performance scales\npositively with data and model size, outperforms alternative approaches, and\nbenefits critically from its self-validation component.",
  "translation": "标题：通过自我演进的批评者实现可扩展的监督\n\n摘要：尽管大型语言模型（LLMs）表现出色，但其发展面临一个关键挑战：在人类评估困难或LLMs超越人类的任务中提供有效反馈。尽管越来越多的人对使用LLMs进行批评感兴趣，但当前方法仍依赖于人类注释或更强大的模型，这使得在没有外部监督的情况下增强批评能力的问题仍未解决。我们引入了SCRIT（自我演进的批评者），一个能够实现批评能力真正自我演进的框架。技术上，SCRIT通过训练合成数据进行自我改进，这些数据由基于对比的自我批评者生成，该批评者使用参考解决方案进行逐步批评，并通过自我验证机制确保批评质量，该机制通过纠正结果来保证批评质量。使用最强大的LLM之一Qwen2.5-72B-Instruct实现，SCRIT在批评纠正和错误识别基准上实现了高达10.3%的改进。我们的分析表明，SCRIT的性能随着数据和模型规模的增加而正向扩展，优于其他方法，并且其自我验证组件对其性能至关重要。",
  "url": "https://huggingface.co/papers/2501.05727"
}