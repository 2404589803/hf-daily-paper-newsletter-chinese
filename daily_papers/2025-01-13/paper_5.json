{
  "title": "Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains",
  "summary": "Large language models (LLMs) have achieved remarkable performance in recent\nyears but are fundamentally limited by the underlying training data. To improve\nmodels beyond the training data, recent works have explored how LLMs can be\nused to generate synthetic data for autonomous self-improvement. However,\nsuccessive steps of self-improvement can reach a point of diminishing returns.\nIn this work, we propose a complementary approach towards self-improvement\nwhere finetuning is applied to a multiagent society of language models. A group\nof language models, all starting from the same base model, are independently\nspecialized by updating each one using data generated through multiagent\ninteractions among the models. By training each model on independent sets of\ndata, we illustrate how this approach enables specialization across models and\ndiversification over the set of models. As a result, our overall system is able\nto preserve diverse reasoning chains and autonomously improve over many more\nrounds of fine-tuning than single-agent self-improvement methods. We\nquantitatively illustrate the efficacy of the approach across a wide suite of\nreasoning tasks.",
  "translation": "标题：多智能体微调：通过多样化推理链实现自我改进\n\n摘要：近年来，大型语言模型（LLMs）取得了显著的性能提升，但其根本上受限于基础训练数据。为了在训练数据之外进一步提升模型性能，最近的研究探索了如何利用LLMs生成合成数据以实现自主的自我改进。然而，连续的自我改进步骤可能会达到收益递减的临界点。在本研究中，我们提出了一种互补的自我改进方法，即对语言模型的多智能体社会进行微调。一组从相同基础模型出发的语言模型，通过模型间的多智能体交互生成的数据进行独立更新，从而实现各自的专门化。通过在每个模型上训练独立的数据集，我们展示了这种方法如何在模型间实现专门化并在模型集合中实现多样化。因此，我们的整体系统能够保留多样化的推理链，并在比单智能体自我改进方法更多的微调轮次中实现自主改进。我们通过一系列推理任务定量地展示了该方法的有效性。",
  "url": "https://huggingface.co/papers/2501.05707"
}