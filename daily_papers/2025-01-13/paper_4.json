{
  "title": "LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs",
  "summary": "Reasoning is a fundamental capability for solving complex multi-step\nproblems, particularly in visual contexts where sequential step-wise\nunderstanding is essential. Existing approaches lack a comprehensive framework\nfor evaluating visual reasoning and do not emphasize step-wise problem-solving.\nTo this end, we propose a comprehensive framework for advancing step-by-step\nvisual reasoning in large language models (LMMs) through three key\ncontributions. First, we introduce a visual reasoning benchmark specifically\ndesigned to evaluate multi-step reasoning tasks. The benchmark presents a\ndiverse set of challenges with eight different categories ranging from complex\nvisual perception to scientific reasoning with over 4k reasoning steps in\ntotal, enabling robust evaluation of LLMs' abilities to perform accurate and\ninterpretable visual reasoning across multiple steps. Second, we propose a\nnovel metric that assesses visual reasoning quality at the granularity of\nindividual steps, emphasizing both correctness and logical coherence. The\nproposed metric offers deeper insights into reasoning performance compared to\ntraditional end-task accuracy metrics. Third, we present a new multimodal\nvisual reasoning model, named LlamaV-o1, trained using a multi-step curriculum\nlearning approach, where tasks are progressively organized to facilitate\nincremental skill acquisition and problem-solving. The proposed LlamaV-o1 is\ndesigned for multi-step reasoning and learns step-by-step through a structured\ntraining paradigm. Extensive experiments show that our LlamaV-o1 outperforms\nexisting open-source models and performs favorably against close-source\nproprietary models. Compared to the recent Llava-CoT, our LlamaV-o1 achieves an\naverage score of 67.3 with an absolute gain of 3.8\\% across six benchmarks\nwhile being 5 times faster during inference scaling. Our benchmark, model, and\ncode are publicly available.",
  "translation": "标题：LlamaV-o1：重新思考大语言模型中的逐步视觉推理\n\n摘要：推理是解决复杂多步骤问题的基本能力，特别是在需要顺序逐步理解的视觉环境中。现有方法缺乏一个全面的框架来评估视觉推理，并且没有强调逐步问题解决。为此，我们提出了一个全面的框架，通过三个关键贡献来推进大语言模型（LMMs）中的逐步视觉推理。首先，我们引入了一个专门设计用于评估多步骤推理任务的视觉推理基准。该基准提出了八种不同类别的多样化挑战，从复杂的视觉感知到科学推理，总共包含超过4k个推理步骤，从而能够对LLMs在多个步骤中执行准确且可解释的视觉推理的能力进行稳健评估。其次，我们提出了一种新颖的指标，该指标在单个步骤的粒度上评估视觉推理的质量，强调正确性和逻辑一致性。与传统的最终任务准确性指标相比，所提出的指标提供了对推理性能的更深入洞察。第三，我们提出了一种新的多模态视觉推理模型，名为LlamaV-o1，该模型使用多步骤课程学习方法进行训练，其中任务逐步组织以促进增量技能获取和问题解决。所提出的LlamaV-o1专为多步骤推理设计，并通过结构化的训练范式逐步学习。大量实验表明，我们的LlamaV-o1优于现有的开源模型，并在与闭源专有模型的比较中表现良好。与最近的Llava-CoT相比，我们的LlamaV-o1在六个基准测试中平均得分为67.3，绝对增益为3.8%，同时在推理扩展期间速度快5倍。我们的基准、模型和代码均已公开。",
  "url": "https://huggingface.co/papers/2501.06186"
}