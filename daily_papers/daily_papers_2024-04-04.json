[{"paper":{"id":"2404.02258","authors":[{"_id":"660e2a251714de5d5bd49825","name":"David Raposo","hidden":false},{"_id":"660e2a251714de5d5bd49826","name":"Sam Ritter","hidden":false},{"_id":"660e2a251714de5d5bd49827","name":"Blake Richards","hidden":false},{"_id":"660e2a251714de5d5bd49828","name":"Timothy Lillicrap","hidden":false},{"_id":"660e2a251714de5d5bd49829","name":"Peter Conway Humphreys","hidden":false},{"_id":"660e2a251714de5d5bd4982a","name":"Adam Santoro","hidden":false}],"publishedAt":"2024-04-02T19:28:11.000Z","title":"Mixture-of-Depths: Dynamically allocating compute in transformer-based\n  language models","summary":"Transformer-based language models spread FLOPs uniformly across input\nsequences. In this work we demonstrate that transformers can instead learn to\ndynamically allocate FLOPs (or compute) to specific positions in a sequence,\noptimising the allocation along the sequence for different layers across the\nmodel depth. Our method enforces a total compute budget by capping the number\nof tokens (k) that can participate in the self-attention and MLP computations\nat a given layer. The tokens to be processed are determined by the network\nusing a top-k routing mechanism. Since k is defined a priori, this simple\nprocedure uses a static computation graph with known tensor sizes, unlike other\nconditional computation techniques. Nevertheless, since the identities of the\nk tokens are fluid, this method can expend FLOPs non-uniformly across the\ntime and model depth dimensions. Thus, compute expenditure is entirely\npredictable in sum total, but dynamic and context-sensitive at the token-level.\nNot only do models trained in this way learn to dynamically allocate compute,\nthey do so efficiently. These models match baseline performance for equivalent\nFLOPS and wall-clock times to train, but require a fraction of the FLOPs per\nforward pass, and can be upwards of 50\\% faster to step during post-training\nsampling.","upvotes":33},"publishedAt":"2024-04-04T04:18:46.165Z","title":"Mixture-of-Depths: Dynamically allocating compute in transformer-based language models","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.02258.png","numComments":0},{"paper":{"id":"2404.02905","authors":[{"_id":"660e129262d63ad000c149de","user":{"avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1625767908838-noauth.jpeg?w=200&h=200&f=face","isPro":false,"fullname":"Keyu Tian","user":"keyu-tian","type":"user"},"name":"Keyu Tian","status":"claimed_verified","statusLastChangedAt":"2024-04-04T07:55:43.864Z","hidden":false},{"_id":"660e129262d63ad000c149df","name":"Yi Jiang","hidden":false},{"_id":"660e129262d63ad000c149e0","name":"Zehuan Yuan","hidden":false},{"_id":"660e129262d63ad000c149e1","name":"Bingyue Peng","hidden":false},{"_id":"660e129262d63ad000c149e2","name":"Liwei Wang","hidden":false}],"publishedAt":"2024-04-03T17:59:53.000Z","title":"Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale\n  Prediction","summary":"We present Visual AutoRegressive modeling (VAR), a new generation paradigm\nthat redefines the autoregressive learning on images as coarse-to-fine\n\"next-scale prediction\" or \"next-resolution prediction\", diverging from the\nstandard raster-scan \"next-token prediction\". This simple, intuitive\nmethodology allows autoregressive (AR) transformers to learn visual\ndistributions fast and generalize well: VAR, for the first time, makes AR\nmodels surpass diffusion transformers in image generation. On ImageNet 256x256\nbenchmark, VAR significantly improve AR baseline by improving Frechet inception\ndistance (FID) from 18.65 to 1.80, inception score (IS) from 80.4 to 356.4,\nwith around 20x faster inference speed. It is also empirically verified that\nVAR outperforms the Diffusion Transformer (DiT) in multiple dimensions\nincluding image quality, inference speed, data efficiency, and scalability.\nScaling up VAR models exhibits clear power-law scaling laws similar to those\nobserved in LLMs, with linear correlation coefficients near -0.998 as solid\nevidence. VAR further showcases zero-shot generalization ability in downstream\ntasks including image in-painting, out-painting, and editing. These results\nsuggest VAR has initially emulated the two important properties of LLMs:\nScaling Laws and zero-shot task generalization. We have released all models and\ncodes to promote the exploration of AR/VAR models for visual generation and\nunified learning.","upvotes":31},"publishedAt":"2024-04-04T03:07:13.758Z","title":"Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.02905.png","numComments":0},{"paper":{"id":"2404.02575","authors":[{"_id":"660e2ce3e23148df7c927a69","user":{"avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64c8f4cec547ed5243ebd0a8/MiOH5YbMg8Gh9KYlQsLmX.jpeg?w=200&h=200&f=face","isPro":false,"fullname":"Hyungjoo Chae","user":"hyungjoochae","type":"user"},"name":"Hyungjoo Chae","status":"claimed_verified","statusLastChangedAt":"2024-04-04T07:56:48.638Z","hidden":false},{"_id":"660e2ce3e23148df7c927a6a","name":"Yeonghyeon Kim","hidden":false},{"_id":"660e2ce3e23148df7c927a6b","name":"Seungone Kim","hidden":false},{"_id":"660e2ce3e23148df7c927a6c","name":"Kai Tzu-iunn Ong","hidden":false},{"_id":"660e2ce3e23148df7c927a6d","name":"Beong-woo Kwak","hidden":false},{"_id":"660e2ce3e23148df7c927a6e","name":"Moohyeon Kim","hidden":false},{"_id":"660e2ce3e23148df7c927a6f","name":"Seonghwan Kim","hidden":false},{"_id":"660e2ce3e23148df7c927a70","name":"Taeyoon Kwon","hidden":false},{"_id":"660e2ce3e23148df7c927a71","name":"Jiwan Chung","hidden":false},{"_id":"660e2ce3e23148df7c927a72","name":"Youngjae Yu","hidden":false},{"_id":"660e2ce3e23148df7c927a73","name":"Jinyoung Yeo","hidden":false}],"publishedAt":"2024-04-03T08:49:11.000Z","title":"Language Models as Compilers: Simulating Pseudocode Execution Improves\n  Algorithmic Reasoning in Language Models","summary":"Algorithmic reasoning refers to the ability to understand the complex\npatterns behind the problem and decompose them into a sequence of reasoning\nsteps towards the solution. Such nature of algorithmic reasoning makes it a\nchallenge for large language models (LLMs), even though they have demonstrated\npromising performance in other reasoning tasks. Within this context, some\nrecent studies use programming languages (e.g., Python) to express the\nnecessary logic for solving a given instance/question (e.g.,\nProgram-of-Thought) as inspired by their strict and precise syntaxes. However,\nit is non-trivial to write an executable code that expresses the correct logic\non the fly within a single inference call. Also, the code generated\nspecifically for an instance cannot be reused for others, even if they are from\nthe same task and might require identical logic to solve. This paper presents\nThink-and-Execute, a novel framework that decomposes the reasoning process of\nlanguage models into two steps. (1) In Think, we discover a task-level logic\nthat is shared across all instances for solving a given task and then express\nthe logic with pseudocode; (2) In Execute, we further tailor the generated\npseudocode to each instance and simulate the execution of the code. With\nextensive experiments on seven algorithmic reasoning tasks, we demonstrate the\neffectiveness of Think-and-Execute. Our approach better improves LMs' reasoning\ncompared to several strong baselines performing instance-specific reasoning\n(e.g., CoT and PoT), suggesting the helpfulness of discovering task-level\nlogic. Also, we show that compared to natural language, pseudocode can better\nguide the reasoning of LMs, even though they are trained to follow natural\nlanguage instructions.","upvotes":28},"publishedAt":"2024-04-04T04:30:28.331Z","title":"Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.02575.png","numComments":1},{"paper":{"id":"2404.02883","authors":[{"_id":"660e17b6bc5f12490fd40f46","name":"Hao Li","hidden":false},{"_id":"660e17b6bc5f12490fd40f47","name":"Yang Zou","hidden":false},{"_id":"660e17b6bc5f12490fd40f48","name":"Ying Wang","hidden":false},{"_id":"660e17b6bc5f12490fd40f49","name":"Orchid Majumder","hidden":false},{"_id":"660e17b6bc5f12490fd40f4a","user":{"avatarUrl":"/avatars/0e17d4f424d67a5983f2915d52c26860.svg","isPro":false,"fullname":"Yusheng Xie","user":"yushx","type":"user"},"name":"Yusheng Xie","status":"claimed_verified","statusLastChangedAt":"2024-04-04T07:55:54.856Z","hidden":false},{"_id":"660e17b6bc5f12490fd40f4b","name":"R. Manmatha","hidden":false},{"_id":"660e17b6bc5f12490fd40f4c","name":"Ashwin Swaminathan","hidden":false},{"_id":"660e17b6bc5f12490fd40f4d","name":"Zhuowen Tu","hidden":false},{"_id":"660e17b6bc5f12490fd40f4e","name":"Stefano Ermon","hidden":false},{"_id":"660e17b6bc5f12490fd40f4f","name":"Stefano Soatto","hidden":false}],"publishedAt":"2024-04-03T17:34:28.000Z","title":"On the Scalability of Diffusion-based Text-to-Image Generation","summary":"Scaling up model and data size has been quite successful for the evolution of\nLLMs. However, the scaling law for the diffusion based text-to-image (T2I)\nmodels is not fully explored. It is also unclear how to efficiently scale the\nmodel for better performance at reduced cost. The different training settings\nand expensive training cost make a fair model comparison extremely difficult.\nIn this work, we empirically study the scaling properties of diffusion based\nT2I models by performing extensive and rigours ablations on scaling both\ndenoising backbones and training set, including training scaled UNet and\nTransformer variants ranging from 0.4B to 4B parameters on datasets upto 600M\nimages. For model scaling, we find the location and amount of cross attention\ndistinguishes the performance of existing UNet designs. And increasing the\ntransformer blocks is more parameter-efficient for improving text-image\nalignment than increasing channel numbers. We then identify an efficient UNet\nvariant, which is 45% smaller and 28% faster than SDXL's UNet. On the data\nscaling side, we show the quality and diversity of the training set matters\nmore than simply dataset size. Increasing caption density and diversity\nimproves text-image alignment performance and the learning efficiency. Finally,\nwe provide scaling functions to predict the text-image alignment performance as\nfunctions of the scale of model size, compute and dataset size.","upvotes":13},"publishedAt":"2024-04-04T03:00:07.972Z","title":"On the Scalability of Diffusion-based Text-to-Image Generation","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.02883.png","numComments":0},{"paper":{"id":"2404.02733","authors":[{"_id":"660e1590076a6255659f92b0","user":{"avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669187672174-637745113a63a2983ffbde13.jpeg?w=200&h=200&f=face","isPro":false,"fullname":"Haofan Wang","user":"wanghaofan","type":"user"},"name":"Haofan Wang","status":"extracted_pending","statusLastChangedAt":"2024-04-04T02:51:01.915Z","hidden":false},{"_id":"660e1590076a6255659f92b1","name":"Qixun Wang","hidden":false},{"_id":"660e1590076a6255659f92b2","name":"Xu Bai","hidden":false},{"_id":"660e1590076a6255659f92b3","name":"Zekui Qin","hidden":false},{"_id":"660e1590076a6255659f92b4","name":"Anthony Chen","hidden":false}],"publishedAt":"2024-04-03T13:34:09.000Z","title":"InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image\n  Generation","summary":"Tuning-free diffusion-based models have demonstrated significant potential in\nthe realm of image personalization and customization. However, despite this\nnotable progress, current models continue to grapple with several complex\nchallenges in producing style-consistent image generation. Firstly, the concept\nof style is inherently underdetermined, encompassing a multitude of elements\nsuch as color, material, atmosphere, design, and structure, among others.\nSecondly, inversion-based methods are prone to style degradation, often\nresulting in the loss of fine-grained details. Lastly, adapter-based approaches\nfrequently require meticulous weight tuning for each reference image to achieve\na balance between style intensity and text controllability. In this paper, we\ncommence by examining several compelling yet frequently overlooked\nobservations. We then proceed to introduce InstantStyle, a framework designed\nto address these issues through the implementation of two key strategies: 1) A\nstraightforward mechanism that decouples style and content from reference\nimages within the feature space, predicated on the assumption that features\nwithin the same space can be either added to or subtracted from one another. 2)\nThe injection of reference image features exclusively into style-specific\nblocks, thereby preventing style leaks and eschewing the need for cumbersome\nweight tuning, which often characterizes more parameter-heavy designs.Our work\ndemonstrates superior visual stylization outcomes, striking an optimal balance\nbetween the intensity of style and the controllability of textual elements. Our\ncodes will be available at https://github.com/InstantStyle/InstantStyle.","upvotes":12},"publishedAt":"2024-04-04T02:51:01.929Z","title":"InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.02733.png","numComments":0},{"paper":{"id":"2404.02893","authors":[{"_id":"660e1b8583bce1a92c00b84d","name":"Yifan Xu","hidden":false},{"_id":"660e1b8583bce1a92c00b84e","name":"Xiao Liu","hidden":false},{"_id":"660e1b8583bce1a92c00b84f","name":"Xinghan Liu","hidden":false},{"_id":"660e1b8583bce1a92c00b850","name":"Zhenyu Hou","hidden":false},{"_id":"660e1b8583bce1a92c00b851","name":"Yueyan Li","hidden":false},{"_id":"660e1b8583bce1a92c00b852","name":"Xiaohan Zhang","hidden":false},{"_id":"660e1b8583bce1a92c00b853","name":"Zihan Wang","hidden":false},{"_id":"660e1b8583bce1a92c00b854","name":"Aohan Zeng","hidden":false},{"_id":"660e1b8583bce1a92c00b855","name":"Zhengxiao Du","hidden":false},{"_id":"660e1b8583bce1a92c00b856","name":"Wenyi Zhao","hidden":false},{"_id":"660e1b8583bce1a92c00b857","name":"Jie Tang","hidden":false},{"_id":"660e1b8583bce1a92c00b858","name":"Yuxiao Dong","hidden":false}],"publishedAt":"2024-04-03T17:51:18.000Z","title":"ChatGLM-Math: Improving Math Problem-Solving in Large Language Models\n  with a Self-Critique Pipeline","summary":"Large language models (LLMs) have shown excellent mastering of human\nlanguage, but still struggle in real-world applications that require\nmathematical problem-solving. While many strategies and datasets to enhance\nLLMs' mathematics are developed, it remains a challenge to simultaneously\nmaintain and improve both language and mathematical capabilities in deployed\nLLM systems.In this work, we tailor the Self-Critique pipeline, which addresses\nthe challenge in the feedback learning stage of LLM alignment. We first train a\ngeneral Math-Critique model from the LLM itself to provide feedback signals.\nThen, we sequentially employ rejective fine-tuning and direct preference\noptimization over the LLM's own generations for data collection. Based on\nChatGLM3-32B, we conduct a series of experiments on both academic and our newly\ncreated challenging dataset, MathUserEval. Results show that our pipeline\nsignificantly enhances the LLM's mathematical problem-solving while still\nimproving its language ability, outperforming LLMs that could be two times\nlarger. Related techniques have been deployed to\nChatGLM\\url{https://chatglm.cn}, an online serving LLM. Related\nevaluation dataset and scripts are released at\nhttps://github.com/THUDM/ChatGLM-Math.","upvotes":10},"publishedAt":"2024-04-04T03:16:22.286Z","title":"ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.02893.png","numComments":2},{"paper":{"id":"2404.02747","authors":[{"_id":"660e1a88e8763d8d1da50fa6","name":"Wentian Zhang","hidden":false},{"_id":"660e1a88e8763d8d1da50fa7","name":"Haozhe Liu","hidden":false},{"_id":"660e1a88e8763d8d1da50fa8","name":"Jinheng Xie","hidden":false},{"_id":"660e1a88e8763d8d1da50fa9","name":"Francesco Faccio","hidden":false},{"_id":"660e1a88e8763d8d1da50faa","name":"Mike Zheng Shou","hidden":false},{"_id":"660e1a88e8763d8d1da50fab","name":"Jürgen Schmidhuber","hidden":false}],"publishedAt":"2024-04-03T13:44:41.000Z","title":"Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion\n  Models","summary":"This study explores the role of cross-attention during inference in\ntext-conditional diffusion models. We find that cross-attention outputs\nconverge to a fixed point after few inference steps. Accordingly, the time\npoint of convergence naturally divides the entire inference process into two\nstages: an initial semantics-planning stage, during which, the model relies on\ncross-attention to plan text-oriented visual semantics, and a subsequent\nfidelity-improving stage, during which the model tries to generate images from\npreviously planned semantics. Surprisingly, ignoring text conditions in the\nfidelity-improving stage not only reduces computation complexity, but also\nmaintains model performance. This yields a simple and training-free method\ncalled TGATE for efficient generation, which caches the cross-attention output\nonce it converges and keeps it fixed during the remaining inference steps. Our\nempirical study on the MS-COCO validation set confirms its effectiveness. The\nsource code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE.","upvotes":9},"publishedAt":"2024-04-04T03:12:11.781Z","title":"Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion Models","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.02747.png","numComments":0},{"paper":{"id":"2404.02514","authors":[{"_id":"660e44a31a20b667fcb380d9","name":"Yisheng He","hidden":false},{"_id":"660e44a31a20b667fcb380da","name":"Weihao Yuan","hidden":false},{"_id":"660e44a31a20b667fcb380db","name":"Siyu Zhu","hidden":false},{"_id":"660e44a31a20b667fcb380dc","name":"Zilong Dong","hidden":false},{"_id":"660e44a31a20b667fcb380dd","name":"Liefeng Bo","hidden":false},{"_id":"660e44a31a20b667fcb380de","name":"Qixing Huang","hidden":false}],"publishedAt":"2024-04-03T07:07:02.000Z","title":"Freditor: High-Fidelity and Transferable NeRF Editing by Frequency\n  Decomposition","summary":"This paper enables high-fidelity, transferable NeRF editing by frequency\ndecomposition. Recent NeRF editing pipelines lift 2D stylization results to 3D\nscenes while suffering from blurry results, and fail to capture detailed\nstructures caused by the inconsistency between 2D editings. Our critical\ninsight is that low-frequency components of images are more\nmultiview-consistent after editing compared with their high-frequency parts.\nMoreover, the appearance style is mainly exhibited on the low-frequency\ncomponents, and the content details especially reside in high-frequency parts.\nThis motivates us to perform editing on low-frequency components, which results\nin high-fidelity edited scenes. In addition, the editing is performed in the\nlow-frequency feature space, enabling stable intensity control and novel scene\ntransfer. Comprehensive experiments conducted on photorealistic datasets\ndemonstrate the superior performance of high-fidelity and transferable NeRF\nediting. The project page is at https://aigc3d.github.io/freditor.","upvotes":8},"publishedAt":"2024-04-04T06:11:51.781Z","title":"Freditor: High-Fidelity and Transferable NeRF Editing by Frequency Decomposition","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.02514.png","numComments":0}]