[{"paper":{"id":"2403.17887","authors":[{"_id":"6603925f759b2e212d08b6bb","user":{"avatarUrl":"/avatars/1705d81d0b16d992833069150796b1d6.svg","isPro":false,"fullname":"Andrey Gromov","user":"gromovand","type":"user"},"name":"Andrey Gromov","status":"extracted_pending","statusLastChangedAt":"2024-03-27T03:28:32.198Z","hidden":false},{"_id":"6603925f759b2e212d08b6bc","user":{"avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/GI73-WJ5WjdB-KUUNGkdv.jpeg?w=200&h=200&f=face","isPro":false,"fullname":"Kushal Tirumala","user":"kushaltirumala","type":"user"},"name":"Kushal Tirumala","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:01:03.657Z","hidden":false},{"_id":"6603925f759b2e212d08b6bd","user":{"avatarUrl":"/avatars/ba34c4fb3b99de36522d0074725750c1.svg","isPro":false,"fullname":"Hassan Shapourian","user":"hassansh","type":"user"},"name":"Hassan Shapourian","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:01:09.596Z","hidden":false},{"_id":"6603925f759b2e212d08b6be","user":{"avatarUrl":"/avatars/c42a72a448dcb9e8bb3a0b41d1b6397a.svg","isPro":false,"fullname":"Paolo Glorioso","user":"pglo","type":"user"},"name":"Paolo Glorioso","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:01:15.478Z","hidden":false},{"_id":"6603925f759b2e212d08b6bf","user":{"avatarUrl":"/avatars/aaa21574bb8aacdc58dc412f5292a260.svg","isPro":false,"fullname":"Dan Roberts","user":"danintheory","type":"user"},"name":"Daniel A. Roberts","status":"extracted_confirmed","statusLastChangedAt":"2024-03-27T03:33:10.995Z","hidden":false}],"publishedAt":"2024-03-26T17:20:04.000Z","title":"The Unreasonable Ineffectiveness of the Deeper Layers","summary":"We empirically study a simple layer-pruning strategy for popular families of\nopen-weight pretrained LLMs, finding minimal degradation of performance on\ndifferent question-answering benchmarks until after a large fraction (up to\nhalf) of the layers are removed. To prune these models, we identify the optimal\nblock of layers to prune by considering similarity across layers; then, to\n\"heal\" the damage, we perform a small amount of finetuning. In particular, we\nuse parameter-efficient finetuning (PEFT) methods, specifically quantization\nand Low Rank Adapters (QLoRA), such that each of our experiments can be\nperformed on a single A100 GPU. From a practical perspective, these results\nsuggest that layer pruning methods can complement other PEFT strategies to\nfurther reduce computational resources of finetuning on the one hand, and can\nimprove the memory and latency of inference on the other hand. From a\nscientific perspective, the robustness of these LLMs to the deletion of layers\nimplies either that current pretraining methods are not properly leveraging the\nparameters in the deeper layers of the network or that the shallow layers play\na critical role in storing knowledge.","upvotes":26},"publishedAt":"2024-03-27T03:28:32.217Z","title":"The Unreasonable Ineffectiveness of the Deeper Layers","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.17887.png","numComments":1},{"paper":{"id":"2403.17888","authors":[{"_id":"66039c0f172fcd3243991d3e","name":"Binbin Huang","hidden":false},{"_id":"66039c0f172fcd3243991d3f","name":"Zehao Yu","hidden":false},{"_id":"66039c0f172fcd3243991d40","user":{"avatarUrl":"/avatars/94c2722e81675b59c55d8ad1ebd0d97c.svg","isPro":false,"fullname":"ANPEI CHEN","user":"apchen","type":"user"},"name":"Anpei Chen","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:50:33.568Z","hidden":false},{"_id":"66039c0f172fcd3243991d41","user":{"avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1644998139538-noauth.jpeg?w=200&h=200&f=face","isPro":false,"fullname":"Andreas Geiger","user":"andreas-geiger","type":"user"},"name":"Andreas Geiger","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:50:39.032Z","hidden":false},{"_id":"66039c0f172fcd3243991d42","name":"Shenghua Gao","hidden":false}],"publishedAt":"2024-03-26T17:21:24.000Z","title":"2D Gaussian Splatting for Geometrically Accurate Radiance Fields","summary":"3D Gaussian Splatting (3DGS) has recently revolutionized radiance field\nreconstruction, achieving high quality novel view synthesis and fast rendering\nspeed without baking. However, 3DGS fails to accurately represent surfaces due\nto the multi-view inconsistent nature of 3D Gaussians. We present 2D Gaussian\nSplatting (2DGS), a novel approach to model and reconstruct geometrically\naccurate radiance fields from multi-view images. Our key idea is to collapse\nthe 3D volume into a set of 2D oriented planar Gaussian disks. Unlike 3D\nGaussians, 2D Gaussians provide view-consistent geometry while modeling\nsurfaces intrinsically. To accurately recover thin surfaces and achieve stable\noptimization, we introduce a perspective-accurate 2D splatting process\nutilizing ray-splat intersection and rasterization. Additionally, we\nincorporate depth distortion and normal consistency terms to further enhance\nthe quality of the reconstructions. We demonstrate that our differentiable\nrenderer allows for noise-free and detailed geometry reconstruction while\nmaintaining competitive appearance quality, fast training speed, and real-time\nrendering. Our code will be made publicly available.","upvotes":13},"publishedAt":"2024-03-27T04:09:54.882Z","title":"2D Gaussian Splatting for Geometrically Accurate Radiance Fields","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.17888.png","numComments":1},{"paper":{"id":"2403.17297","authors":[{"_id":"660396b0126abce2e4d76dc8","user":{"avatarUrl":"/avatars/77f87c52f864e2dd1fbeccd7ced87283.svg","isPro":false,"fullname":"Zheng Cai","user":"zigzagcai","type":"user"},"name":"Zheng Cai","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:02:18.432Z","hidden":false},{"_id":"660396b0126abce2e4d76dc9","name":"Maosong Cao","hidden":false},{"_id":"660396b0126abce2e4d76dca","name":"Haojiong Chen","hidden":false},{"_id":"660396b0126abce2e4d76dcb","name":"Kai Chen","hidden":false},{"_id":"660396b0126abce2e4d76dcc","name":"Keyu Chen","hidden":false},{"_id":"660396b0126abce2e4d76dcd","name":"Xin Chen","hidden":false},{"_id":"660396b0126abce2e4d76dce","name":"Xun Chen","hidden":false},{"_id":"660396b0126abce2e4d76dcf","user":{"avatarUrl":"/avatars/3cdafe03a8295124636347d15a099aaf.svg","isPro":false,"fullname":"Zehui Chen","user":"lovesnowbest","type":"user"},"name":"Zehui Chen","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:13:40.632Z","hidden":false},{"_id":"660396b0126abce2e4d76dd0","name":"Zhi Chen","hidden":false},{"_id":"660396b0126abce2e4d76dd1","name":"Pei Chu","hidden":false},{"_id":"660396b0126abce2e4d76dd2","name":"Xiaoyi Dong","hidden":false},{"_id":"660396b0126abce2e4d76dd3","user":{"avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676546883247-noauth.png?w=200&h=200&f=face","isPro":false,"fullname":"HAODONG DUAN","user":"KennyUTC","type":"user"},"name":"Haodong Duan","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:13:09.092Z","hidden":false},{"_id":"660396b0126abce2e4d76dd4","name":"Qi Fan","hidden":false},{"_id":"660396b0126abce2e4d76dd5","user":{"avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/629ef8544313a7c1dd671130/i5xfHIgELcuO1Ew19ebTw.png?w=200&h=200&f=face","isPro":false,"fullname":"Zhaoye Fei","user":"ngc7293","type":"user"},"name":"Zhaoye Fei","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:15:09.534Z","hidden":false},{"_id":"660396b0126abce2e4d76dd6","name":"Yang Gao","hidden":false},{"_id":"660396b0126abce2e4d76dd7","name":"Jiaye Ge","hidden":false},{"_id":"660396b0126abce2e4d76dd8","name":"Chenya Gu","hidden":false},{"_id":"660396b0126abce2e4d76dd9","user":{"avatarUrl":"/avatars/7ff57a0d2ec93a8b9aec980a2f6e94fd.svg","isPro":false,"fullname":"Yuzhe Gu","user":"Tracygu","type":"user"},"name":"Yuzhe Gu","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:15:36.085Z","hidden":false},{"_id":"660396b0126abce2e4d76dda","name":"Tao Gui","hidden":false},{"_id":"660396b0126abce2e4d76ddb","name":"Aijia Guo","hidden":false},{"_id":"660396b0126abce2e4d76ddc","user":{"avatarUrl":"/avatars/a85635d886c7f157b6723dec5c01c030.svg","isPro":false,"fullname":"Qipeng Guo","user":"QipengGuo","type":"user"},"name":"Qipeng Guo","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:16:02.474Z","hidden":false},{"_id":"660396b0126abce2e4d76ddd","user":{"avatarUrl":"/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg","isPro":false,"fullname":"Conghui He","user":"conghui","type":"user"},"name":"Conghui He","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:16:09.185Z","hidden":false},{"_id":"660396b0126abce2e4d76dde","name":"Yingfan Hu","hidden":false},{"_id":"660396b0126abce2e4d76ddf","name":"Ting Huang","hidden":false},{"_id":"660396b0126abce2e4d76de0","name":"Tao Jiang","hidden":false},{"_id":"660396b0126abce2e4d76de1","name":"Penglong Jiao","hidden":false},{"_id":"660396b0126abce2e4d76de2","name":"Zhenjiang Jin","hidden":false},{"_id":"660396b0126abce2e4d76de3","name":"Zhikai Lei","hidden":false},{"_id":"660396b0126abce2e4d76de4","name":"Jiaxing Li","hidden":false},{"_id":"660396b0126abce2e4d76de5","name":"Jingwen Li","hidden":false},{"_id":"660396b0126abce2e4d76de6","name":"Linyang Li","hidden":false},{"_id":"660396b0126abce2e4d76de7","name":"Shuaibin Li","hidden":false},{"_id":"660396b0126abce2e4d76de8","name":"Wei Li","hidden":false},{"_id":"660396b0126abce2e4d76de9","name":"Yining Li","hidden":false},{"_id":"660396b0126abce2e4d76dea","name":"Hongwei Liu","hidden":false},{"_id":"660396b0126abce2e4d76deb","name":"Jiangning Liu","hidden":false},{"_id":"660396b0126abce2e4d76dec","name":"Jiawei Hong","hidden":false},{"_id":"660396b0126abce2e4d76ded","name":"Kaiwen Liu","hidden":false},{"_id":"660396b0126abce2e4d76dee","name":"Kuikun Liu","hidden":false},{"_id":"660396b0126abce2e4d76def","name":"Xiaoran Liu","hidden":false},{"_id":"660396b0126abce2e4d76df0","name":"Chengqi Lv","hidden":false},{"_id":"660396b0126abce2e4d76df1","name":"Haijun Lv","hidden":false},{"_id":"660396b0126abce2e4d76df2","name":"Kai Lv","hidden":false},{"_id":"660396b0126abce2e4d76df3","name":"Li Ma","hidden":false},{"_id":"660396b0126abce2e4d76df4","name":"Runyuan Ma","hidden":false},{"_id":"660396b0126abce2e4d76df5","name":"Zerun Ma","hidden":false},{"_id":"660396b0126abce2e4d76df6","name":"Wenchang Ning","hidden":false},{"_id":"660396b0126abce2e4d76df7","name":"Linke Ouyang","hidden":false},{"_id":"660396b0126abce2e4d76df8","name":"Jiantao Qiu","hidden":false},{"_id":"660396b0126abce2e4d76df9","name":"Yuan Qu","hidden":false},{"_id":"660396b0126abce2e4d76dfa","name":"Fukai Shang","hidden":false},{"_id":"660396b0126abce2e4d76dfb","name":"Yunfan Shao","hidden":false},{"_id":"660396b0126abce2e4d76dfc","name":"Demin Song","hidden":false},{"_id":"660396b0126abce2e4d76dfd","name":"Zifan Song","hidden":false},{"_id":"660396b0126abce2e4d76dfe","name":"Zhihao Sui","hidden":false},{"_id":"660396b0126abce2e4d76dff","name":"Peng Sun","hidden":false},{"_id":"660396b0126abce2e4d76e00","name":"Yu Sun","hidden":false},{"_id":"660396b0126abce2e4d76e01","name":"Huanze Tang","hidden":false},{"_id":"660396b0126abce2e4d76e02","name":"Bin Wang","hidden":false},{"_id":"660396b0126abce2e4d76e03","name":"Guoteng Wang","hidden":false},{"_id":"660396b0126abce2e4d76e04","name":"Jiaqi Wang","hidden":false},{"_id":"660396b0126abce2e4d76e05","name":"Jiayu Wang","hidden":false},{"_id":"660396b0126abce2e4d76e06","name":"Rui Wang","hidden":false},{"_id":"660396b0126abce2e4d76e07","name":"Yudong Wang","hidden":false},{"_id":"660396b0126abce2e4d76e08","name":"Ziyi Wang","hidden":false},{"_id":"660396b0126abce2e4d76e09","name":"Xingjian Wei","hidden":false},{"_id":"660396b0126abce2e4d76e0a","name":"Qizhen Weng","hidden":false},{"_id":"660396b0126abce2e4d76e0b","name":"Fan Wu","hidden":false},{"_id":"660396b0126abce2e4d76e0c","name":"Yingtong Xiong","hidden":false},{"_id":"660396b0126abce2e4d76e0d","name":"Chao Xu","hidden":false},{"_id":"660396b0126abce2e4d76e0e","name":"Ruiliang Xu","hidden":false},{"_id":"660396b0126abce2e4d76e0f","name":"Hang Yan","hidden":false},{"_id":"660396b0126abce2e4d76e10","name":"Yirong Yan","hidden":false},{"_id":"660396b0126abce2e4d76e11","name":"Xiaogui Yang","hidden":false},{"_id":"660396b0126abce2e4d76e12","name":"Haochen Ye","hidden":false},{"_id":"660396b0126abce2e4d76e13","name":"Huaiyuan Ying","hidden":false},{"_id":"660396b0126abce2e4d76e14","name":"Jia Yu","hidden":false},{"_id":"660396b0126abce2e4d76e15","name":"Jing Yu","hidden":false},{"_id":"660396b0126abce2e4d76e16","name":"Yuhang Zang","hidden":false},{"_id":"660396b0126abce2e4d76e17","name":"Chuyu Zhang","hidden":false},{"_id":"660396b0126abce2e4d76e18","name":"Li Zhang","hidden":false},{"_id":"660396b0126abce2e4d76e19","name":"Pan Zhang","hidden":false},{"_id":"660396b0126abce2e4d76e1a","name":"Peng Zhang","hidden":false},{"_id":"660396b0126abce2e4d76e1b","name":"Ruijie Zhang","hidden":false},{"_id":"660396b0126abce2e4d76e1c","name":"Shuo Zhang","hidden":false},{"_id":"660396b0126abce2e4d76e1d","user":{"avatarUrl":"/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg","isPro":false,"fullname":"Songyang Zhang","user":"zsytony","type":"user"},"name":"Songyang Zhang","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:26:00.336Z","hidden":false},{"_id":"660396b0126abce2e4d76e1e","name":"Wenjian Zhang","hidden":false},{"_id":"660396b0126abce2e4d76e1f","user":{"avatarUrl":"/avatars/18958b8406d1ce492b54c1c839f18c54.svg","isPro":false,"fullname":"Wenwei Zhang","user":"ZwwWayne","type":"user"},"name":"Wenwei Zhang","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:25:05.529Z","hidden":false},{"_id":"660396b0126abce2e4d76e20","name":"Xingcheng Zhang","hidden":false},{"_id":"660396b0126abce2e4d76e21","name":"Xinyue Zhang","hidden":false},{"_id":"660396b0126abce2e4d76e22","name":"Hui Zhao","hidden":false},{"_id":"660396b0126abce2e4d76e23","name":"Qian Zhao","hidden":false},{"_id":"660396b0126abce2e4d76e24","name":"Xiaomeng Zhao","hidden":false},{"_id":"660396b0126abce2e4d76e25","user":{"avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f04fb94a788ed1dd89daf4/5XNPePED6fTMH0SAM2lhj.jpeg?w=200&h=200&f=face","isPro":false,"fullname":"zhoufengzhe","user":"Leymore","type":"user"},"name":"Fengzhe Zhou","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:24:10.251Z","hidden":false},{"_id":"660396b0126abce2e4d76e26","user":{"avatarUrl":"/avatars/c79eb36c4ad96286afda834e260a1c09.svg","isPro":false,"fullname":"zhouzaida","user":"zhouzaida","type":"user"},"name":"Zaida Zhou","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:23:58.339Z","hidden":false},{"_id":"660396b0126abce2e4d76e27","user":{"avatarUrl":"/avatars/8b4d9d847a9e115c3da8cad629bd0a41.svg","isPro":false,"fullname":"Jingming Zhuo","user":"JingmingZ","type":"user"},"name":"Jingming Zhuo","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:23:44.582Z","hidden":false},{"_id":"660396b0126abce2e4d76e28","user":{"avatarUrl":"/avatars/e5fff05f21cdea4e5aebc8ba426fac29.svg","isPro":false,"fullname":"Yicheng Zou","user":"RowitZou","type":"user"},"name":"Yicheng Zou","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:23:36.551Z","hidden":false},{"_id":"660396b0126abce2e4d76e29","user":{"avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1632381702899-61457b8deff2c9fdb4de4988.jpeg?w=200&h=200&f=face","isPro":false,"fullname":"Xipeng Qiu","user":"xpqiu","type":"user"},"name":"Xipeng Qiu","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:23:28.522Z","hidden":false},{"_id":"660396b0126abce2e4d76e2a","name":"Yu Qiao","hidden":false},{"_id":"660396b0126abce2e4d76e2b","user":{"avatarUrl":"/avatars/3db090e101b916d9256d0d3e043db71d.svg","isPro":false,"fullname":"Dahua Lin","user":"lindahua","type":"user"},"name":"Dahua Lin","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:23:14.735Z","hidden":false}],"publishedAt":"2024-03-26T00:53:24.000Z","title":"InternLM2 Technical Report","summary":"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has\nsparked discussions on the advent of Artificial General Intelligence (AGI).\nHowever, replicating such advancements in open-source models has been\nchallenging. This paper introduces InternLM2, an open-source LLM that\noutperforms its predecessors in comprehensive evaluations across 6 dimensions\nand 30 benchmarks, long-context modeling, and open-ended subjective evaluations\nthrough innovative pre-training and optimization techniques. The pre-training\nprocess of InternLM2 is meticulously detailed, highlighting the preparation of\ndiverse data types including text, code, and long-context data. InternLM2\nefficiently captures long-term dependencies, initially trained on 4k tokens\nbefore advancing to 32k tokens in pre-training and fine-tuning stages,\nexhibiting remarkable performance on the 200k ``Needle-in-a-Haystack\" test.\nInternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel\nConditional Online Reinforcement Learning from Human Feedback (COOL RLHF)\nstrategy that addresses conflicting human preferences and reward hacking. By\nreleasing InternLM2 models in different training stages and model sizes, we\nprovide the community with insights into the model's evolution.","upvotes":10},"publishedAt":"2024-03-27T03:46:57.465Z","title":"InternLM2 Technical Report","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.17297.png","numComments":0},{"paper":{"id":"2403.17920","authors":[{"_id":"6603909490c62bb38f073051","user":{"avatarUrl":"/avatars/4cd941cdca6dd829fdc9cb3fb788a99c.svg","isPro":false,"fullname":"Sherwin Bahmani","user":"sherwinbahmani","type":"user"},"name":"Sherwin Bahmani","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:47:20.297Z","hidden":false},{"_id":"6603909490c62bb38f073052","user":{"avatarUrl":"/avatars/e4b4c7dddc5cf962ade45d8bdfa10b1a.svg","isPro":false,"fullname":"Xian Liu","user":"alvinliu0","type":"user"},"name":"Xian Liu","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:47:36.756Z","hidden":false},{"_id":"6603909490c62bb38f073053","name":"Yifan Wang","hidden":false},{"_id":"6603909490c62bb38f073054","user":{"avatarUrl":"/avatars/d7e97a16cfee39e1e50d7a5b747876f1.svg","isPro":false,"fullname":"Ivan Skorokhodov","user":"universome","type":"user"},"name":"Ivan Skorokhodov","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:48:10.431Z","hidden":false},{"_id":"6603909490c62bb38f073055","name":"Victor Rong","hidden":false},{"_id":"6603909490c62bb38f073056","user":{"avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png?w=200&h=200&f=face","isPro":false,"fullname":"Ziwei Liu","user":"liuziwei7","type":"user"},"name":"Ziwei Liu","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:48:20.683Z","hidden":false},{"_id":"6603909490c62bb38f073057","user":{"avatarUrl":"/avatars/50ebc02caa86c37276cc3c5021a0b822.svg","isPro":false,"fullname":"Xihui Liu","user":"XihuiLiu","type":"user"},"name":"Xihui Liu","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:48:26.669Z","hidden":false},{"_id":"6603909490c62bb38f073058","name":"Jeong Joon Park","hidden":false},{"_id":"6603909490c62bb38f073059","name":"Sergey Tulyakov","hidden":false},{"_id":"6603909490c62bb38f07305a","name":"Gordon Wetzstein","hidden":false},{"_id":"6603909490c62bb38f07305b","name":"Andrea Tagliasacchi","hidden":false},{"_id":"6603909490c62bb38f07305c","name":"David B. Lindell","hidden":false}],"publishedAt":"2024-03-26T17:55:11.000Z","title":"TC4D: Trajectory-Conditioned Text-to-4D Generation","summary":"Recent techniques for text-to-4D generation synthesize dynamic 3D scenes\nusing supervision from pre-trained text-to-video models. However, existing\nrepresentations for motion, such as deformation models or time-dependent neural\nrepresentations, are limited in the amount of motion they can generate-they\ncannot synthesize motion extending far beyond the bounding box used for volume\nrendering. The lack of a more flexible motion model contributes to the gap in\nrealism between 4D generation methods and recent, near-photorealistic video\ngeneration models. Here, we propose TC4D: trajectory-conditioned text-to-4D\ngeneration, which factors motion into global and local components. We represent\nthe global motion of a scene's bounding box using rigid transformation along a\ntrajectory parameterized by a spline. We learn local deformations that conform\nto the global trajectory using supervision from a text-to-video model. Our\napproach enables the synthesis of scenes animated along arbitrary trajectories,\ncompositional scene generation, and significant improvements to the realism and\namount of generated motion, which we evaluate qualitatively and through a user\nstudy. Video results can be viewed on our website:\nhttps://sherwinbahmani.github.io/tc4d.","upvotes":10},"publishedAt":"2024-03-27T03:20:55.255Z","title":"TC4D: Trajectory-Conditioned Text-to-4D Generation","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.17920.png","numComments":0},{"paper":{"id":"2403.17898","authors":[{"_id":"66039957c530ce2222488e3c","name":"Kerui Ren","hidden":false},{"_id":"66039957c530ce2222488e3d","user":{"avatarUrl":"/avatars/6d88aa68eccfa07d2009df405f957fd7.svg","isPro":false,"fullname":"Jiang Lihan","user":"lhjiang","type":"user"},"name":"Lihan Jiang","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:54:06.682Z","hidden":false},{"_id":"66039957c530ce2222488e3e","user":{"avatarUrl":"/avatars/ffe72a82aaf42c80eb988344415672ba.svg","isPro":false,"fullname":"Tao Lu","user":"Isaaclt","type":"user"},"name":"Tao Lu","status":"claimed_verified","statusLastChangedAt":"2024-03-27T09:59:21.565Z","hidden":false},{"_id":"66039957c530ce2222488e3f","name":"Mulin Yu","hidden":false},{"_id":"66039957c530ce2222488e40","name":"Linning Xu","hidden":false},{"_id":"66039957c530ce2222488e41","name":"Zhangkai Ni","hidden":false},{"_id":"66039957c530ce2222488e42","user":{"avatarUrl":"/avatars/add48d7e3790a6b500d6c451ef8b0f75.svg","isPro":false,"fullname":"Intelligent Digital Creation","user":"BoDai","type":"user"},"name":"Bo Dai","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:54:51.152Z","hidden":false}],"publishedAt":"2024-03-26T17:39:36.000Z","title":"Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D\n  Gaussians","summary":"The recent 3D Gaussian splatting (3D-GS) has shown remarkable rendering\nfidelity and efficiency compared to NeRF-based neural scene representations.\nWhile demonstrating the potential for real-time rendering, 3D-GS encounters\nrendering bottlenecks in large scenes with complex details due to an excessive\nnumber of Gaussian primitives located within the viewing frustum. This\nlimitation is particularly noticeable in zoom-out views and can lead to\ninconsistent rendering speeds in scenes with varying details. Moreover, it\noften struggles to capture the corresponding level of details at different\nscales with its heuristic density control operation. Inspired by the\nLevel-of-Detail (LOD) techniques, we introduce Octree-GS, featuring an\nLOD-structured 3D Gaussian approach supporting level-of-detail decomposition\nfor scene representation that contributes to the final rendering results. Our\nmodel dynamically selects the appropriate level from the set of\nmulti-resolution anchor points, ensuring consistent rendering performance with\nadaptive LOD adjustments while maintaining high-fidelity rendering results.","upvotes":8},"publishedAt":"2024-03-27T03:58:18.621Z","title":"Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D Gaussians","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.17898.png","numComments":0},{"paper":{"id":"2403.17804","authors":[{"_id":"660394fdcabe90e0a5219cf7","user":{"avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6355d5c460c1b72f62693983/heg2B7OnZ9Yqs3j8g0uTW.jpeg?w=200&h=200&f=face","isPro":false,"fullname":"Oscar Mañas","user":"oscmansan","type":"user"},"name":"Oscar Mañas","status":"extracted_confirmed","statusLastChangedAt":"2024-03-27T15:16:24.376Z","hidden":false},{"_id":"660394fdcabe90e0a5219cf8","user":{"avatarUrl":"/avatars/a66bf45e780d0a82d00035f16c24d4c8.svg","isPro":false,"fullname":"Pietro Astolfi","user":"pietroastolfi","type":"user"},"name":"Pietro Astolfi","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:29:03.588Z","hidden":false},{"_id":"660394fdcabe90e0a5219cf9","name":"Melissa Hall","hidden":false},{"_id":"660394fdcabe90e0a5219cfa","user":{"avatarUrl":"/avatars/98e0d9695527125a0409d1098cdceaa4.svg","isPro":false,"fullname":"Candace Ross","user":"candaceross","type":"user"},"name":"Candace Ross","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:29:13.341Z","hidden":false},{"_id":"660394fdcabe90e0a5219cfb","user":{"avatarUrl":"/avatars/2c6f31b5328309cf4b42262bbb1c6372.svg","isPro":false,"fullname":"Jack Urbanek","user":"JackUrb","type":"user"},"name":"Jack Urbanek","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:29:19.881Z","hidden":false},{"_id":"660394fdcabe90e0a5219cfc","user":{"avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6487618920fb2e386b73e5d2/ZnmBCV-gL3eMeDGC38Rz8.jpeg?w=200&h=200&f=face","isPro":false,"fullname":"Adina Williams","user":"adinawilliams2","type":"user"},"name":"Adina Williams","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:29:27.621Z","hidden":false},{"_id":"660394fdcabe90e0a5219cfd","user":{"avatarUrl":"/avatars/03b367b9033c655129e48bd94bd245ee.svg","isPro":false,"fullname":"Aishwarya Agrawal","user":"aagrawal","type":"user"},"name":"Aishwarya Agrawal","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:29:49.127Z","hidden":false},{"_id":"660394fdcabe90e0a5219cfe","name":"Adriana Romero-Soriano","hidden":false},{"_id":"660394fdcabe90e0a5219cff","name":"Michal Drozdzal","hidden":false}],"publishedAt":"2024-03-26T15:42:01.000Z","title":"Improving Text-to-Image Consistency via Automatic Prompt Optimization","summary":"Impressive advances in text-to-image (T2I) generative models have yielded a\nplethora of high performing models which are able to generate aesthetically\nappealing, photorealistic images. Despite the progress, these models still\nstruggle to produce images that are consistent with the input prompt,\noftentimes failing to capture object quantities, relations and attributes\nproperly. Existing solutions to improve prompt-image consistency suffer from\nthe following challenges: (1) they oftentimes require model fine-tuning, (2)\nthey only focus on nearby prompt samples, and (3) they are affected by\nunfavorable trade-offs among image quality, representation diversity, and\nprompt-image consistency. In this paper, we address these challenges and\nintroduce a T2I optimization-by-prompting framework, OPT2I, which leverages a\nlarge language model (LLM) to improve prompt-image consistency in T2I models.\nOur framework starts from a user prompt and iteratively generates revised\nprompts with the goal of maximizing a consistency score. Our extensive\nvalidation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost\nthe initial consistency score by up to 24.9% in terms of DSG score while\npreserving the FID and increasing the recall between generated and real data.\nOur work paves the way toward building more reliable and robust T2I systems by\nharnessing the power of LLMs.","upvotes":6},"publishedAt":"2024-03-27T03:39:46.464Z","title":"Improving Text-to-Image Consistency via Automatic Prompt Optimization","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.17804.png","numComments":0},{"paper":{"id":"2403.17607","authors":[{"_id":"6603878f527470e01645fe69","name":"Kai Yuan","hidden":false},{"_id":"6603878f527470e01645fe6a","name":"Christoph Bauinger","hidden":false},{"_id":"6603878f527470e01645fe6b","user":{"avatarUrl":"/avatars/ffccf38a96881a7db3dc19954bd55dbe.svg","isPro":false,"fullname":"Xiangyi Zhang","user":"Shawnz516","type":"user"},"name":"Xiangyi Zhang","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:32:01.042Z","hidden":false},{"_id":"6603878f527470e01645fe6c","name":"Pascal Baehr","hidden":false},{"_id":"6603878f527470e01645fe6d","name":"Matthias Kirchhart","hidden":false},{"_id":"6603878f527470e01645fe6e","name":"Darius Dabert","hidden":false},{"_id":"6603878f527470e01645fe6f","name":"Adrien Tousnakhoff","hidden":false},{"_id":"6603878f527470e01645fe70","name":"Pierre Boudier","hidden":false},{"_id":"6603878f527470e01645fe71","name":"Michael Paulitsch","hidden":false}],"publishedAt":"2024-03-26T11:38:39.000Z","title":"Fully-fused Multi-Layer Perceptrons on Intel Data Center GPUs","summary":"This paper presents a SYCL implementation of Multi-Layer Perceptrons (MLPs),\nwhich targets and is optimized for the Intel Data Center GPU Max 1550. To\nincrease the performance, our implementation minimizes the slow global memory\naccesses by maximizing the data reuse within the general register file and the\nshared local memory by fusing the operations in each layer of the MLP. We show\nwith a simple roofline model that this results in a significant increase in the\narithmetic intensity, leading to improved performance, especially for\ninference. We compare our approach to a similar CUDA implementation for MLPs\nand show that our implementation on the Intel Data Center GPU outperforms the\nCUDA implementation on Nvidia's H100 GPU by a factor up to 2.84 in inference\nand 1.75 in training. The paper also showcases the efficiency of our SYCL\nimplementation in three significant areas: Image Compression, Neural Radiance\nFields, and Physics-Informed Machine Learning. In all cases, our implementation\noutperforms the off-the-shelf Intel Extension for PyTorch (IPEX) implementation\non the same Intel GPU by up to a factor of 30 and the CUDA PyTorch version on\nNvidia's H100 GPU by up to a factor 19. The code can be found at\nhttps://github.com/intel/tiny-dpcpp-nn.","upvotes":6},"publishedAt":"2024-03-27T03:35:09.306Z","title":"Fully-fused Multi-Layer Perceptrons on Intel Data Center GPUs","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.17607.png","numComments":0},{"paper":{"id":"2403.17694","authors":[{"_id":"6603ebf72a78438d282ebaca","name":"Huawei Wei","hidden":false},{"_id":"6603ebf72a78438d282ebacb","name":"Zejun Yang","hidden":false},{"_id":"6603ebf72a78438d282ebacc","name":"Zhisheng Wang","hidden":false}],"publishedAt":"2024-03-26T13:35:02.000Z","title":"AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation","summary":"In this study, we propose AniPortrait, a novel framework for generating\nhigh-quality animation driven by audio and a reference portrait image. Our\nmethodology is divided into two stages. Initially, we extract 3D intermediate\nrepresentations from audio and project them into a sequence of 2D facial\nlandmarks. Subsequently, we employ a robust diffusion model, coupled with a\nmotion module, to convert the landmark sequence into photorealistic and\ntemporally consistent portrait animation. Experimental results demonstrate the\nsuperiority of AniPortrait in terms of facial naturalness, pose diversity, and\nvisual quality, thereby offering an enhanced perceptual experience. Moreover,\nour methodology exhibits considerable potential in terms of flexibility and\ncontrollability, which can be effectively applied in areas such as facial\nmotion editing or face reenactment. We release code and model weights at\nhttps://github.com/scutzzj/AniPortrait","upvotes":4},"publishedAt":"2024-03-27T09:50:47.972Z","title":"AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.17694.png","numComments":1},{"paper":{"id":"2403.17237","authors":[{"_id":"6603982c9b248dec28e972c2","user":{"avatarUrl":"/avatars/04984852e6d139db5975015d1a7c9d5f.svg","isPro":false,"fullname":"Lin","user":"Yuanze","type":"user"},"name":"Yuanze Lin","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:55:07.270Z","hidden":false},{"_id":"6603982c9b248dec28e972c3","user":{"avatarUrl":"/avatars/0e81ed3757b4e65be82063b538c3fe49.svg","isPro":false,"fullname":"Ronald Clark","user":"r0nn13","type":"user"},"name":"Ronald Clark","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:55:22.892Z","hidden":false},{"_id":"6603982c9b248dec28e972c4","user":{"avatarUrl":"/avatars/7f95bba9aa7811d56eecb380827abfac.svg","isPro":false,"fullname":"prof philip torr","user":"philiptorr","type":"user"},"name":"Philip Torr","status":"admin_assigned","statusLastChangedAt":"2024-03-27T09:55:33.385Z","hidden":false}],"publishedAt":"2024-03-25T22:34:05.000Z","title":"DreamPolisher: Towards High-Quality Text-to-3D Generation via Geometric\n  Diffusion","summary":"We present DreamPolisher, a novel Gaussian Splatting based method with\ngeometric guidance, tailored to learn cross-view consistency and intricate\ndetail from textual descriptions. While recent progress on text-to-3D\ngeneration methods have been promising, prevailing methods often fail to ensure\nview-consistency and textural richness. This problem becomes particularly\nnoticeable for methods that work with text input alone. To address this, we\npropose a two-stage Gaussian Splatting based approach that enforces geometric\nconsistency among views. Initially, a coarse 3D generation undergoes refinement\nvia geometric optimization. Subsequently, we use a ControlNet driven refiner\ncoupled with the geometric consistency term to improve both texture fidelity\nand overall consistency of the generated 3D asset. Empirical evaluations across\ndiverse textual prompts spanning various object categories demonstrate the\nefficacy of DreamPolisher in generating consistent and realistic 3D objects,\naligning closely with the semantics of the textual instructions.","upvotes":4},"publishedAt":"2024-03-27T03:53:19.598Z","title":"DreamPolisher: Towards High-Quality Text-to-3D Generation via Geometric Diffusion","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.17237.png","numComments":0}]