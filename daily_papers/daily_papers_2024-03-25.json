[{"paper":{"id":"2403.15371","authors":[{"_id":"6600d47a8270811586f9fe3a","user":{"avatarUrl":"/avatars/347f58f15820630e96908a54dd25ed81.svg","isPro":false,"fullname":"Akshay Krishnamurthy","user":"akshaykrish","type":"user"},"name":"Akshay Krishnamurthy","status":"admin_assigned","statusLastChangedAt":"2024-03-25T10:11:50.638Z","hidden":false},{"_id":"6600d47a8270811586f9fe3b","user":{"avatarUrl":"/avatars/21e106c247bf6f027c0384dec8910426.svg","isPro":false,"fullname":"Keegan Harris","user":"keeganharris","type":"user"},"name":"Keegan Harris","status":"claimed_verified","statusLastChangedAt":"2024-03-25T11:42:48.432Z","hidden":false},{"_id":"6600d47a8270811586f9fe3c","name":"Dylan J. Foster","hidden":false},{"_id":"6600d47a8270811586f9fe3d","user":{"avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a6a3a93d8cc9b1183c6593/UF-9ostqBHiAGDOuEtIw_.png?w=200&h=200&f=face","isPro":false,"fullname":"Cyril Zhang","user":"cyrilzhang","type":"user"},"name":"Cyril Zhang","status":"admin_assigned","statusLastChangedAt":"2024-03-25T10:12:55.961Z","hidden":false},{"_id":"6600d47a8270811586f9fe3e","name":"Aleksandrs Slivkins","hidden":false}],"publishedAt":"2024-03-22T17:50:43.000Z","title":"Can large language models explore in-context?","summary":"We investigate the extent to which contemporary Large Language Models (LLMs)\ncan engage in exploration, a core capability in reinforcement learning and\ndecision making. We focus on native performance of existing LLMs, without\ntraining interventions. We deploy LLMs as agents in simple multi-armed bandit\nenvironments, specifying the environment description and interaction history\nentirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5,\nGPT-4, and Llama2, using a variety of prompt designs, and find that the models\ndo not robustly engage in exploration without substantial interventions: i)\nAcross all of our experiments, only one configuration resulted in satisfactory\nexploratory behavior: GPT-4 with chain-of-thought reasoning and an externally\nsummarized interaction history, presented as sufficient statistics; ii) All\nother configurations did not result in robust exploratory behavior, including\nthose with chain-of-thought reasoning but unsummarized history. Although these\nfindings can be interpreted positively, they suggest that external\nsummarization -- which may not be possible in more complex settings -- is\nimportant for obtaining desirable behavior from LLM agents. We conclude that\nnon-trivial algorithmic interventions, such as fine-tuning or dataset curation,\nmay be required to empower LLM-based decision making agents in complex\nsettings.","upvotes":16},"publishedAt":"2024-03-25T01:33:46.684Z","title":"Can large language models explore in-context?","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.15371.png","numComments":0},{"paper":{"id":"2403.15042","authors":[{"_id":"6600fcca4b3187d931c818cd","user":{"avatarUrl":"/avatars/a5b522c18fced97140a156ff3e88d2e8.svg","isPro":false,"fullname":"Nicholas Lee","user":"nicholaslee","type":"user"},"name":"Nicholas Lee","status":"admin_assigned","statusLastChangedAt":"2024-03-25T09:50:14.495Z","hidden":false},{"_id":"6600fcca4b3187d931c818ce","user":{"avatarUrl":"/avatars/be55e57ae14ef817e4393bbfd2ff14d4.svg","isPro":false,"fullname":"Wattanawong","user":"Thanakul","type":"user"},"name":"Thanakul Wattanawong","status":"admin_assigned","statusLastChangedAt":"2024-03-25T09:50:29.744Z","hidden":false},{"_id":"6600fcca4b3187d931c818cf","name":"Sehoon Kim","hidden":false},{"_id":"6600fcca4b3187d931c818d0","name":"Karttikeya Mangalam","hidden":false},{"_id":"6600fcca4b3187d931c818d1","name":"Sheng Shen","hidden":false},{"_id":"6600fcca4b3187d931c818d2","user":{"avatarUrl":"/avatars/316ca348da91ebced86991f36150c959.svg","isPro":false,"fullname":"Gopala Anumanchipalli","user":"gopalakr","type":"user"},"name":"Gopala Anumanchipali","status":"admin_assigned","statusLastChangedAt":"2024-03-25T10:01:05.102Z","hidden":false},{"_id":"6600fcca4b3187d931c818d3","user":{"avatarUrl":"/avatars/350bac7b36ba1db1ac024a0812aa27c4.svg","isPro":false,"fullname":"MICHAEL MAHONEY","user":"Mahoney2022","type":"user"},"name":"Michael W. Mahoney","status":"admin_assigned","statusLastChangedAt":"2024-03-25T10:01:20.313Z","hidden":false},{"_id":"6600fcca4b3187d931c818d4","user":{"avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678041834400-6251bf4b183aa4266924ad91.jpeg?w=200&h=200&f=face","isPro":true,"fullname":"Kurt Keutzer","user":"kurtkeutzer","type":"user"},"name":"Kurt Keutzer","status":"admin_assigned","statusLastChangedAt":"2024-03-25T10:01:29.251Z","hidden":false},{"_id":"6600fcca4b3187d931c818d5","user":{"avatarUrl":"/avatars/6c838f810c5f655d6b0b0b40211cfad7.svg","isPro":false,"fullname":"Amir Gholami","user":"amirgh","type":"user"},"name":"Amir Gholami","status":"admin_assigned","statusLastChangedAt":"2024-03-25T10:01:38.368Z","hidden":false}],"publishedAt":"2024-03-22T08:57:07.000Z","title":"LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement","summary":"Pretrained large language models (LLMs) are currently state-of-the-art for\nsolving the vast majority of natural language processing tasks. While many\nreal-world applications still require fine-tuning to reach satisfactory levels\nof performance, many of them are in the low-data regime, making fine-tuning\nchallenging. To address this, we propose LLM2LLM, a targeted and iterative data\naugmentation strategy that uses a teacher LLM to enhance a small seed dataset\nby augmenting additional data that can be used for fine-tuning on a specific\ntask. LLM2LLM (1) fine-tunes a baseline student LLM on the initial seed data,\n(2) evaluates and extracts data points that the model gets wrong, and (3) uses\na teacher LLM to generate synthetic data based on these incorrect data points,\nwhich are then added back into the training data. This approach amplifies the\nsignal from incorrectly predicted data points by the LLM during training and\nreintegrates them into the dataset to focus on more challenging examples for\nthe LLM. Our results show that LLM2LLM significantly enhances the performance\nof LLMs in the low-data regime, outperforming both traditional fine-tuning and\nother data augmentation baselines. LLM2LLM reduces the dependence on\nlabor-intensive data curation and paves the way for more scalable and\nperformant LLM solutions, allowing us to tackle data-constrained domains and\ntasks. We achieve improvements up to 24.2% on the GSM8K dataset, 32.6% on\nCaseHOLD, 32.0% on SNIPS, 52.6% on TREC and 39.8% on SST-2 over regular\nfine-tuning in the low-data regime using a LLaMA2-7B student model.","upvotes":12},"publishedAt":"2024-03-25T04:25:47.162Z","title":"LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.15042.png","numComments":0},{"paper":{"id":"2403.14781","authors":[{"_id":"660107379e1cf5eb41e51cfd","user":{"avatarUrl":"/avatars/6d6472a52c6d5ddc7d9a8df62d70e567.svg","isPro":false,"fullname":"Shenhao Zhu","user":"ShenhaoZhu","type":"user"},"name":"Shenhao Zhu","status":"admin_assigned","statusLastChangedAt":"2024-03-25T10:16:18.584Z","hidden":false},{"_id":"660107379e1cf5eb41e51cfe","user":{"avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/65d802be7eed9113f6f3e20b/TdjqI_GBn3Bc9bvxgqnDF.jpeg?w=200&h=200&f=face","isPro":false,"fullname":"Junming Chen","user":"Leooo333","type":"user"},"name":"Junming Leo Chen","status":"extracted_confirmed","statusLastChangedAt":"2024-03-25T05:20:33.398Z","hidden":false},{"_id":"660107379e1cf5eb41e51cff","user":{"avatarUrl":"/avatars/8c896b3733e183657009a426834a9dab.svg","isPro":false,"fullname":"Dai Zuozhuo","user":"zyand","type":"user"},"name":"Zuozhuo Dai","status":"admin_assigned","statusLastChangedAt":"2024-03-25T10:16:30.544Z","hidden":false},{"_id":"660107379e1cf5eb41e51d00","name":"Yinghui Xu","hidden":false},{"_id":"660107379e1cf5eb41e51d01","name":"Xun Cao","hidden":false},{"_id":"660107379e1cf5eb41e51d02","name":"Yao Yao","hidden":false},{"_id":"660107379e1cf5eb41e51d03","name":"Hao Zhu","hidden":false},{"_id":"660107379e1cf5eb41e51d04","name":"Siyu Zhu","hidden":false}],"publishedAt":"2024-03-21T18:52:58.000Z","title":"Champ: Controllable and Consistent Human Image Animation with 3D\n  Parametric Guidance","summary":"In this study, we introduce a methodology for human image animation by\nleveraging a 3D human parametric model within a latent diffusion framework to\nenhance shape alignment and motion guidance in curernt human generative\ntechniques. The methodology utilizes the SMPL(Skinned Multi-Person Linear)\nmodel as the 3D human parametric model to establish a unified representation of\nbody shape and pose. This facilitates the accurate capture of intricate human\ngeometry and motion characteristics from source videos. Specifically, we\nincorporate rendered depth images, normal maps, and semantic maps obtained from\nSMPL sequences, alongside skeleton-based motion guidance, to enrich the\nconditions to the latent diffusion model with comprehensive 3D shape and\ndetailed pose attributes. A multi-layer motion fusion module, integrating\nself-attention mechanisms, is employed to fuse the shape and motion latent\nrepresentations in the spatial domain. By representing the 3D human parametric\nmodel as the motion guidance, we can perform parametric shape alignment of the\nhuman body between the reference image and the source video motion.\nExperimental evaluations conducted on benchmark datasets demonstrate the\nmethodology's superior ability to generate high-quality human animations that\naccurately capture both pose and shape variations. Furthermore, our approach\nalso exhibits superior generalization capabilities on the proposed wild\ndataset. Project page: https://fudan-generative-vision.github.io/champ.","upvotes":10},"publishedAt":"2024-03-25T05:10:25.290Z","title":"Champ: Controllable and Consistent Human Image Animation with 3D Parametric Guidance","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.14781.png","numComments":1},{"paper":{"id":"2403.15383","authors":[{"_id":"6600fb883d247974ba253e90","user":{"avatarUrl":"/avatars/f386ac1a826b61a01706d0effae6d8fa.svg","isPro":false,"fullname":"Zhenwei Wang","user":"zhenwei-wang","type":"user"},"name":"Zhenwei Wang","status":"admin_assigned","statusLastChangedAt":"2024-03-25T10:54:22.813Z","hidden":false},{"_id":"6600fb883d247974ba253e91","user":{"avatarUrl":"/avatars/15648407233bf5757262dbc04464d02e.svg","isPro":false,"fullname":"Tengfei Wang","user":"tfwang","type":"user"},"name":"Tengfei Wang","status":"admin_assigned","statusLastChangedAt":"2024-03-25T10:55:00.012Z","hidden":false},{"_id":"6600fb883d247974ba253e92","name":"Gerhard Hancke","hidden":false},{"_id":"6600fb883d247974ba253e93","user":{"avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png?w=200&h=200&f=face","isPro":false,"fullname":"Ziwei Liu","user":"liuziwei7","type":"user"},"name":"Ziwei Liu","status":"admin_assigned","statusLastChangedAt":"2024-03-25T10:55:21.146Z","hidden":false},{"_id":"6600fb883d247974ba253e94","name":"Rynson W. H. Lau","hidden":false}],"publishedAt":"2024-03-22T17:59:01.000Z","title":"ThemeStation: Generating Theme-Aware 3D Assets from Few Exemplars","summary":"Real-world applications often require a large gallery of 3D assets that share\na consistent theme. While remarkable advances have been made in general 3D\ncontent creation from text or image, synthesizing customized 3D assets\nfollowing the shared theme of input 3D exemplars remains an open and\nchallenging problem. In this work, we present ThemeStation, a novel approach\nfor theme-aware 3D-to-3D generation. ThemeStation synthesizes customized 3D\nassets based on given few exemplars with two goals: 1) unity for generating 3D\nassets that thematically align with the given exemplars and 2) diversity for\ngenerating 3D assets with a high degree of variations. To this end, we design a\ntwo-stage framework that draws a concept image first, followed by a\nreference-informed 3D modeling stage. We propose a novel dual score\ndistillation (DSD) loss to jointly leverage priors from both the input\nexemplars and the synthesized concept image. Extensive experiments and user\nstudies confirm that ThemeStation surpasses prior works in producing diverse\ntheme-aware 3D models with impressive quality. ThemeStation also enables\nvarious applications such as controllable 3D-to-3D generation.","upvotes":8},"publishedAt":"2024-03-25T04:20:28.207Z","title":"ThemeStation: Generating Theme-Aware 3D Assets from Few Exemplars","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.15383.png","numComments":0},{"paper":{"id":"2403.15377","authors":[{"_id":"6600d5e3569b30694e4d1080","name":"Yi Wang","hidden":false},{"_id":"6600d5e3569b30694e4d1081","user":{"avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1643872995181-61fb81006374891646732f37.jpeg?w=200&h=200&f=face","isPro":false,"fullname":"Kunchang Li","user":"Andy1621","type":"user"},"name":"Kunchang Li","status":"admin_assigned","statusLastChangedAt":"2024-03-25T10:05:21.549Z","hidden":false},{"_id":"6600d5e3569b30694e4d1082","name":"Xinhao Li","hidden":false},{"_id":"6600d5e3569b30694e4d1083","user":{"avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/tEd3fBjMcEubF4plqzcUz.jpeg?w=200&h=200&f=face","isPro":false,"fullname":"Jiashuo Yu","user":"awojustin","type":"user"},"name":"Jiashuo Yu","status":"admin_assigned","statusLastChangedAt":"2024-03-25T10:05:49.550Z","hidden":false},{"_id":"6600d5e3569b30694e4d1084","user":{"avatarUrl":"/avatars/a9245958cc998a4b4b870bf2490fdaee.svg","isPro":false,"fullname":"Yinan He","user":"yinanhe","type":"user"},"name":"Yinan He","status":"admin_assigned","statusLastChangedAt":"2024-03-25T10:05:56.422Z","hidden":false},{"_id":"6600d5e3569b30694e4d1085","user":{"avatarUrl":"/avatars/9ff824ab02848120aec5e8de6780bcf1.svg","isPro":false,"fullname":"Guo Chen","user":"cg1177","type":"user"},"name":"Guo Chen","status":"claimed_verified","statusLastChangedAt":"2024-03-25T11:50:57.140Z","hidden":false},{"_id":"6600d5e3569b30694e4d1086","name":"Baoqi Pei","hidden":false},{"_id":"6600d5e3569b30694e4d1087","name":"Rongkun Zheng","hidden":false},{"_id":"6600d5e3569b30694e4d1088","user":{"avatarUrl":"/avatars/c99c875a6654e7a8b3958777bcc46d8f.svg","isPro":false,"fullname":"Jilan Xu","user":"Jazzcharles","type":"user"},"name":"Jilan Xu","status":"admin_assigned","statusLastChangedAt":"2024-03-25T10:07:10.497Z","hidden":false},{"_id":"6600d5e3569b30694e4d1089","name":"Zun Wang","hidden":false},{"_id":"6600d5e3569b30694e4d108a","name":"Yansong Shi","hidden":false},{"_id":"6600d5e3569b30694e4d108b","user":{"avatarUrl":"/avatars/de12b6cd34a6149783fba39a947f4295.svg","isPro":false,"fullname":"Tianxiang Jiang","user":"txjiang","type":"user"},"name":"Tianxiang Jiang","status":"admin_assigned","statusLastChangedAt":"2024-03-25T10:07:32.223Z","hidden":false},{"_id":"6600d5e3569b30694e4d108c","name":"Songze Li","hidden":false},{"_id":"6600d5e3569b30694e4d108d","name":"Hongjie Zhang","hidden":false},{"_id":"6600d5e3569b30694e4d108e","name":"Yifei Huang","hidden":false},{"_id":"6600d5e3569b30694e4d108f","name":"Yu Qiao","hidden":false},{"_id":"6600d5e3569b30694e4d1090","name":"Yali Wang","hidden":false},{"_id":"6600d5e3569b30694e4d1091","user":{"avatarUrl":"/avatars/50198ccb02ccd286975a4613fbabee28.svg","isPro":false,"fullname":"Limin Wang","user":"lmwang","type":"user"},"name":"Limin Wang","status":"admin_assigned","statusLastChangedAt":"2024-03-25T10:09:26.539Z","hidden":false}],"publishedAt":"2024-03-22T17:57:42.000Z","title":"InternVideo2: Scaling Video Foundation Models for Multimodal Video\n  Understanding","summary":"We introduce InternVideo2, a new video foundation model (ViFM) that achieves\nthe state-of-the-art performance in action recognition, video-text tasks, and\nvideo-centric dialogue. Our approach employs a progressive training paradigm\nthat unifies the different self- or weakly-supervised learning frameworks of\nmasked video token reconstruction, cross-modal contrastive learning, and next\ntoken prediction. Different training stages would guide our model to capture\ndifferent levels of structure and semantic information through different\npretext tasks. At the data level, we prioritize the spatiotemporal consistency\nby semantically segmenting videos and generating video-audio-speech captions.\nThis improves the alignment between video and text. We scale both data and\nmodel size for our InternVideo2. Through extensive experiments, we validate our\ndesigns and demonstrate the state-of-the-art performance on over 60 video and\naudio tasks. Notably, our model outperforms others on various video-related\ncaptioning, dialogue, and long video understanding benchmarks, highlighting its\nability to reason and comprehend long temporal contexts. Code and models are\navailable at https://github.com/OpenGVLab/InternVideo2/.","upvotes":8},"publishedAt":"2024-03-25T01:39:49.689Z","title":"InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.15377.png","numComments":0},{"paper":{"id":"2403.14773","authors":[{"_id":"6600d2748270811586f8e8cf","user":{"avatarUrl":"/avatars/6a35dbd6f48b6e34a6fc8b8d04e00f6a.svg","isPro":false,"fullname":"Roberto Henschel","user":"robhen","type":"user"},"name":"Roberto Henschel","status":"admin_assigned","statusLastChangedAt":"2024-03-25T10:18:38.071Z","hidden":false},{"_id":"6600d2748270811586f8e8d0","user":{"avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6380b78b5c62156ce7e08c3d/qCc7j9uCMRYowbYJYwASy.jpeg?w=200&h=200&f=face","isPro":false,"fullname":"Levon Khachatryan","user":"lev1","type":"user"},"name":"Levon Khachatryan","status":"admin_assigned","statusLastChangedAt":"2024-03-25T10:19:22.407Z","hidden":false},{"_id":"6600d2748270811586f8e8d1","user":{"avatarUrl":"/avatars/46094173beafd663e29990874058d121.svg","isPro":false,"fullname":"Daniil Hayrapetyan","user":"oldnaari","type":"user"},"name":"Daniil Hayrapetyan","status":"admin_assigned","statusLastChangedAt":"2024-03-25T10:19:28.359Z","hidden":false},{"_id":"6600d2748270811586f8e8d2","user":{"avatarUrl":"/avatars/97ba153584d10d69ee33b71c310b04a0.svg","isPro":false,"fullname":"Hayk Poghosyan","user":"haykpoghos","type":"user"},"name":"Hayk Poghosyan","status":"admin_assigned","statusLastChangedAt":"2024-03-25T10:19:34.305Z","hidden":false},{"_id":"6600d2748270811586f8e8d3","user":{"avatarUrl":"/avatars/c22663168e7b9d0a9fc70bc9c5fc7f02.svg","isPro":false,"fullname":"Vahram Tadevosyan","user":"vahramtadevosyan","type":"user"},"name":"Vahram Tadevosyan","status":"admin_assigned","statusLastChangedAt":"2024-03-25T10:19:40.383Z","hidden":false},{"_id":"6600d2748270811586f8e8d4","name":"Zhangyang Wang","hidden":false},{"_id":"6600d2748270811586f8e8d5","user":{"avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641dadf816ef0e5a7275a5cd/YrieHABBK9VH82VxYPJnp.jpeg?w=200&h=200&f=face","isPro":false,"fullname":"Shant Navasardyan","user":"navasardyanshant","type":"user"},"name":"Shant Navasardyan","status":"admin_assigned","statusLastChangedAt":"2024-03-25T10:20:05.842Z","hidden":false},{"_id":"6600d2748270811586f8e8d6","user":{"avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1642793395811-61e1188afc27c0f5e3641eb3.jpeg?w=200&h=200&f=face","isPro":false,"fullname":"Humphrey Shi","user":"Humphrey","type":"user"},"name":"Humphrey Shi","status":"admin_assigned","statusLastChangedAt":"2024-03-25T10:20:12.061Z","hidden":false}],"publishedAt":"2024-03-21T18:27:29.000Z","title":"StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation\n  from Text","summary":"Text-to-video diffusion models enable the generation of high-quality videos\nthat follow text instructions, making it easy to create diverse and individual\ncontent. However, existing approaches mostly focus on high-quality short video\ngeneration (typically 16 or 24 frames), ending up with hard-cuts when naively\nextended to the case of long video synthesis. To overcome these limitations, we\nintroduce StreamingT2V, an autoregressive approach for long video generation of\n80, 240, 600, 1200 or more frames with smooth transitions. The key components\nare:(i) a short-term memory block called conditional attention module (CAM),\nwhich conditions the current generation on the features extracted from the\nprevious chunk via an attentional mechanism, leading to consistent chunk\ntransitions, (ii) a long-term memory block called appearance preservation\nmodule, which extracts high-level scene and object features from the first\nvideo chunk to prevent the model from forgetting the initial scene, and (iii) a\nrandomized blending approach that enables to apply a video enhancer\nautoregressively for infinitely long videos without inconsistencies between\nchunks. Experiments show that StreamingT2V generates high motion amount. In\ncontrast, all competing image-to-video methods are prone to video stagnation\nwhen applied naively in an autoregressive manner. Thus, we propose with\nStreamingT2V a high-quality seamless text-to-long video generator that\noutperforms competitors with consistency and motion. Our code will be available\nat: https://github.com/Picsart-AI-Research/StreamingT2V","upvotes":7},"publishedAt":"2024-03-25T01:25:12.840Z","title":"StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.14773.png","numComments":0},{"paper":{"id":"2403.15382","authors":[{"_id":"660108c1e73568bf2ee5a8aa","name":"Ruining Li","hidden":false},{"_id":"660108c1e73568bf2ee5a8ab","user":{"avatarUrl":"/avatars/905bccbe96b72862515ae4c27e7de642.svg","isPro":false,"fullname":"chuanxia","user":"lyndonzheng","type":"user"},"name":"Chuanxia Zheng","status":"claimed_verified","statusLastChangedAt":"2024-03-25T11:30:09.059Z","hidden":false},{"_id":"660108c1e73568bf2ee5a8ac","name":"Christian Rupprecht","hidden":false},{"_id":"660108c1e73568bf2ee5a8ad","name":"Andrea Vedaldi","hidden":false}],"publishedAt":"2024-03-22T17:58:59.000Z","title":"DragAPart: Learning a Part-Level Motion Prior for Articulated Objects","summary":"We introduce DragAPart, a method that, given an image and a set of drags as\ninput, can generate a new image of the same object in a new state, compatible\nwith the action of the drags. Differently from prior works that focused on\nrepositioning objects, DragAPart predicts part-level interactions, such as\nopening and closing a drawer. We study this problem as a proxy for learning a\ngeneralist motion model, not restricted to a specific kinematic structure or\nobject category. To this end, we start from a pre-trained image generator and\nfine-tune it on a new synthetic dataset, Drag-a-Move, which we introduce.\nCombined with a new encoding for the drags and dataset randomization, the new\nmodel generalizes well to real images and different categories. Compared to\nprior motion-controlled generators, we demonstrate much better part-level\nmotion understanding.","upvotes":6},"publishedAt":"2024-03-25T05:16:51.472Z","title":"DragAPart: Learning a Part-Level Motion Prior for Articulated Objects","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.15382.png","numComments":0},{"paper":{"id":"2403.14870","authors":[{"_id":"66010210b1e509e1e4b15bf7","user":{"avatarUrl":"/avatars/4857607039bea4fa791643df07b1e998.svg","isPro":false,"fullname":"Mamshad Nayeem Rizve","user":"nayeemrizve","type":"user"},"name":"Mamshad Nayeem Rizve","status":"admin_assigned","statusLastChangedAt":"2024-03-25T10:44:55.192Z","hidden":false},{"_id":"66010210b1e509e1e4b15bf8","name":"Fan Fei","hidden":false},{"_id":"66010210b1e509e1e4b15bf9","name":"Jayakrishnan Unnikrishnan","hidden":false},{"_id":"66010210b1e509e1e4b15bfa","name":"Son Tran","hidden":false},{"_id":"66010210b1e509e1e4b15bfb","name":"Benjamin Z. Yao","hidden":false},{"_id":"66010210b1e509e1e4b15bfc","name":"Belinda Zeng","hidden":false},{"_id":"66010210b1e509e1e4b15bfd","name":"Mubarak Shah","hidden":false},{"_id":"66010210b1e509e1e4b15bfe","name":"Trishul Chilimbi","hidden":false}],"publishedAt":"2024-03-21T22:36:24.000Z","title":"VidLA: Video-Language Alignment at Scale","summary":"In this paper, we propose VidLA, an approach for video-language alignment at\nscale. There are two major limitations of previous video-language alignment\napproaches. First, they do not capture both short-range and long-range temporal\ndependencies and typically employ complex hierarchical deep network\narchitectures that are hard to integrate with existing pretrained image-text\nfoundation models. To effectively address this limitation, we instead keep the\nnetwork architecture simple and use a set of data tokens that operate at\ndifferent temporal resolutions in a hierarchical manner, accounting for the\ntemporally hierarchical nature of videos. By employing a simple two-tower\narchitecture, we are able to initialize our video-language model with\npretrained image-text foundation models, thereby boosting the final\nperformance. Second, existing video-language alignment works struggle due to\nthe lack of semantically aligned large-scale training data. To overcome it, we\nleverage recent LLMs to curate the largest video-language dataset to date with\nbetter visual grounding. Furthermore, unlike existing video-text datasets which\nonly contain short clips, our dataset is enriched with video clips of varying\ndurations to aid our temporally hierarchical data tokens in extracting better\nrepresentations at varying temporal scales. Overall, empirical results show\nthat our proposed approach surpasses state-of-the-art methods on multiple\nretrieval benchmarks, especially on longer videos, and performs competitively\non classification benchmarks.","upvotes":6},"publishedAt":"2024-03-25T04:48:17.087Z","title":"VidLA: Video-Language Alignment at Scale","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.14870.png","numComments":0},{"paper":{"id":"2403.15360","authors":[{"_id":"6600d54e4d7ed9ef948b1277","user":{"avatarUrl":"/avatars/8a5cc786de279726bdf6b194f21ac3cb.svg","isPro":false,"fullname":"Badri Narayana Patro","user":"badripatro","type":"user"},"name":"Badri N. Patro","status":"admin_assigned","statusLastChangedAt":"2024-03-25T10:50:15.761Z","hidden":false},{"_id":"6600d54e4d7ed9ef948b1278","name":"Vijay S. Agneeswaran","hidden":false}],"publishedAt":"2024-03-22T17:22:56.000Z","title":"SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate\n  Time series","summary":"Transformers have widely adopted attention networks for sequence mixing and\nMLPs for channel mixing, playing a pivotal role in achieving breakthroughs\nacross domains. However, recent literature highlights issues with attention\nnetworks, including low inductive bias and quadratic complexity concerning\ninput sequence length. State Space Models (SSMs) like S4 and others (Hippo,\nGlobal Convolutions, liquid S4, LRU, Mega, and Mamba), have emerged to address\nthe above issues to help handle longer sequence lengths. Mamba, while being the\nstate-of-the-art SSM, has a stability issue when scaled to large networks for\ncomputer vision datasets. We propose SiMBA, a new architecture that introduces\nEinstein FFT (EinFFT) for channel modeling by specific eigenvalue computations\nand uses the Mamba block for sequence modeling. Extensive performance studies\nacross image and time-series benchmarks demonstrate that SiMBA outperforms\nexisting SSMs, bridging the performance gap with state-of-the-art transformers.\nNotably, SiMBA establishes itself as the new state-of-the-art SSM on ImageNet\nand transfer learning benchmarks such as Stanford Car and Flower as well as\ntask learning benchmarks as well as seven time series benchmark datasets. The\nproject page is available on this website\n~https://github.com/badripatro/Simba.","upvotes":6},"publishedAt":"2024-03-25T01:37:18.773Z","title":"SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time series","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.15360.png","numComments":0},{"paper":{"id":"2403.15385","authors":[{"_id":"6600fc84bcf0790b8d611a6e","user":{"avatarUrl":"/avatars/310d2a8141d60ff7634dd6e40efe371e.svg","isPro":false,"fullname":"Kevin Xie","user":"kevincxie","type":"user"},"name":"Kevin Xie","status":"claimed_verified","statusLastChangedAt":"2024-03-25T13:58:08.235Z","hidden":false},{"_id":"6600fc84bcf0790b8d611a6f","user":{"avatarUrl":"/avatars/12faffe4a2e4bf411220221375763d76.svg","isPro":false,"fullname":"Jonathan Lorraine","user":"lorraine2","type":"user"},"name":"Jonathan Lorraine","status":"admin_assigned","statusLastChangedAt":"2024-03-25T11:51:21.069Z","hidden":false},{"_id":"6600fc84bcf0790b8d611a70","user":{"avatarUrl":"/avatars/8dab667735111e070cc6d44609dbdaf3.svg","isPro":false,"fullname":"Tianshi Cao","user":"jeanlancel","type":"user"},"name":"Tianshi Cao","status":"admin_assigned","statusLastChangedAt":"2024-03-25T11:51:28.050Z","hidden":false},{"_id":"6600fc84bcf0790b8d611a71","name":"Jun Gao","hidden":false},{"_id":"6600fc84bcf0790b8d611a72","name":"James Lucas","hidden":false},{"_id":"6600fc84bcf0790b8d611a73","name":"Antonio Torralba","hidden":false},{"_id":"6600fc84bcf0790b8d611a74","name":"Sanja Fidler","hidden":false},{"_id":"6600fc84bcf0790b8d611a75","name":"Xiaohui Zeng","hidden":false}],"publishedAt":"2024-03-22T17:59:37.000Z","title":"LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis","summary":"Recent text-to-3D generation approaches produce impressive 3D results but\nrequire time-consuming optimization that can take up to an hour per prompt.\nAmortized methods like ATT3D optimize multiple prompts simultaneously to\nimprove efficiency, enabling fast text-to-3D synthesis. However, they cannot\ncapture high-frequency geometry and texture details and struggle to scale to\nlarge prompt sets, so they generalize poorly. We introduce LATTE3D, addressing\nthese limitations to achieve fast, high-quality generation on a significantly\nlarger prompt set. Key to our method is 1) building a scalable architecture and\n2) leveraging 3D data during optimization through 3D-aware diffusion priors,\nshape regularization, and model initialization to achieve robustness to diverse\nand complex training prompts. LATTE3D amortizes both neural field and textured\nsurface generation to produce highly detailed textured meshes in a single\nforward pass. LATTE3D generates 3D objects in 400ms, and can be further\nenhanced with fast test-time optimization.","upvotes":5},"publishedAt":"2024-03-25T04:24:40.090Z","title":"LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.15385.png","numComments":0},{"paper":{"id":"2403.15246","authors":[{"_id":"6600fdafd157381f16f4297c","user":{"avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6362d9712691058b19de1ba4/c9QrA2oE6lcs_46ShaTY1.jpeg?w=200&h=200&f=face","isPro":false,"fullname":"Orion Weller","user":"orionweller","type":"user"},"name":"Orion Weller","status":"admin_assigned","statusLastChangedAt":"2024-03-25T11:02:38.518Z","hidden":false},{"_id":"6600fdafd157381f16f4297d","name":"Benjamin Chang","hidden":false},{"_id":"6600fdafd157381f16f4297e","user":{"avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666975288423-61dc410f7813d319d7387635.jpeg?w=200&h=200&f=face","isPro":false,"fullname":"Sean MacAvaney","user":"macavaney","type":"user"},"name":"Sean MacAvaney","status":"admin_assigned","statusLastChangedAt":"2024-03-25T11:03:01.317Z","hidden":false},{"_id":"6600fdafd157381f16f4297f","name":"Kyle Lo","hidden":false},{"_id":"6600fdafd157381f16f42980","user":{"avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1599840760465-noauth.png?w=200&h=200&f=face","isPro":false,"fullname":"Arman Cohan","user":"armanc","type":"user"},"name":"Arman Cohan","status":"admin_assigned","statusLastChangedAt":"2024-03-25T11:03:12.243Z","hidden":false},{"_id":"6600fdafd157381f16f42981","name":"Benjamin Van Durme","hidden":false},{"_id":"6600fdafd157381f16f42982","user":{"avatarUrl":"/avatars/b2495c05e0838ad117bf607ccddbb3e4.svg","isPro":false,"fullname":"Dawn Lawrie","user":"dlawrie","type":"user"},"name":"Dawn Lawrie","status":"admin_assigned","statusLastChangedAt":"2024-03-25T11:03:23.291Z","hidden":false},{"_id":"6600fdafd157381f16f42983","user":{"avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5f04d8c45d08220171a0ad32/o-9-ZITCjP5k1UAFzG7yz.png?w=200&h=200&f=face","isPro":false,"fullname":"Luca Soldaini","user":"soldni","type":"user"},"name":"Luca Soldaini","status":"admin_assigned","statusLastChangedAt":"2024-03-25T11:03:29.380Z","hidden":false}],"publishedAt":"2024-03-22T14:42:29.000Z","title":"FollowIR: Evaluating and Teaching Information Retrieval Models to Follow\n  Instructions","summary":"Modern Large Language Models (LLMs) are capable of following long and complex\ninstructions that enable a diverse amount of user tasks. However, despite\nInformation Retrieval (IR) models using LLMs as the backbone of their\narchitectures, nearly all of them still only take queries as input, with no\ninstructions. For the handful of recent models that do take instructions, it's\nunclear how they use them. We introduce our dataset FollowIR, which contains a\nrigorous instruction evaluation benchmark as well as a training set for helping\nIR models learn to better follow real-world instructions. FollowIR builds off\nthe long history of the TREC conferences: as TREC provides human annotators\nwith instructions (also known as narratives) to determine document relevance,\nso should IR models be able to understand and decide relevance based on these\ndetailed instructions. Our evaluation benchmark starts with three deeply judged\nTREC collections and alters the annotator instructions, re-annotating relevant\ndocuments. Through this process, we can measure how well IR models follow\ninstructions, through a new pairwise evaluation framework. Our results indicate\nthat existing retrieval models fail to correctly use instructions, using them\nfor basic keywords and struggling to understand long-form information. However,\nwe show that it is possible for IR models to learn to follow complex\ninstructions: our new FollowIR-7B model has significant improvements (over 13%)\nafter fine-tuning on our training set.","upvotes":4},"publishedAt":"2024-03-25T04:29:36.249Z","title":"FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.15246.png","numComments":0},{"paper":{"id":"2403.15157","authors":[{"_id":"6601032c003d5f16a9022dd0","user":{"avatarUrl":"/avatars/b3c4035c48169c1bfb04a439fce3499f.svg","isPro":false,"fullname":"Chaoyun Zhang","user":"vyokky","type":"user"},"name":"Chaoyun Zhang","status":"admin_assigned","statusLastChangedAt":"2024-03-25T11:10:35.156Z","hidden":false},{"_id":"6601032c003d5f16a9022dd1","name":"Zicheng Ma","hidden":false},{"_id":"6601032c003d5f16a9022dd2","name":"Yuhao Wu","hidden":false},{"_id":"6601032c003d5f16a9022dd3","user":{"avatarUrl":"/avatars/d58fff1a157b189ce2617889ef5f6e2f.svg","isPro":false,"fullname":"Shilin He","user":"shilhe","type":"user"},"name":"Shilin He","status":"admin_assigned","statusLastChangedAt":"2024-03-25T11:11:30.662Z","hidden":false},{"_id":"6601032c003d5f16a9022dd4","name":"Si Qin","hidden":false},{"_id":"6601032c003d5f16a9022dd5","user":{"avatarUrl":"/avatars/c9c5023b030e94c1b1abfb7a1c1dfaf3.svg","isPro":false,"fullname":"MingHua Ma","user":"Gezelligheid520","type":"user"},"name":"Minghua Ma","status":"admin_assigned","statusLastChangedAt":"2024-03-25T11:11:56.365Z","hidden":false},{"_id":"6601032c003d5f16a9022dd6","name":"Xiaoting Qin","hidden":false},{"_id":"6601032c003d5f16a9022dd7","name":"Yu Kang","hidden":false},{"_id":"6601032c003d5f16a9022dd8","name":"Yuyi Liang","hidden":false},{"_id":"6601032c003d5f16a9022dd9","name":"Xiaoyu Gou","hidden":false},{"_id":"6601032c003d5f16a9022dda","name":"Yajie Xue","hidden":false},{"_id":"6601032c003d5f16a9022ddb","name":"Qingwei Lin","hidden":false},{"_id":"6601032c003d5f16a9022ddc","name":"Saravan Rajmohan","hidden":false},{"_id":"6601032c003d5f16a9022ddd","name":"Dongmei Zhang","hidden":false},{"_id":"6601032c003d5f16a9022dde","name":"Qi Zhang","hidden":false}],"publishedAt":"2024-03-22T12:13:16.000Z","title":"AllHands: Ask Me Anything on Large-scale Verbatim Feedback via Large\n  Language Models","summary":"Verbatim feedback constitutes a valuable repository of user experiences,\nopinions, and requirements essential for software development. Effectively and\nefficiently extracting valuable insights from such data poses a challenging\ntask. This paper introduces Allhands , an innovative analytic framework\ndesigned for large-scale feedback analysis through a natural language\ninterface, leveraging large language models (LLMs). Allhands adheres to a\nconventional feedback analytic workflow, initially conducting classification\nand topic modeling on the feedback to convert them into a structurally\naugmented format, incorporating LLMs to enhance accuracy, robustness,\ngeneralization, and user-friendliness. Subsequently, an LLM agent is employed\nto interpret users' diverse questions in natural language on feedback,\ntranslating them into Python code for execution, and delivering comprehensive\nmulti-modal responses, including text, code, tables, and images.\n  We evaluate Allhands across three diverse feedback datasets. The experiments\ndemonstrate that Allhands achieves superior efficacy at all stages of analysis,\nincluding classification and topic modeling, eventually providing users with an\n``ask me anything'' experience with comprehensive, correct and human-readable\nresponse. To the best of our knowledge, Allhands stands as the first\ncomprehensive feedback analysis framework that supports diverse and customized\nrequirements for insight extraction through a natural language interface.","upvotes":3},"publishedAt":"2024-03-25T04:53:01.009Z","title":"AllHands: Ask Me Anything on Large-scale Verbatim Feedback via Large Language Models","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.15157.png","numComments":0},{"paper":{"id":"2403.14714","authors":[{"_id":"66010123e3faf4b4d95c4484","user":{"avatarUrl":"/avatars/3939f9582d882128dc2e64b0220f3844.svg","isPro":false,"fullname":"Dejan Grubisic","user":"dejangrubisic","type":"user"},"name":"Dejan Grubisic","status":"admin_assigned","statusLastChangedAt":"2024-03-25T11:47:04.538Z","hidden":false},{"_id":"66010123e3faf4b4d95c4485","user":{"avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674781395584-noauth.jpeg?w=200&h=200&f=face","isPro":false,"fullname":"Chris Cummins","user":"ChrisCummins","type":"user"},"name":"Chris Cummins","status":"admin_assigned","statusLastChangedAt":"2024-03-25T11:47:11.670Z","hidden":false},{"_id":"66010123e3faf4b4d95c4486","name":"Volker Seeker","hidden":false},{"_id":"66010123e3faf4b4d95c4487","name":"Hugh Leather","hidden":false}],"publishedAt":"2024-03-18T23:25:13.000Z","title":"Compiler generated feedback for Large Language Models","summary":"We introduce a novel paradigm in compiler optimization powered by Large\nLanguage Models with compiler feedback to optimize the code size of LLVM\nassembly. The model takes unoptimized LLVM IR as input and produces optimized\nIR, the best optimization passes, and instruction counts of both unoptimized\nand optimized IRs. Then we compile the input with generated optimization passes\nand evaluate if the predicted instruction count is correct, generated IR is\ncompilable, and corresponds to compiled code. We provide this feedback back to\nLLM and give it another chance to optimize code. This approach adds an extra\n0.53% improvement over -Oz to the original model. Even though, adding more\ninformation with feedback seems intuitive, simple sampling techniques achieve\nmuch higher performance given 10 or more samples.","upvotes":3},"publishedAt":"2024-03-25T04:44:20.465Z","title":"Compiler generated feedback for Large Language Models","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.14714.png","numComments":0}]