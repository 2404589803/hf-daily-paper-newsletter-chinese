[{"paper":{"id":"2403.10301","authors":[{"_id":"65f7b0e9f78419ddb084224b","user":{"avatarUrl":"/avatars/2ee353d8ec2b0ebc5b58d0641beddc26.svg","isPro":false,"fullname":"Hengxing Cai","user":"enjoy","type":"user"},"name":"Hengxing Cai","status":"admin_assigned","statusLastChangedAt":"2024-03-18T08:55:35.867Z","hidden":false},{"_id":"65f7b0e9f78419ddb084224c","user":{"avatarUrl":"/avatars/f8f50390cf767cc62ae612e6b600a340.svg","isPro":false,"fullname":"caixiaochen","user":"caixc97","type":"user"},"name":"Xiaochen Cai","status":"claimed_verified","statusLastChangedAt":"2024-03-18T09:23:50.845Z","hidden":false},{"_id":"65f7b0e9f78419ddb084224d","user":{"avatarUrl":"/avatars/beb000f76e209fffe782ca8411c309bb.svg","isPro":false,"fullname":"shuwen yang","user":"PKUterran","type":"user"},"name":"Shuwen Yang","status":"admin_assigned","statusLastChangedAt":"2024-03-18T08:56:22.333Z","hidden":false},{"_id":"65f7b0e9f78419ddb084224e","user":{"avatarUrl":"/avatars/8d3218b580fb50236660c957f17e10bd.svg","isPro":false,"fullname":"Jiankun Wang","user":"Kuhn95","type":"user"},"name":"Jiankun Wang","status":"admin_assigned","statusLastChangedAt":"2024-03-18T08:56:30.874Z","hidden":false},{"_id":"65f7b0e9f78419ddb084224f","name":"Lin Yao","hidden":false},{"_id":"65f7b0e9f78419ddb0842250","name":"Zhifeng Gao","hidden":false},{"_id":"65f7b0e9f78419ddb0842251","user":{"avatarUrl":"/avatars/869856209b39047c55ec7369be49d354.svg","isPro":false,"fullname":"Junhan Chang","user":"changjh","type":"user"},"name":"Junhan Chang","status":"claimed_verified","statusLastChangedAt":"2024-03-18T08:56:13.416Z","hidden":false},{"_id":"65f7b0e9f78419ddb0842252","user":{"avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/635913a68f38318bfabc90ea/29M50hPy2nMwMjMElAEgA.jpeg?w=200&h=200&f=face","isPro":false,"fullname":"Sihang Li","user":"Sihangli","type":"user"},"name":"Sihang Li","status":"claimed_verified","statusLastChangedAt":"2024-03-18T09:02:20.389Z","hidden":false},{"_id":"65f7b0e9f78419ddb0842253","user":{"avatarUrl":"/avatars/259e5bd353fdb05d2bd102b6d5c2d589.svg","isPro":false,"fullname":"Mingjun Xu","user":"xumingjun","type":"user"},"name":"Mingjun Xu","status":"admin_assigned","statusLastChangedAt":"2024-03-18T08:57:42.147Z","hidden":false},{"_id":"65f7b0e9f78419ddb0842254","user":{"avatarUrl":"/avatars/c9d8704e2e0af13329604416bd3f1d3a.svg","isPro":false,"fullname":"Changxin Wang","user":"foril","type":"user"},"name":"Changxin Wang","status":"admin_assigned","statusLastChangedAt":"2024-03-18T08:57:49.048Z","hidden":false},{"_id":"65f7b0e9f78419ddb0842255","name":"Hongshuai Wang","hidden":false},{"_id":"65f7b0e9f78419ddb0842256","name":"Yongge Li","hidden":false},{"_id":"65f7b0e9f78419ddb0842257","user":{"avatarUrl":"/avatars/84b72949c11eb696fc759a5337d4047c.svg","isPro":false,"fullname":"Mujie Lin","user":"Linmj","type":"user"},"name":"Mujie Lin","status":"admin_assigned","statusLastChangedAt":"2024-03-18T08:58:12.477Z","hidden":false},{"_id":"65f7b0e9f78419ddb0842258","user":{"avatarUrl":"/avatars/4f589f626c204b84f7487ab46ee19a9b.svg","isPro":false,"fullname":"Yaqi Li","user":"lyq9811","type":"user"},"name":"Yaqi Li","status":"admin_assigned","statusLastChangedAt":"2024-03-18T08:58:18.863Z","hidden":false},{"_id":"65f7b0e9f78419ddb0842259","name":"Yuqi Yin","hidden":false},{"_id":"65f7b0e9f78419ddb084225a","name":"Linfeng Zhang","hidden":false},{"_id":"65f7b0e9f78419ddb084225b","user":{"avatarUrl":"/avatars/7296ea9bb301e19c10926022959b2023.svg","isPro":false,"fullname":"Guolin Ke","user":"guolinke","type":"user"},"name":"Guolin Ke","status":"admin_assigned","statusLastChangedAt":"2024-03-18T08:58:48.332Z","hidden":false}],"publishedAt":"2024-03-15T13:43:47.000Z","title":"Uni-SMART: Universal Science Multimodal Analysis and Research\n  Transformer","summary":"In scientific research and its application, scientific literature analysis is\ncrucial as it allows researchers to build on the work of others. However, the\nfast growth of scientific knowledge has led to a massive increase in scholarly\narticles, making in-depth literature analysis increasingly challenging and\ntime-consuming. The emergence of Large Language Models (LLMs) has offered a new\nway to address this challenge. Known for their strong abilities in summarizing\ntexts, LLMs are seen as a potential tool to improve the analysis of scientific\nliterature. However, existing LLMs have their own limits. Scientific literature\noften includes a wide range of multimodal elements, such as molecular\nstructure, tables, and charts, which are hard for text-focused LLMs to\nunderstand and analyze. This issue points to the urgent need for new solutions\nthat can fully understand and analyze multimodal content in scientific\nliterature. To answer this demand, we present Uni-SMART (Universal Science\nMultimodal Analysis and Research Transformer), an innovative model designed for\nin-depth understanding of multimodal scientific literature. Through rigorous\nquantitative evaluation across several domains, Uni-SMART demonstrates superior\nperformance over leading text-focused LLMs. Furthermore, our exploration\nextends to practical applications, including patent infringement detection and\nnuanced analysis of charts. These applications not only highlight Uni-SMART's\nadaptability but also its potential to revolutionize how we interact with\nscientific literature.","upvotes":37},"publishedAt":"2024-03-18T03:11:39.030Z","title":"Uni-SMART: Universal Science Multimodal Analysis and Research Transformer","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.10301.png","numComments":2},{"paper":{"id":"2403.10517","authors":[{"_id":"65f7b20c26f86cf3378c022e","name":"Xiaohan Wang","hidden":false},{"_id":"65f7b20c26f86cf3378c022f","user":{"avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62da55164398e21bf7f0e292/xjKkG8IA2IZZqCdjApSh3.jpeg?w=200&h=200&f=face","isPro":false,"fullname":"Yuhui Zhang","user":"yuhuizhang","type":"user"},"name":"Yuhui Zhang","status":"admin_assigned","statusLastChangedAt":"2024-03-18T09:02:09.951Z","hidden":false},{"_id":"65f7b20c26f86cf3378c0230","user":{"avatarUrl":"/avatars/e7c009e54424c5334a6ae2f20d1576fc.svg","isPro":false,"fullname":"Orr Zohar","user":"orrzohar","type":"user"},"name":"Orr Zohar","status":"admin_assigned","statusLastChangedAt":"2024-03-18T09:02:16.560Z","hidden":false},{"_id":"65f7b20c26f86cf3378c0231","name":"Serena Yeung-Levy","hidden":false}],"publishedAt":"2024-03-15T17:57:52.000Z","title":"VideoAgent: Long-form Video Understanding with Large Language Model as\n  Agent","summary":"Long-form video understanding represents a significant challenge within\ncomputer vision, demanding a model capable of reasoning over long multi-modal\nsequences. Motivated by the human cognitive process for long-form video\nunderstanding, we emphasize interactive reasoning and planning over the ability\nto process lengthy visual inputs. We introduce a novel agent-based system,\nVideoAgent, that employs a large language model as a central agent to\niteratively identify and compile crucial information to answer a question, with\nvision-language foundation models serving as tools to translate and retrieve\nvisual information. Evaluated on the challenging EgoSchema and NExT-QA\nbenchmarks, VideoAgent achieves 54.1% and 71.3% zero-shot accuracy with only\n8.4 and 8.2 frames used on average. These results demonstrate superior\neffectiveness and efficiency of our method over the current state-of-the-art\nmethods, highlighting the potential of agent-based approaches in advancing\nlong-form video understanding.","upvotes":18},"publishedAt":"2024-03-18T03:16:29.771Z","title":"VideoAgent: Long-form Video Understanding with Large Language Model as Agent","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.10517.png","numComments":0},{"paper":{"id":"2403.09919","authors":[{"_id":"65f7a52009cf7381afef6e83","user":{"avatarUrl":"/avatars/7bfbdb1949f73b3d8f88ae2ff73900bb.svg","isPro":false,"fullname":"Aonan Zhang","user":"AonanZhang","type":"user"},"name":"Aonan Zhang","status":"admin_assigned","statusLastChangedAt":"2024-03-18T09:24:31.165Z","hidden":false},{"_id":"65f7a52009cf7381afef6e84","name":"Chong Wang","hidden":false},{"_id":"65f7a52009cf7381afef6e85","name":"Yi Wang","hidden":false},{"_id":"65f7a52009cf7381afef6e86","user":{"avatarUrl":"/avatars/bd0a044b6099d0da1ccee72d287b9256.svg","isPro":false,"fullname":"xuanyu zhang","user":"pku-zxy","type":"user"},"name":"Xuanyu Zhang","status":"admin_assigned","statusLastChangedAt":"2024-03-18T09:26:06.433Z","hidden":false},{"_id":"65f7a52009cf7381afef6e87","name":"Yunfei Cheng","hidden":false}],"publishedAt":"2024-03-14T23:40:56.000Z","title":"Recurrent Drafter for Fast Speculative Decoding in Large Language Models","summary":"In this paper, we introduce an improved approach of speculative decoding\naimed at enhancing the efficiency of serving large language models. Our method\ncapitalizes on the strengths of two established techniques: the classic\ntwo-model speculative decoding approach, and the more recent single-model\napproach, Medusa. Drawing inspiration from Medusa, our approach adopts a\nsingle-model strategy for speculative decoding. However, our method\ndistinguishes itself by employing a single, lightweight draft head with a\nrecurrent dependency design, akin in essence to the small, draft model uses in\nclassic speculative decoding, but without the complexities of the full\ntransformer architecture. And because of the recurrent dependency, we can use\nbeam search to swiftly filter out undesired candidates with the draft head. The\noutcome is a method that combines the simplicity of single-model design and\navoids the need to create a data-dependent tree attention structure only for\ninference in Medusa. We empirically demonstrate the effectiveness of the\nproposed method on several popular open source language models, along with a\ncomprehensive analysis of the trade-offs involved in adopting this approach.","upvotes":11},"publishedAt":"2024-03-18T02:21:20.863Z","title":"Recurrent Drafter for Fast Speculative Decoding in Large Language Models","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.09919.png","numComments":0},{"paper":{"id":"2403.10131","authors":[{"_id":"65f7afd899c842dd937c831a","user":{"avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6374cd6b6ea8da14f8fef8dc/l13bg0tKDjCnUw3I895QZ.png?w=200&h=200&f=face","isPro":false,"fullname":"Tianjun Zhang","user":"tianjunz","type":"user"},"name":"Tianjun Zhang","status":"extracted_pending","statusLastChangedAt":"2024-03-18T03:07:04.684Z","hidden":false},{"_id":"65f7afd899c842dd937c831b","user":{"avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6471b9f6094820190c324eec/yb62BiXzRFY1_pS0GAMGq.jpeg?w=200&h=200&f=face","isPro":false,"fullname":"Shishir Patil","user":"shishirpatil","type":"user"},"name":"Shishir G. Patil","status":"extracted_pending","statusLastChangedAt":"2024-03-18T03:07:04.684Z","hidden":false},{"_id":"65f7afd899c842dd937c831c","name":"Naman Jain","hidden":false},{"_id":"65f7afd899c842dd937c831d","name":"Sheng Shen","hidden":false},{"_id":"65f7afd899c842dd937c831e","name":"Matei Zaharia","hidden":false},{"_id":"65f7afd899c842dd937c831f","name":"Ion Stoica","hidden":false},{"_id":"65f7afd899c842dd937c8320","name":"Joseph E. Gonzalez","hidden":false}],"publishedAt":"2024-03-15T09:26:02.000Z","title":"RAFT: Adapting Language Model to Domain Specific RAG","summary":"Pretraining Large Language Models (LLMs) on large corpora of textual data is\nnow a standard paradigm. When using these LLMs for many downstream\napplications, it is common to additionally bake in new knowledge (e.g.,\ntime-critical news, or private domain knowledge) into the pretrained model\neither through RAG-based-prompting, or fine-tuning. However, the optimal\nmethodology for the model to gain such new knowledge remains an open question.\nIn this paper, we present Retrieval Augmented FineTuning (RAFT), a training\nrecipe that improves the model's ability to answer questions in a \"open-book\"\nin-domain settings. In RAFT, given a question, and a set of retrieved\ndocuments, we train the model to ignore those documents that don't help in\nanswering the question, which we call, distractor documents. RAFT accomplishes\nthis by citing verbatim the right sequence from the relevant document that\nwould help answer the question. This coupled with RAFT's chain-of-thought-style\nresponse helps improve the model's ability to reason. In domain-specific RAG,\nRAFT consistently improves the model's performance across PubMed, HotpotQA, and\nGorilla datasets, presenting a post-training recipe to improve pre-trained LLMs\nto in-domain RAG. RAFT's code and demo are open-sourced at\ngithub.com/ShishirPatil/gorilla.","upvotes":9},"publishedAt":"2024-03-18T03:07:04.709Z","title":"RAFT: Adapting Language Model to Domain Specific RAG","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.10131.png","numComments":0},{"paper":{"id":"2403.10493","authors":[{"_id":"65f7a0f8d64a62e375b1f4f3","user":{"avatarUrl":"/avatars/592366207bc919a6984b3a0d816314c7.svg","isPro":false,"fullname":"Ge Zhu","user":"gzhu06","type":"user"},"name":"Ge Zhu","status":"admin_assigned","statusLastChangedAt":"2024-03-18T09:11:02.008Z","hidden":false},{"_id":"65f7a0f8d64a62e375b1f4f4","name":"Juan-Pablo Caceres","hidden":false},{"_id":"65f7a0f8d64a62e375b1f4f5","user":{"avatarUrl":"/avatars/423682fbf354f085232237cbf0f51bbe.svg","isPro":false,"fullname":"Zhiyao Duan","user":"zhiyaoduan","type":"user"},"name":"Zhiyao Duan","status":"admin_assigned","statusLastChangedAt":"2024-03-18T09:11:26.168Z","hidden":false},{"_id":"65f7a0f8d64a62e375b1f4f6","user":{"avatarUrl":"/avatars/183d2143ba20e1cc8712c63c055aadd7.svg","isPro":false,"fullname":"Nicholas J. Bryan","user":"Njb","type":"user"},"name":"Nicholas J. Bryan","status":"admin_assigned","statusLastChangedAt":"2024-03-18T09:11:32.465Z","hidden":false}],"publishedAt":"2024-03-15T17:27:42.000Z","title":"MusicHiFi: Fast High-Fidelity Stereo Vocoding","summary":"Diffusion-based audio and music generation models commonly generate music by\nconstructing an image representation of audio (e.g., a mel-spectrogram) and\nthen converting it to audio using a phase reconstruction model or vocoder.\nTypical vocoders, however, produce monophonic audio at lower resolutions (e.g.,\n16-24 kHz), which limits their effectiveness. We propose MusicHiFi -- an\nefficient high-fidelity stereophonic vocoder. Our method employs a cascade of\nthree generative adversarial networks (GANs) that convert low-resolution\nmel-spectrograms to audio, upsamples to high-resolution audio via bandwidth\nexpansion, and upmixes to stereophonic audio. Compared to previous work, we\npropose 1) a unified GAN-based generator and discriminator architecture and\ntraining procedure for each stage of our cascade, 2) a new fast, near\ndownsampling-compatible bandwidth extension module, and 3) a new fast\ndownmix-compatible mono-to-stereo upmixer that ensures the preservation of\nmonophonic content in the output. We evaluate our approach using both objective\nand subjective listening tests and find our approach yields comparable or\nbetter audio quality, better spatialization control, and significantly faster\ninference speed compared to past work. Sound examples are at\nhttps://MusicHiFi.github.io/web/.","upvotes":7},"publishedAt":"2024-03-18T02:03:36.849Z","title":"MusicHiFi: Fast High-Fidelity Stereo Vocoding","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.10493.png","numComments":0},{"paper":{"id":"2403.09981","authors":[{"_id":"65f79df901e0f4a95d662065","user":{"avatarUrl":"/avatars/d5f2de814a5f7570ad1710b28c22cf88.svg","isPro":false,"fullname":"Zhiqi Li","user":"lzq49","type":"user"},"name":"Zhiqi Li","status":"admin_assigned","statusLastChangedAt":"2024-03-18T09:16:41.827Z","hidden":false},{"_id":"65f79df901e0f4a95d662066","name":"Yiming Chen","hidden":false},{"_id":"65f79df901e0f4a95d662067","name":"Lingzhe Zhao","hidden":false},{"_id":"65f79df901e0f4a95d662068","name":"Peidong Liu","hidden":false}],"publishedAt":"2024-03-15T02:57:20.000Z","title":"Controllable Text-to-3D Generation via Surface-Aligned Gaussian\n  Splatting","summary":"While text-to-3D and image-to-3D generation tasks have received considerable\nattention, one important but under-explored field between them is controllable\ntext-to-3D generation, which we mainly focus on in this work. To address this\ntask, 1) we introduce Multi-view ControlNet (MVControl), a novel neural network\narchitecture designed to enhance existing pre-trained multi-view diffusion\nmodels by integrating additional input conditions, such as edge, depth, normal,\nand scribble maps. Our innovation lies in the introduction of a conditioning\nmodule that controls the base diffusion model using both local and global\nembeddings, which are computed from the input condition images and camera\nposes. Once trained, MVControl is able to offer 3D diffusion guidance for\noptimization-based 3D generation. And, 2) we propose an efficient multi-stage\n3D generation pipeline that leverages the benefits of recent large\nreconstruction models and score distillation algorithm. Building upon our\nMVControl architecture, we employ a unique hybrid diffusion guidance method to\ndirect the optimization process. In pursuit of efficiency, we adopt 3D\nGaussians as our representation instead of the commonly used implicit\nrepresentations. We also pioneer the use of SuGaR, a hybrid representation that\nbinds Gaussians to mesh triangle faces. This approach alleviates the issue of\npoor geometry in 3D Gaussians and enables the direct sculpting of fine-grained\ngeometry on the mesh. Extensive experiments demonstrate that our method\nachieves robust generalization and enables the controllable generation of\nhigh-quality 3D content.","upvotes":6},"publishedAt":"2024-03-18T01:50:52.632Z","title":"Controllable Text-to-3D Generation via Surface-Aligned Gaussian Splatting","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.09981.png","numComments":0},{"paper":{"id":"2403.09704","authors":[{"_id":"65f7a455fdb0e12d2c46dc43","name":"Swapnaja Achintalwar","hidden":false},{"_id":"65f7a455fdb0e12d2c46dc44","name":"Ioana Baldini","hidden":false},{"_id":"65f7a455fdb0e12d2c46dc45","name":"Djallel Bouneffouf","hidden":false},{"_id":"65f7a455fdb0e12d2c46dc46","name":"Joan Byamugisha","hidden":false},{"_id":"65f7a455fdb0e12d2c46dc47","user":{"avatarUrl":"/avatars/9bfe37cbaa7671cad5dca92a581857b0.svg","isPro":false,"fullname":"Maria Chang","user":"maria-chang","type":"user"},"name":"Maria Chang","status":"admin_assigned","statusLastChangedAt":"2024-03-18T09:03:07.845Z","hidden":false},{"_id":"65f7a455fdb0e12d2c46dc48","name":"Pierre Dognin","hidden":false},{"_id":"65f7a455fdb0e12d2c46dc49","name":"Eitan Farchi","hidden":false},{"_id":"65f7a455fdb0e12d2c46dc4a","user":{"avatarUrl":"/avatars/bdc86d5faea1d3088171cef52cbaf96c.svg","isPro":false,"fullname":"Ndivhuwo Makondo","user":"Ndivhuwo","type":"user"},"name":"Ndivhuwo Makondo","status":"admin_assigned","statusLastChangedAt":"2024-03-18T09:03:22.290Z","hidden":false},{"_id":"65f7a455fdb0e12d2c46dc4b","name":"Aleksandra Mojsilovic","hidden":false},{"_id":"65f7a455fdb0e12d2c46dc4c","user":{"avatarUrl":"/avatars/665e4dd171f7e8c1c483cdeb02b71744.svg","isPro":false,"fullname":"Manish Nagireddy","user":"mnagired","type":"user"},"name":"Manish Nagireddy","status":"admin_assigned","statusLastChangedAt":"2024-03-18T09:03:32.116Z","hidden":false},{"_id":"65f7a455fdb0e12d2c46dc4d","user":{"avatarUrl":"/avatars/b2eae578ea01c37d590036b23f79c8bf.svg","isPro":false,"fullname":"Karthikeyan Natesan Ramamurthy","user":"nrkarthikeyan","type":"user"},"name":"Karthikeyan Natesan Ramamurthy","status":"admin_assigned","statusLastChangedAt":"2024-03-18T09:03:39.304Z","hidden":false},{"_id":"65f7a455fdb0e12d2c46dc4e","name":"Inkit Padhi","hidden":false},{"_id":"65f7a455fdb0e12d2c46dc4f","name":"Orna Raz","hidden":false},{"_id":"65f7a455fdb0e12d2c46dc50","name":"Jesus Rios","hidden":false},{"_id":"65f7a455fdb0e12d2c46dc51","user":{"avatarUrl":"/avatars/e74d64ac5848cc4702b4ff6168def93b.svg","isPro":false,"fullname":"Prasanna Sattigeri","user":"pronics2004","type":"user"},"name":"Prasanna Sattigeri","status":"admin_assigned","statusLastChangedAt":"2024-03-18T09:03:56.432Z","hidden":false},{"_id":"65f7a455fdb0e12d2c46dc52","user":{"avatarUrl":"/avatars/9b760fdd2104a7c9c558ee6f64730a87.svg","isPro":false,"fullname":"Moninder Singh","user":"moninder","type":"user"},"name":"Moninder Singh","status":"admin_assigned","statusLastChangedAt":"2024-03-18T09:04:03.195Z","hidden":false},{"_id":"65f7a455fdb0e12d2c46dc53","name":"Siphiwe Thwala","hidden":false},{"_id":"65f7a455fdb0e12d2c46dc54","name":"Rosario A. Uceda-Sosa","hidden":false},{"_id":"65f7a455fdb0e12d2c46dc55","user":{"avatarUrl":"/avatars/8cc8a06447c871bae4405c837ba0661e.svg","isPro":false,"fullname":"Kush Varshney","user":"kushvarshney","type":"user"},"name":"Kush R. Varshney","status":"admin_assigned","statusLastChangedAt":"2024-03-18T09:04:18.499Z","hidden":false}],"publishedAt":"2024-03-08T21:26:49.000Z","title":"Alignment Studio: Aligning Large Language Models to Particular\n  Contextual Regulations","summary":"The alignment of large language models is usually done by model providers to\nadd or control behaviors that are common or universally understood across use\ncases and contexts. In contrast, in this article, we present an approach and\narchitecture that empowers application developers to tune a model to their\nparticular values, social norms, laws and other regulations, and orchestrate\nbetween potentially conflicting requirements in context. We lay out three main\ncomponents of such an Alignment Studio architecture: Framers, Instructors, and\nAuditors that work in concert to control the behavior of a language model. We\nillustrate this approach with a running example of aligning a company's\ninternal-facing enterprise chatbot to its business conduct guidelines.","upvotes":5},"publishedAt":"2024-03-18T02:17:58.317Z","title":"Alignment Studio: Aligning Large Language Models to Particular Contextual Regulations","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.09704.png","numComments":0},{"paper":{"id":"2403.10395","authors":[{"_id":"65f79f593a8814cf2f4fe9c8","user":{"avatarUrl":"/avatars/773a14ead497813c9d102bea1222d681.svg","isPro":false,"fullname":"Pengkun Liu","user":"pengkunliu","type":"user"},"name":"Pengkun Liu","status":"admin_assigned","statusLastChangedAt":"2024-03-18T09:11:57.663Z","hidden":false},{"_id":"65f79f593a8814cf2f4fe9c9","user":{"avatarUrl":"/avatars/6e5350fd998f0a7a4143d7504218164a.svg","isPro":false,"fullname":"Yikai Wang","user":"yikaiwang","type":"user"},"name":"Yikai Wang","status":"admin_assigned","statusLastChangedAt":"2024-03-18T09:12:27.732Z","hidden":false},{"_id":"65f79f593a8814cf2f4fe9ca","name":"Fuchun Sun","hidden":false},{"_id":"65f79f593a8814cf2f4fe9cb","user":{"avatarUrl":"/avatars/5ffd94289a53b6690f9f7db37557497d.svg","isPro":false,"fullname":"jiafang.li","user":"jiafang","type":"user"},"name":"Jiafang Li","status":"admin_assigned","statusLastChangedAt":"2024-03-18T09:12:44.232Z","hidden":false},{"_id":"65f79f593a8814cf2f4fe9cc","name":"Hang Xiao","hidden":false},{"_id":"65f79f593a8814cf2f4fe9cd","name":"Hongxiang Xue","hidden":false},{"_id":"65f79f593a8814cf2f4fe9ce","user":{"avatarUrl":"https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/t3iK1wPBOP-h_kLBzZA_h.png?w=200&h=200&f=face","isPro":false,"fullname":"Xinzhou Wang","user":"zz7379","type":"user"},"name":"Xinzhou Wang","status":"admin_assigned","statusLastChangedAt":"2024-03-18T09:13:16.794Z","hidden":false}],"publishedAt":"2024-03-15T15:27:58.000Z","title":"Isotropic3D: Image-to-3D Generation Based on a Single CLIP Embedding","summary":"Encouraged by the growing availability of pre-trained 2D diffusion models,\nimage-to-3D generation by leveraging Score Distillation Sampling (SDS) is\nmaking remarkable progress. Most existing methods combine novel-view lifting\nfrom 2D diffusion models which usually take the reference image as a condition\nwhile applying hard L2 image supervision at the reference view. Yet heavily\nadhering to the image is prone to corrupting the inductive knowledge of the 2D\ndiffusion model leading to flat or distorted 3D generation frequently. In this\nwork, we reexamine image-to-3D in a novel perspective and present Isotropic3D,\nan image-to-3D generation pipeline that takes only an image CLIP embedding as\ninput. Isotropic3D allows the optimization to be isotropic w.r.t. the azimuth\nangle by solely resting on the SDS loss. The core of our framework lies in a\ntwo-stage diffusion model fine-tuning. Firstly, we fine-tune a text-to-3D\ndiffusion model by substituting its text encoder with an image encoder, by\nwhich the model preliminarily acquires image-to-image capabilities. Secondly,\nwe perform fine-tuning using our Explicit Multi-view Attention (EMA) which\ncombines noisy multi-view images with the noise-free reference image as an\nexplicit condition. CLIP embedding is sent to the diffusion model throughout\nthe whole process while reference images are discarded once after fine-tuning.\nAs a result, with a single image CLIP embedding, Isotropic3D is capable of\ngenerating multi-view mutually consistent images and also a 3D model with more\nsymmetrical and neat content, well-proportioned geometry, rich colored texture,\nand less distortion compared with existing image-to-3D methods while still\npreserving the similarity to the reference image to a large extent. The project\npage is available at https://isotropic3d.github.io/. The code and models are\navailable at https://github.com/pkunliu/Isotropic3D.","upvotes":5},"publishedAt":"2024-03-18T01:56:46.579Z","title":"Isotropic3D: Image-to-3D Generation Based on a Single CLIP Embedding","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.10395.png","numComments":0},{"paper":{"id":"2403.09977","authors":[{"_id":"65f7ad0f933dd81304b79552","user":{"avatarUrl":"/avatars/9b98943bc609ca974abf0abff3e68633.svg","isPro":false,"fullname":"TerryPei","user":"TerryPei","type":"user"},"name":"Xiaohuan Pei","status":"admin_assigned","statusLastChangedAt":"2024-03-18T09:07:34.813Z","hidden":false},{"_id":"65f7ad0f933dd81304b79553","name":"Tao Huang","hidden":false},{"_id":"65f7ad0f933dd81304b79554","name":"Chang Xu","hidden":false}],"publishedAt":"2024-03-15T02:48:47.000Z","title":"EfficientVMamba: Atrous Selective Scan for Light Weight Visual Mamba","summary":"Prior efforts in light-weight model development mainly centered on CNN and\nTransformer-based designs yet faced persistent challenges. CNNs adept at local\nfeature extraction compromise resolution while Transformers offer global reach\nbut escalate computational demands O(N^2). This ongoing trade-off\nbetween accuracy and efficiency remains a significant hurdle. Recently, state\nspace models (SSMs), such as Mamba, have shown outstanding performance and\ncompetitiveness in various tasks such as language modeling and computer vision,\nwhile reducing the time complexity of global information extraction to\nO(N). Inspired by this, this work proposes to explore the potential\nof visual state space models in light-weight model design and introduce a novel\nefficient model variant dubbed EfficientVMamba. Concretely, our EfficientVMamba\nintegrates a atrous-based selective scan approach by efficient skip sampling,\nconstituting building blocks designed to harness both global and local\nrepresentational features. Additionally, we investigate the integration between\nSSM blocks and convolutions, and introduce an efficient visual state space\nblock combined with an additional convolution branch, which further elevate the\nmodel performance. Experimental results show that, EfficientVMamba scales down\nthe computational complexity while yields competitive results across a variety\nof vision tasks. For example, our EfficientVMamba-S with 1.3G FLOPs improves\nVim-Ti with 1.5G FLOPs by a large margin of 5.6% accuracy on ImageNet.\nCode is available at: https://github.com/TerryPei/EfficientVMamba.","upvotes":4},"publishedAt":"2024-03-18T02:55:14.824Z","title":"EfficientVMamba: Atrous Selective Scan for Light Weight Visual Mamba","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.09977.png","numComments":0},{"paper":{"id":"2403.10242","authors":[{"_id":"65f815f9a46363a9fbab9997","name":"Qijun Feng","hidden":false},{"_id":"65f815f9a46363a9fbab9998","name":"Zhen Xing","hidden":false},{"_id":"65f815f9a46363a9fbab9999","name":"Zuxuan Wu","hidden":false},{"_id":"65f815f9a46363a9fbab999a","name":"Yu-Gang Jiang","hidden":false}],"publishedAt":"2024-03-15T12:24:36.000Z","title":"FDGaussian: Fast Gaussian Splatting from Single Image via\n  Geometric-aware Diffusion Model","summary":"Reconstructing detailed 3D objects from single-view images remains a\nchallenging task due to the limited information available. In this paper, we\nintroduce FDGaussian, a novel two-stage framework for single-image 3D\nreconstruction. Recent methods typically utilize pre-trained 2D diffusion\nmodels to generate plausible novel views from the input image, yet they\nencounter issues with either multi-view inconsistency or lack of geometric\nfidelity. To overcome these challenges, we propose an orthogonal plane\ndecomposition mechanism to extract 3D geometric features from the 2D input,\nenabling the generation of consistent multi-view images. Moreover, we further\naccelerate the state-of-the-art Gaussian Splatting incorporating epipolar\nattention to fuse images from different viewpoints. We demonstrate that\nFDGaussian generates images with high consistency across different views and\nreconstructs high-quality 3D objects, both qualitatively and quantitatively.\nMore examples can be found at our website https://qjfeng.net/FDGaussian/.","upvotes":2},"publishedAt":"2024-03-18T10:22:50.160Z","title":"FDGaussian: Fast Gaussian Splatting from Single Image via Geometric-aware Diffusion Model","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.10242.png","numComments":0},{"paper":{"id":"2403.10425","authors":[{"_id":"65f8196099c842dd939da849","name":"Zhiyong Zhang","hidden":false},{"_id":"65f8196099c842dd939da84a","name":"Huaizu Jiang","hidden":false},{"_id":"65f8196099c842dd939da84b","user":{"avatarUrl":"/avatars/0301164b6fd6ad4247cfad62fd859367.svg","isPro":false,"fullname":"Hanumant Singh","user":"hanumant","type":"user"},"name":"Hanumant Singh","status":"admin_assigned","statusLastChangedAt":"2024-03-18T12:29:15.010Z","hidden":false}],"publishedAt":"2024-03-15T15:58:51.000Z","title":"NeuFlow: Real-time, High-accuracy Optical Flow Estimation on Robots\n  Using Edge Devices","summary":"Real-time high-accuracy optical flow estimation is a crucial component in\nvarious applications, including localization and mapping in robotics, object\ntracking, and activity recognition in computer vision. While recent\nlearning-based optical flow methods have achieved high accuracy, they often\ncome with heavy computation costs. In this paper, we propose a highly efficient\noptical flow architecture, called NeuFlow, that addresses both high accuracy\nand computational cost concerns. The architecture follows a global-to-local\nscheme. Given the features of the input images extracted at different spatial\nresolutions, global matching is employed to estimate an initial optical flow on\nthe 1/16 resolution, capturing large displacement, which is then refined on the\n1/8 resolution with lightweight CNN layers for better accuracy. We evaluate our\napproach on Jetson Orin Nano and RTX 2080 to demonstrate efficiency\nimprovements across different computing platforms. We achieve a notable 10x-80x\nspeedup compared to several state-of-the-art methods, while maintaining\ncomparable accuracy. Our approach achieves around 30 FPS on edge computing\nplatforms, which represents a significant breakthrough in deploying complex\ncomputer vision tasks such as SLAM on small robots like drones. The full\ntraining and evaluation code is available at\nhttps://github.com/neufieldrobotics/NeuFlow.","upvotes":1},"publishedAt":"2024-03-18T10:37:21.175Z","title":"NeuFlow: Real-time, High-accuracy Optical Flow Estimation on Robots Using Edge Devices","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.10425.png","numComments":0}]