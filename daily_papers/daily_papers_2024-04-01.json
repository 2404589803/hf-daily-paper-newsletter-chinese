[{"paper":{"id":"2403.19887","authors":[{"_id":"660a2edb40e346fba5b51b82","name":"Opher Lieber","hidden":false},{"_id":"660a2edb40e346fba5b51b83","name":"Barak Lenz","hidden":false},{"_id":"660a2edb40e346fba5b51b84","name":"Hofit Bata","hidden":false},{"_id":"660a2edb40e346fba5b51b85","name":"Gal Cohen","hidden":false},{"_id":"660a2edb40e346fba5b51b86","name":"Jhonathan Osin","hidden":false},{"_id":"660a2edb40e346fba5b51b87","name":"Itay Dalmedigos","hidden":false},{"_id":"660a2edb40e346fba5b51b88","name":"Erez Safahi","hidden":false},{"_id":"660a2edb40e346fba5b51b89","name":"Shaked Meirom","hidden":false},{"_id":"660a2edb40e346fba5b51b8a","name":"Yonatan Belinkov","hidden":false},{"_id":"660a2edb40e346fba5b51b8b","name":"Shai Shalev-Shwartz","hidden":false},{"_id":"660a2edb40e346fba5b51b8c","name":"Omri Abend","hidden":false},{"_id":"660a2edb40e346fba5b51b8d","name":"Raz Alon","hidden":false},{"_id":"660a2edb40e346fba5b51b8e","name":"Tomer Asida","hidden":false},{"_id":"660a2edb40e346fba5b51b8f","name":"Amir Bergman","hidden":false},{"_id":"660a2edb40e346fba5b51b90","name":"Roman Glozman","hidden":false},{"_id":"660a2edb40e346fba5b51b91","name":"Michael Gokhman","hidden":false},{"_id":"660a2edb40e346fba5b51b92","name":"Avashalom Manevich","hidden":false},{"_id":"660a2edb40e346fba5b51b93","name":"Nir Ratner","hidden":false},{"_id":"660a2edb40e346fba5b51b94","name":"Noam Rozen","hidden":false},{"_id":"660a2edb40e346fba5b51b95","name":"Erez Shwartz","hidden":false},{"_id":"660a2edb40e346fba5b51b96","name":"Mor Zusman","hidden":false},{"_id":"660a2edb40e346fba5b51b97","name":"Yoav Shoham","hidden":false}],"publishedAt":"2024-03-28T23:55:06.000Z","title":"Jamba: A Hybrid Transformer-Mamba Language Model","summary":"We present Jamba, a new base large language model based on a novel hybrid\nTransformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba\ninterleaves blocks of Transformer and Mamba layers, enjoying the benefits of\nboth model families. MoE is added in some of these layers to increase model\ncapacity while keeping active parameter usage manageable. This flexible\narchitecture allows resource- and objective-specific configurations. In the\nparticular configuration we have implemented, we end up with a powerful model\nthat fits in a single 80GB GPU. Built at large scale, Jamba provides high\nthroughput and small memory footprint compared to vanilla Transformers, and at\nthe same time state-of-the-art performance on standard language model\nbenchmarks and long-context evaluations. Remarkably, the model presents strong\nresults for up to 256K tokens context length. We study various architectural\ndecisions, such as how to combine Transformer and Mamba layers, and how to mix\nexperts, and show that some of them are crucial in large scale modeling. We\nalso describe several interesting properties of these architectures which the\ntraining and evaluation of Jamba have revealed, and plan to release checkpoints\nfrom various ablation runs, to encourage further exploration of this novel\narchitecture. We make the weights of our implementation of Jamba publicly\navailable under a permissive license.","upvotes":35},"publishedAt":"2024-04-01T03:49:48.032Z","title":"Jamba: A Hybrid Transformer-Mamba Language Model","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.19887.png","numComments":2},{"paper":{"id":"2403.20041","authors":[{"_id":"660a421540e346fba5b97365","name":"Luchang Li","hidden":false},{"_id":"660a421540e346fba5b97366","name":"Sheng Qian","hidden":false},{"_id":"660a421540e346fba5b97367","name":"Jie Lu","hidden":false},{"_id":"660a421540e346fba5b97368","name":"Lunxi Yuan","hidden":false},{"_id":"660a421540e346fba5b97369","name":"Rui Wang","hidden":false},{"_id":"660a421540e346fba5b9736a","name":"Qin Xie","hidden":false}],"publishedAt":"2024-03-29T08:26:53.000Z","title":"Transformer-Lite: High-efficiency Deployment of Large Language Models on\n  Mobile Phone GPUs","summary":"The Large Language Model (LLM) is widely employed for tasks such as\nintelligent assistants, text summarization, translation, and multi-modality on\nmobile phones. However, the current methods for on-device LLM deployment\nmaintain slow inference speed, which causes poor user experience. To facilitate\nhigh-efficiency LLM deployment on device GPUs, we propose four optimization\ntechniques: (a) a symbolic expression-based approach to support dynamic shape\nmodel inference; (b) operator optimizations and execution priority setting to\nenhance inference speed and reduce phone lagging; (c) an FP4 quantization\nmethod termed M0E4 to reduce dequantization overhead; (d) a sub-tensor-based\ntechnique to eliminate the need for copying KV cache after LLM inference.\nFurthermore, we implement these methods in our mobile inference engine,\nTransformer-Lite, which is compatible with both Qualcomm and MTK processors. We\nevaluated Transformer-Lite's performance using LLMs with varied architectures\nand parameters ranging from 2B to 14B. Specifically, we achieved prefill and\ndecoding speeds of 121 token/s and 14 token/s for ChatGLM2 6B, and 330 token/s\nand 30 token/s for smaller Gemma 2B, respectively. Compared with CPU-based\nFastLLM and GPU-based MLC-LLM, our engine attains over 10x speedup for the\nprefill speed and 2~3x speedup for the decoding speed.","upvotes":19},"publishedAt":"2024-04-01T05:11:50.025Z","title":"Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.20041.png","numComments":0},{"paper":{"id":"2403.20327","authors":[{"_id":"660a3edbdf88d3dbac12b9e2","name":"Jinhyuk Lee","hidden":false},{"_id":"660a3edbdf88d3dbac12b9e3","name":"Zhuyun Dai","hidden":false},{"_id":"660a3edbdf88d3dbac12b9e4","name":"Xiaoqi Ren","hidden":false},{"_id":"660a3edbdf88d3dbac12b9e5","name":"Blair Chen","hidden":false},{"_id":"660a3edbdf88d3dbac12b9e6","name":"Daniel Cer","hidden":false},{"_id":"660a3edbdf88d3dbac12b9e7","name":"Jeremy R. Cole","hidden":false},{"_id":"660a3edbdf88d3dbac12b9e8","name":"Kai Hui","hidden":false},{"_id":"660a3edbdf88d3dbac12b9e9","name":"Michael Boratko","hidden":false},{"_id":"660a3edbdf88d3dbac12b9ea","name":"Rajvi Kapadia","hidden":false},{"_id":"660a3edbdf88d3dbac12b9eb","name":"Wen Ding","hidden":false},{"_id":"660a3edbdf88d3dbac12b9ec","name":"Yi Luan","hidden":false},{"_id":"660a3edbdf88d3dbac12b9ed","name":"Sai Meher Karthik Duddu","hidden":false},{"_id":"660a3edbdf88d3dbac12b9ee","name":"Gustavo Hernandez Abrego","hidden":false},{"_id":"660a3edbdf88d3dbac12b9ef","name":"Weiqiang Shi","hidden":false},{"_id":"660a3edbdf88d3dbac12b9f0","name":"Nithi Gupta","hidden":false},{"_id":"660a3edbdf88d3dbac12b9f1","name":"Aditya Kusupati","hidden":false},{"_id":"660a3edbdf88d3dbac12b9f2","name":"Prateek Jain","hidden":false},{"_id":"660a3edbdf88d3dbac12b9f3","name":"Siddhartha Reddy Jonnalagadda","hidden":false},{"_id":"660a3edbdf88d3dbac12b9f4","name":"Ming-Wei Chang","hidden":false},{"_id":"660a3edbdf88d3dbac12b9f5","name":"Iftekhar Naim","hidden":false}],"publishedAt":"2024-03-29T17:56:40.000Z","title":"Gecko: Versatile Text Embeddings Distilled from Large Language Models","summary":"We present Gecko, a compact and versatile text embedding model. Gecko\nachieves strong retrieval performance by leveraging a key idea: distilling\nknowledge from large language models (LLMs) into a retriever. Our two-step\ndistillation process begins with generating diverse, synthetic paired data\nusing an LLM. Next, we further refine the data quality by retrieving a set of\ncandidate passages for each query, and relabeling the positive and hard\nnegative passages using the same LLM. The effectiveness of our approach is\ndemonstrated by the compactness of the Gecko. On the Massive Text Embedding\nBenchmark (MTEB), Gecko with 256 embedding dimensions outperforms all existing\nentries with 768 embedding size. Gecko with 768 embedding dimensions achieves\nan average score of 66.31, competing with 7x larger models and 5x higher\ndimensional embeddings.","upvotes":18},"publishedAt":"2024-04-01T04:58:04.121Z","title":"Gecko: Versatile Text Embeddings Distilled from Large Language Models","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.20327.png","numComments":1},{"paper":{"id":"2403.20309","authors":[{"_id":"660a2c185294ca0aad3eec0e","name":"Zhiwen Fan","hidden":false},{"_id":"660a2c185294ca0aad3eec0f","name":"Wenyan Cong","hidden":false},{"_id":"660a2c185294ca0aad3eec10","name":"Kairun Wen","hidden":false},{"_id":"660a2c185294ca0aad3eec11","name":"Kevin Wang","hidden":false},{"_id":"660a2c185294ca0aad3eec12","name":"Jian Zhang","hidden":false},{"_id":"660a2c185294ca0aad3eec13","name":"Xinghao Ding","hidden":false},{"_id":"660a2c185294ca0aad3eec14","name":"Danfei Xu","hidden":false},{"_id":"660a2c185294ca0aad3eec15","name":"Boris Ivanovic","hidden":false},{"_id":"660a2c185294ca0aad3eec16","name":"Marco Pavone","hidden":false},{"_id":"660a2c185294ca0aad3eec17","name":"Georgios Pavlakos","hidden":false},{"_id":"660a2c185294ca0aad3eec18","name":"Zhangyang Wang","hidden":false},{"_id":"660a2c185294ca0aad3eec19","name":"Yue Wang","hidden":false}],"publishedAt":"2024-03-29T17:29:58.000Z","title":"InstantSplat: Unbounded Sparse-view Pose-free Gaussian Splatting in 40\n  Seconds","summary":"While novel view synthesis (NVS) has made substantial progress in 3D computer\nvision, it typically requires an initial estimation of camera intrinsics and\nextrinsics from dense viewpoints. This pre-processing is usually conducted via\na Structure-from-Motion (SfM) pipeline, a procedure that can be slow and\nunreliable, particularly in sparse-view scenarios with insufficient matched\nfeatures for accurate reconstruction. In this work, we integrate the strengths\nof point-based representations (e.g., 3D Gaussian Splatting, 3D-GS) with\nend-to-end dense stereo models (DUSt3R) to tackle the complex yet unresolved\nissues in NVS under unconstrained settings, which encompasses pose-free and\nsparse view challenges. Our framework, InstantSplat, unifies dense stereo\npriors with 3D-GS to build 3D Gaussians of large-scale scenes from sparseview &\npose-free images in less than 1 minute. Specifically, InstantSplat comprises a\nCoarse Geometric Initialization (CGI) module that swiftly establishes a\npreliminary scene structure and camera parameters across all training views,\nutilizing globally-aligned 3D point maps derived from a pre-trained dense\nstereo pipeline. This is followed by the Fast 3D-Gaussian Optimization (F-3DGO)\nmodule, which jointly optimizes the 3D Gaussian attributes and the initialized\nposes with pose regularization. Experiments conducted on the large-scale\noutdoor Tanks & Temples datasets demonstrate that InstantSplat significantly\nimproves SSIM (by 32%) while concurrently reducing Absolute Trajectory Error\n(ATE) by 80%. These establish InstantSplat as a viable solution for scenarios\ninvolving posefree and sparse-view conditions. Project page:\ninstantsplat.github.io.","upvotes":9},"publishedAt":"2024-04-01T03:38:01.564Z","title":"InstantSplat: Unbounded Sparse-view Pose-free Gaussian Splatting in 40 Seconds","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.20309.png","numComments":0},{"paper":{"id":"2403.20329","authors":[{"_id":"660a24f565ffe042315e231d","name":"Joel Ruben Antony Moniz","hidden":false},{"_id":"660a24f565ffe042315e231e","name":"Soundarya Krishnan","hidden":false},{"_id":"660a24f565ffe042315e231f","name":"Melis Ozyildirim","hidden":false},{"_id":"660a24f565ffe042315e2320","name":"Prathamesh Saraf","hidden":false},{"_id":"660a24f565ffe042315e2321","name":"Halim Cagri Ates","hidden":false},{"_id":"660a24f565ffe042315e2322","name":"Yuan Zhang","hidden":false},{"_id":"660a24f565ffe042315e2323","name":"Hong Yu","hidden":false},{"_id":"660a24f565ffe042315e2324","name":"Nidhi Rajshree","hidden":false}],"publishedAt":"2024-03-29T17:59:06.000Z","title":"ReALM: Reference Resolution As Language Modeling","summary":"Reference resolution is an important problem, one that is essential to\nunderstand and successfully handle context of different kinds. This context\nincludes both previous turns and context that pertains to non-conversational\nentities, such as entities on the user's screen or those running in the\nbackground. While LLMs have been shown to be extremely powerful for a variety\nof tasks, their use in reference resolution, particularly for\nnon-conversational entities, remains underutilized. This paper demonstrates how\nLLMs can be used to create an extremely effective system to resolve references\nof various types, by showing how reference resolution can be converted into a\nlanguage modeling problem, despite involving forms of entities like those on\nscreen that are not traditionally conducive to being reduced to a text-only\nmodality. We demonstrate large improvements over an existing system with\nsimilar functionality across different types of references, with our smallest\nmodel obtaining absolute gains of over 5% for on-screen references. We also\nbenchmark against GPT-3.5 and GPT-4, with our smallest model achieving\nperformance comparable to that of GPT-4, and our larger models substantially\noutperforming it.","upvotes":9},"publishedAt":"2024-04-01T03:07:34.914Z","title":"ReALM: Reference Resolution As Language Modeling","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.20329.png","numComments":0},{"paper":{"id":"2403.19851","authors":[{"_id":"660a43fff4ab651901c081f3","name":"Niklas Stoehr","hidden":false},{"_id":"660a43fff4ab651901c081f4","name":"Mitchell Gordon","hidden":false},{"_id":"660a43fff4ab651901c081f5","name":"Chiyuan Zhang","hidden":false},{"_id":"660a43fff4ab651901c081f6","name":"Owen Lewis","hidden":false}],"publishedAt":"2024-03-28T21:53:24.000Z","title":"Localizing Paragraph Memorization in Language Models","summary":"Can we localize the weights and mechanisms used by a language model to\nmemorize and recite entire paragraphs of its training data? In this paper, we\nshow that while memorization is spread across multiple layers and model\ncomponents, gradients of memorized paragraphs have a distinguishable spatial\npattern, being larger in lower model layers than gradients of non-memorized\nexamples. Moreover, the memorized examples can be unlearned by fine-tuning only\nthe high-gradient weights. We localize a low-layer attention head that appears\nto be especially involved in paragraph memorization. This head is predominantly\nfocusing its attention on distinctive, rare tokens that are least frequent in a\ncorpus-level unigram distribution. Next, we study how localized memorization is\nacross the tokens in the prefix by perturbing tokens and measuring the caused\nchange in the decoding. A few distinctive tokens early in a prefix can often\ncorrupt the entire continuation. Overall, memorized continuations are not only\nharder to unlearn, but also to corrupt than non-memorized ones.","upvotes":7},"publishedAt":"2024-04-01T05:20:00.597Z","title":"Localizing Paragraph Memorization in Language Models","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.19851.png","numComments":0},{"paper":{"id":"2403.19928","authors":[{"_id":"660a3f8ac50f8393c512058a","name":"Hanting Chen","hidden":false},{"_id":"660a3f8ac50f8393c512058b","name":"Zhicheng Liu","hidden":false},{"_id":"660a3f8ac50f8393c512058c","name":"Xutao Wang","hidden":false},{"_id":"660a3f8ac50f8393c512058d","name":"Yuchuan Tian","hidden":false},{"_id":"660a3f8ac50f8393c512058e","name":"Yunhe Wang","hidden":false}],"publishedAt":"2024-03-29T02:32:15.000Z","title":"DiJiang: Efficient Large Language Models through Compact Kernelization","summary":"In an effort to reduce the computational load of Transformers, research on\nlinear attention has gained significant momentum. However, the improvement\nstrategies for attention mechanisms typically necessitate extensive retraining,\nwhich is impractical for large language models with a vast array of parameters.\nIn this paper, we present DiJiang, a novel Frequency Domain Kernelization\napproach that enables the transformation of a pre-trained vanilla Transformer\ninto a linear complexity model with little training costs. By employing a\nweighted Quasi-Monte Carlo method for sampling, the proposed approach\ntheoretically offers superior approximation efficiency. To further reduce the\ntraining computational complexity, our kernelization is based on Discrete\nCosine Transform (DCT) operations. Extensive experiments demonstrate that the\nproposed method achieves comparable performance to the original Transformer,\nbut with significantly reduced training costs and much faster inference speeds.\nOur DiJiang-7B achieves comparable performance with LLaMA2-7B on various\nbenchmark while requires only about 1/50 training cost. Code is available at\nhttps://github.com/YuchuanTian/DiJiang.","upvotes":7},"publishedAt":"2024-04-01T05:00:59.171Z","title":"DiJiang: Efficient Large Language Models through Compact Kernelization","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.19928.png","numComments":0},{"paper":{"id":"2403.20275","authors":[{"_id":"660a2a29e29bac7275f565db","name":"Mauro Comi","hidden":false},{"_id":"660a2a29e29bac7275f565dc","name":"Alessio Tonioni","hidden":false},{"_id":"660a2a29e29bac7275f565dd","name":"Max Yang","hidden":false},{"_id":"660a2a29e29bac7275f565de","name":"Jonathan Tremblay","hidden":false},{"_id":"660a2a29e29bac7275f565df","name":"Valts Blukis","hidden":false},{"_id":"660a2a29e29bac7275f565e0","name":"Yijiong Lin","hidden":false},{"_id":"660a2a29e29bac7275f565e1","name":"Nathan F. Lepora","hidden":false},{"_id":"660a2a29e29bac7275f565e2","name":"Laurence Aitchison","hidden":false}],"publishedAt":"2024-03-29T16:30:17.000Z","title":"Snap-it, Tap-it, Splat-it: Tactile-Informed 3D Gaussian Splatting for\n  Reconstructing Challenging Surfaces","summary":"Touch and vision go hand in hand, mutually enhancing our ability to\nunderstand the world. From a research perspective, the problem of mixing touch\nand vision is underexplored and presents interesting challenges. To this end,\nwe propose Tactile-Informed 3DGS, a novel approach that incorporates touch data\n(local depth maps) with multi-view vision data to achieve surface\nreconstruction and novel view synthesis. Our method optimises 3D Gaussian\nprimitives to accurately model the object's geometry at points of contact. By\ncreating a framework that decreases the transmittance at touch locations, we\nachieve a refined surface reconstruction, ensuring a uniformly smooth depth\nmap. Touch is particularly useful when considering non-Lambertian objects (e.g.\nshiny or reflective surfaces) since contemporary methods tend to fail to\nreconstruct with fidelity specular highlights. By combining vision and tactile\nsensing, we achieve more accurate geometry reconstructions with fewer images\nthan prior methods. We conduct evaluation on objects with glossy and reflective\nsurfaces and demonstrate the effectiveness of our approach, offering\nsignificant improvements in reconstruction quality.","upvotes":7},"publishedAt":"2024-04-01T03:29:47.651Z","title":"Snap-it, Tap-it, Splat-it: Tactile-Informed 3D Gaussian Splatting for Reconstructing Challenging Surfaces","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.20275.png","numComments":0},{"paper":{"id":"2403.19888","authors":[{"_id":"660a407c4fa3a72a97b28d45","name":"Ali Behrouz","hidden":false},{"_id":"660a407c4fa3a72a97b28d46","name":"Michele Santacatterina","hidden":false},{"_id":"660a407c4fa3a72a97b28d47","name":"Ramin Zabih","hidden":false}],"publishedAt":"2024-03-29T00:05:13.000Z","title":"MambaMixer: Efficient Selective State Space Models with Dual Token and\n  Channel Selection","summary":"Recent advances in deep learning have mainly relied on Transformers due to\ntheir data dependency and ability to learn at scale. The attention module in\nthese architectures, however, exhibits quadratic time and space in input size,\nlimiting their scalability for long-sequence modeling. Despite recent attempts\nto design efficient and effective architecture backbone for multi-dimensional\ndata, such as images and multivariate time series, existing models are either\ndata independent, or fail to allow inter- and intra-dimension communication.\nRecently, State Space Models (SSMs), and more specifically Selective State\nSpace Models, with efficient hardware-aware implementation, have shown\npromising potential for long sequence modeling. Motivated by the success of\nSSMs, we present MambaMixer, a new architecture with data-dependent weights\nthat uses a dual selection mechanism across tokens and channels, called\nSelective Token and Channel Mixer. MambaMixer connects selective mixers using a\nweighted averaging mechanism, allowing layers to have direct access to early\nfeatures. As a proof of concept, we design Vision MambaMixer (ViM2) and Time\nSeries MambaMixer (TSM2) architectures based on the MambaMixer block and\nexplore their performance in various vision and time series forecasting tasks.\nOur results underline the importance of selective mixing across both tokens and\nchannels. In ImageNet classification, object detection, and semantic\nsegmentation tasks, ViM2 achieves competitive performance with well-established\nvision models and outperforms SSM-based vision models. In time series\nforecasting, TSM2 achieves outstanding performance compared to state-of-the-art\nmethods while demonstrating significantly improved computational cost. These\nresults show that while Transformers, cross-channel attention, and MLPs are\nsufficient for good performance in time series forecasting, neither is\nnecessary.","upvotes":6},"publishedAt":"2024-04-01T05:05:01.876Z","title":"MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.19888.png","numComments":0},{"paper":{"id":"2403.20331","authors":[{"_id":"660a1b750b7d615a56a13e19","name":"Atsuyuki Miyai","hidden":false},{"_id":"660a1b750b7d615a56a13e1a","name":"Jingkang Yang","hidden":false},{"_id":"660a1b750b7d615a56a13e1b","name":"Jingyang Zhang","hidden":false},{"_id":"660a1b750b7d615a56a13e1c","name":"Yifei Ming","hidden":false},{"_id":"660a1b750b7d615a56a13e1d","name":"Qing Yu","hidden":false},{"_id":"660a1b750b7d615a56a13e1e","name":"Go Irie","hidden":false},{"_id":"660a1b750b7d615a56a13e1f","name":"Yixuan Li","hidden":false},{"_id":"660a1b750b7d615a56a13e20","name":"Hai Li","hidden":false},{"_id":"660a1b750b7d615a56a13e21","name":"Ziwei Liu","hidden":false},{"_id":"660a1b750b7d615a56a13e22","name":"Kiyoharu Aizawa","hidden":false}],"publishedAt":"2024-03-29T17:59:53.000Z","title":"Unsolvable Problem Detection: Evaluating Trustworthiness of Vision\n  Language Models","summary":"This paper introduces a novel and significant challenge for Vision Language\nModels (VLMs), termed Unsolvable Problem Detection (UPD). UPD examines the\nVLM's ability to withhold answers when faced with unsolvable problems in the\ncontext of Visual Question Answering (VQA) tasks. UPD encompasses three\ndistinct settings: Absent Answer Detection (AAD), Incompatible Answer Set\nDetection (IASD), and Incompatible Visual Question Detection (IVQD). To deeply\ninvestigate the UPD problem, extensive experiments indicate that most VLMs,\nincluding GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying\nextents, highlighting significant room for the improvements. To address UPD, we\nexplore both training-free and training-based solutions, offering new insights\ninto their effectiveness and limitations. We hope our insights, together with\nfuture efforts within the proposed UPD settings, will enhance the broader\nunderstanding and development of more practical and reliable VLMs.","upvotes":5},"publishedAt":"2024-04-01T03:12:25.372Z","title":"Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models","mediaUrl":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.20331.png","numComments":0}]