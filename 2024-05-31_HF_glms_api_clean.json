[
    {
        "url": "https://arxiv.org/abs/2405.20204",
        "content": "这篇文章标题为《Jina CLIP: Your CLIP Model Is Also Your Text Retriever》，由 Andreas Koukounas 和其他 13 位作者共同撰写。文章主要探讨了 Contrastive Language-Image Pretraining (CLIP) 模型，这是一种广泛用于训练模型以将图像和文本对齐到共同嵌入空间的方法。这些模型对于多模态信息检索和相关任务至关重要。然而，CLIP 模型在仅文本任务中的表现通常不如专门的文本模型。这导致在文本_only 和多模态任务中保持单独嵌入和模型的信息检索系统效率低下。文章提出了一种新颖的多任务对比训练方法来解决这个问题，并使用这种方法训练了 jina-clip-v1 模型，使其在文本-图像和文本-文本检索任务上实现了最先进的性能。\n\n搜索结果来自：\n[2405.20204] Jina CLIP: Your CLIP Model Is Also Your Text Retriever - https://arxiv.org/abs/2405.20204"
    },
    {
        "url": "https://arxiv.org/abs/2405.20335",
        "content": "很抱歉，我无法直接访问或检索特定于您提供的URL的内容。但是，我可以根据您提供的信息或一般的查询来提供帮助。如果您能提供这篇文章的标题、作者或主要主题，我可以尝试为您提供更具体的信息。"
    },
    {
        "url": "https://arxiv.org/abs/2405.19893",
        "content": "这篇文章的标题是《Similarity is Not All You Need: Endowing Retrieval Augmented Generation with Multi Layered Thoughts》，由Chunjing Gan和其他11位作者共同撰写，于2024年5月30日提交至arXiv。文章的主要内容集中在大型语言模型（LLMs）在各个领域取得的显著成就，尤其是在知识密集型任务中的应用。然而，由于知识更新的不及时性和成本，以及LLMs存在的幻觉问题，限制了它们在这些任务中的应用。为了解决这些问题，文章提出了一个名为MetRag的框架，即多层思考增强的检索增强生成框架。\n\nMetRag框架的核心思想是，除了现有的基于相似性的思考方式外，还引入了一个小规模的实用性模型，该模型从LLM中获取指导，以实用性为导向进行思考。此外，考虑到检索到的文档集通常非常大，孤立地使用它们难以捕捉它们之间的共性和特点，因此文章建议使用LLM作为任务自适应的摘要器，以赋予检索增强生成以紧凑性为导向的思考。最后，通过前几个阶段的多层思考，LLM被用于知识增强生成。文章通过在知识密集型任务上的大量实验，证明了MetRag框架的优越性。\n\n总的来说，这篇文章探讨了在检索增强生成中，如何通过引入多层思考来提高模型的性能，特别是在处理知识密集型任务时。\n\n搜索结果来自：\n[2405.19893] Similarity is Not All You Need: Endowing Retrieval Augmented Generation with Multi Layered Thoughts - https://arxiv.org/abs/2405.19893\n[2405.19893] Similarity is Not All You Need: Endowing Retrieval Augmented Generation with Multi Layered Thoughts - http://export.arxiv.org/abs/2405.19893"
    },
    {
        "url": "https://arxiv.org/abs/2405.20289",
        "content": "文章《DITTO-2: Distilled Diffusion Inference-Time T-Optimization for Music Generation》探讨了一种名为“DITTO-2”的新方法，用于加速基于推理时优化的控制，并实现比实时更快的音乐生成。这种方法特别适用于各种应用，如音乐修补、外绘、强度、旋律和音乐结构控制。\n\n该研究指出，当前可控音乐生成方法在速度、质量和控制设计方面存在限制。尽管扩散推理时T-优化（DITTO）提供了最先进的结果，但它的速度比实时慢10倍以上，限制了其实际应用。\n\nDITTO-2方法通过以下步骤实现快速、高质量的音乐生成：\n1. 通过高效、修改的一致性或一致性轨迹蒸馏过程，对预训练的扩散模型进行蒸馏，以实现快速采样。\n2. 使用蒸馏后的模型进行推理时优化，其中单步采样作为一种有效的替代优化任务。\n3. 使用估计的噪声潜在变量进行最终的多步采样生成（解码），以实现最佳质量、快速、可控的生成。\n\n研究还发现，DITTO-2方法不仅将生成速度提高了10-20倍，同时还提高了控制一致性和生成质量。此外，该方法还被应用于一个新的应用领域，即最大化文本一致性（CLAP分数），展示了如何将无条件的扩散模型（无文本输入）转换为具有最先进文本控制能力的模型。\n\n这篇文章涵盖了声音（cs.SD）、人工智能（cs.AI）和机器学习（cs.LG）等领域，并由Zachary Novack等作者于2024年5月30日提交至arXiv。\n\n搜索结果来自：\n[2405.20289] DITTO-2: Distilled Diffusion Inference-Time T-Optimization for Music Generation - https://arxiv.org/abs/2405.20289\n[2405.20289] DITTO-2: Distilled Diffusion Inference-Time T-Optimization for Music Generation - http://export.arxiv.org/abs/2405.20289"
    },
    {
        "url": "https://arxiv.org/abs/2405.20327",
        "content": "很抱歉，我无法直接访问或检索特定于您提供的URL的内容。但是，我可以根据您提供的信息或一般的查询来提供帮助。如果您能提供这篇文章的标题、作者或主要主题，我可以尝试为您提供更具体的信息。"
    },
    {
        "url": "https://arxiv.org/abs/2405.20222",
        "content": "这篇文章标题为“MOFA-Video: Controllable Image Animation via Generative Motion Field Adaptions in Frozen Image-to-Video Diffusion Model”，主要介绍了一种名为MOFA-Video的先进可控图像动画方法。这种方法能够利用各种附加的可控信号（如人体关键点参考、手动轨迹和其他提供的视频）或它们的组合，从给定的图像生成视频。与先前只能在特定运动域工作或具有较弱控制能力的方法不同，MOFA-Video设计了几种领域感知的运动场适配器（即MOFA-Adapters），以控制视频生成管道中的生成运动。这些适配器首先从给定的稀疏控制条件生成密集运动流，然后利用给定图像的多尺度特征作为稳定视频扩散生成的引导特征。文章的作者们还分别针对手动轨迹和人体关键点训练了两个运动适配器，因为它们都包含关于控制的稀疏信息。经过训练，不同领域的MOFA-Adapters也可以一起工作，实现更可控的视频生成。\n\n搜索结果来自：\n[2405.20222] MOFA-Video: Controllable Image Animation via Generative Motion Field Adaptions in Frozen Image-to-Video Diffusion Model - https://arxiv.org/abs/2405.20222"
    },
    {
        "url": "https://arxiv.org/abs/2405.19957",
        "content": "这篇文章标题为《PLA4D: Pixel-Level Alignments for Text-to-4D Gaussian Splatting》，由Qiaowei Miao等人于2024年5月30日提交至arXiv。文章主要探讨了文本条件扩散模型（DMs）在图像、视频和3D生成方面取得的突破，以及研究社区关注重点转向更具挑战性的文本到4D合成任务。这种合成引入了时间维度，以生成动态3D对象。\n\n文章指出，用于文本到3D合成的广泛技术——得分蒸馏采样（SDS）——由于其Janus-faced和纹理不真实问题以及高昂的计算成本，成为了文本到4D性能提升的显著障碍。为此，作者提出了PLA4D（Pixel-Level Alignments for Text-to-4D Gaussian Splatting）这一新颖方法，利用文本到视频帧作为显式像素对齐目标，生成静态3D对象并向其注入运动。具体来说，作者介绍了焦点对齐以校准渲染的相机姿态，以及GS-Mesh对比学习，从渲染图像对比中提取几何先验，均在像素级别进行。此外，还开发了使用变形网络的运动对齐来驱动高斯变化，并实施了参考细化以获得平滑的4D对象表面。\n\n这些技术使得4D高斯分层能够在像素级别与生成视频的几何、纹理和运动对齐。与以前的方法相比，PLA4D在更短的时间内产生了具有更好纹理细节的合成输出，并有效缓解了Janus-faced问题。PLA4D完全使用开源模型实现，为4D数字内容创作提供了一个易于访问、用户友好且充满前景的方向。\n\n搜索结果来自：\n[2405.19957] PLA4D: Pixel-Level Alignments for Text-to-4D Gaussian Splatting - https://arxiv.org/abs/2405.19957\n[2405.19957] PLA4D: Pixel-Level Alignments for Text-to-4D Gaussian Splatting - http://export.arxiv.org/abs/2405.19957"
    },
    {
        "url": "https://arxiv.org/abs/2405.20340",
        "content": "很抱歉，我无法直接访问或检索特定于您提供的URL的内容。但是，我可以根据您提供的信息或一般的查询来提供帮助。如果您能提供这篇文章的标题、作者或主要主题，我可以尝试为您提供更具体的信息。"
    },
    {
        "url": "https://arxiv.org/abs/2405.19888",
        "content": "这篇文章标题为《Parrot: Efficient Serving of LLM-based Applications with Semantic Variable》，提交于2024年5月30日，主要探讨了大型语言模型（LLMs）在应用中的高效服务问题。文章指出，随着LLMs的兴起，基于LLM的应用（也称为AI代理或副驾驶）已经成为一种新的软件范式，它结合了LLM和传统软件的优势。不同的LLM应用可能设计出复杂的流程，使用多个LLM请求来完成一个任务。然而，它们必须使用当今公共LLM服务提供的过于简化的请求级API，从而失去了重要的应用级信息。这导致公共LLM服务不得不盲目地优化单个LLM请求，最终导致LLM应用的整体性能次优。\n\n为了解决这一问题，文章介绍了Parrot，这是一个专注于LLM应用端到端体验的LLM服务系统。Parrot提出了“Semantic Variable”这一概念，它是一种统一的抽象，用于向公共LLM服务公开应用级知识。Semantic Variable通过在请求的提示中对输入/输出变量进行注释，并在连接多个LLM请求时创建数据管道，为编程LLM应用提供了一种自然的方式。将Semantic Variables暴露给公共LLM服务使其能够执行传统的数据流分析，以揭示多个LLM请求之间的相关性。这种相关性为LLM应用的端到端性能优化开辟了全新的空间。文章通过广泛的评估表明，Parrot能够为LLM应用的流行和实际用例带来数量级的性能提升。\n\n该文章将在USENIX OSDI 2024上发表，归类于机器学习（cs.LG）和人工智能（cs.AI）领域。\n\n搜索结果来自：\n[2405.19888] Parrot: Efficient Serving of LLM-based Applications with Semantic Variable - https://arxiv.org/abs/2405.19888\n[2405.19888] Parrot: Efficient Serving of LLM-based Applications with Semantic Variable - http://export.arxiv.org/abs/2405.19888"
    },
    {
        "url": "https://arxiv.org/abs/2405.19856",
        "content": "这篇文章标题为《DevEval: A Manually-Annotated Code Generation Benchmark Aligned with Real-World Code Repositories》，主要探讨了如何评估大型语言模型（LLMs）的编码能力。研究发现，现有的基准测试与真实世界的代码仓库不太匹配，不足以评估LLMs的编码能力。为了填补这一知识空白，文章提出了一个新的基准测试，名为DevEval，它有三个优点：\n\n1. DevEval在多个维度上与真实世界的代码仓库保持一致，例如代码分布和依赖分布。\n2. DevEval由13名开发人员手动注释，包含全面的注释（例如需求、原始仓库、参考代码和参考依赖）。\n3. DevEval包含来自117个仓库的1,874个测试样本，涵盖10个流行领域（如互联网、数据库）。\n\n基于DevEval，文章提出了针对仓库级别的代码生成评估，并在DevEval上评估了8个流行的LLMs（例如gpt-4、gpt-3.5、StarCoder 2、DeepSeek Coder、CodeLLaMa）。实验揭示了这些LLMs在真实世界代码仓库中的编码能力。例如，在实验中，gpt-4-turbo的最高Pass@1仅为53.04%。文章还分析了LLMs的失败案例并总结了它们的不足。作者希望DevEval能促进LLMs在真实代码仓库中的发展。DevEval、提示词和LLMs的预测已经发布。\n\n搜索结果来自：\n[2405.19856] DevEval: A Manually-Annotated Code Generation Benchmark Aligned with Real-World Code Repositories - https://arxiv.org/abs/2405.19856"
    },
    {
        "url": "https://arxiv.org/abs/2405.19707",
        "content": "文章《DeMamba: AI-Generated Video Detection on Million-Scale GenVideo Benchmark》讨论了近年来视频生成技术的快速发展，尤其是在社交媒体平台上视频内容的流行，加剧了关于虚假信息传播的担忧。因此，对于能够区分伪造的AI生成视频和真实视频的检测器的需求日益增长。文章指出，由于缺乏来自最先进视频生成器的大规模数据集，开发此类检测器面临障碍。为了解决这一差距，文章介绍了第一个AI生成视频检测数据集GenVideo，该数据集包含超过一百万个AI生成和真实的视频，涵盖了广泛的视频类别和生成技术。\n\n文章还提出了两种评估方法，用于评估检测器在实际场景中的性能：跨生成器视频分类任务评估训练好的检测器在生成器上的泛化能力；降级视频分类任务评估检测器处理在传播过程中质量下降的视频的鲁棒性。此外，文章还介绍了一个名为Detail Mamba（DeMamba）的即插即用模块，通过分析时间和空间维度上的不一致性来识别AI生成的视频，从而增强检测器。实验表明，与现有检测器相比，DeMamba在GenVideo上具有更好的泛化能力和鲁棒性。作者相信，GenVideo数据集和DeMamba模块将显著推进AI生成视频检测领域的发展。\n\n搜索结果来自：\n[2405.19707] DeMamba: AI-Generated Video Detection on Million-Scale GenVideo Benchmark - https://arxiv.org/abs/2405.19707\n[2405.19707] DeMamba: AI-Generated Video Detection on Million-Scale GenVideo Benchmark - http://export.arxiv.org/abs/2405.19707"
    }
]